[
  {
    "objectID": "01_ord_var.html",
    "href": "01_ord_var.html",
    "title": "1  Background",
    "section": "",
    "text": "1.1 Ordinal variables\nOrdinal variables consist of a set of ordered categories. Some examples are the following:\nIn the taxonomy proposed by Stevens (1946), ordinal variables fall in between nominal and interval characteristics. In contrast to nominal variables, the categories are ordered. However, the distance between them is unknown, which sets them apart from interval variables.\nThere are (many) different types of ordinal characteristics. In some cases, the assumption of an underlying (unobserved) latent variable makes sense. This means that the construct of interest can reasonably be thought of as a continuous (rather than a categorical) trait – what makes it an ordinal categorical variable is the way it is measured (its operationalization). This often applies to Likert-type and semantic differential scales. An ordinal scale may also be sequential, which means that units advance through the categories (or stages) and must start from the lowest. Examples are proficiency levels or the position of an item on the grammaticalization cline. Ordinal variables may also result from limitations of data availability and give a coarse categorization of a variable that could have been measured on an interval scale. Examples of such grouped continuous variables are frequency bands, or income deciles. Finally, some ordinal variables may be considered as reflecting two (or more) dimensions: Likert scale items, for instance, reflect direction (agreement vs. disagreement) and intensity of opinion (strong vs. “moderate” (dis-)agreement).\nAs we will see in Section 3.2, the nature of an ordinal variable has consequences for its statistical analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "01_ord_var.html#ordinal-variables",
    "href": "01_ord_var.html#ordinal-variables",
    "title": "1  Background",
    "section": "",
    "text": "Likert-type response scales (strongly agree – agree – neutral – disagree – strongly disagree)\nSemantic differential scales (friendly □ □ □ □ □ unfriendly)\nPosition on grammaticalization cline (content word – grammatical word – clitic – affix)\nGrouped continuous variables, i.e. variables that could have been measured on a continuous scale (e.g. frequency categories: high – moderate – low)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "01_ord_var.html#sec-mrm",
    "href": "01_ord_var.html#sec-mrm",
    "title": "1  Background",
    "section": "1.2 Simplified analysis strategy: Mean response models",
    "text": "1.2 Simplified analysis strategy: Mean response models\nIn practice, ordinal variables are commonly analyzed as though they had been measured on an interval scale (see Sönning 2024; Sönning et al. 2024). Researchers assign numbers to the categories and then calculate averages and/or fit ordinary linear regression models. This technique has received different labels in the literature:\n\nMean response model (MRM), because it relies on averages (means) of the assigned scores.\nInterval-scale analysis, as the ordinal outcome is treated as an interval variable.\nNumeric-conversion approach, since the responses are converted into numeric scores.\n\nThis approach to ordinal data analysis has a number of shortcomings compared to ordinal regression models. An orthodox approach to ordinal data analysis would therefore discard MRMs on the grounds that they are inadequate. This view does not seem constructive, however. In his seminal textbook on ordinal data analysis, Agresti (2010, 5) notes that “we do not take a rigid view about permissible methodology for ordinal variables”. It is much more helpful to make an effort to (i) understand the limitations of MRMs, which may at least to some extent be sidestepped and, equally importantly, (ii) learn from the appeal of MRMs to optimize our use (especially our interpretation and communication) of ordered regression models (see Sönning et al. 2024).\n\n1.2.1 Limitations of mean response models\nIn general, MRMs should only be used if the assumption of an underlying continuous variable makes sense. Even then, a number of issues arise, which are briefly summarized in the current section. For further details, see Long (1997, 35–40, 116–19), Agresti (2010, 5–8, 137–40), Bürkner and Vuorre (2019), and Liddell and Kruschke (2018).\n\n1.2.1.1 Assigning scores to categories\nMRMs require us to specify the distances between categories. If these are unknown, the analysis can give misleading results. In many cases, there is no clear-cut choice for the scores. Most often researchers assume equal distances between categories (i.e. assign scores of, say, 1 to 5 for a 5-point response scale). In certain cases, however, a different set of scores (or, more precisely, a different set of distances between categories) can be motivated on substantive grounds (see Sönning 2024). A custom scoring system may then be used instead.1\n\n\n1.2.1.2 Floor and ceiling effects\nMRMs can also give misleading results due to floor and ceiling effects. A ceiling effect occurs when most observations (or observations in certain parts of the data space) cluster at the upper end of the scale. A floor effect is the reverse pattern. Due to the hard scale bounds, differences between groups will shrink the closer they are to the upper or lower bound of the scale. The variability of the responses within a group also decrease as the group mean approaches the scale limits. To (at least partly) circumvent these distortions, we may try to avoid boundary effects by using extreme categories at both ends of the scale.\n\n\n1.2.1.3 Measurement error\nA particular category on the ordinal scale is typically consistent with a range of values of the underlying attribute that is being measured. To use MRMs, each response category must be represented with a single numeric score. By assigning a single score, we therefore introduce measurement error, since the actual level of the underlying dimension may have been higher or lower than the numeric replacement. The issue of measurement error is more problematic if the number of response categories is rather small – it can be mitigated (somewhat) by using a larger number of categories (5 or more).\n\n\n1.2.1.4 Other points\nMRMs do not yield estimated probabilities for the response categories. This is not a problem for model interpretation if we are only interested in overall trends rather than individual category probabilities. However, it makes it more difficult to check the fit of a model. Further, predictions and estimates based on MRMs can extend beyond the scale limits, above the highest or below the lowest category.\n\n\n\n1.2.2 Advantages of mean response models\nMRMs have a number of advantages over ordinal regression models. To run the analysis, the researcher requires less technical knowledge and no specialized software. MRM techniques are also more widely familiar. Further, MRM analyses are quicker (in terms of computation time), more flexible, and do not cause computational problems (e.g. non-convergence).\nThe critical advantage of MRMs, however, lies in model interpretation and communication. These models are particularly useful for quickly identifying important variables and trends. They also provide simple descriptions of the patterns in the data. Finally, the output of MRMs is easier to interpret.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "01_ord_var.html#interpretation",
    "href": "01_ord_var.html#interpretation",
    "title": "1  Background",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation\nThe choice of data analysis strategy must strike a balance between adequacy and interpretability. While features of the data (such as the measurement scale) may suggest a particular class of procedures, these methods have little value if they produce output that is difficult to interpret and/or communicate. In later sections, we will see that the output of ordered regression models can be communicated just as effectively and transparently as that of mean response models.\nBefore we go further, let us briefly outline some priorities that may guide the analysis of data and the communication of results. Since regression tables, the typical output of statistical software, are difficult to process and interpret, we need additional methods of interpretation. Inferential measures such as p-values provide an incomplete picture, as our linguistic objectives will usually ask for information about the shape and magnitude of patterns in the data. This is also because we are routinely interested in the relative importance of predictors, i.e. we want to compare them in terms of their strength of association with (or “effect on”) the outcome. When interpreting and communicating results, we will therefore make an effort to use meaningful quantities, such as proportions/percentages, and visual means of communication, which are often superior to numeric and discursive modes of presentation. The greatest advantage of MRMs is the option to condense an ordinal scale into a single number (the average), which can be easily compared across subgroups and conditions. This is beneficial when drawing more complex comparisons.\nWe will consider two examples to illustrate the advantages of MRMs for data interpretation and communication.\n\n1.3.1 Example 1: Lexical choices in Maltese English\nThe following data were collected in Malta using the Bamberg Survey of Language Variation and Change. For a total of 68 (near-)synonymous word pairs, respondents were asked to indicate which expression they (tend to) use. Figure 1.1 shows an excerpt from the lexical part of the questionnaire.\n\n\n\n\n\n\n\n\nFigure 1.1: Excerpt from the lexical part of the BSLVC.\n\n\n\n\n\nFor illustration, we will concentrate on 10 lexical pairs (the second variant being the traditionally British one):\n\n\n\n\ntruck/lorry\nsick/ill\npackage/parcel\nchips/crisps\n\nfall/autumn\ntrunk/boot\nfries/chips\n\nreservations/bookings\ncell phone/mobile phone\nsoccer/football\n\n\nWe consider a subset of the data, which includes responses from 200 individuals. The overall distribution of the ratings is given in Table 1.1.\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\n\n\nAmE\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\nBrE\n\n\n\n\n\n\ntruck\n\n\n.69 (134)\n\n\n.12 (23)\n\n\n.12 (24)\n\n\n.01 (2)\n\n\n.06 (12)\n\n\nlorry\n\n\n\n\nsick\n\n\n.47 (92)\n\n\n.16 (31)\n\n\n.32 (62)\n\n\n.02 (3)\n\n\n.04 (8)\n\n\nill\n\n\n\n\npackage\n\n\n.38 (74)\n\n\n.10 (20)\n\n\n.21 (40)\n\n\n.05 (10)\n\n\n.25 (49)\n\n\nparcel\n\n\n\n\nreservations\n\n\n.18 (35)\n\n\n.03 (6)\n\n\n.31 (61)\n\n\n.18 (35)\n\n\n.31 (61)\n\n\nbookings\n\n\n\n\nchips\n\n\n.24 (47)\n\n\n.07 (13)\n\n\n.16 (32)\n\n\n.07 (13)\n\n\n.46 (89)\n\n\ncrisps\n\n\n\n\ntrunk\n\n\n.19 (36)\n\n\n.05 (9)\n\n\n.16 (31)\n\n\n.16 (31)\n\n\n.45 (87)\n\n\nboot\n\n\n\n\nfries\n\n\n.11 (21)\n\n\n.02 (4)\n\n\n.15 (30)\n\n\n.15 (29)\n\n\n.57 (112)\n\n\nchips\n\n\n\n\nfall\n\n\n.06 (12)\n\n\n.02 (3)\n\n\n.07 (13)\n\n\n.10 (20)\n\n\n.75 (145)\n\n\nautumn\n\n\n\n\ncell phone\n\n\n.03 (5)\n\n\n.01 (1)\n\n\n.10 (19)\n\n\n.12 (23)\n\n\n.75 (146)\n\n\nmobile phone\n\n\n\n\nsoccer\n\n\n.03 (6)\n\n\n.00 (0)\n\n\n.05 (9)\n\n\n.09 (17)\n\n\n.84 (163)\n\n\nfootball\n\n\n\n\n\n\nTable 1.1: Overall distribution of the ratings for the selected items.\n\n\n\nFirst, we would like to compare the usage preferences (AmE vs. BrE variant) across all 10 pairs. We have the following questions in mind:\n\nFor which lexical pairs do we see a trend toward the British variant? Which ones show equilibrium, i.e. considerable variation?\nWhich pairs suggest a preference for the American form?\n\nTo answer these questions, we need to compare 10 distributions. The figures below show two visual comparison strategies.\n\n\n\n\n\n\n\n\nFigure 1.2: Visualization of the distribution of the responses using a bar chart and a dot plot showing averages based on a numeric-conversion approach.\n\n\n\n\n\nA horizontal stacked bar chart can be used to show the relative frequency of each category for each item. This preserves information about the distribution of response categories for each item. For instance, we can see that the rate of “no preference” was highest for sick/ill and reservations/bookings.\nA dot plot can be used to condense the information by showing an average calculated from the assigned scores −2, −1, 0, +1, +2. Positive values indicate a tendency towards BrE, negative values point to AmE. These averages are a simple version of a MRM.\nArguably, the dot plot answers our question more clearly. It uses one prominent symbol per item pair and we can quickly assess differences between the 10 pairs. Thus, answers to the questions posed above suggest themselves rather quickly and with ease. Note that information about the relative frequency of response categories is lost at this level of analysis.\nNext, consider a more complex comparison. Assume that our primary interest is in differences between female and male speakers. We might expect women to take on a leading role in ongoing language change, that is, the drift of Maltese English away from its BrE roots towards more globalized or AmE usage. Now we have the following questions in mind:\n\nAre there gender differences for any of the item pairs?\nWhere do female respondents show the expected trend?\nAre gender differences notable or rather minor?\n\nThus, for each lexical pair we would like to compare the tendency (BrE vs. AmE) for male and female respondents. The figures below illustrate the merits of information condensation. By representing the distribution of ratings for each subgroup (i.e. item-gender combination) with a single value and symbol, we can quickly identify those item pairs that conform to the expected pattern, i.e. that slope downward from left to right.\n\n\n\n\n\n\n\n\nFigure 1.3: Visualization of the distribution of the responses for male and female speakers using a bar chart and a line plot.\n\n\n\n\n\n\n\n1.3.2 Example 2: Heaps in Australian English\nAs a second example, consider the usage of heaps in Australian English (data provided by Romina Buttafoco). Here, our interest is in whether the usage of heaps is sensitive to the following factors: the age of the respondent, gender, register, and syntactic function. Heaps is typically considered an informal marker and our engagement with the literature may lead us to expect the following trends:\n\nThe prevalence of heaps is higher among younger speakers.\nWomen are leading the change in progress (from below) and show higher usage rates.\nAs an informal structure, heaps is expected to surface more strongly in less formal registers.\nHeaps is more likely to be used in prototypical functions, i.e. as a quantifier.\n\nInformants were asked to indicate on an ordinal scale how likely they are to use a certain expression in a given social context (reflecting register differences). We will use the subsample of 370 Australian respondents. Figure 1.4 shows an excerpt from the questionnaire; in the stimulus sentence, heaps is used as a quantifier.\n\n\n\n\n\n\n\n\nFigure 1.4: Excerpt from the questionnaire used to collect reported usage rates for heaps.\n\n\n\n\n\nAn important strategy in model interpretation is the inspection of what are variously referred to as “average predictions”, “partial effects”, “marginal effects”, or “predictive margins”. In models that include multiple predictor variables, we are often interested in understanding the relative importance of the individual variables. To this end, we need to assess the unique contribution of each variable to the variation in the outcome. Average predictions allow us to identify the association between a predictor and the outcome while adjusting for the other predictors in the model. This gives us a clear impression of the link between the focal predictor and the outcome. We can summarize the information provided by this procedure in graphical form. An example is given below. Such graphs are sometimes called partial (or marginal) effect displays. They allow us to make two types of comparisons:\n\nWithin-predictor comparisons: Focusing on each predictor individually, we assess how the outcome varies across its levels or values. We ask the following questions: Which predictor values are associated with higher outcomes, which show lower values? How do the levels of a categorical predictor vary? Which ones are rather similar, which ones stick out? How strong, or noticeable, are the patterns that may be discerned? How much statistical uncertainty is there in the estimates?\nBetween-predictor comparisons: Average predictions also allow us to compare the relative importance of predictors. This helps us identify variables that show a relatively strong or weak association with the outcome.\n\n\n\n\n\n\n\n\n\nFigure 1.5: Predictive margins for a quick overview of predictor importance.\n\n\n\n\n\nWithin-predictor and between-predictor comparisons are essential for understanding the association structure suggested by a model. This makes average predictions a very useful strategy for understanding complex models: For each predictor, they facilitate the assessment of its specific pattern and its relative magnitude in comparison with other input variables. Further, they offer information about the statistical uncertainty of the detected patterns in the form of error bars and bands.\nFor ordered regression models, we would like to be able make the same kinds of comparisons with the same level of ease. That is, we need a way of constructing average predictions that allows us to make comparisons within and between predictors.\n\n\n\n\nAgresti, Alan. 2010. Analysis of Ordinal Categorical Data. Hoboken, NJ: John Wiley & Sons.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal Regression Models in Psychology: A Tutorial.” Advances in Methods and Practices in Psychological Science 2 (1): 77–101. https://doi.org/10.1177/2515245918823199.\n\n\nLiddell, Torrin M., and John K. Kruschke. 2018. “Analyzing Ordinal Data with Metric Models: What Could Possibly Go Wrong?” Journal of Experimental Social Psychology 79: 328–48. https://doi.org/https://doi.org/10.1016/j.jesp.2018.08.009.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oakes, CA: Sage.\n\n\nSönning, Lukas. 2024. “Ordinal Response Scales: Psychometric Grounding for Design and Analysis.” Research Methods in Applied Linguistics 3 (3): xx–. https://doi.org/10.1016/j.rmal.2024.100156.\n\n\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht, and Paul Messer. 2024. “Latent-Variable Modelling of Ordinal Outcomes in Language Data Analysis.” Journal of Quantitative Linguistics 31 (2): 77–106. https://doi.org/10.1080/09296174.2024.2329448.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "01_ord_var.html#footnotes",
    "href": "01_ord_var.html#footnotes",
    "title": "1  Background",
    "section": "",
    "text": "In cases where the choice of scores is ambiguous, researchers may decide to do a sensitivity analysis. Repeating the analysis using different scoring systems then reveals the degree to which our conclusions depend on (are sensitive to) the choice of scores.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "02_describe.html",
    "href": "02_describe.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1 Relative frequencies: Category-specific vs. aggregated\nThe distribution of categorical variables is described using relative frequencies such as proportions or percentages. The choice between the two is arbitrary, as they express the same information. We will usually use proportions (see Harrell 2018), and we report them without a zero before the decimal point (i.e. “.10” instead of “0.10”) (APA 2020, 182).\nAnother metric that is often used for categorical data is probability. While probabilities can be understood as meaning the same thing as proportions, they are often used to describe quantities derived from a model, i.e. predicted (or estimated) relative frequencies. Here, we will use the term proportion for data description (observed distributions), and the term probability for model-based predictions or estimates (predicted distributions).\nFor ordinal data, two types of probabilities (or proportions/percentages) are important: Category-specific probabilities (also called response probabilities) and aggregated probabilities. Aggregated probabilities can be broken down into cumulative probabilities which collapse categories starting from the lower end of the scale, and exceedance probabilities, which collapse categories starting from the upper end of the scale. The distinction between category-specific and aggregated frequencies is crucial for the interpretation and communication of results.\nTo illustrate, let’s assume we have measured an outcome on a 5-point Likert-type agreement scale. The observed distribution of responses is shown in Figure 2.1.\nFigure 2.1: Relative frequencies for ordinal variables: Category-specific, cumulative, and exceedance proportions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "02_describe.html#sec-describe-probs",
    "href": "02_describe.html#sec-describe-probs",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1.1 Category-specific proportions\nCategory-specific (or response) proportions give the relative frequency of each of the 5 outcome categories. For instance, we can see that most of the respondents “disagreed” (a share of .40). Listing all individual proportions gives an exhaustive account of the distribution of the outcome. This is useful if we are interested in one (or each) category in its own right, or when detailed information is required about the distribution across categories.\n\n\n2.1.2 Aggregated frequencies: Cumulative and exceedance proportions\nAggregated proportions add up the individual proportions along the ordinal scale: They express the share of observations that are in or below a particular category (cumulative proportions) or above a particular category (exceedance proportions). For instance, looking at the cumulative proportions in Figure 2.1 (middle set of stacked bars), we can state that 30% of respondents either “agreed” or “strongly agreed”. In other words, 30% of the respondents at least “agreed”. This could be used as a measure of agreement.\nExceedance proportions, on the other hand, are the logical counterpart to cumulative proportions. They express the share of observations that exceed a particular category. The exceedance proportions in Figure 2.1 indicate that 50% of respondents either “disagreed” or “strongly disagreed”. For the ordinal scale at hand, exceedance proportions can therefore be used as a measure of disagreement.\nCumulative and exceedance proportions can be helpful in certain settings, since category-specific proportions may offer too much information for some purposes. Individual response proportions also do not take into account the fact that the categories are ordered. Often, we need to condense the distribution into a single number to report on, say, the extent to which respondents agreed, on average. For instance, we may be interested in comparing the level of agreement in three groups – A, B and C. Comparing 15 response probabilities is harder than comparing three numbers summarizing the level of agreement. Thus, we could state that the percentage of respondents who agreed with the statement (i.e. responded “agree” or “strongly agree”) was 60% in group A, 40% in group B, and 35% in group C. Agreement was therefore highest in group A. For more complex comparisons, this condensation to a single number is very helpful.\nCumulative and exceedance proportions have advantages and disadvantages: On the one hand, they discard information in the data. Thus, a cumulative share of .30 does not tell us how many informants agreed strongly (only 5% or as many as 25%?). On the other hand, this reduction in information is beneficial for more complex comparisons, because single scores can be visualized and compared much more easily and effectively. This is a trade-off and each type serves different purposes.\nWhen using cumulative and exceedance proportions for the interpretation and communication of results, two important choices need to be made:\n\nCognitive fit: When talking about an ordinal trait, you should make a decision about how you want (your audience) to think about the outcome. In our example, we could talk about the level of disagreement or the level of agreement. We should choose one, stick to it, and use the matching type of proportion. Here, cumulative proportions reflect agreement, while disagreement is indicated by exceedance frequencies. It is also helpful for the y-axis to have an informative, transparent label (e.g. “Agreement”).\nSplit: If we are looking for a single cumulative proportion to represent the distribution of the ordinal variable, we need to decide where to split the ordinal scale. In our example, we chose to collapse “agree” and “strongly agree” to arrive at a proportion reflecting agreement (e.g. .30). There are different options for splitting an ordinal scale, and the split we choose should be informative for addressing the question we have in mind. While it usually makes sense to use the midpoint of the scale, the number of response categories may be uneven. In our present data, for instance, we have a neutral category in the middle. In such cases we can divide the middle category in half. In a sense, the split is therefore made at the midpoint of the ordinal scale.\n\nIn short, category-specific proportions should be used when interest lies (i) in the detailed distribution of an ordinal outcome or (ii) in individual categories. Aggregated proportions should be used when a condensed summary measure is needed for more complex comparisons (such as trends across a continuous predictor or the comparison of several groups). The choice between cumulative and exceedance probabilities and the split should aim at providing a substantively meaningful summary score.\nTable 2.1 gives different relative frequencies for the variable Syntactic function in the heaps data.\n\n\n\n\n\n\n\n\n\n\nCategory-specific proportions\n\n\n\n\nCumulative proportions\n\n\n\n\nExceedance proportions\n\n\n\n\n\nResponse\n\n\np_adj\n\n\nverb\n\n\nc_adj\n\n\nquant\n\n\np_adj\n\n\nverb\n\n\nc_adj\n\n\nquant\n\n\np_adj\n\n\nverb\n\n\nc_adj\n\n\nquant\n\n\n\n\n\n\n6\n\n\n.11\n\n\n.27\n\n\n.39\n\n\n.47\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n1.0\n\n\n.00\n\n\n.00\n\n\n.00\n\n\n.00\n\n\n\n\n5\n\n\n.08\n\n\n.14\n\n\n.14\n\n\n.18\n\n\n.89\n\n\n.73\n\n\n.61\n\n\n.53\n\n\n.11\n\n\n.27\n\n\n.39\n\n\n.47\n\n\n\n\n4\n\n\n.07\n\n\n.13\n\n\n.12\n\n\n.12\n\n\n.81\n\n\n.58\n\n\n.46\n\n\n.34\n\n\n.19\n\n\n.42\n\n\n.54\n\n\n.66\n\n\n\n\n3\n\n\n.05\n\n\n.06\n\n\n.07\n\n\n.06\n\n\n.74\n\n\n.45\n\n\n.34\n\n\n.22\n\n\n.26\n\n\n.55\n\n\n.66\n\n\n.78\n\n\n\n\n2\n\n\n.18\n\n\n.17\n\n\n.11\n\n\n.08\n\n\n.69\n\n\n.38\n\n\n.27\n\n\n.16\n\n\n.31\n\n\n.62\n\n\n.73\n\n\n.84\n\n\n\n\n1\n\n\n.52\n\n\n.22\n\n\n.17\n\n\n.08\n\n\n.52\n\n\n.22\n\n\n.17\n\n\n.08\n\n\n.48\n\n\n.78\n\n\n.83\n\n\n.92\n\n\n\n\n\n\nTable 2.1: Category-specific, cumulative, and exceedance proportions for the variable Syntactic function in the heaps data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "02_describe.html#sec-describe-vis",
    "href": "02_describe.html#sec-describe-vis",
    "title": "2  Descriptive statistics",
    "section": "2.2 Visualizing ordinal data",
    "text": "2.2 Visualizing ordinal data\nThe visualization of ordinal data is a surprisingly difficult task. In the following, we will survey different display types and options that may be considered. Given the importance of visual means of interpretation and communication, we will reflect on the relative merits of the available options and offer some recommendations.\nThere are a number of points we need to think about when graphing ordinal variables. Much hinges on the decision of whether to show category-specific or aggregated frequencies. The distinction was discussed above. An attractive feature of stacked bar charts and area charts is that they show both types of frequency in the same display. A simplification strategy that should be considered for more complex visualization tasks, where the distribution of an ordinal variable is to be compared across an array of conditions, is to choose a single representative aggregated probability. Such a condensed representation can help us avoid an overly cluttered display, and draw complex comparisons with more ease.\nWhen graphing ordinal data, we need tools for categorical and continuous predictors. Categorical predictors are less problematic. For continuous predictors, on the other hand, we must choose between (i) retaining the variable as a continuous feature, to plot “smooth” trends; or (ii) use binning to create a discretized version of the variable. For strategy (ii), we can make use of the same strategies as for categorical predictors.\nThe following plot types will be useful:\n\nBar charts (stacked and grouped)\nArea charts\nLine plots\nDot plots\n\nLet us now consider different plot types and options for ordinal data visualization.\n\n2.2.1 Categorical predictors\nTo show the distribution of an ordinal variable across the levels of a categorical predictor, we may decide to show category-specific or cumulative proportions.\n\n2.2.1.1 Category-specific proportions\nWhen showing individual response proportions for the levels of a categorical predictor, it makes sense to order the groups along the x-axis to produce a smooth pattern (i.e. monotonically increasing/decreasing probabilities). This will also make it easier to compare the strength of association across predictors including numeric ones. Figure 2.2 shows two graph types that may be used to show response proportions for discrete variables. The color scale runs from red (“never”) to grey (“likely”) – red therefore signals dispreference, and grey reflects acceptance/usage. This choice of fill colors facilitates the interpretation of the ordinal outcome variable.\nThe left-hand plot uses a grouped bar chart. The proportion of grey increases from left to right, which shows that the quantifier use of heaps is most acceptable. While grouped bar charts show the distribution quite well, they quickly become crowded.\nThe right-hand graph is a line plot, which represents individual response proportions as dots and connects them across the graph with lines. This graphical arrangement allows us to see more easily how the proportion of the extreme categories varies across subgroups.\n\n\n\n\n\n\n\n\nFigure 2.2: Grouped bar chart and line plot showing response probabilities for a categorical predictor.\n\n\n\n\n\n\n\n2.2.1.2 Cumulative and exceedance proportions\nWhen showing the distribution of an ordinal variable using aggregated proportions, we must decide on the order of categories within each stack of bars: Should the highest or lowest category appear at the bottom? This decision is critical, since it is the segments that are aligned at the bottom of the display that will be most readily compared. The immediate message in Figure 2.3, for instance, is that there is an increase from left to right. Since the grey segments appear at the bottom of the graph, the primary visual signal is that the share of “grey” increases from left to right. In other words, the acceptability of heaps increases from left to right – it is highest for the quantifier use (“quan”). With this ordering of the categories (grey at the bottom, red at the top), the y-axis in the graph shows acceptability (rather than inacceptability).\n\n\n\n\n\n\n\n\nFigure 2.3: Stacked bar chart showing cumulative probabilities for a categorical predictor.\n\n\n\n\n\n\n\n\n\n\n\nStacked bar charts\n\n\n\nWhen using stacked bar charts, make sure you pay attention to the arrangement of ordinal categories, i.e. which end of the scale appears at the bottom and which at the top. This choice affects the message conveyed by the graph.\n\n\nOn our ordinal scale, red represents the lowest category (“never”) and grey the highest one (“likely”). This means that Figure 2.3 shows the highest category at the bottom, and the lowest at the top. The lines that divide the bars into segments therefore represent exceedance proportions. For the leftmost syntactic function (“p_adj”), the meaning of the cut at about .20 means that about 20% of the responses exceed category 4 – that is, 20% of the responses are in category 5 or 6. The fact that the order of the response categories is reversed in Figure 2.3 makes it quite challenging to recognize this fact.\nA variant of this display type uses diverging bars (see Heiberger and Robbins 2014): Rather than aligning bars at the baseline of 0, they are aligned at (what may be considered) the midpoint of the ordinal scale. Segments above this midpoint extend upward, and segments below the midpoint point downward. Diverging bar charts add another visual cue for interpretation: The vertical (or horizontal) position of each stack.\nThe diverging version of Figure 2.3 appears in Figure 2.4. Note that, to preserve the interpretation of the graph as showing the level of acceptability, the order of the ordinal categories has been reversed: The red segments now appear at the bottom.\n\n\n\n\n\n\n\n\nFigure 2.4: Diverging bar chart showing cumulative probabilities for a categorical predictor.\n\n\n\n\n\n\n\n\n\n\n\nDiverging bar charts\n\n\n\nAttention must again be paid to the arrangement of the ordered categories, i.e. which end of the scale appears at the bottom and which at the top. To encourage the same interpretation as an ordinary stacked bar chart, the order of the categories must be reversed.\n\n\nWe can also use a line plot to show this distribution. The points then mark the exceedance proportions, which means that they show the location of the cutpoints in the stacked bar charts (see Figure 2.3). The points are then connected with lines. An example appears in Figure 2.5. The exceedence probability for the highest category is redundant and can be left out of the graph. This means that line plots of cumulative or exceedance proportions represent the distribution of \\(k\\) ordered categories with \\(k-1\\) linear profiles.\n\n\n\n\n\n\n\n\nFigure 2.5: Line plot showing how the distribution of the ordinal variable varies across the levels of the categorical predictor Syntactic function. Since the highest category appears at the bottom (grey: ‘likely’), the points mark exceedance proportions.\n\n\n\n\n\n\n\n\n\n\n\nAdd alluvial diagram\n\n\n\nAlluvial diagrams discourage the quantitative interpretation of the x-variable. Perhaps write a ggplot function to create them.\n\n\n\n\n\n2.2.2 Numeric predictors\nFor numeric predictors, we can also either look at category-specific or aggregated proportions.\n\n2.2.2.1 Category-specific proportions\nIndividual response probabilities are perhaps best shown using a line plot. Figure 2.6 shows the observed distribution of the category-specific proportions – these are smoothed.\n\n\n\n\n\n\n\n\nFigure 2.6: Line plot showing category-specific sample proportions.\n\n\n\n\n\n\n\n2.2.2.2 Cumulative and exceedance proportions\nAggregated proportions can be graphed using an area chart. Again, the way in which the ordinal categories are arranged from bottom to top is critical for the immediate message conveyed by the graph. Since we would like the y-axis in our graph to show the acceptability of heaps, the highest category, shown in grey, appears at the bottom. This means that the order of the categories is reversed (as in Figure 2.3), since we need our viewers to see how the share of grey increases for younger speakers. Figure 2.9 shows the smoothed distribution of the sample proportions; the amount of smoothing influences the perception of the trends in the data. The lines that divide the rectangle into colored areas are again exceedance proportions, as in Figure 2.3.\n\n\n\n\n\n\n\n\nFigure 2.7: Area chart showing how the distribution of the ordinal variable varies with the numeric predictor Age. Since the highest category appears at the bottom (grey: ‘likely’), the lines that divide up the rectagle show exceedance proportions.\n\n\n\n\n\nWe can also use what is sometimes referred to as a spine plot, which is a stacked bar chart based on a binned version of a continuous variable on the x-axis. The width of the bars in this type of display is proportional to the number of observations in the bin. Such a graph appears in Figure 2.8, where 10-year bins are used. Apart from the change of response proportions across age, the graph also shows that the current dataset includes few respondents younger than 20 or older than 70.\n\n\n\n\n\n\n\n\nFigure 2.8: Area chart showing cumulative proportions for a numeric predictor.\n\n\n\n\n\nWe can also use a line plot to show a reduced version of the area chart in Figure 2.7. To this end, only the smoothed exceedance proportions are shown visually. Similar to Figure 2.5, the distribution of \\(k\\) categories is shown using \\(k-1\\) linear profiles.\n\n\n\n\n\n\n\n\nFigure 2.9: Line plot showing cumulative proportions for a numeric predictor.\n\n\n\n\n\n\n\n\n2.2.3 Complexity\nIllustrate how showing individual aggregated proportions can simplify data description.\nFigure 2.10 shows the distribution of response for each of the 370 informants in the data.\n\n\n\n\n\n\n\n\n\nFigure 2.10: Stacked bar chart showing response proportions for each speaker in the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA, ed. 2020. Publication Manual of the American Psychological Association. 7th ed. Washington, DC: American Psychological Association.\n\n\nHarrell, Frank E. Jr. 2018. “Why i Don’t Like Percents.” Web log post. https://www.fharrell.com/post/percent/.\n\n\nHeiberger, Richard M., and Naomi B. Robbins. 2014. “Design of Diverging Stacked Bar Charts for Likert Scales and Other Applications.” Journal of Statistical Software 57 (5): 1–32.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "03_orm.html",
    "href": "03_orm.html",
    "title": "3  Ordered regression models",
    "section": "",
    "text": "3.1 Three building blocks\nIn general, ordered regression models consist of three building blocks. These can be combined to form different versions of ordered regression models. We will discuss the individual components in turn.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "03_orm.html#sec-orm-building-blocks",
    "href": "03_orm.html#sec-orm-building-blocks",
    "title": "3  Ordered regression models",
    "section": "",
    "text": "3.1.1 Model class\nThere are three major classes of ordinal regression models:\n\ncumulative models (Aitchison and Silvey 1957; McKelvey and Zavoina 1975),\ncontinuation-ratio models (Fienberg 1980), and\nadjacent-category models (Goodman 1983).\n\nTo understand the difference between these approaches, we must recognize that an ordered regression model decomposes the ordinal scale (with K categories) into K - 1 binary variables. A model for 5 outcome categories, for instance, breaks down the 5-point scale into 4 binary comparisons. Under the hood, then, every ordered regression model consists of a set of connected binary regressions. These are estimated simultaneously, with certain constraints on the parameters. The number of binary splits is always K − 1, where K is the number of outcome categories.\nSince ordinal scales can be split in different ways, there are different classes of ordered regression models. The definition of a binary comparison (e.g. categories 1+2 vs. categories 3+4+5; or category 1 vs. category 2) is referred to as a cutpoint equation (Fullerton and Xu 2016, 5). For ordered outcomes, there are three basic possibilities. These are illustrated in Figure 3.1, where the horizontally aligned boxes represent a 5-point ordinal scale. Accordingly, 4 binary comparisons can be drawn, and these comparisons are shown as a stack of aligned boxes. In each chain, the grey box(es) is/are compared to the white box(es). The cutpoint equation states which categories are involved in the comparison.\n\n\n\n\n\n\n\n\nFigure 3.1: Different classes of ordinal regression models divide the ordinal scale into different binary splits.\n\n\n\n\n\n\nFor cumulative models, which appear at the far left in Figure 3.1, we split the ordinal scale at different cumulative probabilities, comparing in each case all categories below the split to all categories above the split. Each binary comparison therefore involves all response categories. This cumulative type of comparison asks: How do cases below a certain threshold compare to those above a certain threshold?\n\nFor continuation-ratio models, the scale is partitioned in a different way: We compare each category (except for the highest one) to all categories above it. Thus, while the lowest category is contrasted with all other categories combined, only two categories remain for the final comparison. This continuation-ratio approach asks: How do cases in a specific category compare to those with a higher value on the ordinal scale?\nIn adjacent-category models, neighboring categories are compared. Each comparison therefore involves only two categories. This strategy asks: How do cases in this specific category compare to those in the next-higher category?\n\nWhile cumulative, continuation-ratio, and adjacent-category models use the ordinal information in the data, the baseline-category model does not. This type of model, which is shown at the far right in Figure 3.1, is therefore typically used for nominal outcomes.\n\n\n3.1.2 (Non-)reliance on the parallel regression assumption\nThe parallel regression assumption (sometimes called the proportional odds assumption) is an important building block of ordered regression models. It makes a fairly strict statements about the relationship between predictor and outcome. It is important to note that this assumption does not apply to the model as a whole, but rather to each predictor individually. Above, we saw that ordered regression models break up an ordinal scale with K categories into K – 1 binary splits. The parallel regression assumption states that the association between the predictor and the outcome is the same across all binary splits.\nWhen assuming parallel regressions for a predictor, this means that the effect of the predictor is the same across all K – 1 binary comparisons. In other words, the parallel regression assumption assumes that the relationship between predictor and outcome is the same at each point along the ordinal scale. For instance, differences in age groups would be assumed to be identical at both ends of the ordinal scale. This is sometimes referred to as the assumption of symmetry.\nWe will discuss the parallel regression assumption in more detail below, including how to assess or test it. It is worth repeating that this assumption is made (or relaxed) for each predictor in the model individually.\nModels where parallel regressions are assumed for all predictors are called parallel models. If the parallel regression assumption is relaxed for all predictors, we call it a non-parallel model (also referred to as a generalized model). The intermediate case, where at least one, but not all of the predictors are allowed to show non-parallel associations, the model is called a partial model.\n\n\n3.1.3 Link function\nLink functions describe the mathematical nuts and bolts of ordered regression models. The most commonly used link functions are the logit, probit, and complementary log-log link. The choice between the logit and probit is basically a question of convenience and/or convention. Results barely differ. The complementary log-log link function, on the other hand, should be handled with care since it is not symmetric. This means that results will differ depending on how you arrange the ordered scale before modeling (in increasing or decreasing order).\n\n\n3.1.4 Summary\nHere is an overview of the three building blocks. Any combination of these features may be used to model ordinal data.\n\n\n\n\nModel type\nCumulative\nAdjacent-category\nContinuation ratio\n\nParallel regressions\nYes (parallel)\nNo (non-parallel)\nFor some predictors (partial)\n\nLink function\nProbit\nLogit\nComplementary log-log\n\n\nThe most widely used form of ordered regression is the parallel cumulative model with a logit link, which is also referred to as a proportional odds model. Non-parallel models are sometimes referred to as generalized models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "03_orm.html#sec-orm-guidelines",
    "href": "03_orm.html#sec-orm-guidelines",
    "title": "3  Ordered regression models",
    "section": "3.2 Selecting a model: Guidelines",
    "text": "3.2 Selecting a model: Guidelines\nThe following recommendation by Long and Freese (2014, 310) is worth keeping in mind: “[…] we suggest that you always compare the results from ordinal models with those from a model that does not assume ordinality,” i.e. a baseline-category model.\nIn general, if the idea of a continuous latent variable is sensible, a cumulative model may be chosen. However, in order to permit simple interpretations based on means of the underlying latent variable, the parallel regression assumption must be met. This is to say that latent variable interpretations are only valid for parallel cumulative models.\n\n3.2.1 Model type\nThe choice of model type depends on the research question and the assumed data-generating process.\n\n3.2.1.1 Cumulative model\nThe cumulative model is useful when the focus is on identifying trends in the outcome, either upward or downward, for different values of the predictors. An advantage of the cumulative model is that it is (approximately) invariant to the choice and number of response categories. This means that the same results would have been obtained had the outcome been measured with fewer (or more) categories.\nPotential problems. The cumulative model usually fits poorly if the variability of an underlying latent variable changes dramatically over the range of observed values. This is the case, say, when attitudes become more (or less) variable (or polarized) in certain sub-populations, e.g. in older respondents. However, it is possible to model the variability of the underlying variable as a function of (the) predictors in the model (see Section #).\n\n\n3.2.1.2 Continuation-ratio model\nA requirement for continuation-ratio models is that the ordinal variable reflects a sequential process. This means that there is a logical starting point and units have to proceed through earlier stages in the sequence in order to reach a higher category. This progression is also assumed to be irreversible. Typical sequential stages are duration and development scales.\nThe focus of a continuation-ratio analysis is to understand the factors that distinguish between those observations that have reached a particular response level (but do not move on) from those observations that do advance to a higher level. Of course, by reversing the category order we can reverse the interpretation, i.e. compare those units that have “made it” to a certain stage to those that haven’t. Thus, the model and its interpretation depend on how the ordinal outcome is coded (in increasing vs. decreasing order).\nDue to the way the ordinal scale is split into binary comparisons (see above), the model provides the probability that an observation moves beyond a stage once a particular stage has been reached (likelihood of advancing or the probability of a transition).\nPotential problems. As illustrated above, the continuation ratio model progressively narrows down the subset of observations involved in binary comparisons. Observations with responses at the same end of the scale may be expected to be more homogenous, on average. This creates the potential problem of sample selection bias, as the continuation-ratio model relies on the assumption that subsamples do not differ systematically with respect to unobserved or omitted variables. Changes in coefficients may thus reflect changes in unobserved heterogeneity rather than “true” effects in the variables.\n\n\n3.2.1.3 Adjacent-category model\nAn advantage of adjacent-category models is that they are also applicable to retrospective (case-control) studies; a retrospective study is one in which sampling depends on the outcome. This is to say that the sample is composed so that there is a certain (prespecified) number of observations for each outcome category. As a result, the observed distribution of the outcome variable is then not representative for the population but deliberately manipulated by the researcher; this sampling procedure is common in medical research, for instance, where the observed sample of the dependent variable is typically skewed in favor of one outcome or the other to achieve a more balanced sample than random sampling would produce. This leads to oversampling of certain outcome categories, and the sample is not representative of the population.\nAdjacent-category and cumulative models usually fit equally well. Other things being equal, the choice between the two depends on whether you prefer effects to refer to individual response categories (adjacent-category model) or groupings of categories using the entire scale (cumulative model). Adjacent-category models can also clarify which explanatory variables might best predict a response being in the next-highest category, thus helping to identify differences between pairs of categories.\nPotential problems. Adjacent-category models rely on the additional assumption of independence of irrelevant attributes (see Long 1997, 182–83; 2014, 177; Cheng 2007).\nFullerton and Xu (2016) state that “adjacent models are better suited than cumulative models for certain types of ordinal variables, including Likert scales and other attitudinal scales that have “additional structure” (Sobel 1997, pp. 215–216).”\n\n\n\n\n\n\nTo do\n\n\n\nRead Sobel 1997.\n\n\n\n\n\n3.2.2 The parallel regression assumption\nAs discussed above, the parallel regression assumption (also called proportional odds restriction) imposes a constraint on the regression coefficients for a specific predictor: They are forced to be constant across the ordinal scale. This means that the change in log odds associated with a change in the predictor is the same for each cutpoint equation. To understand the nature of this constraint, consider Figure 3.2. The graph shows the distribution of responses for female and male informants using a stacked bar chart. The cumulative proportions of the observed distribution are located on the probability scale using black dots. These sample proportions are then mapped onto the logit scale, the scale on which the model operates. These quantities are referred to as cumulative logits. It is on this scale that the two groups are compared. This means that a cumulative model looks at the differences between these cumulative logits. In Figure 3.2, the lines connecting the cumulative logits for male and female informants are nearly parallel, suggesting that the difference between cumulative logits are very similar across the ordinal scale. The parallel regression assumption states that these differences (or slopes) are constant across the scale. Figure 3.2 suggests that this assumption is tenable for the predictor Gender in the dataset at hand.\n\n\n\n\n\n\n\n\nFigure 3.2: Illustration of the cumulative model and the parallel regression constraint: The predictor Gender in the parcel data.\n\n\n\n\n\nLet us also look at the parallel regression assumption for a continuous predictor. We start by creating five Age bins with roughly the same number of speakers. For each age subgroup we then calculate exceedance proportions and convert these into logits. The left-hand graph in Figure 3.3 shows exceedance proportions. The right-hand panel shows them on the logit scale. If the profiles are parallel on the logit scale, the parallel regression assumption is tenable. Clear deviation from a straight-line pattern would suggest that a simple regression line underfits the data.\n\n\n\n\n\n\n\n\nFigure 3.3: Line plot showing cumulative logits for a binned version of Date of birth.\n\n\n\n\n\nThe parallel regression assumption can be assessed in different ways. Formal test, for instance, can be used to compare two models, one which enforces the parallel constraint on a specific predictor (parallel model), and one where the coefficients may vary freely across cutpoint equations (non-parallel model) (see Fullerton and Xu 2016, Ch. 5). These tests often reject the parallel regression assumption and therefore suggest fitting a more complex model with non-parallel slopes.\nBefore we illustrate different ways of assessing the parallel regression assumption, let us emphasize that we should be somewhat hesitant to dismiss the parallel regression assumption. This is for several reasons (see Fullerton and Xu 2016, 9, 109).\n\nTests are oversensitive: Sole reliance on tests is not recommended, as they are overly sensitive to departures from the parallel regression assumption. These tests are also sensitive to other types of model misspecification (see Greene and Hensher 2010, 187–88).\nParsimony: Parallel models have the advantage of being parsimonious: They are less costly in terms of the number of parameters that are “spent” to describe patterns in the data. This advantage is especially important for analyses based on small samples. For reasons of parsimony, a simple model with parallel regression structure is sometimes preferable. Tutz (2012: 241) notes that “[…] with categorical data, parsimonious models are to be preferred because the information content in the response is always low”.\nPractical significance: While a formal test may suggest that the variation in coefficients across cutpoint equations is “statistically significant”, that a non-parallel model provides a better fit, or that it has greater predictive utility, this does not mean that the parallel model leads to substantively different conclusions.\nLatent-variable interpretation: A parallel model can be interpreted on a latent-variable scale, which allows us to effectively condense the patterns of variation (see Sönning et al. 2024). The non-parallel model rules out this interpretative strategy, and dissociates the model from the idea of an underlying continuous process (Greene and Hensher 2010, 190).\n\nIf a formal test rejects the proportional odds restriction, this should always be followed up with an inspection of predicted probabilities, as these give an idea of the importance of the difference (see Kim 2003; Long 2014, 183, 200; Fullerton and Xu 2016, 123).\nWe now consider formal tests and informal assessments of the parallel regression constraint.\n\n3.2.2.1 Formal tests\nFullerton and Xu (2016, Ch. 5) describe a number of tests that can be used to assess the parallel regression assumption. Among these, the likelihood-ratio test (LR test) appears to show the most favorable performance (Peterson and Harrell 1990, 209). This test compares two models:\n\nM0: The reference model with the parallel regression constraint for a specific predictor\nM1: The more complex model, where coefficients for this predictor are free to vary across cutpoint equations\n\nM0 is nested in (i.e. a special case of) M1, which is necessary for a sensible application of the LR test. The test then looks at which model provides a statistically better fit to the data, keeping in mind the number of parameters spent.\nTable 3.1 shows the results of the LR test comparing M0 and M1.\n\n\n\n\n\n\n\nModel\n\n\nParameters\n\n\nAIC\n\n\nLog likelihood\n\n\nTest statistic\n\n\ndf\n\n\np-value\n\n\n\n\n\n\nm0\n\n\n4\n\n\n534.3159\n\n\n-263.1579\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\nm1\n\n\n6\n\n\n533.1532\n\n\n-260.5766\n\n\n5.162656\n\n\n2\n\n\n0.0756734\n\n\n\n\n\n\nTable 3.1: Model comparison using a likelihood-ratio test.\n\n\n\nThe test suggest that M1, the non-parallel model, provides a better fit to the data. This is reflected in the AIC scores (lower values signal greater predictive utility) and the log likelihoods (higher values reflect a statistically better fit). The p-value tells us that M1 fits statistically better, which means that the parallel regression assumption appears to be violated.\n\n\n3.2.2.2 Informal assessments of the parallel regression assumption\nWhen there is indication of a violation of the parallel regression assumption, it is useful to also run informal tests, to inspect the nature of the departure and also appreciate its seriousness (see Kim 2003). Fullerton and Xu (2016, 118–30) suggest three types of checks, which we will discuss next:\n\nModel comparison using information criteria\nComparison of the varying coefficients\nComparison of predicted probabilities\n\n\n3.2.2.2.1 Model comparison using information criteria\nIt should be noted that the BIC is biased towards parsimony (see Fullerton and Xu 2016, 120), which means that it will be more hesitant to dismiss the parallel regression assumption. For the predictor Date of birth, the two information criteria lead us to different conclusions - while the AIC suggests better predictive performance of the M1, BIC suggests a draw between the models.\n\n\n\n\n\n\n\nModel\n\n\nAIC\n\n\nBIC\n\n\nLog.likelihood\n\n\nDeviance\n\n\n\n\n\n\nM0\n\n\n534.3\n\n\n547.4\n\n\n-263.2\n\n\n526.3\n\n\n\n\nM1\n\n\n533.2\n\n\n552.7\n\n\n-260.6\n\n\n521.2\n\n\n\n\n\n\nTable 3.2: Model comparison using a likelihood-ratio test.\n\n\n\n\n\n3.2.2.2.2 Comparison of coefficients across cutpoint equations\nAnother informative assessment is to fit the relevant K – 1 binary regressions separately and compare the coefficients to check whether they vary greatly (or systematically) along the scale. The first step is to create K – 1 indicator variables, which divide the K-point ordinal scale at K – 1 points. For checking the parallel regression assumption on a cumulative model, these binary splits are cumulative, as illustrated in Figure 3.4. Note how the responses exceeding a particular cutpoint may be coded as 1 (for cumulative probabilities) or 0 (for exceedance probabilities). This directionality is necessary in order for the coefficients from the binary regressions to have the same sign as those from the ordinal model.\n\n\n\n\n\n\n\n\nFigure 3.4: The ordinal scale is broken down into four binary comparisons.\n\n\n\n\n\nThe coefficients are listed in Table 3.3.\n\n\n\n\n\n\n\n\n\n\n95% CI\n\n\n\n\n\nCutpoint equation\n\n\nEstimate\n\n\nLower limit\n\n\nUpper limit\n\n\n\n\n\n\n1 vs. (2,3,4,5)\n\n\n−0.83\n\n\n−1.43\n\n\n−0.29\n\n\n\n\n(1,2) vs.(3,4,5)\n\n\n−1.32\n\n\n−1.96\n\n\n−0.76\n\n\n\n\n(1,2,3) vs.(4,5)\n\n\n−1.35\n\n\n−1.92\n\n\n−0.82\n\n\n\n\n(1,2,3,4) vs. 5\n\n\n−1.38\n\n\n−1.96\n\n\n−0.84\n\n\n\n\n\n\nTable 3.3: Regression coefficients (slopes) for the predictor Date of birth for different cutpoint equations.\n\n\n\nFigure 3.5 shows a graphical inspection of the parallel regression assumption in the heaps data. Each estimate is shown as a dot and the error bar indicates statistical uncertainty (95% CI).\n\n\n\n\n\n\n\n\nFigure 3.5: Regression coefficients (slopes) for the predictor Date of birth for different cutpoint equations.\n\n\n\n\n\nThe following questions should guide the interpretation of the figure:\n\nDo the coefficients for each predictor (or across the levels of a categorical predictor such as Function and Register) form a horizontal line? If they do, this means that they are of roughly equal magnitude across the binary splits, which in turn means that their effect is (nearly) constant across the ordinal scale. If the coefficients align horizontally, we can safely maintain the parallel regression assumption.\nIf there is no horizontal pattern, how do the coefficients differ? Do they show an erratic or a systematic pattern? In Figure 3.5, we see a systematic pattern: The association between Date of birth and the ordinal outcome, for instance grows in strength towards the “+2” end of the ordinal scale. This means that differences between cohorts are more pronounced at the upper end of the scale.\nFinally, we should also take into consideration the statistical uncertainty of the coefficient estimates. Estimates that show relatively wide margins of error should caution us against drawing strong conclusion about deviations from the parallel regression assumption.\n\nThis provides information about the plausibility of parallelism for the data.\n\n\n3.2.2.2.3 Comparison of predicted probabilities\nFullerton and Xu (2016, 123) argue that it is more informative to compare predicted probabilities since large differences in log odds (or on the probit scale) may not correspond to large differences in predicted response probabilities. To have an additional point of reference, we also consider a visual summary of the observed data distribution.\nLet us now compare these sample proportions with the predicted probabilities based on the two models. Recall that have fit two models: M0, which enforces the parallel regression assumption for the predictor Date of birth (parallel cumulative probit model), and M1, which relaxes the assumption for this predictor (non-parallel cumulative probit model).\nIn the left-hand panel in Figure 3.6, a line plot shows the observed distribution of response proportions by Date of birth. For this descriptive graph, it is important that we apply smoothing techniques to the data summary – otherwise it would be of little use as a benchmark.1 The right-hand panel in Figure 3.6 presents the predicted response probabilities: solid lines for the parallel model, dotted lines for the non-parallel model. The two models yield similar predictions, but we do observe differences in the extreme categories: the downward trend in apparent time for exclusively BrE parcel (dark blue) is steeper in the non-parallel model. The upward trend for AmE package (dark red), on the other hand, is attenuated in the non-parallel model.\n\n\n\n\n\n\n\n\nFigure 3.6: Comparison of empirical response proportions (left) with model-based predicted probabilities (right) based on the parallel (solid lines) and non-parallel model (dotted lines).\n\n\n\n\n\nNext, we compare the predicted probabilities using area charts. Figure 3.7 shows that the differences between M0 and M1 are relatively minor.\n\n\n\n\n\n\n\n\nFigure 3.7: Area chart comparing empirical cumulative proportions (left) with model-based predicted probabilities based on the parallel model (middle) and the non-parallel model (right).\n\n\n\n\n\nFigure 3.8 shows the cumulative probabilities using a line plot. This allows us to superpose the predicted probabilities from the models in the same display.\n\n\n\n\n\n\n\n\nFigure 3.8: Line plot comparing empirical cumulative proportions (left) with model-based predicted probabilities (right) based on the parallel (solid lines) and non-parallel model (dotted lines).\n\n\n\n\n\n\n\n\n3.2.2.3 Parallel or non-parallel?\nIn our illustrative example, we have seen the while formal tests reject the parallel regression assumption. An inspection of regression coefficients across cutpoint equations showed that the association between Date of birth and the ordinal outcome is somewhat larger at cutpoint 1|2. When we looked at predicted probabilities, however, we noted that the parallel and non-parallel model produce largely similar results. We would therefore maintain the parallel regression assumption for the predictor.\nIn a model with multiple predictors, the decision whether or not to relax the parallel regression assumption must be made for each one individually. If at least one (but not all) predictors are “set free”, the model is called a partial model2. If all predictors are “set free”, the model is called a non-parallel model. Partial and non-parallel models allow for asymmetrical relationships (factors may have a stronger effect at one end of the ordered outcome scale) or erratic relationships. If possible, we should rely on theory to decide whether or not it is reasonable to expect (a)symmetrical relationships. Non-parallel models are equivalent to baseline-category models, which are commonly used for nominal data; this means they have no precision (or “power”) advantage, because no use is made of the ordinal information in the data.\nTable 3.4 shows the trade-off between parsimony and flexibility that is tied to the extent to which the parallel regression assumption is relaxed:\n\n\n\n\n\n\n\n\n\n\nPredictors\n\n\n\n\nNumber of model parameters\n\n\n\n\n\n\n\nModel\n\n\nX1\n\n\nX2\n\n\nX3\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\nFlexibility\n\n\nParsimony\n\n\n\n\n\n\nParallel model\n\n\nParallel\n\n\nParallel\n\n\nParallel\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n−\n\n\n++\n\n\n\n\nConstrained\n\n\nParallel\n\n\nConstrained\n\n\nFree\n\n\n7\n\n\n9\n\n\n11\n\n\n13\n\n\no\n\n\n+\n\n\n\n\nUnconstrained\n\n\nParallel\n\n\nFree\n\n\nFree\n\n\n7\n\n\n9\n\n\n13\n\n\n16\n\n\n+\n\n\no\n\n\n\n\nNon-parallel\n\n\nFree\n\n\nFree\n\n\nFree\n\n\n8\n\n\n12\n\n\n16\n\n\n20\n\n\n++\n\n\n−\n\n\n\n\n\n\nTable 3.4: The parallel regression constraint and model parsimony.\n\n\n\n\n\n\n3.2.3 Link functions\nWhile the logit, probit, and complementary log-log links are applied most often, other link functions are possible. For instance, the cauchit link is less sensitive to outliers and thus provide a more robust fit to the data. However, it requires specialized software. The choice between logit and probit is largely discipline-specific and thus conventional. On the probability scale, the two link functions produce virtually indistinguishable results. The logit link has the (minor) advantage that exponentiated regression coefficients can be interpreted as odds ratios. However, as we will see below, odds ratio are not useful for understanding ordered regression models since they are an unintuitive, unnatural metric that is poorly understood by most audiences.\nSymmetry: The logit and probit link functions are symmetrical, which means that the estimates are unaffected by a reversal of the outcome categories. This does not apply to the complementary log-log link. In the absence of any principled recommendations, the choice of link function can be based on information criteria, which indicate which link achieves the best fit to the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "03_orm.html#model-diagnostics",
    "href": "03_orm.html#model-diagnostics",
    "title": "3  Ordered regression models",
    "section": "3.3 Model diagnostics",
    "text": "3.3 Model diagnostics\n\n3.3.1 Residuals\nHarrell (2015, 314–15) provides a brief discussion of residuals in parallel cumulative models. He notes that partial residuals based on binary regressions for all cutpoint equations may be calculated. For these, smoothed curves are then examined. If the proportional odds assumptions holds, these should be parallel, and if a numeric predictor is represented appropriately in the model, there should be no indication of non-linearity. Partial residuals for binary models are discussed in Landwehr, Pregibon, and Shoemaker (1984).\n\n\n\n\n\n\nTo do\n\n\n\nRead Landwehr, Pregibon, and Shoemaker (1984)\n\n\nLi and Shepherd (2012) proposed a probability-scale residual, which yields one score per unit (or case/observation).\n\n\n\n\n\n\nTo do\n\n\n\nRead Li and Shepherd (2012)\n\n\nFox (2010, 109–10) shows how to calculate residuals for cumulative-link models. Are these really Bayesian latent residuals?\nFigure 3.9 illustrates the calculation of probability-scale residuals. Based on the fitted value for a particular observation, we can calculate the predicted response probabilities. The top panel in Figure 3.9 illustrates this for a case whose fitted value falls in between thresholds 1 and 2. The residual for this observation then depends on the observed response. If the observed response is 1, it is lower then expected (or predicted). The residual is then positive, since the prediction was too high. The value of the residual is a simple difference of two predicted probabilities: The model-based probability of a response lower than the observed response, and the model-based probability of a response higher than the one observed one. These probabilities are shown in Figure 3.9 using grey shading. The fitted value remains the same across all graphs. They show how the residual is calculated for different observed responses. The subtraction of the two densities is done in a way that positive differences indicate that the observed response is higher than expected, and negative difference indicate that it is lower than expected.\n\n\n\n\n\n\n\n\nFigure 3.9: Illustration of the calculation of probability-scale residuals.\n\n\n\n\n\nThe calculation of a latent-scale residual is demonstrated in Fox (2010, 109–10). If we denote the fitted value of observation i as \\(\\theta_i\\) and the observed category as \\(c\\), the latent-scale residual is obtained as follows:\n\\[\nE(\\epsilon_i | Y_i = c, \\theta_i) = \\frac{\\phi(\\tau_{c-1} - \\theta_i) - \\phi(\\tau_{c} - \\theta_i)}{\\Phi(\\tau_{c} - \\theta_i) - \\Phi(\\tau_{c-1} - \\theta_i)}\n\\]\nTo understand this formula, let us simplify it by representing the two quantities in the numerator as \\(D_1\\) and \\(D_2\\) (since they are densities); the difference between the two quantities in the denominator, on the other hand, is just the predicted response probability of the observed response.\n\\[\n\\begin{align}\nE(\\epsilon_i | Y_i = c, \\theta_i) &= \\frac{\\phi(\\tau_{c-1} - \\theta_i) - \\phi(\\tau_{c} - \\theta_i)}{\\Phi(\\tau_{c} - \\theta_i) - \\Phi(\\tau_{c-1} - \\theta_i)} \\\\\n&= \\frac{D_1 - D_2}{P(Y_i = c)}\n\\end{align}\n\\]\nNow we can look at these quantities in a graph. Figure 3.10 shows a normal density centered on \\(\\theta_i\\), the fitted value (marked as a dot on the x-axis). The scenario we are looking at is for an observed response of 3. The expected (or predicted) probability of this response is represented by the shaded area between \\(\\tau_2\\) and \\(\\tau_3\\). It makes sense for this predicted probability to occurs in the denominator: If it is small, the model gives a low probability to the response that was actually observed. In other words, the response is in some sense untypical. The smaller the predicted probability, the larger the residual. The other two quantities, \\(D_1\\) and \\(D_2\\), are marked on the y-axis. Figure 3.10 shows that these are the normal densities of the two thresholds flanking the observed category, in our case \\(\\tau_2\\) (\\(D_1\\)) and \\(\\tau_3\\) (\\(D_2\\)). The subtraction of the two densities is done in a way that positive differences indicate that the observed response is higher than expected, and negative difference indicate that it is lower than expected.\n\n\n\n\n\n\n\n\nFigure 3.10: Illustration of the calculation of latent-scale residuals.\n\n\n\n\n\nLet us apply these two types of residual to the model for parcel. Figure 3.10 shows the distribution of the residuals with a histogram. Note that probability-scale residuals are bounded by –1 and +1. The latent-scale residuals assume a more bell-shaped profile.\n\n\n\n\n\n\n\n\nFigure 3.11: The distribution of probability-scale residuals (left) and latent residuals (right) for the parcel model.\n\n\n\n\n\nNext, we inspect the residuals to detect potential problems with the way our model handles the continuous predictor Date of birth. To this end, Figure 3.12 graphs the residuals against the predictor Date of birth and overlays a smoother, to detect deviation from the assumed linearity. Both residuals seem to suggest that a straight-line trend may oversimplify the pattern in the data, but the deviation from this linear representation is relatively complex.\n\n\n\n\n\n\n\n\nFigure 3.12: Residual plot checking lack of fit for the predictor Date of birth: Probability-scale residuals (left) and latent residuals (right).\n\n\n\n\n\nIt should be possible to use residuals to check for heteroskedasticity by looking at their absolute values against predictor variables.\n\n\n\n\n\n\n\n\nFigure 3.13: Residual plot checking for indications of non-constant variance (i.e. heteroskedasticity) for the predictor Date of birth: Probability-scale residuals (left) and latent residuals (right).\n\n\n\n\n\n\n\n\n\n\n\nTo do\n\n\n\nResiduals for mixed-effects models.\n\n\n\n\n3.3.2 Ordinality assumption\nHarrell (2015) talks about checking the ordinality assumption. A simple way of doing this is to calculate means of the predictor variable for different levels of the ordinal outcome. If the means for two neighboring categories on the ordinal scale are largely equvalent for many of the predictor variables, these categories may be collapsed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "03_orm.html#other-ordered-regression-models",
    "href": "03_orm.html#other-ordered-regression-models",
    "title": "3  Ordered regression models",
    "section": "3.4 Other ordered regression models",
    "text": "3.4 Other ordered regression models\nThe stereotype model can be considered as intermediate between multinomial and ordered regression models. It can be used when you are unsure about the ordering of levels, or when it is suspected that one or more levels can be collapsed. The stereotype model can therefore be used to evaluate if the levels are in their proper order. It provides the (data-based) ordering of levels and quantifies the “closeness” of categories (see Agresti 2010, 103–15; Long and Freese 2014, 445–54).\n\n\n\n\nAgresti, Alan. 2010. Analysis of Ordinal Categorical Data. Hoboken, NJ: John Wiley & Sons.\n\n\nAitchison, J., and S. D. Silvey. 1957. “The Generalization of Probit Analysis to the Case of Multiple Responses.” Biometrika 44 (1/2): 130–40. https://doi.org/10.2307/2333245.\n\n\nCheng, J. Scott, Simon & Long. 2007. “Testing the IIA in the Multinomial Logit Model.” Sociological Methods and Research 35 (4): 583–600.\n\n\nFienberg, S. E. 1980. The Analysis of Cross-Classified Categorical Data. Second. Cambridge, MA: MIT Press.\n\n\nFox, Jean-Paul. 2010. Bayesian Item Response Modeling: Theory and Applications. New York: Springer.\n\n\nFullerton, Andrew S., and Jun Xu. 2016. Ordered Regression Models: Parallel, Partial, and Non-Parallel Alternatives. Boca Raton, FL: CRC Press.\n\n\nGoodman, Leo A. 1983. “The Analysis of Dependence in Cross-Classifications Having Ordered Categories, Using Log-Linear Models for Frequencies and Log-Linear Models for Odds.” Biometrics 39 (1): 149–60. https://doi.org/10.2307/2530815.\n\n\nGreene, William H., and David A. Hensher. 2010. Modeling Ordered Choices: A Primer. Cambridge: Cambridge University Press.\n\n\nHarrell, Frank E. Jr. 2015. Regresion Modeling Strategies. 2nd ed. New York: Springer.\n\n\nKim, Ji-Hyun. 2003. “Assessing Practical Significance of the Proportional Odds Assumption.” Statistics &Amp; Probability Letters 65 (3): 233–39. https://doi.org/10.1016/j.spl.2003.07.017.\n\n\nLandwehr, James M., Daryl Pregibon, and Anne C. Shoemaker. 1984. “Graphical Methods for Assessing Logistic Regression Models.” Journal of the American Statistical Association 79 (385): 61–71. https://doi.org/10.1080/01621459.1984.10477062.\n\n\nLi, Chun, and Bryan E. Shepherd. 2012. “A New Residual for Ordinal Outcomes.” Biometrika 99 (2): 473–80. https://doi.org/10.1093/biomet/asr073.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oakes, CA: Sage.\n\n\n———. 2014. “Regression Models for Nominal and Ordinal Outcomes.” In The Sage Handbook of Regression Analysis and Causal Inference, edited by Christof Best Henning & Wolf, 173–203. London: Sage.\n\n\nLong, J. Scott, and Jeremy Freese. 2014. Regression Models for Categorical Dependent Variables Using Stata. College Station, TX: Stata Press.\n\n\nMcKelvey, Richard D., and William Zavoina. 1975. “A Statistical Model for the Analysis of Ordinal Level Dependent Variables.” The Journal of Mathematical Sociology 4 (1): 103–20. https://doi.org/10.1080/0022250x.1975.9989847.\n\n\nPeterson, Bercedis, and Frank E. Harrell. 1990. “Partial Proportional Odds Models for Ordinal Response Variables.” Applied Statistics 39 (2): 205–17. https://doi.org/10.2307/2347760.\n\n\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht, and Paul Messer. 2024. “Latent-Variable Modelling of Ordinal Outcomes in Language Data Analysis.” Journal of Quantitative Linguistics 31 (2): 77–106. https://doi.org/10.1080/09296174.2024.2329448.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "03_orm.html#footnotes",
    "href": "03_orm.html#footnotes",
    "title": "3  Ordered regression models",
    "section": "",
    "text": "We would usually rely on the “SJ” method for determining the kernel density estimation bandwidth, but here we have set the smoothing parameter manually.↩︎\nPartial models can be further distinguished into “constrained” and “unconstrained” models. Thus, the pattern of the coefficients may be constrained in the sense that they are assumed to vary systematically, i.e. show a certain pattern (such as a stronger effect at the lower end of the scale); thus, constraints are placed on the variation of coefficients across splits (e.g. they could be modeled to vary in a linear fashion).↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordered regression models</span>"
    ]
  },
  {
    "objectID": "04_lvm.html",
    "href": "04_lvm.html",
    "title": "4  A latent variable model",
    "section": "",
    "text": "4.1 Relation between latent variable and ordinal response categories\nIn the latent-variable perspective, the ordinal variable is thought of as an imperfect, coarser measurement of an unobserved (latent) trait. Whenever the variable crosses a threshold, we observe a category change. Thresholds (also called cutpoints) “cut up” the latent scale into intervals that correspond to the ordinal categories. The latent variable (often denoted as y*) ranges from negative to positive infinity. This is illustrated in Figure 4.1, where three thresholds (\\(\\tau_1\\), …, \\(\\tau_3\\)) divide the latent scale into 4 stretches.\nFigure 4.1: Illustration of the mapping from the latent variable y* to the observed ordered categories.\nThe thresholds (often denoted as \\(\\tau\\)) translate the continuous variable into rank-ordered categories. The number of thresholds is always 1 less than the number of response categories. For four categories, there are three thresholds.\nIt makes sense to assume that the trait that is being measured by an ordinal variable is distributed symmetrically in the population of interest – indeed, most ordinal regression models make this assumption. This distribution therefore assumes a bell shape, as illustrated in Figure 4.2. Depending on the specific form of model that is implemented, this may be a standard normal or a standard logistic distribution.\nThe area under the density curve represents all responses in the sample (i.e. 100%). The thresholds divide up this area into four parts, one for each response category. These are shown in Figure 4.2 using grey shading. The area below \\(\\tau_1\\), which appears in dark grey, represents the proportional share of response category 1. In Figure 4.2, it is .16. The share of category 2 – .46 – is represented by the portion between \\(\\tau_1\\) and \\(\\tau_2\\). The response probability for categories 3 (light grey) and 4 (white) are .22 and .16, respectively.\nFigure 4.2 illustrates how a parallel cumulative model represents the category probabilities from a latent-variable perspective. What determines this distribution is (i) the location of the threshold parameters, and (ii) the center of the bell curve. The center of the density curve can be thought of as the model intercept.\nFigure 4.2: Latent trait interpretation of the cumulative model: Relationship between the latent variable *y** and the observed ordinal variable y.\nThe predictor variables model the location of the density curve. For a subgroup that shows a larger proportion of higher responses, the density curve is shifted rightward. This means that its center shifts upwards along the latent scale. This is illustrated in Figure 4.3, where the center is shifted by +0.7 to the right – the average is now +1.7. The response probabilities for categories 1 to 4 are now .04, .30, .27, and .38. The dotted curve represents the density in Figure 4.2, which is centered about 1.\nFigure 4.3: Change in response probabilities resulting from a shift of the latent mean.\nOne way to model ordinal data is to assume that the underlying latent variable has a standard normal distribution, with a mean of 0, and a standard deviation of 1. The model then determines the location of the cutpoints. Instead of assuming equal distances between them (as is the case in MRMs), the data decide where to cut up the latent scale. Importantly, the normally distributed trait has a mean, which varies as a function of the predictors in the model. In this regard, the latent variable model for ordered outcomes is similar to MRMs. Thus, if a predictor shows an association with the outcome, it shifts the mean of the latent trait upwards or downwards on the latent response scale.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#the-data-generating-mechanism-represented-by-a-lvm",
    "href": "04_lvm.html#the-data-generating-mechanism-represented-by-a-lvm",
    "title": "4  A latent variable model",
    "section": "4.2 The data-generating mechanism represented by a LVM",
    "text": "4.2 The data-generating mechanism represented by a LVM\nWe can also think of the latent-variable interpretation of cumulative models as dividing the data-generating process into two parts (see Grilli and Rampichini 2012, 393). The first is the underlying continuous representation of the trait that is being measured, including an assumption about the shape of its distribution in the population. The most commonly used link functions (logit and probit) stipulate a symmetric, bell-shaped profile. The second part is a measurement model, which represents the way in which this continuous trait is being measured. In some sense, then, the first part represents the construct of interest, i.e. a sensation (or perception/attitude/opinion) that may vary between individuals and contexts, and the second part represents the instrument used to measure this construct, e.g. design features of the questionnaire (e.g. number and wording of response categories).\nFigure 4.4 illustrates the difference between these two components. Let’s assume a researcher is interested in the extent to which respondents agree to a particular statement. We can think of agreement as a continuous variable – opinions vary, and some individuals will agree (or disagree) more strongly than others. The latent-variable model assumes that the continuous feature of interest (here: agreement) has a bell-shaped distribution. The density curve at the top of Figure 4.4 shows the assumed distribution of the underlying trait in a sample of speakers.\nTo measure the level of agreement, the researcher may use different response scales, which can vary in (i) the number of response categories and (ii) their wording. This means that the instrument may be designed in different ways to measure the variable of interest. Figure 4.4 shows three scenarios: a 3-point scale, a 5-point scale, and a 7-point scale. On the left side, the design of the ordinal response scale is shown. On the far right, the observed distribution of responses across the categories is visualized using a grouped bar chart. The density curves in the middle illustrate how the latent-variable model represents both components: the distribution of the latent trait in the sample of informants (bell-shaped curve), and the design of the ordinal scale, i.e. the number and perceived meaning of the verbal scale labels (number and location of thresholds).\n\n\n\n\n\n\n\n\nFigure 4.4: Illustration of the data-generating mechanism: While the assumed distribution of the continuous variable in the population remains the same, differences in the number and phrasing of response categories will produce different observed distributions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#potential-for-confusion-scaling-of-latent-variable-vs.-cumulative-proportions",
    "href": "04_lvm.html#potential-for-confusion-scaling-of-latent-variable-vs.-cumulative-proportions",
    "title": "4  A latent variable model",
    "section": "4.3 Potential for confusion: Scaling of latent variable vs. cumulative proportions",
    "text": "4.3 Potential for confusion: Scaling of latent variable vs. cumulative proportions\nA potentially confusing feature of cumulative models is the directionality of the model scale, and its relation to the cumulative probabilities. To recognize the issue, consider Figure 4.5. The horizontal axis represents the latent variable. Higher values correspond to a higher value on the underlying continuum (e.g. a higher level of agreement). The thresholds divide the scale into four intervals, which represent the response categories of the ordered scale. Two densities are shown – one for the average latent score of 0, and the second one with an average of 0.7, which is therefore displaced to the right. The averages are shown with filled circles (marked “A” and “B”). We note that group B shows a higher average on the latent scale. On the ordinal response scale, this means that group B will show a greater share of responses in the higher response categories.\n\n\n\n\n\n\n\n\nFigure 4.5: Cumulative models can be formulated on the basis of (a) cumulative probabilities, i.e. the probability of a response below a threshold; and (b) exceedance probabilities, i.e. the probability of a response above a threshold. Only formulation (b) produces latent scores that are consistent with the directionality of the ordinal scale.\n\n\n\n\n\nScaling issues can arise because the cumulative model is formulated in terms of cumulative probabilities, and for a given threshold, there are two probabilities we can foreground: the probability of observing a category below the threshold, or the probability of observing a category above the threshold. The probability of observing a category below the threshold is usually referred to as a cumulative probability, and the probability of observing a category above the threshold as an exceedance probability (i.e. 1 – cumulative probability). Both model formulations are found in the literature.\nTo see how this may cause confusion, let us consider a simple example. In Figure 4.5, threshold \\(\\tau_{1}\\) divides the scale into those categories below \\(\\tau_{1}\\) (1) and those above \\(\\tau_{1}\\) (2, 3, 4). Panel (a) foregrounds the probability of observing a response below the threshold. This probability is shown using grey shading. We see that this shaded area is smaller in group B (.04) than in group A (.16). This means that the probability of observing a category below the threshold decreases as the latent score increases. The two types of quantity (cumulative probability, latent score) therefore have a reverse interpretation, which can cause confusion.\nIn a model that describes the distribution of cumulative probabilities, the latent score generated by the linear predictor is therefore subtracted from the thresholds. This ensures that the coefficients of the model will be consistent with the ordinal scale.\n\\[\nln\\left(\\frac{Pr(y \\le m)}{Pr(y &gt; m)}\\right) = \\tau_m - \\textbf{x}\\beta\n\\]\nIn panel (b), on the other hand, the probability of observing a response above the threshold is foregrounded. Now the shaded area is larger in group B (.96) than in group A (.84), which means that the probability of observing a category above the threshold increases as the latent score increases. The two types of quantity (exceedance probability, latent score) therefore have the same directionality of interpretation.\nIn a model that describes the distribution of exceedance probabilities, the thresholds are therefore subtracted from the latent score generated by the linear predictor. This again ensures that the coefficients of the model will be consistent with the ordinal scale.\n\\[\nln\\left(\\frac{Pr(y \\ge m)}{Pr(y &lt; m)}\\right) =  \\textbf{x}\\beta - \\tau_m\n\\]\nThis means that choice of formulation affects the meaning of the thresholds. Let us illustrate this with a simple regression model for the heaps data, with Age as the only predictor variable. We fit two models to the data – one based on cumulative and one based on exceedance probabilities. The models return the exact same coefficient for the predictor Age. The thresholds, on the other hand, are mirror images of one another.\nThis is illustrated in Figure 4.6. First note that the y-axis is scaled identically in both panels. The trend lines for Age coincide. The threshold coefficients, however, run in opposite directions. For cumulative probabilities, thresholds increase with the latent scale – they increase from bottom to top. For exceedance probabilities, on the other hand, they decrease from bottom to top. In fact, the thresholds in the right panel are the exact mirror image of those in the left, with zero as the mirror axis.\n\n\n\n\n\n\n\n\nFigure 4.6: The location and meaning of the thresholds depends on whether cumulative or exceedance probabilities are modeled.\n\n\n\n\n\nThe thresholds for cumulative probabilities, which appear in the left panel, allow for a natural interpretation of the latent scale: Thresholds represent cutpoints that divide the continuous latent scale into an ordered set of categories.\nBoth formulations exist in the literature. The advantage of the exceedance-probability formulation is that the threshold coefficients are consistent with the intercepts we obtain from running a series of binary regressions on the cutpoint equations along the ordinal scale (Harrell 2015, 313). This makes it easier to run model checks. An advantage of the cumulative-probability formulation, on the other hand, is that it makes transparent the latent-variable perspective. This is because the threshold coefficients can be interpreted as the cutpoints that discretize the underlying continuous variable.\nWhen using cumulative models (or, indeed any other form of ordered regression), it is important to know which formulation is implemented in the software. Table Table 4.1 shows which type of probability is foregrounded in a number of R packages.\n\n\n\n\n\n\n\nR package\n\n\nFunction\n\n\nReference\n\n\nProbability\n\n\n\n\n\n\nordinal\n\n\nclm(), clmm()\n\n\nChristensen (2018)\n\n\nCumulative\n\n\n\n\nMASS\n\n\npolr()\n\n\nVenables and Ripley (2002)\n\n\nCumulative\n\n\n\n\nVGAM\n\n\nvglm()\n\n\nYee (2015)\n\n\nCumulative\n\n\n\n\nbrms\n\n\nbrm()\n\n\nBürkner (2017)\n\n\nCumulative\n\n\n\n\nrms\n\n\norm()\n\n\nHarrell (2015)\n\n\nExceedance\n\n\n\n\n\n\nTable 4.1: The formulation of cumulative models in different R packages.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#latent-variable-analysis-of-the-heaps-data",
    "href": "04_lvm.html#latent-variable-analysis-of-the-heaps-data",
    "title": "4  A latent variable model",
    "section": "4.4 Latent-variable analysis of the heaps data",
    "text": "4.4 Latent-variable analysis of the heaps data\nFor illustration, we apply a LVM to the heaps data. Here, we focus on model interpretation and how the distribution of the ordinal responses is represented using thresholds and shifted density curves. We will therefore directly move on to illustrative graphs of the fitted model.\n\n4.4.1 Category proportions under the density curve\nIn the following diagrams, the (vertical) y-axis shows the underlying trait. For the current data set, this is the reported “likelihood of usage”, ranging from “never” to “likely”. The questionnaire offered six tick boxes (see Figure 1.4), so we have five thresholds. Let us consider each predictor in turn.\nIn Figure 4.7, the horizontal axis shows the predictor Age, which varies from around 80 years (left margin) to about 20 years (right margin). The distribution of the latent trait is shown for three discrete values of age (75, 50, and 25 years). There is a clear association between Age and the usage of heaps: The center of the density curve shifts upwards. For 75-year-olds, the response “never” has the largest share (red), while for 25-year-olds, the response “likely” has the highest probability (grey). The three snapshots we have chosen are points on a continuum, which is represented by the trend line.\n\n\n\n\n\n\n\n\nFigure 4.7: Distribution of the latent variable *y** at different years of age.\n\n\n\n\n\nThe model-based density curves for the different levels of Register are shown in Figure 4.8. The black dots trace the mean of the latent variable across the levels of Register. We observe that while the proportional share of “likely” is highest when talking to friends, the different types of interlocutor account for relatively little variation in the probability of outcome responses.\n\n\n\n\n\n\n\n\nFigure 4.8: Distribution of the latent variable *y** for different registers.\n\n\n\n\n\nDifferences among syntactic functions are more pronounced. As Figure 4.9 shows, the bell-shaped distributions shift across the levels of this variable. The shaded area under the curve shows how the responses are distributed across the syntactic functions. heaps has the highest level of acceptability when used as a quantifier. As an intensifier of an adjective in the positive form, it is clearly dispreferred.\n\n\n\n\n\n\n\n\nFigure 4.9: Distribution of the latent variable for different syntactic functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#condensed-representation-focus-on-the-average",
    "href": "04_lvm.html#condensed-representation-focus-on-the-average",
    "title": "4  A latent variable model",
    "section": "4.5 Condensed representation: Focus on the average",
    "text": "4.5 Condensed representation: Focus on the average\nThe latent variable model allows us to interpret and communicate an ordered regression model in very similar ways as MRMs. To this end, we can construct predictive margins on the latent variable scale for each predictor in the model. Figure 4.10 shows the means of the distribution of the latent variable across different levels or values of the predictors in the model, adjusting for all other predictors in the model. These adjusted predictions should be compared to the MRM version in Figure 1.5.\n\n\n\n\n\n\n\n\nFigure 4.10: Predictive margins on the latent-variable scale for a side-by-side comparison of predictors.\n\n\n\n\n\n\n4.5.0.1 A model with an interaction\nWe can include an interaction between Age and Syntactic function.\n\n\n\n\n\n\n\n\nFigure 4.11: Predictive margins on the latent-variable scale for an inspection of the interaction between Age and Syntactic function.\n\n\n\n\n\n\n\n4.5.1 Thresholds\nThe default approach to the estimation of thresholds is to grant the model full flexibility when locating them. This means that their spacing is determined empirically, based on the data. While this may at first seem like a sensible approach, it is also the most costly in terms of the number of parameters needed to record their location. For \\(k\\) response categories, we need \\(k-1\\) threshold parameters. It therefore makes sense to consider alternative threshold structures.\nWhether it makes sense to use the default approach and fit flexible thresholds depends on the design of the ordinal scale. It turns out that in many cases, additional information can be used to fit thresholds. Supplementary information may be available on scale symmetry or equidistance. By building this information into the model, fewer parameters are needed to estimate the cutpoints.\nLet us consider symmetry first. Some ordinal scales are constructed in a symmetric way, which is reflected in parallel wording to both sides of the (implicit) scale midpoint. The illustrative agreement scales in Figure 4.4, for instance, are symmetric. The thresholds are therefore spaced symmetrically around their midpoint. Further examples, which are taken from the linguistic research literature, appear in Figure 4.12. Note how each set of verbal scale point labels is folded at the center.\n\n\n\n\n\n\n\n\nFigure 4.12: Examples of diverging ordinal scales with symmetric scale point labels.\n\n\n\n\n\nVerbalizations that create symmetric scales are relatively frequent in the literature. In a recent survey of the use of ordinal response scales in the linguistic research literature, Sönning (2024) looked at 4,441 articles published between 2012 and 2022 in 17 linguistic journals. In total, 473 ordinal response scales were identified. Of these, 89 (19%) showed a symmetric verbalization, the most typical variant being Likert-type agreement scales.\nThe estimation of thresholds may also be directed by information on equidistance. If an ordinal scale consists of a sequence of boxes which are labeled only at the endpoints, the spacing between thresholds can arguably be considered equidistant. This kind of layout is typical for semantic differential scales, for instance. Figure 4.13 shows a number of examples from the literature. Out of the 473 ordinal response scales surveyed in Sönning (2024), 245 (52%) permit an equidistant spacing of the thresholds.\n\n\n\n\n\n\n\n\nFigure 4.13: Examples of response scales with verbal labels only at the endpoints, which suggest the use of equidistant thresholds.\n\n\n\n\n\nSymmetric thresholds divide the latent scale into proportionate stretches. As Figure 4.14 illustrates, the distance between thresholds is the same to both sides of the scale midpoint. When we constrain the thresholds to be symmetric, we need fewer parameters to describe their location. For 6 outcome categories, for instance, we need 3 (instead of 5) parameters.\n\n\n\n\n\n\n\n\nFigure 4.14: Examples of different symmetric threshold structures for a 5- and a 6-point ordered scale.\n\n\n\n\n\nThe equidistance assumption allows us to fit an even more parsimonious threshold model. Table 4.2 compares the three methods in terms of the number of parameters they require for response scales with 4 to 8 categories. The flexible approach always requires \\(k - 1\\) parameters, where \\(k\\) is the number of categories. The equidistant approach always requires 2 parameters, irrespective of the length of the ordinal scale. The symmetric approach is in between, and its parsimony advantage grows with the number of response categories.\n\n\n\n\n\n\n\n\n\nFigure 4.15: Examples of equidistant thresholds for ordered scales with 4 to 7 response categories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of response categories\n\n\n\n\n\nMethod\n\n\nThree\n\n\nFour\n\n\nFive\n\n\nSix\n\n\nSeven\n\n\nEight\n\n\n\n\n\n\nFlexible\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\nSymmetric\n\n\n2\n\n\n2\n\n\n3\n\n\n3\n\n\n4\n\n\n4\n\n\n\n\nEquidistant\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n2\n\n\n\n\n\n\nTable 4.2: Number of model parameters needed to represent different threshold structures.\n\n\n\nLet us briefly consider what threshold structures we may wish to use for our case studies. For the BrE-AmE lexical pairs, the response scale is verbalized symmetrically (see Figure 1.1). We therefore use symmetric thresholds when analyzing these data.\nFor the heaps data, the layout of the response scale is more complex (see Figure 4.16). Only three categories of the 6-point scale are labeled. The sequence from “likely” to “unlikely” can be considered equidistant. The distance between “unlikely” and “never”, on the other hand, should remain flexible.\n\n\n\n\n\n\n\n\nFigure 4.16: Excerpt from the questionnaire used to collect reported usage rates for heaps.\n\n\n\n\n\nThis means that we would ideally like to use custom thresholds when modeling these data. The custom set, which is illustrated in Figure 4.17, includes a threshold parameter \\(a\\) for the equidistant sequence, and a threshold parameter \\(b\\) for the flexible spacing at the right end of the scale. Since the implementation of custom thresholds requires considerable additional work, we will instead use flexible thresholds when modeling these data.\n\n\n\n\n\n\n\n\nFigure 4.17: Custom threshold structure for the heaps data: While the spacings labeled ‘a’ may be considered constant, spacing ‘b’ is best represented using an additional parameter.\n\n\n\n\n\n\n\n4.5.2 Model identification: Thresholds and intercept\nIn a cumulative model, the intercept and the threshold parameters cannot be estimated simultaneously (see, e.g. Rabe-Hesketh and Skrondal 2021, 642–44). In a model with 5 outcome categories, for example, it is not possible to estimate an intercept as well as all four threshold parameters. Instead, the location of one of these parameters must be fixed to 0, which is equivalent to dropping it from the model. Since different options exist, there are different parameterizations of cumulative models.\nThe intercept can be set to 0, which means that all threshold parameters are estimated. This is the default approach in virtually all software packages. Alternatively, one of the thresholds can be set to 0, and then the remaining thresholds and the intercept are estimated. This option is attractive if the ordinal scale is diverging (perhaps symmetric), with a logical (or natural) midpoint. What makes this option attractive for these kind of scale layouts is the fact that we can give the latent scale a meaningful zero point. Positive scores on the latent scale then reflect sentiments above the scale midpoint, while negative scores signal the opposite direction. With an even number of response categories, the middle threshold can be mapped to zero. With an uneven number of response categories, we can set the mean of the two central thresholds to 0. These two strategies are illustrated in Figure 4.14 above.\nFigure 4.18 illustrates two basic parameterizations of cumulative models. The model does not include any predictors but instead models a set of observed category proportions: 1 (.16), 2 (.53), 3 (.15), 4 (.16). These probabilities correspond to the areas under the density curve between two thresholds (see Figure 4.2).\nIn the parameterization on the left, the location of the intercept is set equal to zero. It therefore appears in grey, with a dotted line. This means that all three thresholds are estimated (\\(\\tau_1\\) = –1, \\(\\tau_2\\) = +0.5 and \\(\\tau_3\\) = +1). These three parameters then apportion the probability mass appropriately.\n\n\n\n\n\n\n\n\nFigure 4.18: Two parameterizations of the cumulative model: (a) The intercept or (b) the first threshold is fixed to zero and thereby excluded from the model.\n\n\n\n\n\nIn the parameterization on the right, it is the location of the first threshold that is fixed to zero; the intercept is therefore grayed out. The other two cutpoints (\\(\\tau_2\\) and \\(\\tau_3\\)) are estimated (+1.5 and +2, respectively), and the intercept is also estimated as +1. This again yields the appropriate distribution of the probability mass.\nIf we compare these two graphs, we observe that the bell-shaped curves are not aligned. In panel (a), the density is centered at 0; in panel (b), it is centered at +1. The location of the density curve on the latent scale depends on the parameterization, i.e. which parameter is fixed to zero.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#variance-components",
    "href": "04_lvm.html#variance-components",
    "title": "4  A latent variable model",
    "section": "4.6 Variance components",
    "text": "4.6 Variance components\nIn mixed-effects models, a standard deviation parameter is estimated for each clustering variable that is represented with random intercepts. While the random-effects SD parameter is estimated from the data, the residual (level-1) SD parameter remains fixed at 1. This is a somewhat odd feature of mixed-effects LVMs for ordinal data. As a result, both measures of spread must be understood in the context of the threshold parameters, which usually change systematically when alternating between a simple and a mixed-effects model.\nTo see why this is so, consider a simple regression model that is applied to a hierarchically structured set of data with multiple observations from each subject. If the variable Subject is not included as a random effect in the model, the between-subject variability is soaked up by the residual (level-1) error term. Since it is fixed to 1, however, the location of the thresholds must represent the surplus variation, i.e. the additional variability due variation to between subjects. The thresholds will be spaced more tightly, which results in the probability mass shifting to more extreme response categories. If a random intercept term is added to the model, the spacing of the threshld parameters will increase, to reflect the reduction in the residual (level-1) error term.\n\n\n\n\n\n\n\n\nFigure 4.19: Level-1 and level-2 error terms.\n\n\n\n\n\n\n\n\n\nBürkner, Paul-Christian. 2017. “Brms: An r Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nChristensen, Rune Haubo B. 2018. “Cumulative Link Models for Ordinal Regression with the r Package Ordinal.” Technical University of Denmark.\n\n\nGrilli, Leonardo, and Carla Rampichini. 2012. “Multilevel Models for Ordinal Data.” In Modern Analysis of Customer Surveys: With Applications Using R, edited by Ron S. Kenett and Silvia Salini, 391–411. New York: Wiley.\n\n\nHarrell, Frank E. Jr. 2015. Regresion Modeling Strategies. 2nd ed. New York: Springer.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.\n\n\nSönning, Lukas. 2024. “Ordinal Response Scales: Psychometric Grounding for Design and Analysis.” Research Methods in Applied Linguistics 3 (3): xx–. https://doi.org/10.1016/j.rmal.2024.100156.\n\n\nVenables, William N., and Brian D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer.\n\n\nYee, Thomas W. 2015. Vector Generalized Linear and Additive Models: With an Implementation in r. New York: Springer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "04_lvm.html#footnotes",
    "href": "04_lvm.html#footnotes",
    "title": "4  A latent variable model",
    "section": "",
    "text": "(Largely) synonymous terms in the literature are underlying propensity/liability/sentiment/index function/utility.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>A latent variable model</span>"
    ]
  },
  {
    "objectID": "05_meth_int.html",
    "href": "05_meth_int.html",
    "title": "5  Methods of interpretation",
    "section": "",
    "text": "5.1 Regression coefficients\nThe output of an ordered regression model is a table of coefficients. These typically provide relatively little and rather obscure information about relations in the data. Here is the printout for our current model:\nformula: rating_int ~ dob_c + gender\ndata:    d\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit symmetric 193  -260.79 531.59 5(1)  3.82e-13 2.8e+01\n\nCoefficients:\n        Estimate Std. Error z value Pr(&gt;|z|)    \ndob_c    -1.1840     0.2415  -4.903 9.44e-07 ***\ngenderm  -0.5843     0.2698  -2.166   0.0303 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n          Estimate Std. Error z value\ncentral.1 -0.81539    0.21223  -3.842\ncentral.2  0.20360    0.20495   0.993\nspacing.1  0.39671    0.06906   5.744\nThere are two ways of making coefficients of an ordered regression model more meaningful: standardization and, in the case of a logit link, re-expression as odds ratios. The aim of standardization is to establish comparability between coefficients. We will not cover standardized coefficients here. For more information, see Long (1997, 60–71, 127–30) and Long and Freese (2014, 180–83, 332–35).\nWhen a logit link is used, odds ratios can be used for interpretation. Odds ratios are obtained through exponentiation of the regression coefficients. The coefficient for the variable Gender, for instance, is -0.58, where “female” is the reference level. This is a difference on the log odds scale. After exponentiation, this becomes an odds ratio of 0.56. This means that, for male speakers, the odds of responding below a particular threshold on the response scale are 0.56 as high as for female speakers. Put differently, the odds for female speakers are higher by a factor of 1.79. For Date of birth, the difference in log odds associated with an increase of 25 years is 0.2, which is an odds ratio of 0.31.\nOdds ratios can assist in the comparison of the relative strength of association between predictors and the outcome. It should be noted, however, that odds ratios are counterintuitive for many people, which is why we should perhaps give preference to other strategies of interpretation.\nLet us briefly consider the limitations of odds ratios and assume we have two proportions we wish to compare numerically: A (.40) and B (.20). These proportions are shown visually in Figure 5.1. Two straightforward ways of comparing .40 and .20 are to state (i) that the difference is .20 (or 20 percentage points); or (ii) that the proportion in group A is twice as high, or greater by a factor of 2.\nFigure 5.1: Two proportions that are to be compared.\nTo compare these proportions using an odds ratio, on the other hand, we first need to convert them into odds. We obtain the odds for group A by dividing the proportion (.40) by its analogue, i.e. 1 – .40 = .60. The odds in group A are therefore .40/.60 = 0.67, and those in group B are .20/.80 = 0.25. The ratio of these odds is 0.67/0.25 = 2.67. This means that an odds ratio is a ratio of ratios of probabilities – a rather obscure way of quantifying the difference between the two conditions.1\nIt therefore makes sense to translate regression coefficients into more meaningful quantities. The natural metric for understanding categorical outcomes are probabilities, to which we now turn.\nCumulative proportions play a special role for ordered variables. Here are examples of communicating the association between a predictor and the outcome using probabilities:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methods of interpretation</span>"
    ]
  },
  {
    "objectID": "05_meth_int.html#sec-meth-int-coef",
    "href": "05_meth_int.html#sec-meth-int-coef",
    "title": "5  Methods of interpretation",
    "section": "",
    "text": "On average, the proportion of neutral responses was .32 among 20-year-olds and .17 among 60-year-olds, a difference of .15.”\n“The proportion of responses above the neutral midpoint for the adjectival and quantifier function was .07 and .89, a difference of .82 in absolute terms.”\n“For an average native Australian native speaker, the probability of being “likely” to use heaps in a conversation with friends is .30. This compares with .01 in a conversation with a superior.”\n“For female respondents, the probability of agreement (i.e. the categories “agree” and “strongly agree” combined) exceeds that of male respondents by a factor of 1.8.”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methods of interpretation</span>"
    ]
  },
  {
    "objectID": "05_meth_int.html#sec-meth-int-pp",
    "href": "05_meth_int.html#sec-meth-int-pp",
    "title": "5  Methods of interpretation",
    "section": "5.2 Predictions on the probability scale",
    "text": "5.2 Predictions on the probability scale\nTranslating tables of coefficients into probabilities/proportions requires some effort, as the model output needs to be processed to arrive at such quantities. The R packages {marginaleffects} (Arel-Bundock, Greifer, and Heiss Forthcoming) and {emmeans} (Lenth 2024) greatly facilitate this task.\nBefore we go further, we must consider a fundamental issue that complicates the use of predicted probabilities for the interpretation of categorical regression models (see Long and Freese 2014, 133–36). While the scale on which the model is fit – the link scale – is linear, the data scale on which we interpret the model – the probability scale – is non-linear. To understand what is meant by “linear” vs. “non-linear” in this case, consider Figure 5.2. It shows a hypothetical analysis with two predictors, Age and Gender. Each panel shows two trend lines, one for male and one for female speakers. In the left panel, they are shown on the logit scale, where they are straight. This is what is meant by “linear”. The right-hand panel shows them after back-transformation to the probability scale, where they appear as curves and are therefore “non-linear”.\n\n\n\n\n\n\n\n\nFigure 5.2: Predictions and comparisons on (a) the linear logit scale and (b) the non-linear probability scale.\n\n\n\n\n\nThe non-linearity of the probability scale is an issue if we are looking for a single number to summarize the difference between male and female speakers. The dotted grey lines in Figure 5.2 show this difference at four locations (20, 40, 60, and 80 years). On the logit scale (left panel), they are constant: The difference between male and female speakers is 1.5 logits, irrespective of age. This greatly simplifies the summary of the model. On the probability scale (right panel), on the other hand, we note that the difference between male and female speakers depends on which age group we are considering: For 20-year-olds, the difference is small (.06), for 80 year-olds much larger (.35). On the probability scale, then, the difference between male and female speakers depends on where in the data space (here: for which age group) we evaluate it.\nThe same problem applies to the comparison of age groups. The triangles in the left panel in Figure 5.3 shows that speaker groups that are 20 years apart (i.e. moving 20 years horizontally) differ by the same amount, i.e. a vertical step of 1.5 units on the logit scale. It does not matter which age groups we compare (80- vs. 60-year-olds or 40- vs. 20-year-olds) and it also does not matter whether we look at male or female speakers. All triangles look the same. The right-hand panel, on the other hand, shows that the difference in predicted probability associated with an age gap of 20 years depends on which part of the predictor space we look at. For male speaker aged 40 vs. 20, the difference is as small as .03. For female speakers aged 80 vs. 60, it is as large as .24. The difference therefore varies by gender and across the age range.\n\n\n\n\n\n\n\n\nFigure 5.3: Predictions and comparisons on (a) the linear logit scale and (b) the non-linear probability scale.\n\n\n\n\n\nThe basic issue, then, is that when making comparisons on the probability scale, we must keep in mind that it matters where in the data space the comparison is evaluated. To get a sense of the average difference associated with certain predictor values, we therefore usually average over the distribution of differences on the probability scale.\nThe current section on the use of predicted probabilities for model interpretation is divided into four parts. We will first deal with predictions and then with comparisons between predictions. For each quantity, we distinguish two types: Predictions/comparisons made for individual units vs. averages over such unit-level estimates.\n\n5.2.1 Predicted category-specifc vs. cumulative probabilities\nWe can predict both types of probabilities\n\n\n5.2.2 Predictions for each observed unit\nA useful first step is to compute predicted probabilities for the estimation sample, i.e. the data that were used to fit the model (see Long and Freese 2014, 138–39, 339–40). This means that we ask the model to generate predictions for each observation in the sample. An examination of this distribution of predictions is helpful for two reasons. First, the variation in predictions gives us an idea of the model’s capacity to capture variation in the response variable. If predictions vary little, the explanatory variables in our model are largely inert. If predicted values vary substantially, on the other hand, these variables effectively capture variation in the distribution of the responses. Further, the distribution of predictions may point to interesting or potentially problematic features in the data or model.\nWe can visualize the distribution of these predicted probabilities with a dot diagram. Note how the caption of Figure 5.4 clarifies how these predictions were obtained, i.e. how the predictor variables were handled. In the present case, the are treated as-observed, which means that the observed values in the estimation sample are used to calculate predictions.\n\n\n\n\n\n\n\n\nFigure 5.4: Predicted category-specific probabilities for the observations in the estimation sample.\n\n\n\n\n\nThis first assessment is useful for getting an impression of the predictive capacity of the model, i.e. the extent to which the observed variation in the predictor variables is linked to variation in the outcome probabilities. Figure 5.4 shows large variation in the extreme categories, where probabilities range from .15 to .70 (category 1) and .10 to .55 (category 5), suggesting that the predictors in our model are associated with considerable variation in the ordinal outcome. In such a dot diagram, we would also look for outliers, which may point to problems in the data (or model).\nWe can also provide summary statistics in a table. Table 5.1 reports the average predicted response probability as well as the in-sample variability pf the response probabilities. Note again how the table caption clarifies how predictor variables were handled.\n\n\n\n\n\n\n\nResponse\n\n\nMean\n\n\nSD\n\n\nMin\n\n\nMax\n\n\n\n\n\n\n1\n\n\n.38\n\n\n.15\n\n\n.05\n\n\n.64\n\n\n\n\n2\n\n\n.09\n\n\n.02\n\n\n.02\n\n\n.10\n\n\n\n\n3\n\n\n.21\n\n\n.03\n\n\n.11\n\n\n.25\n\n\n\n\n4\n\n\n.07\n\n\n.02\n\n\n.04\n\n\n.10\n\n\n\n\n5\n\n\n.25\n\n\n.15\n\n\n.09\n\n\n.74\n\n\n\n\n\n\nNote:   Adjusted to: Date of birth (as-observed), Gender (as-observed).\n\n\n\n\n\n\nTable 5.1: Summary of the distribution of the predicted category-specific probabilities for the observations in the estimation sample.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf the distribution of predictor variables in our estimation sample is unbalanced, the data may not provide a good representation of the population of interest. In that case, obtaining predictions for the estimation sample is still of interest, though less informative. Predictions may instead be generated for a hypothetical set of observations, designed to provide a miniature version of the population of interest.\n\n\nIf the distribution of predictions for the observed data shows variation in the outcome probabilities, the next step is to obtain predictions at specific points in the predictor space. This allows us to understand the nature of this variation, i.e. which predictors show particularly strong association with the outcome.\n\n\n5.2.3 Unit-level predictions\nA prediction is made at the unit level if it is generated for a specific predictor profile, i.e. a combination of predictor values. While these predictor values may represent sample averages, the important point is that no averaging takes place for the predicted probabilities. This contrasts unit-level predictions from average predictions.\nThe predictor profiles we use may be designed with different priorities in mind. We will outline three strategies for creating custom profiles: predictions (i) at specified values, (ii) for the “average unit”, and (iii) for ideal types.\n\n5.2.3.1 Predictions at specified values\nIt is often informative to obtain predictions at substantively interesting values. We will follow Long and Freese (2014) and refer to a specified condition as a profile. A profile is a combination of predictor values and it describes a (hypothetical) unit – say, a female speaker born in 1975.\nWith only two predictors, we could decide to construct profiles that sound out the predictor space, varying Gender across its observed levels and Date of birth from 1940 to 2000, in 20-year increments. These predictions are shown in Table 5.2, which shows that predicted probabilities vary considerably by Date of birth.\n\n\n\n\n\n\n\n\n\n\nFemale\n\n\n\n\nMale\n\n\n\n\n\nDate of birth\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n1940\n\n\n.05\n\n\n.02\n\n\n.11\n\n\n.07\n\n\n.74\n\n\n.09\n\n\n.04\n\n\n.16\n\n\n.09\n\n\n.62\n\n\n\n\n1960\n\n\n.13\n\n\n.05\n\n\n.20\n\n\n.10\n\n\n.53\n\n\n.21\n\n\n.07\n\n\n.24\n\n\n.10\n\n\n.38\n\n\n\n\n1980\n\n\n.27\n\n\n.09\n\n\n.25\n\n\n.09\n\n\n.30\n\n\n.40\n\n\n.10\n\n\n.23\n\n\n.07\n\n\n.19\n\n\n\n\n2000\n\n\n.49\n\n\n.10\n\n\n.21\n\n\n.06\n\n\n.14\n\n\n.64\n\n\n.09\n\n\n.16\n\n\n.04\n\n\n.09\n\n\n\n\n\n\nTable 5.2: Predicted response probabilities for different combinations of Gender and Date of birth.\n\n\n\nWe can also visualize these predictions. Category-specific probabilities can be visualized with a grouped bar chart, which appears as Figure 5.5.\n\n\n\n\n\n\n\n\nFigure 5.5: Grouped bar chart showing the predicted category-specific probabilities for different combinations of Gender and Date of birth.\n\n\n\n\n\nAlternatively, we can show them using a line plot. As illustrated in Figure 5.6, we draw predictions for male and female speakers into different panels, or into the same panel.\n\n\n\n\n\n\n\n\nFigure 5.6: Line plot showing the predicted category-specific probabilities for different combinations of Gender and Date of birth.\n\n\n\n\n\nCumulative probabilities can be shown using a stacked bar chart or a line plot.\n\n\n\n\n\n\n\n\nFigure 5.7: Stacked bar chart showing the predicted cumulative probabilities for different combinations of Gender and Date of birth.\n\n\n\n\n\n\n\n5.2.3.2 Predictions for the “average unit”\nWe may also wish to form predictions for what may be considered the “average unit” in the data, i.e. a central and/or representative point in the predictor space. The values at which predictors are being held are sometimes referred to as the base values or the specified values of the predictors. For quantitative variables, we may choose the mean (or median), for categorical variables the mode, i.e. the level with the highest frequency in the sample. In the current data, this would be a female speaker born in 1973.34. The predictions for this hypothetical “average” individual are shown in Table 5.3:\n\n\n\n\n\n\n\nResponse\n\n\nPr(y)\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n.30\n\n\n[.22, .38]\n\n\n\n\n2\n\n\n.09\n\n\n[.06, .12]\n\n\n\n\n3\n\n\n.25\n\n\n[.18, .32]\n\n\n\n\n4\n\n\n.09\n\n\n[.06, .12]\n\n\n\n\n5\n\n\n.28\n\n\n[.20, .36]\n\n\n\n\n\n\nNote:   Adjusted to: Date of birth = −0.07 (mean), Gender = female (mode).\n\n\n\n\n\n\nTable 5.3: Predicted response probabilities for the “average” speaker in the sample.\n\n\n\nWe may not be happy with setting categorical variables to their modal values, as we thereby essentially ignore all cases in the less well-represented group(s) (see Long and Freese 2014, 245). If we want to instead hold them at the average value they assume in the sample, we need to average over predictions made for male and female speakers. This is described in the section on aggregated predictions.2\nIt should be noted that predicted probabilities for the “average unit” in the sample will be different from units on average, i.e. the average over the predicted probabilities for all units in the sample.\n\n\n5.2.3.3 Predictions for ideal types\nIf the predictor values at which predictions are calculated are chosen strategically, we may refer to them as ideal types. Long and Freese (2014, 270) note that ideal types are “particularly illustrative for interpretation when independent variables are substantially correlated”.3 This is because the predictor values may be chosen to represent realistic combinations. If a predictions are formed for a particular subgroup in the data, an option is to use local means as base values for peripheral variables (see Long and Freese 2014, 270–80 for discussion).4\nFor illustration, we will define two ideal types:\n\nA young female speaker (born in 2000)\nAn old male speaker (born in 1940)\n\nPredicted category probabilities for these ideal types appear in Table 5.4.\n\n\n\n\n\n\n\n\n\n\nYoung female speaker(Gender = female, Date of birth = 2000)\n\n\n\n\nOld male speaker(Gender = male, Date of birth = 1940)\n\n\n\n\n\nResponse\n\n\nPr(y)\n\n\n95% CI\n\n\nPr(y)\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n.09\n\n\n[.01, .17]\n\n\n.49\n\n\n[.37, .61]\n\n\n\n\n2\n\n\n.04\n\n\n[.01, .07]\n\n\n.10\n\n\n[.06, .13]\n\n\n\n\n3\n\n\n.16\n\n\n[.07, .26]\n\n\n.21\n\n\n[.14, .27]\n\n\n\n\n4\n\n\n.09\n\n\n[.05, .13]\n\n\n.06\n\n\n[.03, .08]\n\n\n\n\n5\n\n\n.62\n\n\n[.40, .83]\n\n\n.14\n\n\n[.08, .21]\n\n\n\n\n\n\nTable 5.4: Predicted response probabilities for two specific profiles (ideal types).\n\n\n\n\n\n\n5.2.4 Average predictions\nInstead of forming predictions for specific conditions, we often want to aggregate over predicted probabilities obtained at specified values. This aggregation is carried out on the probability scale. In this section, we will discuss two types of averages: An overall average that represents the estimation sample and/or the target population, and averages for subgroups in the data. Average predictions always aggregate over unit-level predictions, so a crucial question is how the underlying unit-level predictions are computed, i.e. which predictor profiles are considered.\n\n5.2.4.1 Unit-level predictions: as-observed vs. specified values\nThere are two general strategies for designing the predictor profiles for which unit-level predictions are calculated and then averaged. For each predictor, we may either choose to specify values manually (specified values) or we may instead rely on its distribution in the estimation sample (as-observed values).\nFor predictors whose distribution in the sample closely corresponds to the distribution in the target population, it makes sense to consider using the as-observed approach, as this adds realism to predictions and their averages. Custom values can instead be specified for predictors whose distribution is unbalanced in the data set.\nConsider, as an example, the distribution of the variables Date of birth and Gender in the current data set. The histograms in Figure 5.8 show that while subjects in the data are roughly balanced on Gender (53% female, 47% male), the variable Date of birth is distributed very unevenly: Younger speakers are overrepresented, with 70% of the individuals born 1980 or later. Since the sample is clearly not representative of the population, we should hesitate to treat Date of birth as-observed when calculating unit-level predictions that will feed into averages.\n\n\n\n\n\n\n\n\nFigure 5.8: Distribution of the speakers in our sample by Date of birth and Gender.\n\n\n\n\n\nIn such cases, we may instead use specified values to control the representation of subgroups. While it is often the case that we want to assigning equal weights to all levels, there may be substantive reasons for weighting them differently. In the case of Gender in the current data, we would opt for equal weights.\nFor continuous predictors, there are a number of options, which may also be roughly divided into simple and weighted representations. In both cases, the aim is to find a handful of values that offer the representation we seek. If we are interested in assigning the same weight to values across the observed range, we may opt for equally-spaced locations. This approach makes sense for the variable Date of birth in the present setting, and we could use the set [1940, 1950, …, 2000] for adequate coverage of the observed age groups. Note that the specified values will usually not span the actual range of the numeric variable, since this span may be distorted by even a single outlier. Instead, a restricted range, say from the 5th to the 95th percentile makes sense (see Long and Freese 2014, p.).\nFor other numeric variables, it makes more sense to weight values in proportion to their representation in the data or the target population. A viable strategy is to use decile midpoints, which are illustrated in Figure 5.9. These locations are found by first dividing the target distribution into deciles, i.e. 10 bins with (nearly) the same number of cases. These deciles are delimited with grey vertical lines. The midpoints (i.e. median) of these bins are the decile midpoints - they appear as black dots below the histograms.\n\n\n\n\n\n\n\n\nFigure 5.9: Decile midpoints for a symmetric and a skewed distribution.\n\n\n\n\n\nLet us apply the two strategies to our model. Table 5.5 shows that the choice of adjustment strategy does not matter much for Gender, but for Date of birth we note an appreciable effect on the category-specific probabilities.\n\n\n\n\n\n\n\n\n\n\nGender: as-observedDate of birth: as-observed\n\n\n\n\nGender: specified valuesDate of birth: as-observed\n\n\n\n\nGender: specified valuesDate of birth: specified values\n\n\n\n\n\nResponse\n\n\nPr(y)\n\n\n95% CI\n\n\nPr(y)\n\n\n95% CI\n\n\nPr(y)\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n.38\n\n\n[.32, .44]\n\n\n.37\n\n\n[.30, .44]\n\n\n.29\n\n\n[.23, .34]\n\n\n\n\n2\n\n\n.09\n\n\n[.06, .11]\n\n\n.09\n\n\n[.06, .13]\n\n\n.07\n\n\n[.04, .09]\n\n\n\n\n3\n\n\n.21\n\n\n[.16, .27]\n\n\n.24\n\n\n[.17, .30]\n\n\n.20\n\n\n[.14, .25]\n\n\n\n\n4\n\n\n.07\n\n\n[.05, .09]\n\n\n.08\n\n\n[.05, .10]\n\n\n.08\n\n\n[.05, .10]\n\n\n\n\n5\n\n\n.25\n\n\n[.19, .31]\n\n\n.23\n\n\n[.17, .29]\n\n\n.37\n\n\n[.29, .46]\n\n\n\n\n\n\nNote:   Adjusted to: Gender = as-balanced (50% male, 50% female); Date of birth = as-balanced (1940, 1960, 1980, 2000).\n\n\n\n\n\n\nTable 5.5: Predicted response probabilities using different adjustment strategies: as-observed vs. specified values.\n\n\n\nWe have seen that there are a number of different adjustment strategies, i.e. ways of handling those predictors over which we wish to average, or – more generally – which we wish to adjust for. Here is an overview:\nas-observed\n\ncomplete dataset\nsubset\n\nspecified values\n\nsingle value\n\nquantitative variables: mean/median (based on sample or population)\ncategorical variables: mode (based on sample or population)\n\nmultiple values\n\nquantitative variables\n\nbalanced: at equal steps\nweighted\n\nat equal steps, then weighted (based on sample or population)\nat quantiles (based on sample)\n\n\ncategorical variables\n\nbalanced: simple average\nweighted: weighted average (based on sample or population)\n\n\n\nWhen averaging over predictions on a non-linear scale, adjustment by using a single specified value for a peripheral variable (e.g. the sample mean or mode) it is generally not recommended. This is because the predicted probability depends on the values of all predictors in the model. Holding them at their means (or modes) may not be representative of the characteristics of the observations in the sample. Many authors therefore prefer the as-observed approach (Long 1997, 74; Cameron 2005, 467; Greene and Hensher 2010, 143; Hanmer and Kalkan 2013, 3).\nIt should be noted that the recommendation of these scholars is to give preference to the as-observed approach over the specified-single-values approach. Our listing above shows that there is another option, the use of specified-multiple-values. This strategy can copy the advantages of the as-observed approach by taking into consideration the entire data space. A further advantage of this strategy vis-à-vis observed values is that it does not force us to rely on sample characteristics when forming adjusted predictions. Instead, the specified values may be informed by external (or population) data. In settings where the sample does not provide a good representation of the distribution of certain variables in the target population, the specified-multiple-values approach is our preferred adjustment strategy.\n\n\n5.2.4.2 Average predictions for a quick overview of predictor importance\nAverage predictions can also help us compare predictors in terms of their strength of association with the outcome. This may be done by forming two average predictions for each variable, which should give a good representation of their “effective range”. For binary inputs, it is natural to compute average predictions at the two levels. For continuous variables, we must find a span that may also be considered as reflecting the typical difference we observe between two units. This span may be determined empirically or on subject-matter grounds. Common choices for data-based spans are based on the standard deviation. Arguments can be made for a 1-SD or a 2-SD span (see Gelman 2007).\nIn our data, we use a hybrid approach. We will opt for a 30-year step, which may be considered as representing a generational shift in usage patterns. In addition, however, we are interested in how usage patterns differ across the entire range of birth years that are represented in our data. Accordingly, we will form average predictions at three locations: 1940, 1970, and 2000.\nTable 5.6 arrange these average predictions in a way that allows us to compare the two predictors in terms of their strength of association with the outcome.\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\nLevel\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nDate of birth\n\n\n\n\n1940\n\n\n.07\n\n\n.03\n\n\n.14\n\n\n.08\n\n\n.68\n\n\n\n\n1970\n\n\n.24\n\n\n.08\n\n\n.24\n\n\n.09\n\n\n.34\n\n\n\n\n2000\n\n\n.56\n\n\n.09\n\n\n.18\n\n\n.05\n\n\n.11\n\n\n\n\nGender\n\n\n\n\nFemale\n\n\n.23\n\n\n.07\n\n\n.20\n\n\n.08\n\n\n.43\n\n\n\n\nMale\n\n\n.33\n\n\n.08\n\n\n.21\n\n\n.08\n\n\n.31\n\n\n\n\n\n\nNote:   Adjusted to: Gender = as-balanced; Date of birth = as-balanced (1940, 1950, 1960, 1970, 1980, 1990, 2000).\n\n\n\n\n\n\nTable 5.6: Average predictions for the variables in the model.\n\n\n\n\n\n5.2.4.3 Average predictions for subgroups in the data\nWhen averaging over predictions at the two levels of Gender, there are different options for handling the peripheral variable Date of birth. Two general strategies are (i) the as-observed approach, which uses the values recorded in the estimation sample, and (ii) the use of specified values. When dealing with subgroups of the data, a further question is whether the peripheral variables should be treated identically or differently in the two subgroups. When using specified values, for instance, custom values may be chosen within each subgroup. And for the as-observed approach, the peripheral variables can likewise be adjusted to the values they assume in the relevant subset of the data.\nTable 5.7 compares the results of these different averaging strategies. In the first set of average predictions (as-observed, identical), the row “Female” contains the average predicted response probabilities for a scenario in which all respondents in our estimation sample are female (irrespective of their actual gender). The row “Male”, on the other hand, shows the average estimates supposing that all respondents are male.\nIn the second set of average predictions (as-observed, different), the row “Female” contains the average predicted response probabilities for a group of female speakers with a date-of-birth distribution that is identical to the one in the current sample of female speakers. The same applies to male speakers.\nThe final set of predictions averages over predictions made for specified values of Date of birth. Since these are 10-year increments, equal weight is given to each cohort,\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\nLevel\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nAs-observed, identical\n\n\n\n\nFemale\n\n\n.32 [.24, .40]\n\n\n.08 [.06, .11]\n\n\n.23 [.16, .29]\n\n\n.08 [.05, .10]\n\n\n.29 [.22, .37]\n\n\n\n\nMale\n\n\n.45 [.36, .54]\n\n\n.09 [.06, .12]\n\n\n.20 [.15, .26]\n\n\n.06 [.04, .08]\n\n\n.20 [.13, .26]\n\n\n\n\nAs-observed, different\n\n\n\n\nFemale\n\n\n.31 [.23, .39]\n\n\n.08 [.06, .11]\n\n\n.23 [.16, .29]\n\n\n.08 [.05, .11]\n\n\n.30 [.22, .37]\n\n\n\n\nMale\n\n\n.46 [.37, .55]\n\n\n.09 [.06, .12]\n\n\n.20 [.14, .26]\n\n\n.06 [.04, .08]\n\n\n.19 [.13, .26]\n\n\n\n\nSpecified valuesᵃ, identical\n\n\n\n\nFemale\n\n\n.23 [.17, .29]\n\n\n.07 [.04, .09]\n\n\n.20 [.14, .26]\n\n\n.08 [.05, .11]\n\n\n.43 [.33, .52]\n\n\n\n\nMale\n\n\n.33 [.25, .41]\n\n\n.08 [.05, .10]\n\n\n.21 [.15, .27]\n\n\n.08 [.05, .10]\n\n\n.31 [.21, .42]\n\n\n\n\n\n\nNote:   ᵃAdjusted to: Date of birth = as-balanced (1940, 1950, 1960, 1970, 1980, 1990, 2000).\n\n\n\n\n\n\nTable 5.7: Average predictions for the variables in the model.\n\n\n\nWhether the as-observed approach treats the subgroups identically or differently doesn’t make much of a difference. However, using specified values of Date of birth changes the predictions markedly. This is because the as-observed approach produces average predictions that are biased towards the response behavior of younger participants (who are overrepresented in the sample).\n\n\n5.2.4.4 Using local means for adjustment\nLong and Freese (2014, 273–74, 303–8) discuss a useful strategy for subgroup comparisons. Typically, subgroups are defined by varying the targeted variables and holding all peripheral variables at specified (or observed) values. Critically, however, the peripheral variable assume identical values across the subgroups. If this is unrealistic, peripheral variable may instead be set to their subgroup-specific values, or local means. By using such local means for adjustment, we may observe how robust our conclusions are to assumptions about the levels of other variables.\n\n\n\n\n\n\nNote\n\n\n\nTo form average predictions for subgroups in the data, Stata offers the option “over”, which holds other variable at subgroup-specific means.\n\n\n\n\n\n5.2.5 Comparisons\nComparisons are useful for expressing the strength of association between a predictor and the outcome. As such, a comparison is defined as a function of two (or more) model-based predictions.The term “function” refers to different ways of comparing two numbers: We can calculate a difference, a ratio, or other quantities. Since a model allows us to form different types of predictions, there is a corresponding variety of comparisons that may be drawn.\nTypically, a comparison focuses on a single variable. This means that the two predictions that are contrasted differ on that variable only. When talking about a comparison, it therefore makes sense to distinguish between the targeted (focal) variable and peripheral (adjustment) variables.\nA comparison can be conceptualized in two ways. First, the predictor profiles involved in the comparison may be understood as representing two different units. These hypothetical units are, however, identical with regard to all peripheral variables and only differ on the targeted variable. In that sense, it is an attempt to compare “like with like”. This way of thinking about comparisons is typical when the aims of a study are descriptive.\nAlternatively, the predictor profiles may be considered as representing two versions of one and the same unit. Viewed in this way, a comparison is interpreted as expressing what would happen if we were able to change the value of the targeted variable for the same (hypothetical) unit. This is a thought experiment, and therefore often termed a “counterfactual comparison”. This way of thinking about comparisons is typical of causal explanation.\n\n5.2.5.1 Choice of predictor values\nThe choice of predictor values for the targeted variable needs some thought. For binary predictors, it seems natural to compare the two levels of the variable. For continuous predictors, on the other hand, the choice of locations at which to compare predictions is less straightforward.\nOne strategy is to determine two values manually, which means that predictions are formed for two profiles only: One profile where the numeric variable assumes the higher value (for instance the upper quartile, or 1 SD above the mean), and one where it is held at the lower value (e.g. the lower quartile, or 1 SD below the mean). This way of computing a comparison answers the following question: What is the difference between individuals with a relatively high value on the continuous variable (e.g. 1 SD above the mean) compared to those with a relatively low value (e.g. 1 SD below the mean). This is a descriptive question.\nAnother strategy is to use the observed values of the numeric variable and increase its value for each unit in the sample by a specific amount (e.g. 1, or by 1 SD). This means that for the first set of profiles, the targeted variable assumes its observed distribution in the data. In the second set of profiles, it is shifted upwards by a fixed amount, as though the targeted variable had increased for the units in the estimation sample. This way of computing a comparison answers the following question: If we shift the value of the numeric variable by X units for each individual in the sample, what is the average difference we observe in the outcome probability. This maps onto a causal research question, which envisages some sort of intervention that may be applied to the units in the data.\nFigure 5.10 illustrates these different strategies for the variable Date of birth in our analysis. The shifted histograms at the top represent the as-observed approach, which uses the distribution of the variable in the sample and shifts it upward by a certain amount. An increment of +1 is the default setting in the comparisons() function. The bars below the histogram illustrate different options for the approach relying on specified values. Shown in monospace font at the left margin of the plot are shortcuts that can be used with the {marginaleffects} package. These will be illustrated shortly.\nLet us pause to consider these two strategies (observed vs. specified values). It appears that using the observed values to evaluate a comparison for a numeric predictor makes most sense if we intend to draw causal conclusions about the effect of a hypothetical intervention, which would be applied at the unit level, i.e. to the units whose predictor values are shifted. It should be kept in mind, however, that for some variables it does not make sense to envisage an intervention. Date of birth is an obvious example. If the focus is instead on description, it is likely that the comparison of two hypothetical individuals with different values on the targeted variable is more informative.\n\n\n\n\n\n\n\n\nFigure 5.10: Different types of comparison for numeric variables.\n\n\n\n\n\nNevertheless, an advantage of the observed-values approach is that it gives us an idea of how much the difference varies across the span of the numeric variable. It therefore makes sense to consider an intermediate approach, which allows us to appreciate how much the difference associated with a 20-year gap in Date of birth depends on where in the predictor space (including the target variable itself) it is evaluated. The approach is shown visually in Figure 5.11. We first settle on the span we wish to evaluate, in our case the gap in years. We could rely on sample statistics, but in the present data set this does not seem very sensible due to its imbalanced composition. We instead decide to consider a 30-year gap, which roughly corresponds to one generation. The comparison between birth dates that are one generation (i.e. 30 years) apart is then evaluated at a specified number of locations across the (possibly restricted) range of the numeric variable.\n\n\n\n\n\n\n\n\nFigure 5.11: Comparisons for a numeric variable: An intermediate strategy that combines the strenghts of the as-observed and the as-balanced approach.\n\n\n\n\n\nSimilar to the calculations of predictions, there are different ways of handling peripheral variable: They may be held at their means, treated in an as-balanced manner, or allowed to assume their observed values in the data.\n\n\n5.2.5.2 Comparisons for each observed unit\nWe should bear in mind that by making comparisons for each unit in the estimation sample, we are essentially running a thought experiment – in comparing two versions of the same unit, we are imagining some kind of intervention. This might not make sense for certain variable, with Gender being a case in point. We can nevertheless think of the distribution of differences as reflecting the extent to which male and female speakers differ on the probability scale at different points in the predictor space, in our case across different birth years.\nWe can inspect this distribution with a dot diagram; the scaling of the x-axis is chosen in a way that permits direct comparison with Figure 5.13 below.\n\n\n\n\n\n\n\n\nFigure 5.12: Gender: Distribution of differences between predicted response probabilities for the observations in the estimation sample.\n\n\n\n\n\nFor a numeric variable, we can also compare two version of the same unit. For instance, we can add 25 years to the speaker’s actual date of birth. We should bear in mind that it also does not make sense to think of Date of birth as changing value within an individual. Plus, for some speakers this shifts the year of birth into the future, and thereby extrapolated into an unobserved region of the data space. Nevertheless, the distribution of differences gives us some idea of how much the difference associated with a 1-unit (i.e. 25-year) shift in year of birth varies in the data space. Further below, we will use a more sensible set of specified values to reveal this feature of our data and model.\n\n\n\n\n\n\n\n\nFigure 5.13: Date of birth: Distribution of differences between predicted response probabilities for the observations in the estimation sample.\n\n\n\n\n\n\n\n5.2.5.3 Comparisons for numeric predictors\nFor numeric predictors, we illustrated the use of different locations and values of the numeric variable itself see 5.10. Table 5.8 lists the differences in response probabilities for each of the approaches illustrated in Figure 5.10, broken down by Gender.\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\nLevel\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nobserved → observed + 1 (default)\n\n\n\n\nFemale\n\n\n+.28 [+.17, +.40]\n\n\n+.00 [−.02, +.02]\n\n\n−.07 [−.13, −.01]\n\n\n−.04 [−.06, −.02]\n\n\n−.17 [−.23, −.11]\n\n\n\n\nMale\n\n\n+.28 [+.18, +.38]\n\n\n−.03 [−.05, −.00]\n\n\n−.10 [−.16, −.05]\n\n\n−.04 [−.06, −.02]\n\n\n−.11 [−.16, −.07]\n\n\n\n\nobserved → observed + 0.5\n\n\n\n\nFemale\n\n\n+.14 [+.08, +.19]\n\n\n+.01 [−.00, +.02]\n\n\n−.02 [−.05, +.01]\n\n\n−.02 [−.03, −.01]\n\n\n−.10 [−.14, −.06]\n\n\n\n\nMale\n\n\n+.15 [+.09, +.20]\n\n\n−.01 [−.02, +.01]\n\n\n−.05 [−.08, −.02]\n\n\n−.02 [−.03, −.01]\n\n\n−.07 [−.10, −.04]\n\n\n\n\n−1 → +1\n\n\n\n\nFemale\n\n\n+.41 [+.27, +.55]\n\n\n+.06 [+.03, +.09]\n\n\n+.06 [−.02, +.13]\n\n\n−.03 [−.06, +.00]\n\n\n−.50 [−.68, −.32]\n\n\n\n\nMale\n\n\n+.50 [+.34, +.65]\n\n\n+.03 [−.00, +.06]\n\n\n−.05 [−.13, +.02]\n\n\n−.06 [−.09, −.03]\n\n\n−.41 [−.61, −.22]\n\n\n\n\nmean − SD → mean + SD\n\n\n\n\nFemale\n\n\n+.29 [+.18, +.41]\n\n\n+.03 [+.01, +.06]\n\n\n−.01 [−.06, +.04]\n\n\n−.04 [−.06, −.01]\n\n\n−.28 [−.39, −.17]\n\n\n\n\nMale\n\n\n+.34 [+.21, +.46]\n\n\n+.00 [−.02, +.03]\n\n\n−.08 [−.14, −.03]\n\n\n−.05 [−.08, −.02]\n\n\n−.21 [−.31, −.11]\n\n\n\n\nmean − SD/2 → mean + SD/2\n\n\n\n\nFemale\n\n\n+.15 [+.09, +.21]\n\n\n+.02 [+.01, +.03]\n\n\n−.00 [−.03, +.02]\n\n\n−.02 [−.04, −.01]\n\n\n−.14 [−.20, −.08]\n\n\n\n\nMale\n\n\n+.17 [+.11, +.24]\n\n\n+.00 [−.01, +.02]\n\n\n−.05 [−.08, −.01]\n\n\n−.03 [−.04, −.01]\n\n\n−.10 [−.15, −.05]\n\n\n\n\nlower quartile → upper quartile\n\n\n\n\nFemale\n\n\n+.18 [+.11, +.26]\n\n\n+.02 [+.00, +.04]\n\n\n−.01 [−.05, +.02]\n\n\n−.03 [−.04, −.01]\n\n\n−.16 [−.23, −.10]\n\n\n\n\nMale\n\n\n+.21 [+.13, +.29]\n\n\n+.00 [−.02, +.02]\n\n\n−.06 [−.10, −.02]\n\n\n−.03 [−.05, −.01]\n\n\n−.12 [−.18, −.06]\n\n\n\n\nminimum → maximum\n\n\n\n\nFemale\n\n\n+.44 [+.30, +.58]\n\n\n+.07 [+.04, +.11]\n\n\n+.10 [+.02, +.17]\n\n\n−.01 [−.05, +.02]\n\n\n−.60 [−.79, −.40]\n\n\n\n\nMale\n\n\n+.54 [+.40, +.69]\n\n\n+.05 [+.01, +.08]\n\n\n−.01 [−.10, +.08]\n\n\n−.05 [−.09, −.02]\n\n\n−.53 [−.75, −.31]\n\n\n\n\n\n\nTable 5.8: Different options for comparisons for numeric predictors.\n\n\n\n\n\n5.2.5.4 Comparisons at specified values\nComparisons can also be made at specified values. For speakers born in 1975:\n\nInterpretation: “For speakers born in 1975, the average difference between male and female speakers is…”\n\n\n\n\n\n\n\n\n\n\n\nMale − Female\n\n\n\n\n\nResponse\n\n\nΔ Pr(y)\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n+.12\n\n\n[+.01, +.23]\n\n\n\n\n2\n\n\n+.02\n\n\n[+.00, +.03]\n\n\n\n\n3\n\n\n+.00\n\n\n[−.02, +.02]\n\n\n\n\n4\n\n\n−.02\n\n\n[−.03, +.00]\n\n\n\n\n5\n\n\n−.12\n\n\n[−.23, −.01]\n\n\n\n\n\n\nNote:   Adjusted to: Date of birth = 0 (1975).\n\n\n\n\n\n\nTable 5.9: Comparison between male and female speakers born in 1975.\n\n\n\n\n\n5.2.5.5 Comparisons for the “average unit”\nWe can set peripheral variables equal to their means (numeric predictors) or modes (categorical predictors).\n\nInterpretation: For speakers with the average date of birth in the sample (2120.07), the difference between male and female speakers is…\n\n\n\n\n\n\n\n\n\n\n\nMale − Female\n\n\n\n\n\nResponse\n\n\nΔ Pr(y)\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n+.13\n\n\n[+.01, +.26]\n\n\n\n\n2\n\n\n+.01\n\n\n[−.00, +.02]\n\n\n\n\n3\n\n\n−.02\n\n\n[−.05, +.01]\n\n\n\n\n4\n\n\n−.02\n\n\n[−.04, −.00]\n\n\n\n\n5\n\n\n−.10\n\n\n[−.19, −.01]\n\n\n\n\n\n\nNote:   Adjusted to: Date of birth = −0.07 (mean).\n\n\n\n\n\n\nTable 5.10: Comparison between the male and female “average” speaker in the sample.\n\n\n\n\n\n5.2.5.6 Comparisons for ideal types\nThe calculation of comparison for (or between) ideal types works accordingly.\n\n\n\n5.2.6 Average comparisons\nSince a comparison is based on two specific profiles, it is a conditional quantity. This means that its magnitude depends on where in the predictor space it is evaluated, i.e. on the setting(s) of the peripheral variable(s). Since the units in the estimation sample have different values on the predictor variables, there is a distribution of differences (or ratios, etc.) in the sample. It often makes sense to inspect this distribution, and to evaluate a comparison at different points in the predictor space, to consider both its average magnitude as well as its variability across conditions (see Maddala 1983, 24).\nBefore we go further, we should note that an average comparison may be calculated in two ways, depending on the point at which we average. The difference is illustrated in Figure 5.14. The starting point are two sets of predictions, one for each value of the targeted variable. These two sets are shown using grey tiles in Figure 5.14.\n\nCompare and average: We start with a unit-level comparison of the two sets of predictions, which would result in a set of unit-level comparisons. This set is of the same length as each of the sets of predictions. We then take the average over these unit-level comparisons. This means that we first compare, and then average.\nAverage and compare: Alternatively, we could start by averaging over each set of predictions to obtain two average predictions, which we then compare. This means that we first average and then compare.\n\n\n\n\n\n\n\n\n\nFigure 5.14: Forming average comparisons: Compare and average vs. average and compare.\n\n\n\n\n\nImportantly, these two procedures yield different results in non-linear models. In general, the first approach (compare and average) is preferable. It also allows us to see how the magnitude of, say, the difference of interest varies across the conditions considered.\nThe compare-and-average approach has been variously referred to as the “average marginal effect” (Long 1997; Long and Freese 2014), the “average predictive comparison” (Gelman and Hill 2007), and the “average discrete change” (Long and Mustillo 2017).\n\n5.2.6.1 Average comparisons: Observed vs. specified values\nThe points in the predictor space where the comparison is made can be determined in two ways. The values for the peripheral variables may be specified manually (specified values) or we may use the values they assume in the estimation sample (observed values). This means that we can average them over the sample or compute them at fixed values.\nWe can show these average comparisons graphically:\n\n\n\n\n\n\n\n\nFigure 5.15\n\n\n\n\n\nRecall that the distribution of Date of birth in our sample is very unbalanced. It therefore makes sense to specify its values manually, to obtain estimates that are more representative of the target population. Since the target population includes all speakers of Maltese English irrespective of their year of birth, we wish to treat them equally when reporting a population-level average. We will evaluate the comparison at seven locations that are 10 years apart (1940, 1950, …, 2000). Table 5.11 juxtaposes average comparisons for these different treatment of the peripheral variable Date of birth.\n\n\n\n\n\n\n\n\n\n\nDate of birth = as-observed\n\n\n\n\nDate of birth = as-balancedᵃ\n\n\n\n\n\nResponse\n\n\nDiff\n\n\n95% CI\n\n\nDiff\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n+.13\n\n\n[+.01, +.24]\n\n\n+.10\n\n\n[+.01, +.19]\n\n\n\n\n2\n\n\n+.00\n\n\n[−.00, +.01]\n\n\n+.01\n\n\n[−.00, +.02]\n\n\n\n\n3\n\n\n−.02\n\n\n[−.04, +.00]\n\n\n+.01\n\n\n[−.00, +.02]\n\n\n\n\n4\n\n\n−.02\n\n\n[−.03, −.00]\n\n\n−.01\n\n\n[−.01, +.00]\n\n\n\n\n5\n\n\n−.10\n\n\n[−.18, −.01]\n\n\n−.11\n\n\n[−.21, −.01]\n\n\n\n\n\n\nNote:   ᵃAdjusted to: Date of birth = (1940, 1950, 1960, 1970, 1980, 1990, 2000).\n\n\n\n\n\n\nTable 5.11: Average comparisons for Gender using different adjustment methods for Date of birth.\n\n\n\nInterpretation: “The average difference in predicted probability between male and female respondents, evaluated at (and averaged over) the observed values of age, is\n\n\n5.2.6.2 Average comparisons for a quick overview of predictor importance\nTo be able to directly compare the two variables Gender and Date of birth, we must first decide what a “fair comparison” may be. It seems that a 30-year span, which represents a generational shift, makes sense. Further, we must decide whether to evaluate it at two fixed points (e.g. 1990 vs. 1960) or whether we should embrace the observed range of the date-of-birth scale to also take into consideration the fact that the difference associated with a 30-year gap will depend on where in the predictor space it is evaluated (including which birth years are considered). We will use our intermediate approach (see Figure 5.11).\nTable 5.12 shows average comparisons for the two predictors. We note that the predicted response probabilities vary more with the date of birth.\n\n\n\n\n\n\n\n\n\n\nDate of birthᵃ(Difference: 30 years)\n\n\n\n\nGenderᵇ(Female − Male)\n\n\n\n\n\nResponse\n\n\nDiff\n\n\n95% CI\n\n\nDiff\n\n\n95% CI\n\n\n\n\n\n\n1\n\n\n+.25\n\n\n[+.18, +.32]\n\n\n+.10\n\n\n[+.01, +.19]\n\n\n\n\n2\n\n\n+.04\n\n\n[+.01, +.06]\n\n\n+.01\n\n\n[−.00, +.02]\n\n\n\n\n3\n\n\n+.03\n\n\n[−.02, +.07]\n\n\n+.01\n\n\n[−.00, +.02]\n\n\n\n\n4\n\n\n−.02\n\n\n[−.04, −.00]\n\n\n−.01\n\n\n[−.01, +.00]\n\n\n\n\n5\n\n\n−.29\n\n\n[−.41, −.18]\n\n\n−.11\n\n\n[−.21, −.01]\n\n\n\n\n\n\na Adjusted to: Gender = as-balanced; Date of birth = as-balanced (1970 − 1940, 1980 − 1950, 1990 − 1960, 2000 − 1970).\n\n\n\n\nb Adjusted to: Gender = as-balanced; Date of birth = as-balanced (1940, 1950, 1960, 1970, 1980, 1990, 2000).\n\n\n\n\n\n\nTable 5.12: Average comparisons to compare the two predictors in terms of their strength of association with the outcome.\n\n\n\nLong and Freese (2014, 344–51) show these differences graphically, which makes sense if there are a large number of predictors in the model. Figure 5.16 mimics the kind of display they use.\n\n\n\n\n\n\n\n\nFigure 5.16: Visualization of average comparisons for side-by-side comparisons of predictors.\n\n\n\n\n\n\n\n5.2.6.3 Average comparisons for subgroups in the data\nSo far we have used average comparisons to obtain a single summary for a given predictor, to quantify its overall strength of association with the outcome. As we saw in Figure 5.2 and Figure 5.3, the difference between predicted probabilities are bound to vary across the predictor space. This is because differences between predictions near the endpoints of the probability scale (i.e. near 0 or 1) will be smaller. Since this local compression is scale-induced, it occurs even if the model does not include any interaction terms.\nIt is therefore often informative to calculate average comparisons for subgroups in the data. These subgroups may be formed based on the peripheral variables, but also based on the predictor of interest, the targeted variable. This allows us to note how the predictive capacity of a variable varies across regions of the data space.\nLet us first consider the change in outcome probabilities associated with a 30-year difference in year of birth. We will evaluate this 30-year step at three different locations of the predictor variable:\n\n1970 vs. 1940\n1985 vs. 1955\n2000 vs. 1970\n\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\nComparison\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n1970 vs. 1940\n\n\n+.17 [+.14, +.20]\n\n\n+.05 [+.03, +.07]\n\n\n+.10 [+.03, +.18]\n\n\n+.01 [−.02, +.05]\n\n\n−.33 [−.46, −.21]\n\n\n\n\n1985 vs. 1955\n\n\n+.28 [+.19, +.37]\n\n\n+.03 [+.01, +.06]\n\n\n+.00 [−.04, +.04]\n\n\n−.03 [−.05, −.01]\n\n\n−.28 [−.41, −.16]\n\n\n\n\n2000 vs. 1970\n\n\n+.32 [+.20, +.44]\n\n\n+.01 [−.00, +.03]\n\n\n−.06 [−.10, −.02]\n\n\n−.05 [−.07, −.02]\n\n\n−.23 [−.32, −.14]\n\n\n\n\n\n\nTable 5.13: Average comparisons for a 30-year gap in Date of birth at different locations of the Date-of-birth scale.\n\n\n\nWe can also evaluate it separately for male and female speakers. For a balanced representation of the focal variable Date of birth, we use the intermediate approach illustrated in Figure 5.11.\n\n\n\n\n\n\n\n\n\n\nResponse category\n\n\n\n\n\nGender\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nf\n\n\n+.22 [+.15, +.29]\n\n\n+.04 [+.02, +.06]\n\n\n+.06 [+.01, +.11]\n\n\n−.01 [−.03, +.01]\n\n\n−.31 [−.43, −.20]\n\n\n\n\nm\n\n\n+.28 [+.20, +.36]\n\n\n+.03 [+.01, +.05]\n\n\n−.00 [−.06, +.05]\n\n\n−.03 [−.05, −.01]\n\n\n−.27 [−.40, −.15]\n\n\n\n\n\n\nTable 5.14: Average comparisons for a 30-year gap in Date of birth at different levels of the variable Gender.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methods of interpretation</span>"
    ]
  },
  {
    "objectID": "05_meth_int.html#sec-meth-int-latent",
    "href": "05_meth_int.html#sec-meth-int-latent",
    "title": "5  Methods of interpretation",
    "section": "5.3 Predictions on the latent-variable scale",
    "text": "5.3 Predictions on the latent-variable scale\nRecall that an interpretation in terms of a latent variable is only valid for parallel cumulative models. If the assumptions of this model type hold, the latent variable allows for simple strategies for model interpretation.\nImportantly, model-based predictions on the latent-variable scale only make sense in the context of the thresholds, which means that it will usually be necessary to visualize them. This is because we have to know the spacing of the thresholds to appreciate the meaning of differences on the latent scale. If a probit link is used, the coefficients on the latent-variable scale may be interpreted as standardized effect size measures, the benchmark being the residual variation. In multilevel models, there will be residuals at different levels, which means that the level-1 error variation may not be the appropriate yardstick for all predictors in the model.\nWhen we interpret predictions on the latent scale, we no longer have to confront the fundamental problem of linear vs. non-linear response scales – all predictions are linear on the latent scale. This means that there are no scale-induced interactions. For a quantitative predictor, the as-observed and the specified-single-value approach therefore produce the same results. This means that the task of model interpretation is greatly simplified.\nIt should be noted, however, that the issue becomes relevant when the model includes interaction terms, and when a numeric variable is modeled using polynomials or flexible smooths.\nWe fit a model that includes an interaction:\n\nm &lt;- clm(\n  rating_int ~ dob_c * gender, \n  data = d, \n  threshold = \"symmetric\")\n\nThis is the regression table:\n\n\nformula: rating_int ~ dob_c * gender\ndata:    d\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit symmetric 193  -260.60 533.20 5(1)  5.96e-13 8.1e+01\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \ndob_c          -1.3333     0.3443  -3.872 0.000108 ***\ngenderm        -0.6857     0.3151  -2.176 0.029559 *  \ndob_c:genderm   0.2916     0.4681   0.623 0.533339    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n          Estimate Std. Error z value\ncentral.1 -0.86376    0.22847  -3.781\ncentral.2  0.15823    0.21999   0.719\nspacing.1  0.39761    0.06923   5.744\n\n\n\n5.3.1 Predictions for each observed unit\nWe can obtain predictions for the estimation sample to obtain an overview of the predictive capacity of the variables in our model. Figure 5.17 shows the distribution of these predictions for the observations in our sample.\n\n\n\n\n\n\n\n\n\nFigure 5.17: Predicted scores on the latent-variable scale for the observations in the estimation sample.\n\n\n\n\n\n\n5.3.2 Unit-level predictions\nWe can calculate predictions on the latent scale for a male and a female speaker born in 2000.\n\n\n\n\n\n\n\n\n\n\n95% CI\n\n\n\n\n\nGender\n\n\nPrediction\n\n\nLower limit\n\n\nUpper limit\n\n\n\n\n\n\nf\n\n\n−0.98\n\n\n−1.56\n\n\n−0.4\n\n\n\n\nm\n\n\n−1.37\n\n\n−1.95\n\n\n−0.8\n\n\n\n\n\n\nNote:   Adjusted to: Date of birth = +1 (2000).\n\n\n\n\n\n\nTable 5.15: Predictions for a male and a female speaker born in 2000.\n\n\n\nIt is hard to interpret these predictions without reference to the thresholds. It is therefore best to consider them visually:\n\n\n\n\n\n\n\n\nFigure 5.18: Predictions for a male and a female speaker born in 2000.\n\n\n\n\n\nWe can also look at how the predictions for male and female speakers vary across the entire date-of-birth range – that is, for each combination of Gender and Date of birth. Figure 5.19 shows two trends lines, one for male and one for female speakers.\n\n\n\n\n\n\n\n\nFigure 5.19: Predictions for male and female speakers across the span of the variable Date of birth.\n\n\n\n\n\nSince the model only includes two predictors, Figure 5.19 shows the complete predictor space: Every condition that is represented by the model appears in the figure. This means that no predictor was backgrounded, or averaged over.\n\n\n5.3.3 Average predictions\nWe can also average predictions across the values of a peripheral variable. This allows us to compare predictors side by side, to assess their relative strength of association with the outcome.\nWe calculate average predictions for Date of birth by forming simple averages over the two levels of Gender. This means that the two trend lines in Figure 5.19 merge into one. Since we give the same weight to male and female speakers, they “meet” half way. When forming average predictions for Gender, on the other hand, we hold Date of birth at 1975, which is close to the center of the distribution. These two sets of average predictions can then be displayed side by side, as is done in Figure 5.20. This side-by-side arrangement allows us to compare them in terms of their average strength of association with the outcome. It is clear from Figure 5.19 that Date of birth has a much stronger association with the lexical preference for package over parcel.\n\n\n\n\n\n\n\n\nFigure 5.20: Average predictions for the two predictors in the model.\n\n\n\n\n\n\n\n5.3.4 Unit-level comparisons\nRather than compare unit-level predictions side by side as in Figure 5.18, we can express the comparison numerically, by calculating the difference between the two. In Figure 5.18, we saw predictions for male and female speakers born in 2000. Female speakers have a higher latent mean: −0.98, 95% CI [−1.56, −0.4] compared to −1.37, [−1.95, −0.8] for male speakers. The difference between female and male speakers is 0.39, [−0.4, 1.19].\nInstead of comparing male and female speakers with the same birth year, we may also ask how the difference between the subgroups varies across levels of Date of birth. Since the model includes an interaction between these predictors, the difference is not constant across the scale. As Figure 5.19 showed, male and female speakers have different slopes. To see how the estimated difference between male and female speakers varies with birth year, we can obtain unit-level comparisons at several locations – i.e. for several birth years. The results is visualized in Figure 5.21: For 1940, the difference is about +1, for the birth year 2000 it is about +0.5.\n\n\n\n\n\n\n\n\nFigure 5.21: Comparison between male and female speakers across the span of the variable Date of birth.\n\n\n\n\n\nWe can also form comparisons for the variable Date of birth: The difference between female speakers born in 1950 (1.69, 95% CI [0.73, 2.64]) vs. 2000 (−0.98, 95% CI [−1.56, −0.4]) is 2.67, 95% CI [1.32, 4.02].\n\n\n5.3.5 Average comparisons\nThe comparisons so far involved no averaging – all predictors were held at specific values. In models that include a larger number of predictor variables, we often need to average over one or several peripheral variables. To generate average comparisons, we first create two or more average predictions which we then compare.\nFigure 5.20 showed average predictions for both predictors. We can compare speakers born in 2000 and 1940, averaging over Gender (as-balanced, i.e. simple average).\nLikewise, we can compare male and female speakers, averaging over Date of birth.\nand then use pairs() to obtain relevant contrasts.\n\n\n dob_c emmean    SE  df asymp.LCL asymp.UCL\n  -1.4   1.67 0.439 Inf     0.812     2.533\n   1.0  -1.18 0.214 Inf    -1.596    -0.759\n\nResults are averaged over the levels of: gender \nConfidence level used: 0.95 \n\n\n\n\n contrast             estimate    SE  df z.ratio p.value\n (dob_c-1.4) - dob_c1     2.85 0.582 Inf   4.896  &lt;.0001\n\nResults are averaged over the levels of: gender \n\n\n\n\n\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. Forthcoming. “How to Interpret Statistical Models Using marginaleffects in R and Python.” Journal of Statistical Software, Forthcoming. https://marginaleffects.com.\n\n\nCameron, Pravin K., A. Colin & Trivedi. 2005. Microeconometrics: Methods and Applications. New York: Cambridge University Press.\n\n\nGelman, Andrew. 2007. “Scaling Regression Inputs by Dividing by Two Standard Deviations.” Statistics in Medicine 27 (15): 2865–73. https://doi.org/10.1002/sim.3107.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Linear Models. Cambridge: Cambridge University Press.\n\n\nGreene, William H., and David A. Hensher. 2010. Modeling Ordered Choices: A Primer. Cambridge: Cambridge University Press.\n\n\nHanmer, Michael J., and Kerem Ozan Kalkan. 2013. “Behind the Curve: Clarifying the Best Approach to Calculating Predicted Probabilities and Marginal Effects from Limited Dependent Variable Model.” American Journal of Political Science 57: 263–77.\n\n\nLenth, Russell V. 2024. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oakes, CA: Sage.\n\n\nLong, J. Scott, and Jeremy Freese. 2014. Regression Models for Categorical Dependent Variables Using Stata. College Station, TX: Stata Press.\n\n\nLong, J. Scott, and Sarah Mustillo. 2017. “Comparing Groups in Binary Regression Models Using Predictions.” Working paper. http://www.indiana.edu/jslsoc/files_research/groupdif/long-mustillo-comparing-groups-brm-2017-06-06.pdf.\n\n\nMaddala, S., Gangadharrao. 1983. Limited-Dependent and Qualitative Variables in Econometrics. Cambridge: Cambridge University Press.\n\n\nOsbourne, Jason W. 2015. Best Practices in Logistic Regression. London: Sage.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methods of interpretation</span>"
    ]
  },
  {
    "objectID": "05_meth_int.html#footnotes",
    "href": "05_meth_int.html#footnotes",
    "title": "5  Methods of interpretation",
    "section": "",
    "text": "As Osbourne (2015, 34) puts it, “odds ratios are problematic in that researchers, practitioners, and the lay public often don’t intuitively understand odds – although they often think they do […] [R]atios of things that people don’t understand are necessarily even more fraught with difficulty.”↩︎\nAn alternative strategy may also be used if categorical predictors are represented with dummy variables, i.e. indicator variables coded as 0/1. Gender, for instance, may be expressed as an indicator variable Female, with a value of 1 representing a female speakers, and a value of 0 a male speaker. The in-sample averages for this variable Female is 0.53 (i.e. 53% of the speakers are female). We can then form predictions for values between 0 and 1 and thereby give different weight to the two levels. Note, however, that the resulting predicted probabilities differ from the ones we would have obtained by averaging over predicted probabilities generated for male and female speaker. This is because the averaging was done on different scales (logit vs. probability scale).↩︎\nFor instance, frequency and dispersion as indicators of usage patterns are interrelated and covary strongly enough to consider them jointly as reflecting prototypes: Current and pervasive items vs. infrequent and specialized items. Ideal types may even me modeled on the basis of existing exemplars.↩︎\nInteresting: Long and Freese (2014, 272) argue against mixing as-observed and as-specified approaches to defining ideal types.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Methods of interpretation</span>"
    ]
  },
  {
    "objectID": "09_r_workbench.html",
    "href": "09_r_workbench.html",
    "title": "6  R workbench",
    "section": "",
    "text": "6.1 Descriptive statistics",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R workbench</span>"
    ]
  },
  {
    "objectID": "09_r_workbench.html#descriptive-statistics",
    "href": "09_r_workbench.html#descriptive-statistics",
    "title": "6  R workbench",
    "section": "",
    "text": "6.1.1 Relative frequencies\nWe can obtain category-specific and cumulative proportions in R using the {dplyr} package. The first step is to obtain the category-specific counts, then the corresponding proportions, and finally aggregated proportions. We apply this sequence of steps to the heaps dataset.\n\nd |&gt; \n  group_by(rating) |&gt; \n  dplyr::summarize(\n    n = n()) |&gt; \n  mutate(\n    prop = prop.table(n),\n    cum_prop = cumsum(prop)\n  )\n\nWe can also form cumulative proportions for subgroups in the data, by adding a grouping variable to the group_by() function. Note that the grouping variable must be listed first.\n\nd |&gt; \n  group_by(gender, rating) |&gt; \n  dplyr::summarize(\n    n = n()) |&gt; \n  mutate(\n    prop = prop.table(n),\n    cum_prop = cumsum(prop)\n  )\n\n\n\n6.1.2 Visualizing ordinal data\n\n6.1.2.1 Categorical predictors\nWhen drawing a bar chart as shown in Figure 2.2, we first need to decide on how to order subgroups (in this case: syntactic functions) along the x-axis. Before visualizing the data, we convert the variable synt_fun into and ordered factor manually:\n\nd$synt_fun &lt;- factor(\n  d$synt_fun, \n  levels = c(\"p_adj\", \"verb\", \"c_adj\", \"quan\"),\n  ordered = TRUE)\n\nWe can now draw a grouped bar chart.\n\nd |&gt; \n  ggplot(aes(\n    x = synt_fun, \n    fill = rating)) +\n  geom_bar(position = \"dodge\") +\n  scale_fill_brewer(\n    palette = \"RdGy\")\n\nTo draw the line plot in Figure 2.2, we need more code. Note that the first part merely creates the category-specific proportions.\n\nd |&gt;                                    \n  group_by(synt_fun, rating) |&gt;  \n  dplyr::summarize(                     \n    n = n()) |&gt;                         \n  mutate(                               \n    prop = prop.table(n),               \n    cum_prop = cumsum(prop)             \n  ) |&gt;                                \n  ggplot(aes(\n    x = synt_fun, \n    y = prop, \n    color = rating, \n    group = rating)) +\n  geom_line() +\n  geom_point() +\n  scale_colour_brewer(\n    palette = \"RdGy\")\n\nHere is the code to draw a bar chart showing cumulative proportions (see Figure 2.3).\n\nd |&gt; \n  ggplot(aes(\n    x = synt_fun, \n    fill = rating)) +\n  geom_bar(\n    position = \"fill\") +\n  scale_fill_brewer(\n    palette = \"RdGy\")\n\nTo change the order of the categories (i.e. to have red segments at the bottom), we can reverse the y-axis using the additional function scale_y_reverse().\n\nd |&gt; \n  ggplot(aes(\n    x = synt_fun, \n    fill = rating)) +\n  geom_bar(\n    position = \"fill\") +\n  scale_fill_brewer(\n    palette = \"RdGy\") +\n  scale_y_reverse()\n\nWe may then want to change the tick mark labels manually, so that 0 marks the bottom of the graph and 1 the top.\n\nd |&gt; \n  ggplot(aes(\n    x = synt_fun, \n    fill = rating)) +\n  geom_bar(\n    position = \"fill\") +\n  scale_fill_brewer(\n    palette = \"RdGy\") +\n  scale_y_reverse(\n    breaks = c(0, .5, 1), \n    labels = c(\"1\", \".5\", \"1\"))\n\nDiverging bar charts require more work. Note that the first part only creates the appropriate summary statistics (like above).\n\n# create summary\nds &lt;- d |&gt;                              \n  group_by(synt_fun, rating) |&gt;         \n  dplyr::summarize(                      \n    n = n()) |&gt;                         \n  mutate(                               \n    prop = prop.table(n),               \n    cum_prop = cumsum(prop)) \n\n# draw graph\nds |&gt;  \n  ggplot(aes(\n    x = reorder(synt_fun, prop * as.numeric(rating), mean),\n    y = prop, fill = rating)) +\n  geom_col(data = dplyr::filter(\n    ds, rating %in% c(\"4\", \"5\", \"6\")),\n    position = position_stack(reverse = TRUE)) +\n  geom_col(data = dplyr::filter(\n    ds, rating %in% c(\"1\", \"2\", \"3\")),\n    aes(y = -prop),\n    position = position_stack(reverse = FALSE)) +\n  scale_fill_brewer(palette = \"RdGy\", drop = FALSE)\n\nThe following code creates the line plot in Figure 2.5. We start by creating a summary table listing the relevant exceedance proportions. Then we exclude those for the highest category, as these are 0 and redundant. Then we draw the graph, where we explicitly define the limits of the y-axis.\n\n# create summary\nds &lt;- d |&gt;                                    \n  group_by(synt_fun, rating) |&gt;  \n  dplyr::summarize(                      \n    n = n()) |&gt;                         \n  mutate(                               \n    prop = prop.table(n),               \n    exc_prop = 1 - cumsum(prop)) \n\n# exclude exceedance proportions for highest category\nds &lt;- ds |&gt; dplyr::filter(rating != \"6\")\n\n# draw graph\nds |&gt;  \n  ggplot(aes(\n    x = synt_fun, \n    y = exc_prop, \n    color = rating, \n    group = rating)) +\n  geom_line() +\n  geom_point(size = 1) +\n  scale_colour_brewer(\n    palette = \"RdGy\") +\n  ylim(0,1)\n\n\n\n6.1.2.2 Numeric predictors\nUnfortunately, we don’t know how to use {ggplot2} to directly create a plot showing category-specific proportions across the variable Date of birth (see Figure 2.6). Our current workaround is to extract the category-specific proportions from an object created using the function cdplot() and then pass on these quantities to {ggplot}\n\n# create smoothed cumulative proportions\ntmp &lt;- cdplot(\n  rating ~ age, \n  data = d, \n  bw = 3, \n  plot = FALSE)\n\n# change these into category-specific proportions and arrange as a data frame\nds &lt;- data.frame(\n  age = 18:80,\n  rating_1 = tmp[[1]](18:80),\n  rating_2 = tmp[[2]](18:80) - tmp[[1]](18:80),\n  rating_3 = tmp[[3]](18:80) - tmp[[2]](18:80),\n  rating_4 = tmp[[4]](18:80) - tmp[[3]](18:80),\n  rating_5 = tmp[[5]](18:80) - tmp[[4]](18:80),\n  rating_6 = 1 - tmp[[5]](18:80)\n) |&gt;  \n  gather(rating_1:rating_6, key = \"response\", value = \"prob\")\n\n# draw line plot\nds|&gt; \n  ggplot(aes(\n    x = age, \n    y = prob, \n    color = response)) +\n  geom_line() +\n  scale_colour_brewer(\n    type = \"div\", \n    palette = \"RdGy\") \n\nThe following code produces an area chart showing aggregated proportions (see Figure 2.7).\n\nd |&gt; \n  ggplot(aes(\n    x = age, \n    after_stat(count), \n    fill = rating)) +\n  geom_density(\n    position = \"fill\", \n    adjust = 1.5, \n    colour = 1, \n    outline.type = \"full\") +\n  scale_fill_brewer(\n    palette = \"RdGy\") \n\nTo create a spine plot (see Figure 2.8) using {ggplot2}, we need need the geom_mosaic() function in the {ggmosaic} package. The first step is to discretize the variable Age into 10-year bins. To produce our desired visual arrangement (the highest category “6” in grey at the bottom), we need to reverse the order of the response categories (using fct_rev()) and then alse reverse the order of the fill colors using the argument direction in the function scale_fill_brewer().\n\nd |&gt; mutate(\n  age_bin = factor(\n    cut(age,\n        seq(10, 90, by = 10)))) |&gt; \n  mutate(\n    rating = fct_rev(rating)\n  ) |&gt; \n  count(rating, age_bin) %&gt;%\n    ggplot() +\n    geom_mosaic(aes(\n      weight = n,\n      x = product(age_bin),\n      fill = rating)) +\n    scale_fill_brewer(\n      palette = \"RdGy\",\n      direction = -1)\n\nThe following code produces the line plot version of this area chart (see Figure 2.9).\n\nd |&gt; \n  ggplot(aes(\n    x = age, \n    after_stat(count), \n    color = rating)) +\n  geom_density(\n    position = \"fill\",\n    adjust = 1.5, \n    outline.type = \"upper\") +\n  scale_colour_brewer(\n    palette = \"RdGy\") +\n  ylim(0, 1)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R workbench</span>"
    ]
  },
  {
    "objectID": "09_r_workbench.html#ordered-regression-models",
    "href": "09_r_workbench.html#ordered-regression-models",
    "title": "6  R workbench",
    "section": "6.2 Ordered regression models",
    "text": "6.2 Ordered regression models\n\n6.2.1 Model fitting\nWe can fit cumulative models using the function the {ordinal} package. It offers two functions for model fitting:\n\nclm() for simple models\nclmm() for mixed-effects models (more recent version)\n\nThe function clmm2() is an older version for mixed-effects models, which currently offers extended functionality but will eventually be superseded by clmm().\nWe start by loading the data and make sure that the outcome variable rating is represented as an ordered factor.\n\nd_parcel  &lt;- read_tsv(\n  here(\"data/analysis_data\",\n       \"malta_data_parcel.tsv\"))\n\nd_parcel$rating &lt;- factor(d_parcel$rating)\n\nThe following code fits a model to the parcel-package data with a single predictor variable, Date of birth. We use the argument threshold to request symmetric thresholds.\n\nm &lt;- clm(\n  rating ~ dob_centered, \n  data = d_parcel, \n  threshold = \"symmetric\")\n\nWe can print a regression table using summary():\n\nsummary(m)\n\nformula: rating ~ dob_centered\ndata:    d_parcel\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit symmetric 193  -263.16 534.32 5(1)  5.40e-13 1.9e+01\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \ndob_centered   -1.212      0.242  -5.008 5.49e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n          Estimate Std. Error z value\ncentral.1 -0.55013    0.17209  -3.197\ncentral.2  0.44757    0.17158   2.609\nspacing.1  0.38966    0.06784   5.744\n\n\nThe function Anova() in the {car} package (Fox and Weisberg 2019) produces an analysis of deviance table:\n\ncar::Anova(m, type=\"III\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: rating\n             Df  Chisq Pr(&gt;Chisq)    \ndob_centered  1 25.085  5.486e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere are other functions for model comparison and evaluation. Nested models can be compared using anova(), and stepwise model selection can be done with drop1() and add1().\n\n\n6.2.2 The parallel regression assumption\nIn R, we can run the LR test as follows. We first fit the two models using the clm() function from the package {ordinal} (Christensen 2023). To fit the non-parallel model, we use the additional argument nominal =, where we specify the predictor(s) for which we want to fit freely varying coefficients.\n\nm_np &lt;- clm(\n  rating ~ 1, \n  nominal = ~ dob_centered, \n  data = d_parcel,\n  threshold = \"symmetric\")\n\nThen we compare the two models using the function anova():\n\nanova(m, m_np)\n\nLikelihood ratio tests of cumulative link models:\n \n     formula:              nominal:      link: threshold:\nm    rating ~ dob_centered ~1            logit symmetric \nm_np rating ~ 1            ~dob_centered logit symmetric \n\n     no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nm         4 534.32 -263.16                        \nm_np      6 533.15 -260.58  5.1627  2    0.07567 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nNon-parallel mixed models not implemented in {ordinal}.\n\n\n\nIt is currently not possible to fit non-parallel mixed-effects models using the package {ordinal} – the clmm() function does not have the nominal argument.\n\n\n\n6.2.2.1 Model comparison using information criteria\nWe can use the functions AIC() and BIC() to obtain information measures for both models.\n\nBIC(m)\n\n[1] 547.3666\n\n\n\nBIC(m_np)\n\n[1] 552.7294\n\n\n\n\n6.2.2.2 Comparison of coefficients across cutpoint equations\nTo compare coefficients across cutpoint equations, we need to create four new indicator variables that represent the binary splits underlying cumulative models (see Figure 3.1). We add these to the data frame.\n\nd_parcel$split_1 &lt;- ifelse(as.numeric(d_parcel$rating) &lt;= 1, 0, 1)\nd_parcel$split_2 &lt;- ifelse(as.numeric(d_parcel$rating) &lt;= 2, 0, 1)\nd_parcel$split_3 &lt;- ifelse(as.numeric(d_parcel$rating) &lt;= 3, 0, 1)\nd_parcel$split_4 &lt;- ifelse(as.numeric(d_parcel$rating) &lt;= 4, 0, 1)\n\nThen we fit a logistic regression model to each cutpoint equation:\n\nm_s1 &lt;- glm(\n  split_1 ~ dob_centered, \n  data = d_parcel, \n  family = binomial(link = \"logit\"))\n\nm_s2 &lt;- glm(\n  split_2 ~ dob_centered, \n  data = d_parcel, \n  family = binomial(link = \"logit\"))\n\nm_s3 &lt;- glm(\n  split_3 ~ dob_centered, \n  data = d_parcel, \n  family = binomial(link = \"logit\"))\n\nm_s4 &lt;- glm(\n  split_4 ~ dob_centered, \n  data = d_parcel, \n  family = binomial(link = \"logit\"))\n\nThe coefficients can then be compared in tabular and/or graphical form. We can use the function coef() to access the coefficients of interest, and the function confint() to obtain confidence intervals for them. The following code produces a simpl table (see Table 3.3).\n\ndata.frame(\n  split = 1:4,\n  estimate = round(\n    c(coef(m_s1)[2], \n      coef(m_s2)[2], \n      coef(m_s3)[2], \n      coef(m_s4)[2]), \n    2),\n  ci_lower = round(\n    c(confint(m_s1)[2,1], \n      confint(m_s2)[2,1],  \n      confint(m_s3)[2,1], \n      confint(m_s4)[2,1]), \n    2),\n  ci_upper = round(\n    c(confint(m_s1)[2,2], \n      confint(m_s2)[2,2],\n      confint(m_s3)[2,2], \n      confint(m_s4)[2,2]), \n    2))\n\n  split estimate ci_lower ci_upper\n1     1    -0.83    -1.43    -0.29\n2     2    -1.32    -1.96    -0.76\n3     3    -1.35    -1.92    -0.82\n4     4    -1.38    -1.96    -0.84\n\n\n\n\n6.2.2.3 Comparison of predicted probabilities\nWe use the emmeans() function to obtain average predicted response probabilities for models m and m_np.\n\nap_m &lt;- emmeans(\n  m, \n  ~ rating | dob_centered, \n  mode = \"prob\",\n  at = list(\n    dob_centered = seq(-1.4, 1, by = .1))\n  ) |&gt; \n  data.frame()\n\nap_m_np &lt;- emmeans(\n  m_np, \n  ~ rating | dob_centered, \n  mode = \"prob\",\n  at = list(\n    dob_centered = seq(-1.4, 1, by = .1))\n  ) |&gt; \n  data.frame()\n\nThen we draw the graph using {ggplot2} (see Figure 3.6).\n\nap_m |&gt; \n  ggplot(aes(\n    x = dob_centered, \n    y = prob, \n    color = rating)) +\n  geom_line() +\n  geom_line(data = ap_m_np, linetype = \"22\") +\n  scale_colour_brewer(palette = \"RdBu\")\n\nExtract category-specific probabilities from model.\n\nemmeans(\n  m_np, \n  ~ rating | dob_centered, \n  mode = \"prob\",\n  at = list(\n    dob_centered = 0)\n  )\n\nExtract cumulative probabilities from model.\n\nemmeans(\n  m_np, \n  ~ dob_centered | cut, \n  mode = \"cum.prob\",\n  at = list(\n    dob_centered = 0)\n  )\n\nExtract exceedance probabilities from the model:\n\nemmeans(\n  m_np, \n  ~ dob_centered | cut, \n  mode = \"exc.prob\",\n  at = list(\n    dob_centered = 0)\n  )\n\n\n\n\n6.2.3 Model diagnostics\nThe presid() function in the {PResiduals} package can be used to calculate probability-scale residuals. Since presid() does not (yet) work on models fit using the package {ordinal}, we write a function that extracts probability-scale residuals from models fit using clm():\n\npresid.clm &lt;- function(object, ...) {\n  pfun &lt;- switch(object$link,\n                 logit = plogis,\n                 probit = pnorm,\n                 cloglog = pGumbel,\n                 cauchit = pcauchy)\n  n &lt;- object$nobs\n  q &lt;- length(object$Theta)\n  lp &lt;- as.numeric(model.matrix(object)$X[,-1] %*% coef(object)[-(1:length(object$alpha))])\n\n  cumpr &lt;- cbind(0, matrix(pfun(matrix(object$Theta, n, q, byrow = TRUE) - lp),, q), 1)\n  y &lt;- as.integer(object$y)\n  lo &lt;- cumpr[cbind(seq_len(n), y)]\n  hi &lt;- 1 - cumpr[cbind(seq_len(n), y+1L)]\n  res &lt;- lo - hi\n  res &lt;- naresid(object$na.action, res)\n  return(res)\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R workbench</span>"
    ]
  },
  {
    "objectID": "09_r_workbench.html#methods-of-interpretation",
    "href": "09_r_workbench.html#methods-of-interpretation",
    "title": "6  R workbench",
    "section": "6.3 Methods of interpretation",
    "text": "6.3 Methods of interpretation\nWe now look at how to apply different methods of interpretation using R. The following sections are structured identically to the ones in Chapter 5. The R code given here produces figures and tables similar to the ones in that earlier chapter. The relevant items are cross-referenced. To see the intended output, hover over the hyperlink – the image or table will then appear.\nThis is the model we will concentrate on:\n\nm &lt;- clm(\n  rating ~ dob_c + gender, \n  data = d_parcel, \n  threshold = \"symmetric\")\n\n\n6.3.1 Predictions on the probability scale (R)\n\n6.3.1.1 Predictions for each observed unit (R)\nIn R, the in-sample predicted probabilities for the outcome categories can be computed with the function predictions() in the {marginaleffects} package.\n\npredictions(m)\n\nThe following code produces the dot diagram in Figure 5.4. note how piping is used to pass on the output of the predictions() function to the ggplot() function. The function geom_dotplot() draws a dot diagram. We manually changed the argument binwidth, since the default setting (30 bins) is too coarse for the large number of dots that need to be displayed.\n\npredictions(m)  |&gt; \n  ggplot(aes(x = estimate, y = group)) +\n  geom_dotplot(\n    binwidth = .005)\n\nTo create a by-category summary table (Table 5.1), we use the function group_by() to identify the subgroups we wish to compare (here: the response categories, which are generically referred to as “group” by the predictions() function). Then we use the function summarize() in the {dplyr} package to obtain the summary statistics we need.\n\npredictions(m) |&gt; \n  group_by(group) |&gt; \n  dplyr::summarize(\n    Mean = mean(estimate),\n    SD = sd(estimate),\n    Min = min(estimate),\n    Max = max(estimate)\n  )\n\n\n\n6.3.1.2 Unit-level predictions (R)\n\n6.3.1.2.1 Predictions at specified values (R)\nWe can use the predictions() function to obtain predictions at specified values. We start by creating a “grid”, i.e. data frame which holds this specific profile we are interested in.\n\ngrid &lt;- data.frame(\n  dob_c = 0,\n  gender = \"f\")\n\ngrid\n\n  dob_c gender\n1     0      f\n\n\nBy supplying this grid to the newdata argument in the predictions() function, we can predict the response probabilities for this profile:\n\npredictions(m, newdata = grid)\n\nThe same can be done using the {emmeans} package:\n\nemmeans(           # compute from\n  m,               # model m\n  ~ rating,        # category-specific predictions\n  mode = \"prob\",   # on the probability scale\n  at = list(       # for the following profile:\n    dob_c = 0,     #  - date of birth = 0 (year 1975)\n    gender = \"f\")) #  - gender = female\n\nThe datagrid() function in the {marginaleffects} package makes it easy to define different profiles. Note how the datagrid() function returns all combinations of the specified values:\n\ngrid &lt;- datagrid(\n  gender = c(\"m\", \"f\"), \n  dob_c = c(-1.4, -0.6, 0.2, 1), \n  model = m)\n\ngrid\n\n  gender dob_c rowid\n1      m  -1.4     1\n2      m  -0.6     2\n3      m   0.2     3\n4      m   1.0     4\n5      f  -1.4     5\n6      f  -0.6     6\n7      f   0.2     7\n8      f   1.0     8\n\n\nWe then pass the new grid to the predictions() function, to obtain the predicted probabilities summarized in Table 5.2.\n\npredictions(\n  m, \n  newdata = grid)\n\nWe can also use shortcut functions: range to define the endpoints of the date-of-birth distribution, and unique to include all attested levels (or values) of Gender:\n\ngrid &lt;- datagrid(\n  gender = unique, \n  dob_c = range, \n  model = m)\n\ngrid\n\n  gender dob_c rowid\n1      f  -1.4     1\n2      f   1.0     2\n3      m  -1.4     3\n4      m   1.0     4\n\n\n\n\n6.3.1.2.2 Predictions for the “average unit” (R)\nWhen locating a central and/or representative point in the predictor space, {marginaleffects} uses the mean for all numeric variables, and the mode for all categorical variables. The package allows us to pass the \"mean\" shortcut to the newdata argument, which returns predictions for a (hypothetical) female speaker born in 1973.34 (Table 5.3):\n\npredictions(\n  m, \n  newdata = \"mean\")\n\n\n\n6.3.1.2.3 Predictions for ideal types (R)\nTo form predictions for the two ideal types (Table 5.4), we specify the relevant profiles using the newdata argument of the predictions() function:\n\npredictions(\n  m, \n  newdata = datagrid(\n    dob_c = -1.4,\n    gender = \"m\"\n  ))\n\npredictions(\n  m, \n  newdata = datagrid(\n    dob_c = 1,\n    gender = \"f\"\n  ))\n\n\n\n6.3.1.2.4 Extra: Plotting unit-level predictions using {marginaleffects}\nMale and female speaker born in 2000:\n\nplot_predictions(\n  m,\n  condition = list(\n    \"gender\", \"group\", \"dob_c\" = 1)) + \n  scale_y_continuous(\n    limits = c(0, 1), expand = c(0, 0),\n    breaks = c(0, .5, 1), label = c(\"0\", \".5\", \"1\")) +\n  scale_color_manual(values = fill_cols_label) +\n  theme_ls() +\n  ylab(\"Probability\") + xlab(NULL) +\n  labs(caption = \"Adjusted to: Date of birth = 2000.\") +\n  theme(strip.text = element_blank())\n\nFull range of conditions:\n\nplot_predictions(\n  m,\n  condition = c(\"dob_c\", \"gender\", \"group\")) + \n  scale_y_continuous(\n    limits = c(0, 1), expand = c(0, 0),\n    breaks = c(0, .5, 1), label = c(\"0\", \".5\", \"1\")) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  facet_grid(. ~ group) +\n  theme_ls() +\n  ylab(\"Probability\") +\n  xlab(\"Date of birth (rescaled)\")\n\n\n\n\n\n6.3.2 Average predictions (R)\n\n6.3.2.1 Unit-level predictions: as-observed vs. specified values (R)\nBy default, the function avg_predictions() uses the as-observed approach and calculates the average prediction over the sample. In {marginaleffects} terminology, such average predictions are said to have been obtained using an empirical grid.\n\navg_predictions(m)\n\nRather than use the observed distribution for the variable Gender, we may take control of the value(s) at which to evaluate predictions. For instance, to treat the two levels (male and female) equally, we need to supply this variable to the newdata argument. All variables that are not specified in the newdata argument will still be treated as-observed. The grid is said to be partly synthetic, i.e. partly manually specified.\n\navg_predictions(\n  m,\n  newdata = datagrid(\n    gender = unique\n  ))\n\nWe can also determine the value of Age at which unit-level prediction are to be formed. By also defining the values of Age we are specifying a fully synthetic grid.\n\navg_predictions(\n  m,\n  newdata = datagrid(\n    dob_c = seq(-1.4, 1, by = .8),\n    gender = unique\n  ))\n\n\n\n\n\n\n\nTip\n\n\n\nWe have created two custom functions for handling quantitative predictors with datagrid(). These allow you to select specific points of the observed (empirical) distribution of numeric variables:\n\nsd_mean_sd() returns three values: the mean surrounded by two values at +/- 1 standard deviation. The function mad() is used to obtain a robust version of the SD, and a 10% trimmed mean is used by default.\ndecile_midpoints() returns the midpoints of n bins, each of which contain roughly the same number of cases in the estimation sample.\n\n\n\n\n\n\n6.3.2.2 Average predictions for a quick overview of predictor importance (R)\nWe can compute average predictions for a specific predictor using the avg_predictions() function. The argument variables specifies the predictor of interest, and the argument newdata controls the teatment of the peripheral variables, i.e. it defines the locations of the data space over which predictions are averaged. The average predictions reported in Table 5.6 for the variable Gender can be obtained as follows:\n\navg_predictions(\n  m,\n  variables = \"gender\",\n  newdata = datagrid(\n    dob_c = seq(-1.4, 1, length = 7)\n  ))\n\nFor quantitative variables, we have to specify the values of interest in a list. Otherwise avg_predictions() will only return a single average prediction calculated at the in-sample mean of the variable.\n\navg_predictions(\n  m,\n  variables = list(\n    \"dob_c\" = c(-1.4, -.2, 1)),\n  newdata = datagrid(\n    gender = unique\n  ))\n\n\n\n6.3.2.3 Average predictions for subgroups in the data (R)\nTo compute average predictions for the subgroups using the same as-observed values for all groups we need to use the argument variables in the avg_predictions() function. The “as-observed, identical” average predictions are obtained as follows:\n\navg_predictions(\n  m,\n  variables = \"gender\")\n\nIf we instead use the by argument, the peripheral variables are again treated as-observed, but a separate set of observed values is used for each subgroup defined by the by argument. The “as-observed, different” average predictions are obtained as follows:\n\navg_predictions(\n  m,\n  by = \"gender\")\n\nFor average predictions that give equal weight to cohorts, we can use the newdata argument to specify the age values at which to generate predictions.\n\navg_predictions(\n  m,\n  variables = \"gender\", \n  newdata = datagrid(\n    dob_c = seq(-1.4, 1, by = .8)\n  ))\n\n\n\n6.3.2.4 Using local means for adjustment (R)\n\n\n6.3.2.5 Extra: Plotting average predictions using {marginaleffects}\nThe {marginaleffects} package makes it easy to visualize average predictions.\nAverage predictions by Gender, treating Date of birth as-observed:\n\nplot_predictions(\n  m, \n  by = c(\"gender\", \"group\"))\n\nAverage predictions by Gender, treating Date of birth as-balanced:\n\nplot_predictions(\n  m, \n  by = c(\"gender\", \"group\"),\n  newdata = datagrid(\n    gender = unique,\n    dob_c = seq(-1.4, 1, .8)))\n\nAverage predictions by Date of birth, treating Gender as-balanced:\n\nplot_predictions(\n  m, \n  by = c(\"dob_c\", \"group\"),\n  newdata = datagrid(\n    gender = unique,\n    dob_c = seq(-1.4, 1, .05)))\n\n\n\n\n6.3.3 Comparisons (R)\n\n6.3.3.1 Comparisons for each observed unit (R)\nThe function comparison() in the {marginaleffects} package can be used to make comparisons. The targeted variable is specified with the argument variable. By default, all other variables are treated as-observed, which means that a comparison is computed for each observation in the sample. The following function call therefore returns a large table of differences in predicted probability, which compare the predictions obtained for the male and the female version of each subject in the sample.\n\ncomparisons(\n  m,\n  variables = \"female\")\n\nTo draw a dot diagram (Figure 5.12), we run the following code:\n\ncomparisons(\n  m,\n  variables = \"gender\") |&gt;  \n  ggplot(aes(x = estimate, y = group)) +\n  geom_dotplot(binwidth = .001) +\n  geom_vline(xintercept = 0)\n\nFor numeric variables, the default behavior of the comparisons() function is to use the observed values of the targeted numeric variable: The variable is changed from its observed value to its observed value plus 1.\n\ncomparisons(\n  m,\n  variables = \"dob_c\") |&gt;  \n  ggplot(aes(x = estimate, y = group)) +\n  geom_dotplot(binwidth = .001) +\n  geom_vline(xintercept = 0)\n\n\n\n6.3.3.2 Comparisons for numeric predictors (R)\nWe will consider the options shown in Figure 5.10 in turn, from top to bottom. The output is not shown here. Note that the first two strategies return a batch of predictions, one for each observation in the estimation sample:\n\nDefault: (observed + 1) – observed\n\n\ncomparisons(\n  m,\n  variables = \"dob_c\",\n  newdata = datagrid(\n    gender = unique))\n\n\n(observed + 0.5) – observed\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = 0.5),\n  newdata = datagrid(\n    gender = unique))\n\n\n–1 -&gt; 1\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = c(-1, 1)),\n  newdata = datagrid(\n    gender = unique))\n\n\n(mean + SD) – (mean – SD)\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = \"2sd\"),\n  newdata = datagrid(\n    gender = unique))\n\n\n(mean + SD/2) – (mean – SD/2)\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = \"sd\"),\n  newdata = datagrid(\n    gender = unique))\n\n\nupper quartile – lower quartile\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = \"iqr\"),\n  newdata = datagrid(\n    gender = unique))\n\n\nmaximum – minimum\n\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = \"minmax\"))\n\nFor the intermediate approach illustrated in Figure 5.11, we need to specify two things: (i) the span we want to consider, which is done in the same way as above; and (ii) the base values, i.e. values of the numeric variable to which the span will be added:\n\ncomparisons(\n  m,\n  variables = list(\n    \"dob_c\" = 30/25),\n  newdata = datagrid(\n    dob_c = seq(-1.4, -.2, length = 4)\n  )\n)\n\nTo evaluate the comparison at custom locations, we can also use functions to supply specific values of the distribution. To apply the comparison to the restricted range, from the 5th to the 95th percentile (see Long and Freese 2014, 250–51), we could type the following:\n\ncomparisons(m,\n  variables = list(\n    \"dob_c\" = c(\n      quantile(d$dob_c, .05),\n      quantile(d$dob_c, .95)\n    )\n  ))\n\n\n\n6.3.3.3 Comparisons at specified values (R)\n\ncomparisons(\n  m,\n  variables = \"gender\",\n  newdata = datagrid(\n    dob_c = 0))\n\n\n\n6.3.3.4 Comparisons for the “average unit” (R)\nWe can set peripheral variables equal to their means (numeric predictors) or modes (categorical predictors) by specifying the argument newdata = \"mean\":\n\ncomparisons(\n  m,\n  variables = \"gender\",\n  newdata = \"mean\")\n\n\n\n6.3.3.5 Comparisons for ideal types (R)\n\n\n\n6.3.4 Average comparisons (R)\nThe {marginaleffects} package computes average comparisons by first calculating unit-level comparisons on a specified scale and then averaging over these.\n\n6.3.4.1 Average comparisons: Observed vs. specified values (R)\nBy default, the avg_comparisons() function treats peripheral variables as-observed, which means that the comparison is calculated for each observation in the data set and then averaged. For binary variables, it considers a change from one level to the other. The values we obtain are those in Table 5.11 (left-hand side).\n\navg_comparisons(\n  m,\n  variables = \"gender\")\n\nWe can graph these with the following code (see Figure 5.15).\n\navg_comparisons(\n  m,\n  variables = \"gender\") |&gt;  \n  ggplot(aes(x = estimate, y = group)) +\n  geom_point() +\n  geom_linerange(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0)\n\nTo produce average comparisons over specified values of the peripheral variable Date of birth, we need to specify its values using the newdata argument (see right-hand side of Table 5.11).\n\navg_comparisons(\n  m,\n  variables = \"gender\",\n  newdata = datagrid(\n    dob_c = seq(-1.4, 1, length = 7)\n  ))\n\n\n\n6.3.4.2 Average comparisons for a quick overview of predictor importance (R)\nThe summaries reported in Table 5.12 can be obtained as follows. To calculate average comparisons for the variable Gender, treating Date of birth as-balanced, we need to pass the nae of the tageted variable gender to the variables argument, and manually define the values of Date of birth using the newdata argument.\n\navg_comparisons(\n  m,\n  variables = \"gender\",\n  newdata = datagrid(\n    dob_c = seq(-1.4, 1, length = 7)\n  ))\n\nTo calculate average comparisons for Date of birth using the intermediate approach outlined in Figure 5.11, we need to specify the step size in a list passed to the argument variables. In the newdata argument, we define the start locations of Date of birth.\n\navg_comparisons(\n  m,\n  variables = list(\n    \"dob_c\" = 30/25),\n  newdata = datagrid(\n    dob_c = seq(-1.4, -.2, length = 4),\n    gender = unique\n  ))\n\nInstead of treating a peripheral variable as-observed, we can determine its values manually:\n\navg_comparisons(\n  m, \n  variables = \"gender\")\n\n\navg_comparisons(\n  m, \n  by = \"gender\")\n\n\navg_comparisons(\n  m, \n  variables = \"female\",\n  newdata = datagrid(\n    dob_c = quantile_midpoints\n  ))\n\n\n\n6.3.4.3 Average comparisons for subgroups in the data (R)\nAverage comparisons for Date of birth, for Gender subgroups\n\nDOB: as-observed (+1)\nGender: treated identically\n\n\navg_comparisons(\n  m,\n  variables = \"dob_c\",\n  by = \"gender\")\n\nAverage comparisons for Date of birth, for Gender subgroups\n\nDOB: -1 =&gt; +1\nGender: treated identically\n\n\navg_comparisons(\n  m,\n  variables = list(\n    \"dob_c\" = c(-1, 1)),\n  by = \"gender\")\n\nAverage comparisons for Date of birth, for Gender subgroups\n\nDOB: intermediate approach\nGender: treated identically\n\n\navg_comparisons(\n  m,\n  variables = list(\n    \"dob_c\" = 30/25),\n  newdata = datagrid(\n    dob_c = seq(-1.4, -.2, length = 4),\n    gender = unique),\n  by = \"gender\") \n\n\n\n\n6.3.5 Predictions on the latent-variable scale (R)\nSince model-based predictions on the latent-variable scale only make sense in the context of the thresholds, we have written two custom functions for adding thresholds and category labels to {ggplot} graphs:\n\nadd_thresholds()\nadd_threshold_labels()\n\n\nadd_thresholds &lt;- function(\n    thresholds, \n    col_lines = \"black\",\n    vertical = FALSE){\n  thresholds_c &lt;- as.numeric(thresholds) - mean(as.numeric(thresholds))\n  \n  if (vertical == FALSE){\n    ggplot2::geom_hline(\n      yintercept = thresholds_c,\n      color = col_lines\n    )\n  } else {\n    ggplot2::geom_vline(\n      xintercept = thresholds_c,\n      color = col_lines\n    )\n  }\n  \n}\n\nadd_threshold_labels &lt;- function(\n    thresholds, \n    padding_labels = .1, \n    x_location = 1, \n    adj_label = 1,\n    col_labels = \"black\",\n    custom_labels = NULL,\n    size_labels = 3,\n    vertical = FALSE){\n  \n  thresholds_c &lt;- as.numeric(thresholds) - mean(as.numeric(thresholds))\n  \n  n_thresholds &lt;- length(thresholds)\n\n  if (is.null(custom_labels)){\n    category_labels &lt;- c(\n      str_split(dimnames(thresholds)[[2]][1:n_thresholds], pattern = \"\\\\|\", simplify = TRUE)[,1],\n      str_split(dimnames(thresholds)[[2]][1:n_thresholds], pattern = \"\\\\|\", simplify = TRUE)[n_thresholds,2]\n    )\n  } else {\n    category_labels &lt;- custom_labels\n  }\n  \n  location_labels &lt;- c(\n    min(thresholds_c) - diff(range(thresholds_c))*padding_labels,\n    zoo::rollmean(thresholds_c, k = 2),\n    max(thresholds_c) + diff(range(thresholds_c))*padding_labels)\n  \n  if (vertical == FALSE){\n    ggplot2::annotate(\n      \"text\", \n      x = x_location,\n      y = location_labels,\n      label = category_labels,\n      adj = adj_label,\n      color = col_labels,\n      size = size_labels\n    )\n  } else {\n    ggplot2::annotate(\n      \"text\", \n      y = x_location,\n      x = location_labels,\n      label = category_labels,\n      adj = adj_label,\n      color = col_labels,\n      size = size_labels\n    )\n  }  \n}\n\n\n6.3.5.1 {emmeans} package\nUnfortunately, the {marginaleffects} package does not allow us to produce predictions on the latent-variable scale. We must therefore switch to the {emmeans} package (Lenth 2024), which offers similar functionality to {marginaleffects} package, but differs in a few key respects. The most important difference (apart from syntax) concerns the default treatment of peripheral variables.\nAs we have seen above, {marginaleffects} treats peripheral variables as-observed, unless we actively specify their values. {emmeans}, on the other hand, defaults to an as-balanced treatment of categorical variables. This means that it returns simple averages, giving equal weight to all levels of the variable. The default treatment of continuous predictors, on the other hand, is to hold them at their in-sample average. Importantly, {emmeans} does not support as-observed predictions. The key differences are summarized in Figure 6.1.\n\n\n\n\n\n\n\n\nFigure 6.1\n\n\n\n\n\n\n\n\n6.3.6 Predictions for each observed unit (R)\nThe {emmeans} package does not allow us to generate as-observed predictions. This means that we must construct them manually:\n\npo_lv_m &lt;- data.frame(\n  estimate = as.numeric(\n    t(as.matrix(coef(m)[4:5])) %*% t(model.matrix(m)$X[,-1]) - mean(m$Theta)))\n\nThen we can plot them using a dot diagram. The custom functions (defined above) add_thresholds() and add_threshold_labels() are used to annotate the graph.\n\npo_lv_m |&gt; \n  ggplot(aes(x = estimate)) + \n  geom_dotplot(\n    method = \"histodot\", \n    binwidth = .09) +\n  add_thresholds(m$Theta, vertical = TRUE) +\n  add_threshold_labels(m$Theta, vertical = TRUE)\n\n\n\n6.3.7 Unit-level predictions (R)\nWhile {emmeans} is not designed to generate unit-level predictions, we can nevertheless force it to avoid averaging by manually specifying value for all predictor variables in the model. Let’s say we wish to obtain predictions for a male speaker born in 2000. We pass the variables that are used to define the condition to the spec argument and then, critically, we specify the values for both in the at argument. To compute a prediction for a male speaker born in 2000, we run the following:\n\nemmeans(\n  m,\n  spec = c(\"dob_c\", \"gender\"),\n  at = list(\n    dob_c = 1,\n    gender = \"m\"))\n\nTo add conditions, we specify additional values in the at argument. To obtain predictions for both a male and female speaker born in 2000:\n\nemmeans(\n  m,\n  spec = c(\"dob_c\", \"gender\"),\n  at = list(\n    dob_c = 1,\n    gender = c(\"f\", \"m\")))\n\nAnd we can also add predictions for two additional years (1950 and 1975):\n\nemmeans(\n  m,\n  spec = c(\"dob_c\", \"gender\"),\n  at = list(\n    dob_c = c(-1, 0, 1),\n    gender = c(\"f\", \"m\")))\n\n\n\n6.3.8 Average predictions (R)\nWe can use the emmeans() function to get average predictions for male and female speakers. If we do not specify a birth year, we get at-means predictions, based on the in-sample average.\n\nemmeans(\n  m,\n  spec = \"gender\")\n\nSince the distribution of this variable is not representative of the target population, we avoid sample statistics and instead ask for predictions for the year 1975:\n\nemmeans(\n  m,\n  spec = \"gender\",\n  at = list(\n    dob_c = 0))\n\nWe can likewise obtain average predictions for the variable Date of birth. By default, {emmeans} treats Gender (the peripheral categorical variable) as-balanced, giving the same weight to male and female speakers. This is what we want. When requesting average predictions for numeric predictors, we also need to specify the locations we wish to consider. Otherwise we will only get a single estimate for the in-sample mean. Here, we choose 20-year steps:\n\nemmeans(\n  m,\n  spec = \"gender\",\n  at = list(\n    dob_c = c(-1.4, -.6, .2, 1)))\n\nIf we wish to instead obtain weighted averages over peripheral categorical variables, we can use the weights argument. Setting it to \"proportional\", the levels are weighted according to their representation in the estimation sample.\n\nemmeans(\n  m,\n  spec = \"dob_c\",\n  at = list(\n    dob_c = c(-1.4, -.6, .2, 1)),\n  weights = \"proportional\")\n\n\n\n6.3.9 Unit-level comparisons (R)\nTo obtain unit-level comparisons, the first step is to create unit-level predictions. For instance, to compare female and male speakers born in 2000, we use the emmeans() function to calculate unit-level predictions. We assign these to a new object ap_gender:\n\nap_gender &lt;- emmeans(\n  m,\n  spec = \"gender\",\n  at = list(\n    dob_c = 1))\n\nap_gender\n\n gender emmean    SE  df asymp.LCL asymp.UCL\n f      -0.878 0.247 Inf     -1.36    -0.395\n m      -1.462 0.258 Inf     -1.97    -0.957\n\nConfidence level used: 0.95 \n\n\nTo compare these predictions, use the function pairs():\n\npairs(ap_gender)\n\n contrast estimate   SE  df z.ratio p.value\n f - m       0.584 0.27 Inf   2.166  0.0303\n\n\nThe output of pairs() does not include confidence intervals. To obtain these, we have to wrap the function confint() around it:\n\nconfint(\n  pairs(ap_gender))\n\n contrast estimate   SE  df asymp.LCL asymp.UCL\n f - m       0.584 0.27 Inf    0.0555      1.11\n\nConfidence level used: 0.95 \n\n\nBy default, pairs() runs all pairwise comparisons among predictions contained in the objects created by emmeans(). We can simplify the output with the argument simple. For instance, if we wish to obtain differences between male and female speakers at different birth years, we start by forming a set of 6 predictions:\n\nap_gender &lt;- emmeans(\n  m,\n  spec = c(\"dob_c\", \"gender\"),\n  at = list(\n    dob_c = c(-1, 0, 1),\n    gender = c(\"f\", \"m\")))\n\nap_gender\n\n dob_c gender emmean    SE  df asymp.LCL asymp.UCL\n    -1 f       1.490 0.363 Inf     0.778     2.201\n     0 f       0.306 0.195 Inf    -0.076     0.688\n     1 f      -0.878 0.247 Inf    -1.361    -0.395\n    -1 m       0.906 0.380 Inf     0.161     1.650\n     0 m      -0.278 0.217 Inf    -0.704     0.147\n     1 m      -1.462 0.258 Inf    -1.968    -0.957\n\nConfidence level used: 0.95 \n\n\nWe are not interested in all pairwise comparisons, only in the comparisons between the levels of Gender, but for each birth year. The argument simple allows us to specify the focal variable for the contrast, which gives us what we want:\n\npairs(\n  ap_gender, \n  simple = \"gender\")\n\ndob_c = -1:\n contrast estimate   SE  df z.ratio p.value\n f - m       0.584 0.27 Inf   2.166  0.0303\n\ndob_c =  0:\n contrast estimate   SE  df z.ratio p.value\n f - m       0.584 0.27 Inf   2.166  0.0303\n\ndob_c =  1:\n contrast estimate   SE  df z.ratio p.value\n f - m       0.584 0.27 Inf   2.166  0.0303\n\n\nThe following code produces an interaction plot.\n\nemmip(\n  m, \n  gender ~ dob_c,\n  at = list(\n    dob_c = seq(-1.4, 1, by = .1)\n    ),\n  CIs = TRUE, \n  col = NA) +\n  add_thresholds(m$Theta, col_lines = \"grey\") +\n  add_threshold_labels(m$Theta, x_location = 1.2, adj_label = 1, col_labels = \"grey\") +\n  theme_classic()  +\n  geom_ribbon(aes(ymin = LCL, ymax = UCL, fill = gender), alpha = 0.2, colour = NA) +\n  geom_line(lty = 1)\n\n\n\n6.3.10 Average comparisons (R)\nGenerating average comparisons works in a similar way. We first create an object containing average predictions and then use pairs() to obtain relevant contrasts.\n\nap_dob &lt;- emmeans(\n  m,\n  spec = \"dob_c\",\n  at = list(\n    dob_c = c(-1.4, 1)))\n\nap_dob\n\n dob_c emmean    SE  df asymp.LCL asymp.UCL\n  -1.4   1.67 0.437 Inf     0.815     2.527\n   1.0  -1.17 0.213 Inf    -1.588    -0.752\n\nResults are averaged over the levels of: gender \nConfidence level used: 0.95 \n\n\n\npairs(ap_dob)\n\n contrast             estimate   SE  df z.ratio p.value\n (dob_c-1.4) - dob_c1     2.84 0.58 Inf   4.903  &lt;.0001\n\nResults are averaged over the levels of: gender \n\n\n\n\n\n\nChristensen, Rune H. B. 2023. Ordinal—Regression Models for Ordinal Data. https://CRAN.R-project.org/package=ordinal.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion to Applied Regression. Third. Thousand Oaks CA: Sage.\n\n\nLenth, Russell V. 2024. Emmeans: Estimated Marginal Means, Aka Least-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLong, J. Scott, and Jeremy Freese. 2014. Regression Models for Categorical Dependent Variables Using Stata. College Station, TX: Stata Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R workbench</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 2010. Analysis of Ordinal Categorical Data.\nHoboken, NJ: John Wiley & Sons.\n\n\nAitchison, J., and S. D. Silvey. 1957. “The Generalization of\nProbit Analysis to the Case of Multiple Responses.”\nBiometrika 44 (1/2): 130–40. https://doi.org/10.2307/2333245.\n\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. Forthcoming.\n“How to Interpret Statistical Models Using\nmarginaleffects in R and\nPython.” Journal of Statistical Software,\nForthcoming. https://marginaleffects.com.\n\n\nAssociation, American Psychological, ed. 2020. Publication Manual of\nthe American Psychological Association. 7th ed. Washington, DC:\nAmerican Psychological Association.\n\n\nBürkner, Paul-Christian. 2017. “Brms: An r Package for Bayesian\nMultilevel Models Using Stan.” Journal of Statistical\nSoftware 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nBürkner, Paul-Christian, and Matti Vuorre. 2019. “Ordinal\nRegression Models in Psychology: A Tutorial.”\nAdvances in Methods and Practices in Psychological Science 2\n(1): 77–101. https://doi.org/10.1177/2515245918823199.\n\n\nCameron, Pravin K., A. Colin & Trivedi. 2005. Microeconometrics:\nMethods and Applications. New York: Cambridge University Press.\n\n\nCheng, J. Scott, Simon & Long. 2007. “Testing the IIA in the\nMultinomial Logit Model.” Sociological Methods and\nResearch 35 (4): 583–600.\n\n\nChristensen, Rune H. B. 2023. Ordinal—Regression Models for Ordinal\nData. https://CRAN.R-project.org/package=ordinal.\n\n\nChristensen, Rune Haubo B. 2018. “Cumulative Link Models for\nOrdinal Regression with the r Package Ordinal.” Technical\nUniversity of Denmark.\n\n\nFienberg, S. E. 1980. The Analysis of Cross-Classified Categorical\nData. Second. Cambridge, MA: MIT Press.\n\n\nFox, Jean-Paul. 2010. Bayesian Item Response Modeling:\nTheory and Applications. New York: Springer.\n\n\nFox, John, and Sanford Weisberg. 2019. An R Companion\nto Applied Regression. Third. Thousand Oaks CA: Sage.\n\n\nFullerton, Andrew S., and Jun Xu. 2016. Ordered Regression Models:\nParallel, Partial, and Non-Parallel Alternatives. Boca Raton, FL:\nCRC Press.\n\n\nGelman, Andrew. 2007. “Scaling Regression Inputs by Dividing by\nTwo Standard Deviations.” Statistics in Medicine 27\n(15): 2865–73. https://doi.org/10.1002/sim.3107.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Linear Models. Cambridge:\nCambridge University Press.\n\n\nGoodman, Leo A. 1983. “The Analysis of Dependence in\nCross-Classifications Having Ordered Categories, Using Log-Linear Models\nfor Frequencies and Log-Linear Models for Odds.”\nBiometrics 39 (1): 149–60. https://doi.org/10.2307/2530815.\n\n\nGreene, William H., and David A. Hensher. 2010. Modeling Ordered\nChoices: A Primer. Cambridge: Cambridge University Press.\n\n\nGrilli, Leonardo, and Carla Rampichini. 2012. “Multilevel Models\nfor Ordinal Data.” In Modern Analysis of Customer Surveys:\nWith Applications Using R, edited by Ron\nS. Kenett and Silvia Salini, 391–411. New York: Wiley.\n\n\nHanmer, Michael J., and Kerem Ozan Kalkan. 2013. “Behind the\nCurve: Clarifying the Best Approach to Calculating Predicted\nProbabilities and Marginal Effects from Limited Dependent Variable\nModel.” American Journal of Political Science 57:\n263–77.\n\n\nHarrell, Frank E. Jr. 2015. Regresion Modeling Strategies. 2nd\ned. New York: Springer.\n\n\n———. 2018. “Why i Don’t Like Percents.” Web log post. https://www.fharrell.com/post/percent/.\n\n\nHeiberger, Richard M., and Naomi B. Robbins. 2014. “Design of\nDiverging Stacked Bar Charts for Likert Scales and Other\nApplications.” Journal of Statistical Software 57 (5):\n1–32.\n\n\nKim, Ji-Hyun. 2003. “Assessing Practical Significance of the\nProportional Odds Assumption.” Statistics &Amp;\nProbability Letters 65 (3): 233–39. https://doi.org/10.1016/j.spl.2003.07.017.\n\n\nLandwehr, James M., Daryl Pregibon, and Anne C. Shoemaker. 1984.\n“Graphical Methods for Assessing Logistic Regression\nModels.” Journal of the American Statistical Association\n79 (385): 61–71. https://doi.org/10.1080/01621459.1984.10477062.\n\n\nLenth, Russell V. 2024. Emmeans: Estimated Marginal Means, Aka\nLeast-Squares Means. https://CRAN.R-project.org/package=emmeans.\n\n\nLi, Chun, and Bryan E. Shepherd. 2012. “A New Residual for Ordinal\nOutcomes.” Biometrika 99 (2): 473–80. https://doi.org/10.1093/biomet/asr073.\n\n\nLiddell, Torrin M., and John K. Kruschke. 2018. “Analyzing Ordinal\nData with Metric Models: What Could Possibly Go\nWrong?” Journal of Experimental Social Psychology 79:\n328–48. https://doi.org/https://doi.org/10.1016/j.jesp.2018.08.009.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited\nDependent Variables. Thousand Oakes, CA: Sage.\n\n\n———. 2014. “Regression Models for Nominal and Ordinal\nOutcomes.” In The Sage Handbook of Regression Analysis and\nCausal Inference, edited by Christof Best Henning & Wolf,\n173–203. London: Sage.\n\n\nLong, J. Scott, and Jeremy Freese. 2014. Regression Models for\nCategorical Dependent Variables Using Stata. College Station, TX:\nStata Press.\n\n\nLong, J. Scott, and Sarah Mustillo. 2017. “Comparing Groups in\nBinary Regression Models Using Predictions.” Working paper. http://www.indiana.edu/jslsoc/files_research/groupdif/long-mustillo-comparing-groups-brm-2017-06-06.pdf.\n\n\nMaddala, S., Gangadharrao. 1983. Limited-Dependent and Qualitative\nVariables in Econometrics. Cambridge: Cambridge University Press.\n\n\nMcKelvey, Richard D., and William Zavoina. 1975. “A Statistical\nModel for the Analysis of Ordinal Level Dependent Variables.”\nThe Journal of Mathematical Sociology 4 (1): 103–20. https://doi.org/10.1080/0022250x.1975.9989847.\n\n\nOsbourne, Jason W. 2015. Best Practices in Logistic Regression.\nLondon: Sage.\n\n\nPeterson, Bercedis, and Frank E. Harrell. 1990. “Partial\nProportional Odds Models for Ordinal Response Variables.”\nApplied Statistics 39 (2): 205–17. https://doi.org/10.2307/2347760.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and\nLongitudinal Modeling Using Stata. College Station,\nTX: Stata Press.\n\n\nSönning, Lukas. 2024. “Ordinal Response Scales:\nPsychometric Grounding for Design and Analysis.”\nResearch Methods in Applied Linguistics 3 (3): xx–. https://doi.org/10.1016/j.rmal.2024.100156.\n\n\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht,\nand Paul Messer. 2024. “Latent-Variable Modelling of Ordinal\nOutcomes in Language Data Analysis.” Journal of Quantitative\nLinguistics 31 (2): 77–106. https://doi.org/10.1080/09296174.2024.2329448.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nVenables, William N., and Brian D. Ripley. 2002. Modern Applied\nStatistics with s. Fourth. New York: Springer.\n\n\nYee, Thomas W. 2015. Vector Generalized Linear and Additive Models:\nWith an Implementation in r. New York: Springer.",
    "crumbs": [
      "References"
    ]
  }
]