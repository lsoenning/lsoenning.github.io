[
  {
    "objectID": "00_intro.html",
    "href": "00_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "The focus of the present study is on corpus-based variationist research, where corpus data are used to shed light on some type of alternation phenomenon. Classic examples of this type of research are the dative, genitive, or comparative alternation in Present-day English.\nThe goal is to work toward a unified approach to the statistical analysis of this type of corpus data. The framework I have chosen is multilevel (mixed-effects) regression, which is one of the most widely used tools for modeling variationist data. As I will try to lay out in the following chapters, it is also a remarkably useful tool since is allows us to summarize data in an insightful way that is capable of closely integrating the researcher’s objectives and our knowledge about the structure of the data. The current treatment elaborates on earlier accounts of multilevel modeling of language data by giving special consideration to these two guideposts of language data modeling.\nOne of the key themes, which is dealt with in Chapter 2, is the role of the researcher’s objectives when analyzing variationist corpus data. It turns out that scientific (and therefore statistical) inferences can be categorized along two dimensions, in terms of their type and scope. Being clear about how the goals of a study relate to these dimensions helps us withe the specification and interpretation of statistical models.\nChapter 3 then describes the way in which statistical models can help us pursue our linguistic goals, and how their form may vary depending on the type and scope of inference. The discussion is supported by illustrative demonstrations that allow us to recognize how alterations to a model affect the meaning and interpretation of statistical uncertainty estimates such as confidence intervals. I will also deal with two fundamental concepts in statistical theory – the methodological device of random sampling and the notion of a population – and lay out their role and relevance in variationist corpus research.\nChapter 4 turns to the second key feature of corpus data, their structural layout. By this, I mean what may be considered a variationist data universal: The tokens, or instances of the structure of interest, are always clustered in the sense that there will be multiple tokens from the same text (speaker or author). Depending on the linguistic structure studied, there may also be multiple tokens per item (word form, lexical item, or lemma). I will refer to this as the structural component of the data and contrast it with the systematic component, which includes the set of predictor variables that are assumed to show an association with the choices speakers make. Borrowing heavily from the literature on the design and analysis of experiments, I will present a simple template that researchers may fill in to recognize the relation between the variables (structural and systematic) in their data. This template then provides guidance for model specification, as it brings into view the components that could in principle be included in a mixed-effects model, and whose exclusion brings with it an additional assumption that the model is forced to make.\nChapter 5 elaborates on the dialogue between model specification and research objectives and pays particular attention to the use of random effects in data analysis. I will discuss the distinction between fixed and random effects, which remains a source of confusion and dispute throughout the language sciences. We will lay out the dimensions along which these two classes have been contrasted, which relate to very different features of empirical research, including study design, characteristics of the variable as such, and the researcher’s objectives. A variable’s status as fixed vs. random may therefore vary across dimensions, which helps us make sense of the confusion in the literature. I will consider fixed effects and random effects as prototypes, which allows us to appreciate where on a continuum a specific variable may be located.\nChapter 6 then turns to the concrete task of specifying a regression model for variationist corpus data. The implications of the template presented in Chapter 4 will be illustrated using a case study on the variable (ING), sometimes referred to as “g-dropping”.\nChapter 7 will deal in more detail with the treatment of what I will refer to as token-level predictors. These are variables that are coded at the level of the individual corpus hits (rather than attributes of higher-level units such as texts of lexical items). Such token-level predictors play a special role in multilevel modeling, since their association with the outcome can be partitioned into what is referred to as a between-cluster and within-cluster component. While this partition is discussed prominently in other fields of study (e.g. research on education), it has – to my knowledge – so far been largely neglected in language research. As I will illustrate, however, this partitioning of the variation of token-level predictors allows us to address informative research questions.\nChapter 8 discusses the potential of mixed-effects models to fruitfully combine two fundamentally different approaches to empirical work, namely what has been referred to as the nomothetic vs. idiographic orientation. The former seeks generalizations, the latter represent a case-study-type approach that looks in depth at a (much) smaller number of subjects. A brief historical digression will show that the relevance of both perspectives has found its way into variationist research, including the more recent use of mixed-effects regression analysis. I will demonstrate how this modeling framework may be used to this end and will draw attention to an important pitfall that does not seem to have received much attention by practitioners.\nChapter 9 is practical in nature and deals with various modeling tactics.\nChapter 10 turns to an important topic in mixed-effects modeling of categorical outcome variables: The different types of model-based predictions (or estimates) on the proportion scale. It discusses the important distinction between what are often referred to as conditional (or cluster-specific) and marginal (or population-averaged) predictions, which is arguably discussed too rarely in the methodological literature on language data analysis. I will also lay out and contrast different ways of adjusting for variables in the model when forming predictions for model interpretation.\nChapter 11 and Chapter 11 present two case studies.\n\n\n…\n\n\nHow can language data be thought about in a systematic way\nTry and see some sort of unity that will be helpful and constructive\nHow can language data be studied quantitatively in a way that advances understanding\nCorpus data is analyzed with a variety of methodological approaches\nMixed-effects regression modeling has emerged as one of the chief methodological approaches.\nWhy research on alternation phenomena is useful [Arppe_etal2010, pp. 13-15]\ncausal inference ought to be an increasingly prominent concern, to which corpus-based analyses do not always pay sufficient heed\nrecognize archetypes\nconsider approaches with a common language\nprototypical analyses\n“too often, framework are siloed within specific disciplines, clouded by domain-specific language”\n“this makes methods harder to discover, and hides their general applicability”\n“it can be hard for practitioners outside of these fields to recognize when the problem they are facing fits one of these paradigms”",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_research_objectives.html",
    "href": "01_research_objectives.html",
    "title": "2  Research objectives",
    "section": "",
    "text": "2.1 Inferences from data\nAn important part of research work involves the use of empirical observation to make inferences. According to Merriam-Webster, the word inference denotes “the act or process of reaching a conclusion about something from known facts or evidence”. In the following, we will distinguish between different kinds of conclusions that are often made by researchers. The kind of inferences that are made depend on the objectives of the researcher.\nFor our present purposes, we will classify inferences along two dimensions, which are shown graphically in Figure Figure 2.1. The horizontal dimension is concerned with the type of inference, largely distinguishing between the numerical description of the observed data (descriptive inference) and the substantive explanation of patterns in the data (analytic inference). The vertical dimension marks the scope of inference, the breadth of validity that is attached to a conclusion – here, we may distinguish between broad and narrow inferences. The endpoints of each continuum list related concepts and ideas. We now take a closer look at the two dimensions.\nFigure 2.1: Classification of inferences, Type of inference (vertical dimension) and scope of inference (horizontal dimension). The diagram is adapted from N. H. Anderson (2001, 9) and complemented with terminology used by different authors to refer to similar notions (see text for details).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research objectives</span>"
    ]
  },
  {
    "objectID": "01_research_objectives.html#sec-inferences",
    "href": "01_research_objectives.html#sec-inferences",
    "title": "2  Research objectives",
    "section": "",
    "text": "2.1.1 Type of inference\n\n\n…\n\n\nLanguage data (Evert 2006; Baroni & Evert 2009)\nDescriptive inference: Extensional view of language – “language as an infinite body of text, comprising all the utterances that have ever been made or will ever be made by the relevant group of speakers” (Evert 2006: 179)\nAnalytic inference: Intensional view of language – Competence of human speakers, the properties of language as a formal system\nE-language vs. I-language (Chomsky 1986: Knowledge of language: Its nature, origin, and use.)\nextrastatistical inference: Wilk & Kempthorne 1955 (cited in Anderson 1961: 312)\n\n\nThe distinction along the vertical dimension contrasts two fundamentally different types of conclusions that can be made based on data. The first kind, which we refer to as descriptive inference, is a statement about some quantity of interest (e.g. a percentage or some other average score). This type of inference focuses on the outcome quantity which may, in certain practical applications, be of direct interest. Descriptive inferences are typical of applied research, where findings often inform real-world decisions and serve as a basis for action. A central concern is therefore the predictive accuracy of the result at hand, i.e. whether we can make sufficiently accurate predictions for unobserved cases or future outcomes. Descriptive inference can rely on statistical theory – tools such as p-values and confidence intervals are statistical uncertainty statements about the inferred or predicted outcome quantity.\nThe second kind of conclusion we can make based on data is an analytic inference. It is not primarily concerned with numbers, but rather with their conceptual interpretation and substantive meaning. We seek a deeper understanding of patterns in the data, and try to offer an explanation for our observations. Interest is therefore in the kind of process or mechanism that generated the data – why these patterns emerge and how they came about. This kind of conclusion, which is necessarily tentative, reflects our scientific interest in the underlying system, or forces, that are mirrored in the observed data. Natural language use – as documented in corpora – reflects language-internal forces (e.g. cognitive constraints, language universals) and language-external ones (e.g. social factors). Analytic inference is therefore concerned with explanation or causation rather than description or prediction, and is typical for basic research with a theoretical orientation. Depending on the research context, the notion of a process may refer to a mechanism or operation in the real world (as in industry) or in the conceptual, theoretical world (such as cognitive constraints). For analytic inferences, statistical theory is of no direct help. Rather, such inferences are extra-statistical – they rely on knowledge of the subject matter.\nLet us consider some linguistic examples of each type. Descriptive inference, with its focus on a measured quantity and (possibly) its predictive accuracy, is typical of certain forms of language data work. For instance, descriptive grammars or dictionaries may include quantitative information on certain questions of language use. Examples are (i) the usage rate of coordinators in initial position (Biber et al. 1999, 84), (ii) the positional distribution of adverbials (Quirk et al. 1985, 501), and (iii) the pronunciation of words (e.g. schedule) across varieties and age groups (Wells 2008, 717). Other areas that rely on descriptive inference include applied linguistics, e.g. the comparative evaluation of teaching methods, and certain domains of natural language processing, e.g. the development of automatic taggers or translation tools. In all these applications, the interest is restricted to the outcome quantity (e.g., for schedule, the preference of /sk-/ /∫-/, expressed as a percentage) or the predicted outcome (e.g. the most likely part-of-speech tag for a given form).\nAnalytic inference, on the other hand, is typical of work with a theoretical orientation. Interest does not primarily center on the specific outcome quantity, but on a deeper understanding of the patterns we see. The focus is on linguistic interpretation. For instance, we may be interested in working towards an explanation for distributional patterns of variants, i.e. different ways of saying the same thing. Examples are the dative, genitive, or comparative alternation in English. We might be interested in whether certain contextual constraints, such as constituent length or animacy, may be operative and incline the speaker towards one variant over the other. Further examples include the psycholinguistic notions of frequency effects (e.g. type and token counts as clues to the differential productivity of morphological patterns) and complexity effects (e.g. the use of support strategies to alleviate working memory load).\nThe distinction between descriptive and analytic inference serves to highlight two important points for empirical research. The first concerns their discordant nature – they are very different research goals. Obviously, both have their merits, and it would seem that, in certain research settings, we would like to pursue both objectives. As N. H. Anderson (2001) [p. 10-11] notes, however, descriptive and analytic inferences require different strategies in study design and data analysis. Analytic inferences rely on restriction and simplification strategies, to isolate the process of interest. A typical example is an experimental study eliciting responses under tightly controlled, artificial circumstances. Descriptive inference, on the other hand, aims for “realistic” observation that is representative of the real-world settings to which research findings are to be applied. The important point is that aiming for both types of inference requires the researcher to make compromises, which may lead to fuzzy inferences. As noted by N. H. Anderson (2001, 10), “[a]ttempts to achieve both goals are likely to achieve neither” (see also Sidman (1960), p. 194]. The second essential point that is clarified by the distinction between descriptive and analytic inferences is the fact that they rely on very different sources of information and reasoning aids. While descriptive inferences can be formed using statistical procedures, the kind of analytic inferences that are typical of the cognitive and behavioral sciences rest almost entirely on extra-statistical grounds. There are no statistical tests or procedures that allow us to attach uncertainty intervals to our inferences. These kinds of research conclusions hinge on experience and judgement – they are backed by knowledge of the subject-matter and research area.\nCorpus data and analytic inferences\nThe question of whether corpus data can be used to make inferences on speakers’ underlying grammars has received some attention in the literature (e.g. Arppe et al. 2010; Baayen and Arppe 2011; Divjak, Dąbrowska, and Arppe 2016; Divjak and Arppe 2013)\nIn variationist corpus analyses, regression analyses produce a set of probabilistic weights that attach to the predictors studied.[^We will use the label predictor or factor to refer to a variable that is assumed to show an association with the outcome. A predictor or factor can be continuous or categorical. Our usage of the label “factor” contrasts with the terminology used in variable rule analysis, where a categorical factor (or predictor) is referred to as a “factor group”, the label “factor” being used to denote the individual categories. In accordance with standard statistical terminology, these categories will here be referred to as predictor or factor levels .] A key question for linguistic interpretation is what meaning should be given to these probabilistic weights. We may contrast two extreme views on the matter: the naïve and the skeptical view. First, the naïve view, understands the set of probabilistic weights as a direct reflection of the underlying grammar. Regression coefficients are then assumed to have directs analogues in the mind. This reification of regression coefficients was perhaps characteristic of early variationist research (e.g. Cedergren and Sankoff 1974). It was criticized early on by Bickerton (1971, 461), who noted the need to name a “recognizable mental process”. This criticism was readily adopted by variationists. Sankoff (1978, 235–36) concedes that “the regularities and tendencies modeled by these probabilities are of different kinds and come from different sources”, and that the probabilistic weights are “simply quantitative generalizations”, “analytical abstractions rather than components of language” (Sankoff 1978, 235). Likewise, Sankoff and Labov (1979, 217) state: “we do not make the error of confusing the set of rules we write with the grammatical processes that people use.”\nThe skeptical perspective, on the other hand, questions the cognitive plausibility of these probabilistic weights, on the grounds that the way they are derived from data is at odds with principles of human cognition and learning.\n\n\n…\n\n\n“how to understand the statistical results from a cognitive perspective” (Baayen and Arppe 2011, 8)\ncharacteristics of mental grammars\nlinguistic cognitive processes\nlinguistic knowledge that is represented in the brain\n“characteristics observable in language usage reflect characteristics of the mental processes and structures yielding usage, even though we do not know the exact form of these mental representations”\nposited underlying language system governing usage\nDivjak and Arppe (2013, 229–30, 235–37)\nKlavan and Divjak (2016) consider the cognitive plausibility of corpus-based models. They consider a model to be cognitively plausible if it shows the same predictive ability as humans. This is a very global assessment and fails to consider deeper questions, such as whether the prediction weights bear any realism.\nUniformity of mental grammars? Divjak, Dąbrowska, and Arppe (2016, 27) note that due the characteristic multicollinearity of variables (and resulting redundancy), speakers may in fact have internalized different grammars that nevertheless arrive at the same production behavior. (some references to be found there)\n\n\n\n\n2.1.2 Scope of inference\nA second dimension along which inferences vary is their scope, which refers their range of validity. Studies are typically based on a limited set of entities, i.e. a sample. The basic distinction here is whether interest is restricted to the sample of entities at hand, or whether the aim is to extend the breadth of interpretation beyond the cases at hand. The scope of inference, sometimes referred to as th inference space (V. E. Anderson and McLean 1974, 57) Accordingly, we may distinguish between narrow inference and broad inference1. The label “entities” can refer to different, sometimes co-occurring classes of units in our study – speakers/texts, words, languages, varieties, or (micro)genres. Within a single study, the intended scope of inference may therefore vary for different classes of units.\n1 We adopt these terms from (McLean, Sanders, and Stroup 1991; see also Stroup 2013, 70, 90–99)We will use the term narrow inference to refer to situations where inferences are made only concerning the set of entities studied. Thus, we could decide to restrict our statements to the particular set of genres represented in our data, to the selection of words we happened to obtain, or even to the specific set of individuals we observed. Our statistical conclusions would then be specific to this narrow set. The entities that form the target of narrow inference assume a special status, since our deliberate restriction in scope can sharpen our knowledge about these individual units. A focus on individual speakers (or writers) corresponds to an idiographic research perspective Windelband (1998). Since language resides in the individual speaker’s mind, there is clearly some merit to this approach to linguistic study. From a statistical perspective, the modesty of our inferential ambitions is awarded with more accurate estimates, i.e. narrower uncertainty bounds around our key quantities.\nThe term broad inference, on the other hand, will be used to refer to situations where we use our sample as a stepping stone to make statements about the populations of entities that this sample represents. We then make generalizations based on our sample data. Questions of external validity then arise – that is, to what extent can we extrapolate these findings to unobserved entities. The individual entities are then used to form summary measures (such as averages), which are then used as estimates of population features. This entails a shift from an idiographic to a nomothetic perspective Windelband (1998), a more general level of description, which averages over individual and unique entities. Statistical theory offers tools to compute and express the additional uncertainty resulting from the extrapolation to unobserved entities. However, the adequacy of these methods hinges on certain requirements. Thus, statistical theory only offers guarantees of adequate performance if the sample of entities at hand is a random sample from the underlying population. Since this is rarely the case in everyday research, broad inference from non-random samples is to some (perhaps a considerable) extent extra-statistical and therefore relies on knowledge about the research area.\nThe distinction between broad and narrow inference highlights two aspects of empirical research. First, how wide to span the inference space is a decision made by the researcher. Broad inference is an optional move: We can deliberately restrict the intended level of generality of our research findings. This may be a good call if the data at hand do not offer the type and/or amount of information that is necessary to derive adequate uncertainty bounds for broad-inference estimates. Thus, in sparse data settings, broad inference yields but a very blurry picture of generalized patterns, perhaps too blurry to be of any use. In other settings the data may not include information about certain sources of variability that would be needed to determine the uncertainty surrounding our statistical projections. Sometimes, then, the data provide a weak or unreliable inductive base for broad inference, which would prompt a circumspect researcher to backtrack and rely on narrow-inference estimates. It is also not difficult to imagine settings where both types of estimates may be combined. The second aspect of empirical research that is highlighted by the inference space is the difference between statistical and extra-statistical sources of information for drawing inferences.\n\n\n2.1.3 Type and scope of inference: Co-occurrence patterns\nThere is a tendency for the type and scope of inference to show an association in particular research settings. Typical connections are shown in Figure Figure 2.2). Basic research, with its purely scientific, theory-oriented approach aimed at understanding causal systems, shows a tendency to resort to narrow inference (Mook 1983; Cox 1958, 10–11). Methodologically, this research style relies on experimentation and randomization schemes for the isolation of causal forces. Applied research, on the other hand, serves as a basis for action or decisions in the real world. It shows a more pronounced interest in the representativeness of the observations under study and in the applicability of conclusions to new conditions and circumstances. Statistical techniques that feature prominently in this branch of science are survey sampling and random sampling schemes.\n\n\n\n\n\n\n\n\n\nFigure 2.2: Co-occurrence tendencies of type and scope of inference.}\n\n\n\n\n\nSee also Rabe-Hesketh and Skrondal (2021), p. 100-102: In the survey-sampling literature, analytic inferences about the data-generating mechanism are referred to as superpopulation inference (or infinite population inference) since even an exhaustive sample still represents only a sample from the data-generating model. Descriptive inferences about finite population parameters only need to take into account sampling variability. Such inferences take into account design features and are therefore called design-based inferences. In contrast, model-based inferences target parameters of the data-generating mechanism. Note however, that the mechanism does not refer to a genuine scientific causal mechanism, rather a descriptive mechanism.\n\n\n\n2.1.4 Type and scope of inference: Literature\nDiscussions of different types and scopes of inference appear in many places throughout the literature. Figure Figure 2.1) is adapted from N. H. Anderson (2001, 9). He refers to the illustration as a validity diagram and labels the vertical ends “outcome” and “process”, and the horizontal ends “internal” and “external”. This distinction is helpful since it distinguishes two conceptually distinct dimensions, which are sometimes mixed to varying degrees by other writers.\nThus, in the domain of industrial experimentation and quality control, Deming Deming (1975) introduced the distinction between enumerative and analytic studies, the vertical dimension in Figure Figure 2.1).2 This distinction was formulated with applied contexts in mind, where research findings serve as a basis for action – in the words of Mook (1983, 380) “a real-life setting in which one wants to know what to do”. Examples are medicine, public service and industry. A criterion that helps distinguish between enumerative and analytic tasks is the notion of a 100% sample (Deming 1975, 147). If we were able to do an exhaustive analysis of the population of interest, we would have the perfect state of information about the population. If our interest were, instead, in a process that produced certain patterns in the data (and the population), our knowledge of this process would still be incomplete. Deming Deming (1975) does not distinguish between internal and external validity; rather, the focus is on applications where the aim is to generalize (see also Gitlow et al. 1989).\n2 Hahn and Meeker (1993), p. 4 take a broader view of analytic studies as research that “is not dealing with a finite, identifiable, unchanging collection of units, and, thus is concerned with a process, rather than a population”.In the field of psychology, on the other hand, large parts of the discussion center on the distinction between a process-based and a population-based interpretation of statistical inference Frick (1998), again referring to the vertical dimension in Figure Figure 2.1). Here, analytical inferences (process model) are linked to questions of internal validity, whereas descriptive inference (population model) are intertwined with questions of external validity.\n\n\n2.1.5 Implications for language data analyis\nHow do these settings differ from typical corpus-linguistic applications? Most corpus-based work is concerned with learning about language, its use and acquisition. That is, the interest is typically theoretical. Our findings do not inform real-world decisions. Rather, the data we collect serve to provide some insight into the workings of language. This is not to say that generalizability to a specific population is irrelevant. We do, however, need to think about which kind of generalization we wish to make.\nObservational studies fall in between the two poles (Kish 1987, 20)\n\n\n2.1.6 Statistical vs. extra-statistical reasoning\nWe have just distinguished different kinds of inferences that research workers are often interested in making. Let us now summarize to which extent inferences can be made on statistical grounds.\nIt turns out that statistics is relevant only for certain inferential tasks. Analytic inferences, for instance, are outside the purview of statistical methods. While quantitative tools may provide aids for the reasoned interpretation of data, there is no way of attaching statistical uncertainties to the truth value of a scientific explanation. Even experiments do not produce genuinely analytic inferences. They allow us to infer causal relations, which, however, remain at a descriptive level. A distinction is therefore sometimes made between causal description and causal explanation. In general, then, analytic inferences rest on subject-matter grounds.\nDescriptive inferences, on the other hand, can be made on statistical grounds. Narrow inferences are restricted to the set of conditions and contexts actually observed, and the data at hand therefore (usually) provide ample grounds for narrow descriptive inferences. In making broad inferences, on the other hand, we wish to extend our conclusions to unobserved settings and circumstances. The degree to which the data at hand can buttress these extrapolations cannot be answered on statistical grounds. It is a matter of judgement and hinges on our knowledge of the research area and the empirical experience we have accumulated (Sidman 1960, 59; MacKay and Oldford 2000, 276).\n\n\n\n\n\n\n\n\n\nFigure 2.3: Scope and type of inference: Statistical vs. extrastatistical reasoning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research objectives</span>"
    ]
  },
  {
    "objectID": "01_research_objectives.html#sec-generalization",
    "href": "01_research_objectives.html#sec-generalization",
    "title": "2  Research objectives",
    "section": "2.2 Generalization",
    "text": "2.2 Generalization\n\n2.2.1 Generality of what?\nfrom Sidman (1960: 46-47) - Which aspect of the data does one wish to test for generality? - With which features of the data are we particularly concerned? - Generality of what? - Which type of generality do we wish to determine? - Directionality of effect - Functional shape, shape of distribution - Quantitative values\nthe current state of development of the science.\n\n\n2.2.2 Levels of generality\nSince generalizations can be made at different levels of descriptions and analysis, the distinction between broad and narrow inference can be made at different levels. Figure Figure 2.4) (adapted from Runyan (1982, 7)) provides a sketch of certain layers of generality.\nGeneralizations applying to specific individuals. Let us start at the bottom by first considering the individual speaker. Thus, we may obtain a certain amount of spoken or written material produced by this individual. This excerpt is a sample from their language use, and may serve as a basis for inferences about the general patterns of language use, or the mental grammar, of this speaker. At this level of analysis, broad inference would target the linguistic system of this individual, from which we have observed a limited amount of output.\n\n\n\n\n\n\n\n\nFigure 2.4: Broad vs. narrow inference for speakers, speaker groups and communicative situations: Levels of generality. Adapted from Runyan (1982, 7).\n\n\n\n\n\nGroup-specific generalizations. At a higher level of analysis, we might be interested in the linguistic behavior of a particular socio-demographic subgroup, say, the speech of London teenagers. Let us assume we have data from 15 speakers. Broad inferences are generalizations to this speaker group, and the 15 individuals serve to inform our summary measures about this population, i.e. London teenagers in general. If we instead decide to restrict the generality of our statements to the 15 speakers at hand, this would constitute a shift to narrow inference.\nLanguage-specific generalizations. Inferences may aim for even broader levels of validity, for instance a particular language. For Present-Day spoken British English, for instance, socio-demographic subgroups would then be considered as a sample, informing conclusions at the more general level. For written language use, on the other hand, individual genres could be considered as a sample of a larger population. Depending on the focus of our study, our interest may center on these broad inferences, i.e. average trends across subgroups or genres; however, our linguistic attention could also be focused on these lower-level units, in which case we would be interested in making narrow inferences.\nUniversal generalizations. We can pursue still broader generality by trying to identify language-universal features. Such generalizations across languages would concentrate on features common to all natural languages. These may result from shared functional-cognitive constraints, or, depending on the theoretical viewpoint adopted, innate architectures.3 At this highest level of linguistic generality, a sample consists of a set of languages, which inform our generalizations.\n3 Note that the question of what gives rise to commonalities between languages are answered by analytic inferences.As these examples illustrate, the scope of inference is relative; it depends on the descriptive level at which generalizations are advanced. Regularities may be found at each level, and our research objectives identify the tier at which (or to which) we wish to generalize. Lower-level units then usually form the sample that informs higher-level, broad-inference statements. The conceptual relationship between sample and population therefore surfaces at different elevations in Figure Figure 2.4), and the epistemological status of a given unit (i.e. grey box), depends on the linguistic question being asked. A specific language, for instance, may be a broad-inference target, or it could be considered as a unit of analysis that informs generalizations at a still broader level of inference.\nSo far we have considered generality with regard to speakers, speaker groups, and communicative situations. The scope and levels of inference were oriented towards language-external units of description and analysis. A similar hierarchy may be stipulated for language-internal entities. Figure Figure 2.5) shows increasing levels of generality based on lexical units as the lowest level of generality. The ideas and relationship we have discussed for language-external tiers of generality directly apply to this language-internal system.\n\n\n\n\n\n\n\n\nFigure 2.5: Broad vs. narrow inference for lexical structures: Levels of generality. Inspired by Runyan (1982, 7).\n\n\n\n\n\nTo recapitulate, there are different levels at which identify regularities and advance generalizations. Two pyramidal constellations, with different altitudes of generality, were sketched: (i) a language-external one, resting on the individual speakers as the lowest-level units, and (ii) a language-internal one , with lexical units constituting the base of the hierarchy. As we move upwards, we extend the scope of inference. Moving the target of inference brings about changes in what exactly constitutes sample vs. population. The distinction between broad and narrow inference, then, is only meaningful if we are clear about the level of generality, both language-externally (Figure Figure 2.4)) and -internally (Figure Figure 2.5)).\nEvaluate Shadish et al.’s 4-step ladder of validity\n\n\n2.2.3 Generalizability Theory\nSome basic notions from generalizability theory will be helpful.\n\nUniverse\nFacet",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research objectives</span>"
    ]
  },
  {
    "objectID": "01_research_objectives.html#sec-severe-testing",
    "href": "01_research_objectives.html#sec-severe-testing",
    "title": "2  Research objectives",
    "section": "2.3 Statistical inference as severe testing",
    "text": "2.3 Statistical inference as severe testing\n\nSimple tools: If little or nothing has been done to rule out flaws in inferring a claim, then it has not passed a severe test.\nProbability as a measure of how capable methods are at uncovering and avoiding erroneous interpretations of data. That’s what it means to view statistical inference as severe testing.\nClaims may be probable, but terribly tested by the data at hand.\n\n\n\n\n\nRoland Schäfer is currently working on a statistics textbook for linguists revolving around the notion of severe testing.\n\n\nAllport, Gordon W. 1937. Personality: A Psychological Interpretation. New York: Henry Holt.\n\n\nAnderson, Norman H. 2001. Empirical Direction in Design and Analysis. Mahwah, NJ: Lawrence Erlbaum.\n\n\nAnderson, Virgil E., and Robert A. McLean. 1974. Design of Experiments: A Realistic Approach. New York: Marcel Dekker.\n\n\nArppe, Antti, Gaëtanelle Gilquin, Dylan Glynn, Martin Hilpert, and Arne Zeschel. 2010. “Cognitive Corpus Linguistics: Five Points of Debate on Current Theory and Methodology.” Corpora 5 (1): 1–27. https://doi.org/10.3366/E1749503210000341.\n\n\nBaayen, R. Harald, and Antti Arppe. 2011. “Statistical Classification and Principles of Human Learning.” In Proceedings of Quantitative Investigations in Theoretical Linguistics 4 (QITL-4), 8–11. Berlin: Humboldt-Universität zu Berlin. https://doi.org/10.1145/2858036.2858558.\n\n\nBiber, Douglas, Stig Johansson, Geoffrey Leech, Susan Conrad, and Edward Finegan. 1999. Longman Grammar of Spoken and Written English. Harlow: Pearson Education.\n\n\nBickerton, Derek. 1971. “Inherent Variability and Variable Rules.” Foundations of Language 7 (4): 457–92. http://www.jstor.org/stable/25000558.\n\n\nCedergren, Henrietta J., and David Sankoff. 1974. “Variable Rules: Performance as a Statistical Reflection of Competence.” Language 50 (2): 333–55. https://doi.org/10.2307/412441.\n\n\nCox, David R. 1958. Planning of Experiments. New York: Wiley.\n\n\nDeming, W. Edwards. 1950. Some Theory of Sampling. New York: Wiley.\n\n\n———. 1953. “On the Distinction Between Enumerative and Analytic Surveys.” Journal of the American Statistical Association 48 (262): 244–55. https://doi.org/10.2307/2281285.\n\n\n———. 1975. “On Probability as a Basis for Action.” The American Statistician 29 (4): 146–52. https://doi.org/10.2307/2683482.\n\n\nDivjak, Dagmar, and Antti Arppe. 2013. “Extracting Prototypes from Exemplars: What Can Corpus Data Tell Us about Concept Representation?” Cognitive Linguistics 24 (2): 221–74. https://doi.org/10.1515/cog-2013-0008.\n\n\nDivjak, Dagmar, Ewa Dąbrowska, and Antti Arppe. 2016. “Machine Meets Man: Evaluating the Psychological Reality of Corpus-Based Probabilistic Models.” Cognitive Linguistics 27 (1): 1–33. https://doi.org/10.1515/cog-2015-0101.\n\n\nFrick, Robert W. 1998. “Interpreting Statistical Testing: Process and Propensity, Not Population and Random Sampling.” Behavior Research Methods: Instruments & Computers 30 (3): 527–35. https://doi.org/10.3758/BF03200686.\n\n\nGitlow, Howard, Shelly Gitlow, Alan Oppenheim, and Rosa Oppenheim. 1989. Tools and Methods for the Improvement of Quality. Boston: Irwin.\n\n\nHahn, Gerald J., and William Q. Meeker. 1993. “Assumptions for Statistical Inference.” The American Statistician 47 (1): 1–11. https://doi.org/10.2307/2684774.\n\n\nKish, Leslie. 1987. Statistical Design for Research. Hoboken, NJ: Wiley.\n\n\nKlavan, Jane, and Dagmar Divjak. 2016. “The Cognitive Plausibility of Statistical Classification Models: Comparing Textual and Behavioral Evidence.” Folia Linguistica 50 (2): 355–48. https://doi.org/10.1515/flin-2016-0014.\n\n\nMacKay, R. Jock, and R. Wayne Oldford. 2000. “Scientific Method, Statistical Method and the Speed of Light.” Statistical Science 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nMcLean, Robert A., William L. Sanders, and Walter W. Stroup. 1991. “A Unified Approach to Mixed Linear Models.” The American Statistician 45 (1): 54–64. https://doi.org/10.2307/2685241.\n\n\nMook, Douglas G. 1983. “In Defense of External Invalidity.” American Psychologist 38 (4): 379–87. https://doi.org/10.1037/0003-066X.38.4.379.\n\n\nQuirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. London: Longman.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.\n\n\nRunyan, William M. 1982. Life Histories and Psychobiography: Explorations in Theory and Method. New York: Oxford University Press.\n\n\nSankoff, David. 1978. “Probability and Linguistic Variation.” Synthese 37 (2): 217–38. https://www.jstor.org/stable/20115257.\n\n\nSankoff, David, and William Labov. 1979. “On the Uses of Variable Rules.” Language in Society 8 (2): 189–222. https://doi.org/10.1017/S0047404500007430.\n\n\nSidman, Murray. 1960. Tactics of Scientific Research: Evaluating Experimental Data in Psychology. New York: Basic Books.\n\n\nSohn, David. 1992. “Knowledge in Psychological Science: That of Process or of Population?” The Journal of Psychology 126 (1): 5–16. https://doi.org/10.1080/00223980.1992.10543336.\n\n\nStroup, Walter W. 2013. Generalized Linear Mixed Models: Modern Concepts, Methods and Applications. Boca Raton: CRC Press.\n\n\nThomae, Hans. 1999. “The Nomothetic-Idiographic Issue: Some Roots and Recent Trends.” International Journal of Group Tensions 20 (1/2): 187–215. https://doi.org/10.1023/a:1021891506378.\n\n\nWells, John C. 2008. Longman Pronunciation Dictionary. Harlow: Pearson Longman.\n\n\nWindelband, Wilhelm. 1894. “Geschichte Und Naturwissenschaft.” In Rektoratsreden Der Universität Strassburg, 193–208. Strassburg: Heitz und Mündel. https://doi.org/10.11588/diglit.20767.\n\n\n———. 1998. “History and Natural Science.” Theory & Psychology 8 (1): 5–22. https://doi.org/10.1177/0959354398081001.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Research objectives</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html",
    "href": "02_statistical_inference.html",
    "title": "3  Statistical inferences",
    "section": "",
    "text": "3.1 Relation to scientific objectives\nLet us start by clarifying the supplementary function of statistical inferences in the context of our scientific objectives (see Figure 2.1)). First, statistical inferences are descriptive inferences: They produce uncertainty statements on the outcome scale. We will therefore use the terms “statistical inference” and “descriptive inference” interchangeably. As for the second dimension, statistical inferences allow us to generalize beyond the observed set of units, and are therefore geared towards broad inference. Recall, however, the different levels of generality illustrated in Figure 2.4 and Figure 2.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html#rationale",
    "href": "02_statistical_inference.html#rationale",
    "title": "3  Statistical inferences",
    "section": "3.2 Rationale",
    "text": "3.2 Rationale\nTo illustrate the rationale behind statistical inference, let us consider the ideal setting: There is a well-defined, large population about which we wish to make inferences. Of interest is a certain feature of this population, which can be expressed quantitatively, say, using a percentage. If we were able to study the entire population, this quantity, called the population parameter, would be known with certainty. However, we can only study a much smaller subset of the population. We will discuss two settings. In the first, the population is unstructured in the sense that, for the inferential task at hand, there are no relevant groups, or sub-populations. In the second setting, the population can be partitioned into meaningful subgroups – we first select groups and then sample from these groups.\n\n3.2.1 Sampling from a population\nLet us first consider the unstructured setting. To estimate the population parameter, we draw a random sample, say, of size 80. Random sampling means that each unit in the population has the same probability of being selected. This sample of 80 units then forms the basis of our statistical inferences about the underlying population. We calculate the quantity of interest using our sample data. This yields a sample statistic, our best guess at the population parameter.\nInferential statistics is based on the following idea: While we have ended up with our specific sample, we could, in principle, have obtained a different set of observations from the population. In that sample, the percentage (our estimate) would have been different. This sample-to-sample variation leads to uncertainty surrounding our sample statistic: Our sample-based estimate will differ from the population parameter due to sampling variation and sampling error.\nSampling variation can be illustrated using simulation. We first need to define a population and a population parameter (e.g. a percentage). From this population we then repeatedly draw samples of size 80 and observe how the sample-based estimates fluctuate around the true value, i.e. how much they are in error. Figure 3.1 shows a simulated sampling setting, with the population parameter set to 1/3 (or 33%). We draw 100 samples from this population. The estimated proportions, each based on 80 observations, seesaw around the population parameter of 1/3, which is marked with a grey horizontal line. The dot diagram at the right margin collects these 100 estimated proportions and shows their distribution around the target parameter. Panel (b) shows the same simulation setting, but with smaller sample sizes (20 instead of 80). With smaller samples, we observe greater variation among estimates. On average, the estimates are further from the true parameter.\nEach point estimate in Figure 3.1 comes with an uncertainty estimate, in the present case a 95% confidence interval. Confidence intervals are a statistical tool that can be used to express the uncertainty surrounding a sample statistic. For the present data setting, where the outcome is binary and the estimated quantity is a proportion, statistics offers methods for computing confidence intervals. This method is designed to perform as shown in Figure 3.1): In the long run, 95% of the intervals it produces include the target parameter. However, statistical theory guarantees this performance only if the samples are drawn at random from the underlying population.\n\n\n\n\n\n\n\n\nFigure 3.1: Illustration of random sampling, sampling distributions, and confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html#sec-stat-inf-sample-struc-pop",
    "href": "02_statistical_inference.html#sec-stat-inf-sample-struc-pop",
    "title": "3  Statistical inferences",
    "section": "3.3 Sampling from a structured population",
    "text": "3.3 Sampling from a structured population\nLet us now consider a situation where we have not one set of independent units, but rather observations that are grouped in some meaningful way. These groups of data points could represent different categorical features. They could be different genres, different speakers, different semantic classes of nouns, or different words. These variables differ in terms of the number of existing groups, or levels of the grouping variable. The set of semantic classes for nouns is finite, and a study could include all levels of this variable. The number of genres, on the other hand, while finite, is much larger. A study is unlikely to be able to include even a near-exhaustive collection of genres. The same is true for words. Finally, the set of speakers is usually even larger.\nThese classificatory variables also differ in terms of the amount of research interest that attaches to the individual levels that are included in a study. Thus, each semantic class will be of interest, as well as comparisons among them. The same is usually true for genres. As for individual words, the researcher only sometimes attends to the individual forms under investigation. Finally, particular speakers are usually not of direct interest.\nIn terms of the horizontal dimension in Figure 2.1, then, we usually wish to generalize across speakers – they usually serve to inform broad-inference estimates. For semantic classes, on the other hand, our concern is with the set at hand, and generalizing beyond a fixed set of semantic categories might be linguistically meaningless. Depending on the focus of a study, genre and word fall somewhere in between these two poles – words perhaps leaning towards the broad-inference pole, and genres to the narrow-inference end.\nWe will therefore distinguish two canonical cases. In the narrow-inference case, the set of groups under study is of interest in itself. We do not intend to generalize beyond the observed groups, to other unobserved groups. This could be because the set of groups in our sample is exhaustive, and generalization may be therefore be meaningless; or perhaps we have the deliberate decision to restrict our focus to the subset at hand. In the narrow-inference case, we refer to the groups as strata. In the other setting, the broad-inference case, generalization beyond the set of groups observed is a primary concern. This would be the case if the set of groups is a small fraction of a much larger population of groups. In the broad-inference case, we will refer to the groups as clusters.\n\n3.3.1 Stratified sampling\n\n\n\n\n\n\n\n\nFigure 3.2: Illustration of stratified sampling.\n\n\n\n\n\n\nillustrates stratified sampling. There are 5 groups, or strata, and for each simulation run, we sample 100 observations from each stratum. Groups are distinguished using different colors, and the values produced by a single simulation run are connected by a thin vertical line. Each stratum has its own underlying population parameter: .30 (grey), .35 (blue), .50 (red), .60 (green), and .75 (yellow). Similar to the sampling variation shown in Figure 3.1), there is sample-to-sample variation around these population parameters. The dot diagrams in the far right panel (c) show, for each stratum, the distribution of simulated sample statistics. The dot diagram in panel (b) collapses these into a single distribution of estimates.\n\nIn panel (a), estimates of the overall population average, i.e. the simple average over the five strata is shown using black filled circles. The average proportion over the five sub-population parameters is .50. The estimates hover tightly around this true value, and the statistical uncertainty, as indicated by the error intervals, is quite small. These uncertainty intervals are narrow-inference intervals: They refer only to the five strata and statistical inferences therefore also extend only to these five groups – in this case, the simple average over the five groups.\n\n\n3.3.2 Cluster sampling\n\n\n\n\n\n\n\n\nFigure 3.3: Illustration of cluster sampling.\n\n\n\n\n\nFigure 3.3 illustrates cluster sampling. Each simulation run samples 100 observations from each of five groups, or clusters – similar to the setting just discussed. In contrast to stratified sampling, however, we do not repeatedly sample from the same five groups. Instead, the groups themselves are also a random sample from a much larger population of groups. This means that there is now sampling variation at two levels: From each group that is selected, we draw a random sample of 100 observations. This leads to the kind of sample-to-sample variation that we saw in Figure 3.1), and which also appear is 1), in the for of the colored dot diagrams in panel (c). For cluster sampling, there is a second source of sampling variation: The five groups are selected from a population of groups. This population of groups has a true underlying population average (also set to .50 in the current example), and a dispersion parameter indicating the variation among groups (here a standard deviation of 0.65 on the logit scale). The amount of variation among groups in the population is indicated in panel (b), which collects all cluster estimates. Drawing five clusters from this population will produce considerable fluctuation around the population parameter of .50. This is evident from the much more lively vacillation of the simulated group sets, which are again connected by a thin vertical line, as well as the variation of the averages based on the simulated cluster sets.\nThe uncertainty intervals in panel (a) are narrow-inference estimates, and therefore of the same type as those shown in 1). These error intervals fail to provide adequate coverage of the true population parameter of .50. Each interval does provide a valid uncertainty indication for the 5 clusters that happened to occur in a particular simulation run. For statistical inferences reaching beyond these five units, the error intervals are not adequate, since they do not take into account the extra sampling variation that is due to the fact that our 5 groups are also a random sample from a population of groups. Panel (c) shows the same point estimates as panel (a), but with uncertainty intervals adjusted from this additional source of variation. These are broad-inference estimates.\n\n\n3.3.3 Cluster sampling: Estimating averages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 Cluster sampling: Estimating differences",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html#random-sampling-as-a-methodological-device",
    "href": "02_statistical_inference.html#random-sampling-as-a-methodological-device",
    "title": "3  Statistical inferences",
    "section": "3.4 Random sampling as a methodological device",
    "text": "3.4 Random sampling as a methodological device\n\n3.4.1 The role of random sampling\nThe statistical theory underlying the calculation of uncertainty estimates rests on certain idealizations. Thus, one critical assumption is that the sample at hand is a random sample from the population of interest. A sample can be considered random if each unit in the population has the same probability of being selected. This random, equal-probability feature guarantees, in the long run, the absence of systematic distortions in the resulting samples and thereby avoids systematic musrepresentation of the population. It means that the kinds of samples obtained via this process are, on average, unbiased in the sense that they do not over- or underrepresent certain parts of the population. The sample does not differ in some systematic way from the population. This does not mean that the random subset we obtain can be considered a miniature version of the target population. It just means that there is no systematic discrepancy.\nIn practice, random sampling is a methodological device. It is a data collection strategy that avoids biased samples – there is no reason to question the regresentativeness of a random sample. It allows the researcher to justify generalizations to the population on statistical grounds, and take statistical inferences at face value. It is a long-run guarantee of valid descriptive inferences. It also guards against methodological criticism.\nOnly in such idealized settings, then, can the researcher appeal to statistical theory to justify generalizations from sample to population. These idealized settings are captured in the set of assumptions underlying a statistical procedure – random sampling is one of these assumptions. From the viewpoint of statistical theory, these assumptions are prerequisites to valid descriptive inferences. Statistical theory can provide long-run guarantees only if prerequisites are met.\n\n\n3.4.2 Relation to inference types\nUnbiased samples are essential for descriptive inference and broad inference. They are less essential for analytic inference and narrow inference.\nMook (1983) Mook (1982) Cox (1958)\nHowever, we need to carefully weigh the pros and cons of this procedural decision (Mook 1982, 33–38).\n\n\n3.4.3 Inferences from non-random samples\nBeyond these idealized circumstances, statistical theory does not provide safeguards against erroneous inferences. Whenever the data at hand depart from the assumptive ideal, extra-statistical knowledge must be brought to bear on the inferential task. The question of how much meaning we may attach to statistical inferences is a matter of judgement. The researcher must decide to which extent the prerequisites to (or assumptions for) valid inferences are met and what consequences imperfections have for our linguistic conclusions. Such judgement is necessarily based on incomplete knowledge. As (Cobb 1998, 385) notes: “Then (as always), you can cross your fingers and substitute an assumption for the information you wish you had, being careful to remind yourself that the more untested assumptions you make, the more tentative your conclusions should be.”\nWhen working with non-random samples, the important question is whether the sampling procedure that was used has given rise to an unrepresentative sample. A sample is unrepresentative if it differs from the population in terms of relevant factors (Bottai 2014, 229), that is, factors to which the outcome quantity is sensitive. The question of whether the sampling process was biased in some way can only be answered by knowledge about the sampling method used, the target population and relevant factors. If we genuinely conclude that, to the best of our current knowledge, our sample does not suffer from systematic distortions, we can consider the set of observations at hand as representative of the target population. Given this working state of knowledge, methods of statistical inference may be applied. In this context, Fisher (1956, 33) talks of “necessary ignorance” as a prerequisite to descriptive inferences. As (Johnstone 1989, 449, emphasis in original) notes, “[h]owever, this ignorance must be bona fide. It remains a cardinal methodological sin to suppress or ignore relevant information, either actively or by leaving particular stones unturned.”\nIn the Bayesian approach to statistical inference, the assumption of random sampling corresponds to the notion of exchangeability (Lindley and Novick (1981); Draper et al. (1993)], which is the assumption of similarity (and therefore, for practical purposes the interchangeability) of observed units in the sample and the unobserved units in the population. A biased sampling process would lead to dissimilar, non-exchangeable sets of entities.\nIn cases where we discern differences between our sample and the target population, we are unable to produce valid inferences for the population of interest. In such cases, we first need to clearly state the discrepancies. The sample will then not represent the entire target population, but a particular subset of it. This subset is often called the sampled population (Cochran 1983, 4) or the ad-hoc population (Mook 1982, 37), and it is the conceptual population to which we may extend our descriptive inferences.\nBefore we consider questions of random sampling in the context of corpus data analysis, let us briefly summarize the main points so far. Random sampling is helpful methodologically, but what ultimately matters for valid statistical inference is the absence of bias from the sample. If the researcher’s line of argument depends (in part) on statistical inferences, they will need to defend their validity. If random sampling was used, we can appeal to statistical theory. In the absence of random sampling, we have to convince ourselves and our audience that the sampling process we used was unbiased. If we detect systematic mismatches between sample and target population, we can specify the sampled population to which our inferences may legitimately apply.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html#corpus-data-non-random-samples",
    "href": "02_statistical_inference.html#corpus-data-non-random-samples",
    "title": "3  Statistical inferences",
    "section": "3.5 Corpus data: Non-random samples",
    "text": "3.5 Corpus data: Non-random samples\nStatistical inference can also be applied for non-random samples. Then, however, statistical theory no longer offers a protective belt against discrepancies between the target of our inferences and the information at hand. There is no guarantee of absence of bias in our sample. This does not mean that statistical inferences cannot be made. But with non-random samples, the researcher’s expertise and judgement is called for in order to decide which interpretations can be attached to inferential results. This could involve an explicit formulation of the sampled population, i.e. the population that our non-random sample does represent, and how it differs from the population of interest, or target population.\nThe question of corpus data and random sampling can be addressed from two perspective: (i) the compilation of the corpus and (ii) the set of observations extracted to address a particular research question.\nConstraints on generality\n“Very often the attempt to select a representative sample of some population is unnecessary , or more trouble than it is worth, or even impossible in principle.” (Mook 1982, 38)\n\n3.5.1 Corpus compilation\nFor corpus compilation, it seems difficult to carry out actual random sampling. It is impossible to enumerate the units in the population of interest – for a spoken corpus, these are the speakers of a particular language (variety); for a written corpus, the collection of written texts produced by the population of interest. This enumeration, however, is necessary to assign equal probability to each unit in the population. Nevertheless, a researcher can take precautions to strengthen the regresentativeness and therefore the validity of statistical inferences. Through careful definition of the target population, the arrangement of a sampling frame, and the use of near-random selection procedures, an effort can be made to avoid systematic discrepancies between sample and population. A further desirable feature is to give equal weight to the sampled units, which would mean that roughly the same number of words are taken from each speaker or text.1 However, potential imbalances can also be addressed at the analysis stage, where the units can be treated even-handedly.\n1 Spoken BNC2014\n\n3.5.2 Corpus data\nWhen using corpora as a source of data on a particular linguistic phenomenon, we extract relevant hits from a corpus. This set of hits then constitutes the sample on which our analyses and interpretations are based. If the compilation of the corpus took methodological precautions against bias, it appears reasonable to assume that this will carry forward into the set of corpus hits at hand. Note, however, that the structure of interest may be used at different rates by different speakers. Thus, a perfectly balanced corpus, with the same number of words sampled, say, from each text, does not guarantee balanced token counts for the structure of interest. This, again, would need to be addressed at the analysis stage.\nDepending on the type of structure we are studying, there may be additional populations from which we are effectively sampling. For instance, when studying certain type of syntactic pattern, say ditransitive constructions, these will be instantiated by different lexical elements. We are therefore also sampling from the population of ditransitive verbs. Or consider the English comparative alternation, where we are sampling from the population of gradable adjectives. This kind of sampling occurs on a language-internal level, and the researcher has no control over the kinds of lexical units returned by a corpus query. In fact, the set of corpus hits will show decisively non-random features, due to universal distributional features of language use. Overall, the selection of units obtained will be biased towards higher-frequency forms, which are more likely to be observed and therefore make it into our sample. Further, the token distribution across lexical units (e.g. gradable adjectives) will be uneven, and very likely skewed.\nIt is fair to say that corpora and corpus data rarely (if ever) represent truly random samples. When sampling speakers or texts for corpus compilation, methodological efforts can be made to work towards the idealized setting of a randomly selected, representative sample of language use. The set of units that we obtain at the language-internal level, however, is bound to be systematically biased. This means that, for corpus data analysis, there are no methodological precautions against certain distortions in our portrayal of the population. Such issue require careful linguistic attention.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "02_statistical_inference.html#populations-in-language-data-analysis",
    "href": "02_statistical_inference.html#populations-in-language-data-analysis",
    "title": "3  Statistical inferences",
    "section": "3.6 Populations in language data analysis",
    "text": "3.6 Populations in language data analysis\n(Spiegelhalter 2019, 91–92) distinguishes three types of populations:\n\nA literal population: The usual state of affairs.\nA virtual population: Measurement error.\nA metaphorical population.\n\nConceptual populations\nIn survey sampling, it is also conceded that interest may extend beyond finite populations (e.g. Cochran (1977, 39); ).\nsuperpopulation\nLohr (2022, 47) mentions the notion of a superpopulation as an alternative universes in which circumstances were slightly different.\nWhat is a population?\n“Population is an abstract concept fundamental to statistics. It refers to the totality of numbers that would result from indefinitely many repetitions of the same process of selecting objects, measuring or classifying them, and recording the results. A population is, thus, a fixed body of numbers, and it is this general body of numbers about which we would like to know.” (Wallis and Roberts 1956, 126)\n\n\n\n\nBottai, Matteo. 2014. “Lessons in Biostatistics: Inferences and Conjectures about Average and Conditional Treatment Effects in Randomized Trials and Observational Studies.” Journal of Internal Medicine 276 (3): 229–37. https://doi.org/10.1111/joim.12283.\n\n\nCobb, George W. 1998. Introduction to Design and Analysis of Experiments. New York: Springer.\n\n\nCochran, William G. 1977. Sampling Techniques. New York: Wiley.\n\n\n———. 1983. Planning and Analysis of Observational Studies. New York: Wiley.\n\n\nCox, David R. 1958. Planning of Experiments. New York: Wiley.\n\n\nDraper, David, James S. Hodges, Colin L. Mallows, and Daryl Pregibon. 1993. “Exchangeability and Data Analysis.” Journal of the Royal Statistical Society A 156 (1): 9–37. https://doi.org/10.2307/2982858.\n\n\nFisher, Ronald A. 1956. Statistical Methods and Scientific Inference. Edinburgh: Oliver & Boyd.\n\n\nJohnstone, David J. 1989. “On the Necessity for Random Sampling.” The British Journal for the Philosophy of Science 40 (4): 443–57. https://www.jstor.org/stable/687735.\n\n\nLindley, Deniis V., and Melvin R. Novick. 1981. “The Role of Exchangeability in Inference.” The Annals of Statistics 9 (1): 45–58. https://doi.org/10.1214/aos/1176345331.\n\n\nLohr, Sharon L. 2022. Sampling: Design and Analysis. Boca Raton, FL: CRC Press.\n\n\nMook, Douglas G. 1982. Psychological Research: Strategy and Tactics. New York: Harper; Row.\n\n\n———. 1983. “In Defense of External Invalidity.” American Psychologist 38 (4): 379–87. https://doi.org/10.1037/0003-066X.38.4.379.\n\n\nSpiegelhalter, David. 2019. The Art of Statistics: Learning from Data. London: Penguin.\n\n\nWallis, W. Allen, and Harry V. Roberts. 1956. Statistics: A New Approach. Glencoe, IL: The Free Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical inferences</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html",
    "href": "03_data_structure.html",
    "title": "4  Data structure",
    "section": "",
    "text": "4.1 The structural component\nThe first step in corpus-based work is to think carefully about the (prospective) structure of the data. We use the term data structure to refer to the underlying organization of observations (i.e. corpus hits) that exists prior to coding and classifying the data accoring to the variables of primary interest (the predictors, which are part of the systematic component). More specifically, the structural component may include two ways in which the individual observations in our set of data are grouped, or clustered: (i) they may stem from the same speaker or writer, i.e. the same source, and/or (ii) they may be grouped on lexical grounds, i.e. several tokens represent, or involve, the same lexical unit. In such cases, we say that there is clustering in the data. If a set of corpus hits can be arranged by speaker, for instance, we will refer to Speaker as a clustering factor, and to individual speakers as clustering units, or simply units. Clustering by source is a pervasive feature of corpus data. Whether we are also dealing with clustering by lexical unit depends on the linguistic structure or phenomenon we are studying. Both layout features of corpus data become more transparent by considering various examples of each.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#the-structural-component",
    "href": "03_data_structure.html#the-structural-component",
    "title": "4  Data structure",
    "section": "",
    "text": "4.1.1 Clustered data structure: Examples\nTo illustrate typical structural features of natural language data, we first consider, as an illustrative example, the sociolinguistic variable (ING). Let us briefly consider this research context: Words ending in &lt;ing&gt; may be pronounced as /ɪŋ/ or /ɪn/. The latter variant instantiates what is referred to as g-dropping. The objective is to describe and understand under which circumstances speakers show g-dropping.\nLet’s assume we have compiled a corpus consisting of unstructured interviews with 66 speakers, with interviews varying in length. We search this corpus for relevant tokens, i.e. words ending in &lt;-ing&gt;. In total, we obtain 6314 instances. Our search returns multiple hits per speaker. Figure 4.1 shows distribution of token counts per speaker. Most informants contribute between about 70 and 150 tokens. This illustrates clustering by source: The 6314 observations that make up our data set can be arranged by Speaker.\n“The data have a two-level structure with tokens as units at level 1 and subjects as clusters at level 2.”\n\n\n\n\n\n\n\n\n\nFigure 4.1: Distribution of the number of tokens per Speaker; data from Forrest (2017).\n\n\n\n\nThe tokens we have extracted from our corpus can also be broken down by lexical unit; we will use the shorter label Item. We consider identical forms with different word classes as different words. Thus, there are 795 unique forms, but some belong to different word classes - for instance, beginning and building (verb or noun), and demanding and entertaining (verb and adjective). WIth this definition of “word”, we arrive at 1024 units. For the data at hand, the 6314 tokens in our data set are distributed very unevenly across these 0 items. ?fig-ing-tokens-per-word) shows the distribution. Among the high-frequency forms that stand out are going (386 tokens), doing (226 tokens), being (NA tokens), and working (NA tokens). Of the 0 words, 544 (i.e. %) only occur once in the corpus. Such items are often referred to as hapaxes (short for hapax legomena). This illustrates clustering by lexical unit: The 6314 observations that make up our data set can be grouped by Item.\n\n\n\n\n\n\n\n\nFigure 4.2: Distribution of the number of tokens per Item; data from Forrest (2017).\n\n\n\n\n\nClustering by source almost always materializes in corpus data. Thus, the hits we extract from a corpus can usually be grouped according to source: We may have multiple observations per speaker (spoken corpus) or text (written corpus). The second way in which observations may be organized is by lexical unit. Whether this type of clustering will surface in our data depends on the linguistic structure we are studying. If the construction/phenomenon of interest has variable slots and is therefore instantiated by different words, then observations will be clustered by lexical unit. Examples are listed in Table @ref(tab-clustering-word-examples).\n\n\n\n\n\n\n\n4.1.2 Clustering produces higher-level units\nWhenever we have clustered (or hierarchical) data structures, we can identify higher-level units in our data. At the lowest level in our data, we have the individual corpus hits, i.e. the instances that we have extracted from the collection of texts. We will refer an observations at this lowest level as a token, and sometimes, for variety, as a corpus hit, or observation.2 Each token therefore occupies a single line in the concordance table.\n2 In the literature on the design and analysis of experiments, a critical distinction is that between the unit of measurement and the experimental unit. The distinction is made based on the design employed and on the type of randomization (actually) implemented when running the experiment. In the survey sampling literature, the notion of a sampling unit plays a central role, and a distinction is made between primary and secondary sampling units.In other literatures, what we refer to as tokens may be called “level-1 units” or “observations at level 1”. We will try to will use the term token consistently and reserve the label “unit” for higher-level collections of data points, i.e. speakers/writers and lexical units. Since they sit at a higher level in our data layout, they are sometimes referred to as level-2 units, or simply groups or clusters. We will usually try to use concrete labels for these units, i.e. speakers (or texts) and words or types. And use the term unit when talking about higher-level entities more generally. The hierarchical organization of the tokens in our data set means that tokens can be arranged into clusters, with clusters differing in size. When describing such data layouts, we will say that there are higher-level units in our data, and that tokens are clustered (by Speaker, or by Item).\nDue to the uncontrolled, observational nature of corpus data, it is typical for units to differ in size. This is true for both structural factors. The variability of token counts across speakers will depend mostly on corpus design – the number of tokens per speaker may vary, even considerably, depending on how much material a speaker or writer contributed to a corpus. We can sidestep highly uneven distributions by balancing the word count across speakers or texts at the stage of corpus compilation. Nevertheless, even if word counts are perfectly balanced, the number of relevant tokens for analysis will still show variation, since it also depends on how often a speaker/writer used the structure of interest. In general, then, we can only exert some control over the distribution of token counts over speakers. Usually, uneven tallies across speakers are a nuisance and of little substantive interest.\nThe distribution of token counts across words, on the other hand, will almost certainly vary markedly. There are likely to be a handful of high-frequency units (or types), accounting for the bulk of the data, and very many low-frequency words, some (perhaps a substantial proportion) of which appear only once in our data. Such skewed distributions are a typical footprint of natural language data – the frequency profile then resembles a Zipfian distribution. The extent of the skew (or imbalance) depends on the structure we are studying. It is, however, a near-universal feature of natural language use. Importantly, we have no way of exerting control over the distribution of token counts over words. That is, there are no corpus compilation strategies that will even out lexical occurrence rates. In contrast to unbalances tallies across speakers, however, the observed frequency distribution often bears linguistic meaning and is of substantive interest. The skewed profile we see in Figure 4.2) is a typical feature of natural language data.\nFor the (ING) data, the two types of clustering produce vastly different distributions of token counts. This is not untypical for natural language data. Between-speaker imbalances in token counts largely depend on corpus design. Asymmetries in token counts across words, on the other hand, reflect the natural workings of language. Before we go further, a brief comment on terminology: While it would make sense, in principle, to say that tokens are “nested” within Speaker (or Item), the notion of “nesting” is usually used to describe a relation between factors, not between observations and factors.3\n3 There are exceptions in the literature, however, e.g. Rabe-Hesketh and Skrondal (2021), p. 75.\n\n4.1.3 Clustered data structure: Crossed clustering\nFor a set of corpus data that show both types of clustering, by source and by lexical unit, the hierarchical grouping of tokens is in fact a kind of cross-classification. In our illustrative (ING) study, we have two clustering factors: Speaker and Item. We could create a table with columns for speakers and rows for words; then, each token would fall into a particular cell, i.e. reflect a certain speaker-word combination. Figure 4.3) gives a visual representation of the cross-classified hierarchical arrangement of tokens. Due to space limitations, words occurring fewer then 5 times are not included in the diagram. The distributions at the margins, represented by spikes, correspond to those shown in Figure 4.1) and Figure 4.2) above. The size of the circles in the grid is proportional to the number of tokens in the cell.\n\n\n\n\n\n\n\n\nFigure 4.3: Cross-classified clustering structure: Speaker is crossed with word (only words with 5 or more tokens are displayed).\n\n\n\n\n\nIn principle, any speaker-word combination could occur. However, there are very many empty cells, especially for infrequent lexical items. This is a logical consequence of the skewed marginal distribution of word token counts: The majority of words have fewer tokens than there are speakers in the data set, and by necessity each hapax stems from a single speaker, leaving 65 empty cells in the table.\nIn the current data layout, Speaker and Item are crossed, which leads to the two-way arrangement shown in Figure 4.3). More specifically, as is evident from the patchy distribution of tokens across cells, they are partially crossed, which means that some combinations, though possible, are not observed. This is a common distributional feature of natural language data. It is in stark contrast to the kind of data structures produced by designed experiments, which are the size of units is usually (roughly) balanced.\nThe structural component of the model not only reflects the internal organization of observations, but also identifies different levels at which processes influencing the response may occur, or more cautiously, different levels at which we may observe systematic variation. This brings us to the next component.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#the-systematic-component",
    "href": "03_data_structure.html#the-systematic-component",
    "title": "4  Data structure",
    "section": "4.2 The systematic component",
    "text": "4.2 The systematic component\nWe refer to variables that are the primary focus of a study as predictor variables; they constitute the systematic component of a data set. In the case of (ING), the probability of g-dropping has been observed to vary with a set of factors. These can be grouped into two classes: language-internal and language-external (or social). Social variables include attributes of the speaker – in our illustration of the (ING) data, these are Date of Birth, Sex, and Education. Internal features, on the other hand, are linguistic in nature. They may be attributes of the immediate environment in which a token was observed, e.g. Following Context – more specifically, the place of articulation of the following segment (coronal: saying [t]o her; velar: saying [g]ood-bye). They could also be properties of the lexical unit “carrying” the ending -ing – e.g. its Word Class, Frequency, or the consonant preceding -ing (coronal: waiting, velar: working).\n\n4.2.1 Different units of analysis\nPredictor variables in the systematic component must be considered in light of the structural component – that is, they must be linked to the level at which they are meaured or observed. Thus, predictors can be measured at the level of the individual token, or at the level of a higher-level unit (a particular speaker or word):\n\nThe predictors Date of Birth, Sex, and Education are attributes of the speaker, so they are measured at the level of the speaker. We call them speaker-level variables.\nPredictors that represent attributes of words are Word Class, Frequency, and the Preceding Consonant – they are item-level variables.\nFinally, the Following Context is a property of the context in which a specific token appears and is measured by inspecting the individual corpus hit. It is a token-level variable.\n\nIt is important to be clear about the mapping between the systematic and the structural component. Each predictor must be linked to the appropriate level, and to the appropriate unit in the structural component. This leads to the distinction between token-level predictors (level-1 predictors) on the one hand, and speaker-level and word-level predictors (level-2 predictors) on the other.\nThis distinction has consequences for the analysis of the data. Thus, the unit of analysis for Age and Sex is the speaker.4 This means that the sample size for inferences about these social variables is the number of speakers in our corpus. Figure 4.4) shows the relevant units of analysis, the speakers, broken down by Date of Birth and Sex. Figure 4.5) stratifies speakers by Date of Birth and Education.\n4 The term unit of analysis corresponds to the experimental unit in the literature on the design and analysis of experiments. For corpus data, the identification of the appropriate unit of analysis is usually fairly straightforward, since it follows logically from what a variable expresses and how it is measured. In data from experiments, the identification of relevant units can be much more difficult, since it depends both on the design and the method of random assignment used when conducting the experiment.5 In some disciplines, this is referred to as pseudo-replication (Hurlbert).From the point of view of these speaker-level predictors, the multiple tokens produced by a certain speaker do not add much information to our statistical conclusions. The relevant sample size is the number of different speakers. For each speaker, we have multiple tokens, however. This constitutes a form of sub-sampling. The notion of sub-sampling is relevant when considering level-2 predictors, which are measured on the speaker or word. We talk about sub-sampling when the data include multiple observations from the same unit. In the context of corpus data, then, sub-sampling may apply to speaker-level and word-level predictors and must be taken into account during the analysis of the data.5 This is because, say, additional tokens per speaker are of marginal relevance for the analysis of differences between age groups – they merely contribute to the state of information we have about one individual.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Distribution of speakers across the speaker-level predictors Date of Birth and Sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Distribution of speakers across the speaker-level predictors Date of Birth and Education.\n\n\n\n\nThe unit of analysis for Word Class and Frequency, on the other hand, is the word. Again, the relevant sample size for these word-level predictors is the number of unique words in our data set. Figure 4.6) shows how the 0 unique words are distributed across word classes and the frequency spectrum. Note that Frequency is shown on a log-scaled x-axis.\n\n\n\n\n\n\n\n\nFigure 4.6: Distribution of words across the word-level predictors Lexical Category and Frequency.\n\n\n\n\n\nFinally, the level of analysis for the Following Context is the specific corpus hit. This is because the predictor is measured by looking at the immediate phonetic context in which the token appears. It is therefore an attribute of the smallest level of observation: the token, or corpus hit.\n\n\n\n\n\n\n\n4.2.2 Speakers/words: Nested factors\nThe two clustering factors in the (ING) data set are Speaker and Item. The relationship that holds between these factors and the corresponding level-2 predictors is referred to as one of nesting. Consider the factor Speaker. Each level of the factor (i.e. each speaker in our data) belongs to one particular level of a speaker-level predictor, for instance a particular age group. A speaker cannot be observed at different values or levels of a speaker-level predictor. Likewise, each word belongs to a certain lexical category, a word-level predictor. It cannot be observed in different categories. Whenever we have a nesting relationship between two factors, we can say that one factor (e.g. Speaker) is nested within the other (e.g. Date of Birth). The factor Item, then, is nested within Word Class. More generally, a nested relation means that each level of the nested factor (i.e. Speaker and Item) can only be observed in combination with one particular level of the outer factor, the level-2 factor.\nIn general, there are two important structural relations that can hold between factors; nesting is one of these. The other is referred to as crossing. When two factors are crossed any combination of the levels is possible. We have already come across a crossing relationship: that between Speaker and Item (see Figure 4.3). We will return to crossing shortly.\nIn corpus data, nesting relations between clustering factors and cluster-level (level-2) predictors are not difficult to discern. This is because they arise naturally – we can identify them on the basis of our background knowledge. Thus, it is quite obvious that a speaker can belong to only one value of a speaker-level predictor. In our data set, for instance, a given speaker has a certain sex and date of birth.6 Likewise, a given word belong to a particular lexical category. More generally, then, we can state the following:\n6 We will disregard the fact that certain speaker-level variables can also change over time – examples are the number of foreign languages spoken, the place of residence, or gender. At the time of data collection, speaker-level factors assume a particular value, which remains constant across the observations collected from this individual.\nThe levels of the factor Speaker (i.e. the individual speakers in the data sets) are nested within the levels (or values) of speaker-level predictors.\nThe levels of the factor Item (i.e. the individual words in the data sets) are nested within the levels (or values) of word-level predictors.\n\nWe can depict nested relationships between factors visually. Thus, Figure 4.7) shows two-way classifications of tokens in the data: (i) by Speaker and Sex (top); and (ii) by Speaker and Age Group (bottom). Speakers are ordered by Sex, Age Group, and the number of tokens they produced; the size of the circles is proportional to this token count (see Figure 4.1)). Figure 4.7) illustrates what nested relationships between factors look like: Cross-tables between factors that enter into a nesting relationship have a systematic arrangement of empty cells, since the observable combinations of levels are limited.\n\n\n\n\n\n\n\n\nFigure 4.7: The nesting relationship between Speaker and the speaker-level predictors Sex and Age Group.\n\n\n\n\n\nIn corpus data, nesting relationships are intrinsic attributes of the data. Once we have established a mapping between the systematic and the structural component, the identification of nesting relationships is fairly straightforward. In the statistical literature, Speaker and Item are referred to as nested factors7. The factors within which Speaker and Item are nested are referred to as nesting factors8.\n7 Other labels are inner factor and hierarchical factor.8 Other labels are outer factor, cluster-level factor, cluster-constant factor. In longitudinal data: time-invariant variable\n\n4.2.3 Token-level predictors: Crossed with Speaker/Word\n\nenvironmental factor\nphonetic nature of the following segment\npreceding and following phonological segment\non the immediate left, on the immediate right\n\nToken-level predictors such as the Following Context always enter into a crossing relationship with clustering factors – in our case, with both Speaker and Item. This is because all values of the token-level predictor (i.e. coronal, velar, pause, other) may occur with each speaker and each word: Speakers may produce tokens in each phonetic context, and words may likewise appear in each phonetic context.\nLet us visualize these cross-classifications of tokens. Figure 4.8 illustrates the crossed relationship between Speaker and Following Context. The 66 speakers in our data are arranged from left to right, and the levels of the factor Following Context are ordered by rate of occurrence, from top to bottom. Looking at the occurrence rate of the observed places of articulation, the general cline (other &gt; coronal &gt; pause &gt; velar) appears to be fairly consistent across speakers. Figure 4.8 also lists the tokens counts observed in each cell. Even though there are many sparsely populated cells, every possible combination of the two classification variables, Speaker and Following Context, occurs in the current data set. The two factors are therefore said to be fully crossed.\n\n\n\n\n\n\n\n\nFigure 4.8: Distribution of token-level predictor Following Context across speakers.\n\n\n\n\n\nNext, consider the crossed relationship between Item and Following Context. We will restrict our attention to words that occur at least 20 times in our data set (n = 57). A visual impression of the two-way counts is provided in Figure 4.9. Note that the general frequency cline (other &gt; coronal &gt; pause &gt; velar) does not hold across lexical items – the observed frequencies across the grid show pronounced peaks and gaps. The top-ranking going (v), for instance, is most likely to be observed before coronal sounds, contrary to the general trend in the tallies; trying (v), on the other hand, ranking 7th, is exclusively followed by coronal sounds.\nIn statistical terms, there is an association between Item and Following Context. What this means is that there is a tendency for words to occur in different phonetic contexts. Such distributional asymmetries are unsurprising, and – in the present case – to a certain extent predictable: They could reflect collocational or collostructional preferences (e.g. going to and trying to, each with a high string frequency), or, in our case, variation among word classes in terms of the likelihood of preceding certain high-frequency function words (e.g. the and a would be more likely after verbs; of would be expected to occur more often after nouns).\n\n\n\n\n\n\n\n\nFigure 4.9: Distribution of token-level predictor Following Context across words with 20 or more tokens.\n\n\n\n\n\nWhen breaking down the corpus hits by Item and Following Context, we end up with quite a few empty cells — not every possible combination of the two classification variables occurs in the current data set. The two factors are therefore said to be partially crossed.\nIn general, then, the crossing relationship between clustering factors and token-level predictor will be one of full crossing or partial crossing. For the two-way distribution of tokens, we can state that, for the factor Speaker, we would usually expect no systematic differences in the occurrence rate of token-level predictor values (see Figure 4.3). For the factor Item, on the other hand, the rates at which token-predictor values occur may vary between lexical units. This variation reflects language-internal distributional patterns, and will usually make sense linguistically.\n\n\n4.2.4 Level-2 predictors in cross-classified clustering: Crossing relationships\nIn cases where the structural component of a data set shows a cross-classified layout, a level-2 predictor measured on one clustering factor (e.g. Speaker) is crossed with the other clustering factor (e.g. Item). For instance, a word-level predictor such as Word Class is crossed with Speaker. This is because every level of the predictor Word Class may be observed in each speaker. The two-way frequency distribution is is visualized in Figure 4.10), at the top. Speaker and Word Class are partially crossed – the low-frequency word classes noun and adjective are not observed in every speaker. Figure 4.10) also offers a visual cross-classification of Figure 4.10) with Preceding Consonant and Frequency (binned into three frequency bands with breaks at 10 and 100). For both word-level predictors, we observe full crossing with Speaker.\n\n\n\n\n\n\n\n\nFigure 4.10: Distribution of item-level predictor Word Class across speakers.\n\n\n\n\n\nThe same structural relationship holds the other way around, namely between speaker-level predictors such as Date of Birth and the clustering factor Item.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#data-structure-systematic-component",
    "href": "03_data_structure.html#data-structure-systematic-component",
    "title": "4  Data structure",
    "section": "4.3 Data structure: Descriptive statistics",
    "text": "4.3 Data structure: Descriptive statistics\n\n4.3.1 Structural component\nThe total number of observation is … The number of clusters is and there are an average of … tokens per cluster in the dataset.\nNeed to plot\n\n\n4.3.2 Systematic component\nIn clustered data settings, we can distinguish between two different kinds of averages: The overall mean for a variable disregards any clustering structure and simply averages over the entire set of tokens in the data. The computation of the grand mean, on the other hand, involves an intermediate step: First, cluster-specific averages are computed, and then we average over these cluster-specific averages.\n\noverall standard deviation: SD of all observations from the overall mean\nbetween standard deviation: SD of the cluster means from the overall mean\nwithin standard deviation: SD of the observations from the cluster means\n\nHere, it probably doesn’t make sense to compute the between-cluster variability for words in this way, because of the many hapaxes. Probably better to use a multilevel model here. We could do something similar for the other linguistics predictors in the data.\n\n\n\n\n\n\nVariable\nLevel\nMean\nSD_word\nSD_subject\n\n\n\n\n1\nFollowing context\nOverall\n0.17\n0.38\n0.38\n\n\n3\n\nBetween (MLM)\nNA\n0.12\n0.05\n\n\n5\n\nWithin (MLM)\nNA\n0.23\n0.24\n\n\n\n\n\nWe see that Following context varies much more within speakers than between speakers.\nWe can express the distribution of level-2 variables at two different levels: Either across tokens, or across the respective units (or clusters).\nThus, we could compute the proportion of tokens that are verbs. And we can compute the proportion of words that are verbs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#data-structure-terminology-across-literatures",
    "href": "03_data_structure.html#data-structure-terminology-across-literatures",
    "title": "4  Data structure",
    "section": "4.4 Data structure: Terminology across literatures",
    "text": "4.4 Data structure: Terminology across literatures\nHierarchical data structures occur in almost any domain of statistics. As a result, the terminology that has evolved to describe relevant aspects of a set of data varies considerably. We will make an effort here to draw parallels between different strands.\nThe terms crossing and nesting are applied independently of the substantive research area.\nDoE: Whole plots, subplots; whole units, subunits\neconometrics: panels",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#drawing-a-plot-plan-for-natural-language-data",
    "href": "03_data_structure.html#drawing-a-plot-plan-for-natural-language-data",
    "title": "4  Data structure",
    "section": "4.5 Drawing a plot plan for natural language data",
    "text": "4.5 Drawing a plot plan for natural language data\nTo coordinate and communicate information about corpus data structure, it helps to sketch the data layout. This section introduces a template that may be used to document the important structural features of a data set. Here, we will explain how to fill in this template. In Chapter @ref(model-specification} we will see how this organized arrangement will allow us to specify regression models in an informed manner.\nStroup (2013) notes that every set of data can be considered as having a plot plan. This term originates from agricultural experimentation, where treatments (e.g. different fertilizers) are often applied to plots of land.9 We will adopt this term to describe the data layout, which we often sketch visually.\n9 Within the design of experiments literature, the field of agriculture has been an important source of technical terms. This is due to the fact that R.A. Fisher, who invented many of the designs and techniques still in use today, did applied work in this field.As we will see in the next chapter, this plot plan will be of help when making decisions about how to analyze a set of data. A plot plan brings together the structural and the systematic component of a set of data and identifies the relationships that hold between the factors in our data – i.e. that of nesting and (partial) crossing. These data features have consequences for the specification of regression models.\n\n4.5.1 Step 1: Map the structural component\nThe first step is to identify clustering variables in our data. As we have discussed above, these can generally be of two kinds. Language-external clustering by source is almost always discernible in a set of corpus hits – they may be from the same Speaker (spoken corpus) or Text (written corpus). To get a feel for the data structure, we should count the number of unique speakers/texts that contribute tokens to our data, and then the number of tokens from each speaker/text (cf. Figure 4.2)).\nDepending on the type of structure under investigation, there may, in addition, be clustering by lexical unit, where tokens extracted from a corpus represent the same Item. Again, it makes sense to determine the number of unique lexemes that feature in our data, and to inspect a token frequency profile (see Figure 4.2)). This will reveal to which extent token counts are skewed, and it is worthwhile to identify the top-ranking items.\nIf we observe both kinds of grouping structures, the clustering factors will be crossed. Cross-tabulations and diagrams may help us perceive features of this two-way distributions of tokens.\nIn general, two common structural layouts seem to be the following:\n\nClustering by source only\nCrossed clustering by source and on lexical grounds\n\nThese two situations lead to two different templates for our plot plan.\n\n\n4.5.2 Step 2: Determine the unit of analysis for each predictor\nNext, we turn to the systematic component. Take the list of predictors and determine, for each one, the appropriate unit of analysis. Two related questions help us arrive at an appropriate classification. Predictors represent attributes of units. The first question is what kind of unit a predictor classifies or measures: Is it an attribute of the speaker (or text), an attribute of a lexeme, or an attribute of the immediate context of the corpus hit. The second question is how the attribute is determined. Is it measured based on information about the speaker or text, based on the lexeme at hand, or is it determined by looking at the specific context in which the token occurs. Usually, there are three possibilities. The unit of analysis can be (i) the speaker, (i) the word, or (iii) the context.\n\n\n4.5.3 Step 3: Sort the predictors into token-level and cluster-level variables\nHaving identified the appropriate unit of analysis for each predictor, we can distinguish between token-level predictors (attributes of the context of occurrence), speaker-level predictors (attributes of the speaker), and word-level predictors (attributes of the word). We enter these variables into the plot plan at the appropriate places: Token-level predictors are listed inside of the box, speaker-level predictors are given above the box, and word-level predictors are appear to the left of the box.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#tools",
    "href": "03_data_structure.html#tools",
    "title": "4  Data structure",
    "section": "4.6 Tools",
    "text": "4.6 Tools\nTo obtain preliminary insights into the structure of the data, the following tools are helpful:\n\ndot diagrams\nbubble charts\nline plots\n\n\n4.6.1 Graphing the data structure using R\nOnce the plot plan is set up, we tabulate and visualize the distributional features of our data. Two chart types are particularly useful to this end: dot diagrams and bubble charts.\n\n\n4.6.2 The structural component\nAn important first step is to get an insight into the distribution of tokens across clusters. The cluster variable needs to be available as a column in the data set. We start by producing a table of token counts by speaker and sort this table:\n\ning_sample |&gt; \n  group_by(speaker) |&gt; \n  summarize(token_count = n()) |&gt; \n  arrange(-token_count)\n\nThis will return the following table:\n\n\n# A tibble: 66 × 2\n   speaker token_count\n   &lt;fct&gt;         &lt;int&gt;\n 1 142             186\n 2 376             137\n 3 116             135\n 4 368             135\n 5 379             135\n 6 174             133\n 7 328             133\n 8 383             132\n 9 370             131\n10 369             130\n# ℹ 56 more rows\n\n\nWe can use the data in this table to draw a dot diagram:\n\ning_sample |&gt; \n  group_by(speaker) |&gt; \n  summarize(token_count = n()) |&gt; \n  arrange(-token_count) |&gt; \n  ggplot(aes(x=token_count)) +\n  geom_dotplot()\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nDot diagrams are a useful tool. The figure we just produced already gives us important insight into the structure of the data. Token counts can vary widely across speakers. This is a typical feature of natural language data.10\n10 Certain structures are more prone to produce unequal token counts across speakers. We can get an idea of how prone a structure is by inspecting the variation in rates across text samples of equal size. It would be helpful to have some sort of typology of language structures at some point.11 Again, a typology would be useful to have, based on linguistic criteria.The second step is to do the same for the different lexical items in your data. Determine the overall number of unique lexemes, and then the number of tokens for each lexeme. This gives you a second important perspective on the structure of your data. Again, you produce a dot diagram. Depending on the type of structure you are investigating, the distribution of token counts across lexemes could be highly uneven. It could show a Zipfian distribution.11\n\ning_sample |&gt; \n  group_by(word_2) |&gt; \n  summarize(token_count = n()) |&gt; \n  arrange(-token_count)\n\nThis will return the following table:\n\n\n# A tibble: 1,024 × 2\n   word_2                token_count\n   &lt;fct&gt;                       &lt;int&gt;\n 1 going_verb                    386\n 2 doing_verb                    226\n 3 being_gerund                  161\n 4 working_verb                  159\n 5 trying_verb                   147\n 6 being_verb                    133\n 7 interesting_adjective         132\n 8 coming_verb                   127\n 9 getting_verb                  125\n10 going_gerund                  119\n# ℹ 1,014 more rows\n\n\n\n\n4.6.3 Cluster-level predictors\nNext, consider the speaker-level and word-level predictors in turn. Produce cross-classifications for all speaker-related variables and observe how speakers are distributed across the categories. You may need to discretize continuous variables. Look for problematic distributions. Then do the same for word-level predictors.\n\n\n4.6.4 Token-level predictors\nThe distribution of token-level predictors should be inspected at different levels. The overall distribution disregards clustering factors and reports the distribution of the variable throughout the entire data set. Two further dimensions that are of interest take into account the data structure. Thus, we can partition the overall distribution into two components, one between clusters and one within clusters. The important insight here is the extent of the between-cluster variation, since it signals the extent to which clusters vary in their constellation with regard to this token-level variable. As we will see later on, it is important to be aware of differences among clusters if they exist.\nFor natural language data, we need to be careful when separating these sources of variability since clusters may differ dramatically in size. The best way seems to be to run a multilevel regression model with the token-level predictor as the outcome, and the clustering factors as random intercepts. This allows us to separate these two variance components, i.e. the within- and the between-dimension. For categorical token-level variables, we need to invoke the latent-variable interpretation of binary regression models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "03_data_structure.html#natural-language-use",
    "href": "03_data_structure.html#natural-language-use",
    "title": "4  Data structure",
    "section": "4.7 Natural language use",
    "text": "4.7 Natural language use\nLanguage-internal factors vary naturally\n\n\n\n\nForrest, Jon. 2017. “The Dynamic Interaction Between Lexical and Contextual Frequency: A Case Study of (ING).” Language Variation and Change 29 (2): 129–56. https://doi.org/10.1017/S0954394517000072.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.\n\n\nStroup, Walter W. 2013. Generalized Linear Mixed Models: Modern Concepts, Methods and Applications. Boca Raton: CRC Press.\n\n\nWelham, S. J., S. J. Gezan, S. J. Clark, and A Mead. 2014. Statistical Methods in Biology: Design and Analysis of Experiments and Regression. Boca Raton: CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data structure</span>"
    ]
  },
  {
    "objectID": "04_statistical_models.html",
    "href": "04_statistical_models.html",
    "title": "5  Statistical models: Integration of research objectives and data structure",
    "section": "",
    "text": "5.1 The inherent flexibility of corpus data analysis\nTo start with, we need to distinguish between three components of statistical work: (i) the research question(s) motivating the analysis, (ii) the statistical analysis of the data and (iii) the linguistic interpretation of the results of this analysis. While these three aspects are intrinsically linked, the research question assumes a primary role. The analysis responds to the objectives of the study and aims to produce results that serve as an appropriate basis for interpretation.\nIn experimental research, these three aspects form a coherent whole and are linked via the research design: Based on the specific question to be addressed, an experimental design and data collection plan is devised. The design aims to strike a balance between efficiency and validity considerations, and the analysis of the data usually follows from the design.\nIn corpus-based work, the situation is different. Since we are using data that is already there, we have much less room for maneuver in terms of study design. As a result, the interplay between these three aspects – research question, statistical analysis, linguistic interpretation – is fluid. Sometimes, our engagement with the data at hand will force us to modify our research questions and subsequent interpretation. If the data may turn out to be too sparse to support intended interpretations or levels of generality, we restrict our research question either by focusing it or by narrowing the scope of the intended interpretations. In corpus-based work, there is much more feedback going on between these three aspects. This feedback is mediated by information coming from the data and our background knowledge of a particular research area.\nCorpus data analysis therefore often involves navigating back and forth between these three components and making adjustments in the light of the data. The aim is to ensure that the linguistic interpretations we are distilling from our data have a sufficient empirical footing. To make appropriate adjustments, we must have some knowledge about the basic elements of statistical analysis, and the consequences of certain decisions. This means that data-analytic skills required for corpus data analysis are quite different compared to those that are necessary to analyze experimental data. Using the data to make informed decisions during analysis and knowing how these decisions affect the conclusions that are warranted by our analysis requires statistics and science to go hand in hand.\nFigure 5.1: Components of empirical research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical models: Integration of research objectives and data structure</span>"
    ]
  },
  {
    "objectID": "04_statistical_models.html#data-structure-and-model-specification",
    "href": "04_statistical_models.html#data-structure-and-model-specification",
    "title": "5  Statistical models: Integration of research objectives and data structure",
    "section": "5.2 Data structure and model specification",
    "text": "5.2 Data structure and model specification\nThe specification of a model for data analysis requires careful negotiation between our research objectives (see Chapter 2) and the structure and information in our data (see Chapter 4). The systematic component, which represents the linguistic variables that are of primary concern, must be related to the structural component, which represents the hierarchical layout of a set of corpus hits. The way in which these two components of data structure are combined can be the subject of heated debate. The underlying research objectives can only provide some guidance here.\nOnce we have drawn for our set of corpus data a plot plan of all structural and systematic factors, the existing relationships among the factors can be used to construct a model. Thus, from the crossing and nesting relationships that hold between factors, we can derive a set of parts that may be included in our model specification. In regression models, these parts correspond to terms in the model. The full set of terms can be considered the ceiling in terms of model complexity, since it will include all terms that can, in principle, be justified on the basis of the plot plan, or data layout. We will refer to such a model as the full model. It is rarely reasonable to go ahead and work directly with this full model, since the data may not provide enough information to respond adequately to this level of complexity. If the data cannot meet the demands of a complex model, the precision and validity of estimates and inferences can deteriorate.\nOn the other hand, the full model also provides us with a reference point for model interpretation. Its form, i.e. the set of terms it includes, instantiates the scope of inference for model-based estimates. Thus, the distinction between broad and narrow inference has concrete consequences for the terms in the model. By adding and removing certain parts, we can navigate back and forth between different levels of generality. The full model tends to be overly ambitious in the sense that it sets its inferential targets quite broadly. Broad inferences, however, rely on adequate data. The specification of a sensible model hinges on a close consideration of the informational content of the data and our knowledge of the research area. It is often sensible to reduce model complexity. The important point, however, is that such reductions entail changes in the inferential meaning of the model output by restricting the scope of our inferences. These simplifications often shift the statistical grounds for inference from broad to narrow, and therefore entail certain constraints on generality. Such simplifications may be made to bring a model in line with our research objectives, or they could be emphatically suggested by the data, which may provide feeble grounds for the intended level of generality.\nA sensible and constructive approach to model specification, then, is to ask what consequences certain changes to our model have for the meaning that is transported by a given statistical inference. We have discussed the critical distinction between broad and narrow inference, and in fact modeling decision often boil down to this difference: Many modeling decisions, especially about how the structural component should be represented, are decision about the scope of inference, i.e. broad vs. narrow inference.\nThe controversial issue is whether the structural component can and should represent the full data layout, or whether certain simplifications are necessary or even desirable. Given that this issue has not been fully resolved across the experimental disciplines, which usually deal with relatively clean data architectures, it seems unlikely that consensus may be reached in domains dealing with observational data, which create much more complex and untidy data arrangements.\nAs a result, it is not our aim to advocate, or prescribe, a certain approach to corpus data analysis. Instead, we will try to make clear what consequences certain data-analytic decisions have for the interpretation of model-based inferences. Our strategy will be to think about a model formulation that captures all of the features that we recognize in our plot plan for the data. Often, the resulting model is too complex to be useful. This means we have to make deliberate simplifications to our model. A simplification, then, is the reduction of the structural component of our model. Every simplifying move entails changes to the conclusions we draw in the end. We will try to make clear how changes in the structural component narrow the scope of our statistical conclusions and how we can learn from the data whether a certain simplification is warranted. Importantly, we will also try to warn against biases that may arise from uncareful reductions of the structural component.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical models: Integration of research objectives and data structure</span>"
    ]
  },
  {
    "objectID": "04_statistical_models.html#fixed-vs.-random-factors",
    "href": "04_statistical_models.html#fixed-vs.-random-factors",
    "title": "5  Statistical models: Integration of research objectives and data structure",
    "section": "5.3 Fixed vs. random factors",
    "text": "5.3 Fixed vs. random factors\nAn important distinction in statistical analysis is that between fixed and random effects. From a purely statistical perspective, the difference between fixed and random effects can be explained and illustrated in a relatively straightforward way. In statistical theory, the difference between the two types is defined for abstract, idealized circumstances. The difference between these artificial settings is not difficult to grasp. What may cause headaches, instead, is the question of which of the two best represents the empirical situation at hand. Sometimes, a concrete data setting may be situated somewhere along the continuum between these idealized end points.\nThis section aims to provide some orientation for the analysis of corpus data. We will first consider the basic statistical ideas underlying the distinction between fixed and random effects. This will allow us to understand the statistical meaning that is carried by fixed and random effects. Then, we will connect this statistical meaning to the our research objectives, specifically, the difference between broad and narrow inference. This is followed by a survey of how fixed and random effects are described in the methodological literature. This overview helps us characterize prototypical instances of fixed and random effects. Finally, we will have a look at some factors in linguistic study that fall somewhere in between these prototypes.\n\n5.3.1 The basic statistical idea\nThe general idea surrounding the distinction between fixed and random factors can be illustrated with the help of the two sampling situations that were discussed in Section 3.3. We therefore again turn to simulation to clarify inferential meaning.\nRecall that, in stratified sampling, sampling occurred from the exact same set of groups. Each average across the five groups was calculated from the ame five groups. In other words, the factor Group was fixed. As we saw in Figure 3.2, sampling variation occurred at the level of each group (see colored sampling distributions at the right margin). The uncertainty surrounding the average over the five groups therefore only reflects sampling variation resulting from selecting a sample of units from each group.\nIn cluster sampling, on the other hand, we were not dealing with a fixed set of five groups. Rather, the five groups that happened to end up in our study were themselves a random sample from a larger population of groups. This is why, in this situation, the factor Group is called random because its levels (i.e. the individual groups) are a random sample from a population. Each average across the five groups is therefore computed based on a different selection of units. This means that there is an additional source of sampling variation in this set-up. The sample-to-sample variability of our average is also affected by this sampling process at the level of the population of groups.\nThe fixed-effect average is therefore sensitive to only one source of sampling variation. Its uncertainty interval reflects this source, and provides adequate coverage for long-run sampling from exactly these five groups. The random-effect average, on the other hand, is affected by a second source of sampling variation. Its uncertainty bounds incorporate both sources of sampling variation and provide adequate coverage for long-run two-stage sampling processes.\n\n\n5.3.2 Tools for broad vs. narrow inference\nmultilevel model has greater capacity for generality random intercepts are treated as coming from a higher-level distribution that relates to a larger underlying population\nIn a concrete data-analysis setting, we may find ourselves in a situation where our tokens can be broken down into 5 meaningful groups, similar to the simulation setting in the previous chapter. We then need to decide whether we are interested in a fixed-effect or a random-effect estimate of the average over the 5 groups.\nIf we are interested in only the five groups at hand, we do not intend to generalize beyond these, to other unobserved groups (perhaps because there are none). We would then treat Group as a fixed factor. The resulting fixed-effect inferences for the average over the five groups correspond to the filled circles in Figure 3.2, which also illustrates the long-run conceptualization of these statistical inferences. In terms of the scope of inference, this can be considered a case of narrow inference, since we are not reaching beyond the 5 groups at hand. In regression modeling, narrow-inference estimates are obtained via fixed effects.\nIt may also be the case that the five groups at hand are not of particular interest in themselves, but serve to represent a larger population of groups. This means that we would consider the data set as having been generated by a two-stage sampling process – a sample of groups from a population of groups, and then a sample of observations from each sampled group (i.e. samples withing samples). In this case, we would treat Group as a random factor, to cushion our estimates with appropriate uncertainty intervals – i.e. error bars that reflect both sources of sampling variation. The random-effect inferences compare with the filled black circles in Figure 3.3, which portray the long-run meaning of the statistical inferences. Since we are extending our statistical conclusions to the population level, we are making broad inferences. In general, broad-inference estimates absorb multiple sources of sampling variation. In regression modeling, broad-inference estimates require the specification of appropriate random effects, which provide the statistical grounds for extending the scope of inference.\n\n\n\n\n\n\n\n\nFigure 5.2: Broad vs. narrow inference.\n\n\n\n\n\nFigure 5.2 juxtaposes broad-inference estimates (based on a random-factor analysis of Group) and narrow-inference estimates (based on a fixed-factor analysis of Group). Let us first consider panel (a). The empty circles show the observed proportions of the five groups. Below these, the narrow-inference estimate of the average across the five groups is shown. Above these, we see the broad-inference estimates. To produce broad-inference estimates, the model looks at the observed variation among the sampled groups. It uses the dispersion of scores to measure the amount of between-group variation. It does so by taking a measure of variability such as the standard deviation. This measure is then used for an approximate calculation of the population-level variation among groups. The distribution among groups in this projected population is shown by the density curves at the top. The population is sketched so as to form, in terms of variability, the most likely constellation of units from which the sample at hand may have been drawn. The estimated population-level measure of variability, along with the sample size at hand, is then incorporated into the broad-inference estimate.\nIn panel (b), the five groups differ more with regard to the outcome proportions – the variation among groups is larger. This leads to a different projection of the population-level variability among groups: The grey density curve is wider. This additional variability is worked into the broad-inference estimate, which receives wider uncertainty bounds. Note that the fixed-effect estimate does not change. Panel (c), on the other hand, features a set of groups that are quite homogeneous. Accordingly, the projected population likewise shows smaller between-group variability and the broad-inference estimate is therefore tighter.\nIt is instructive to refer back to Figure 3.3. The adjusted uncertainty intervals, shown at the far right in panel (c), are broad-inference estimate constructed based on the sample of 5 groups. Notice how the width of the uncertainty intervals varies with the variability among the observed groups that inform the population projection, which in turn feeds into the broad-inference estimate. Wide intervals results form heterogeneous samples, and tight intervals result from homogeneous samples. Note, in particular, the estimate with the tightest confidence interval. It is located roughly in the middle of the line-up, and underestimates the true proportion by about .10. Its uncertainty interval badly misses the target. Looking at panel (a), we see that the five groups happened to be very similar in this particular simulation draw. The between-group variability was therefore small, which led to an underestimate of the population-level variability among groups. This yielded an overly optimistic broad-inference estimate.\nThe modification of a statistical model to produce broad and narrow inferences is one of the roles of fixed vs. random effects in regression modeling. As we will see further below, fixed and random effects also have other functions in data analysis. A point we wish to stress here is that the choice between these two ways of representing a grouping variable may largely depend on our research objectives, that is, the level of generality that we wish to attach to an estimate.\n\n\n5.3.3 Decisions\nWhen using regression modeling for data analysis, we (implicitly) decide for each predictor whether it should be treated as fixed or random. It is fair to say that these decisions have stirred debates and controversies in the literature. The controversy arises in the application of these statistical concepts to actual data analysis settings, and discussions therefore take place in the methodological realms of substantive disciplines. Statistical theory can unfortunately only be of some help. As discussed in the previous section, the input provided by statistical theory is this: Fixed-effect estimates generate narrow inferences that do not reach beyond the specific groups (or units) at hand. Random-effect estimates, on the other hand, embrace multiple sources of sampling variation and therefore use the information in the data to generate broad inferences, thereby reaching beyond the particular set of groups (or units) observed.\nIn a concrete research setting, the choice between fixed and random effects cannot be made on statistical grounds. It rests on subject-matter considerations and the researcher’s objectives. Drawing a line between fixed and random effects therefore proves to be difficult without a concrete context in mind. Nevertheless, the way in which the categorization is made in the literature is instructive, since authors vary in terms of the distinctive features they foreground (see Gelman (2005)). The distinction can be made along several dimensions. We will try to relate these to our discussion of research objectives.\nIt is also useful to consider the fixed-random dichotomy from the viewpoint of prototype theory. We can state the prototypical features of each category, while allowing for fuzzy category boundaries. Prototypes then represent the two endpoints of a continuum from fixed to random, with many factors sitting comfortably in between these poles. Real-world examples of prototypical representatives of each type can serve as heuristic reference points in less familiar settings, where we are unsure about the distinction and where we need some guidance for our reasoning.\nOur discussion of distinctions, categorizations, and prototypes should not distract from the most critical point. Both options provide valid statistical conclusions – but they answer different questions. The focus should therefore not be so much on whether a certain data-analytic decision, e.g. to set up a factor as fixed or random, is “correct”. Instead, we should be clear about the type of question answered by either approach and the meaning transported by the resulting inferences. There will be situations where both classifications are reasonable, each providing a statistical answer to a different question. In many cases, it will be reasonable to derive and compare both types of inferences.\nFirst, we may deliberately choose to restrict the scope of our inferences by using fixed effects. This decision must resonate in the discussion of our results, and it should ideally be stated in a constraints on generality section preceeding the findings. Second, random effects do not simply “buy” you broader inferences. The inferences extend to the set of units that are represented by the sample of units in your data. So it is important to draw a line between the sampled population and the target population, which may very well differ.\nWe should also stress that the consequences of the distinction between these two types of predictor categories vary depending on where in the data hierarchy the predictor in question is located. In other words, sometimes the decision matters very little, sometimes a lot. We will try to offer some kind of guidance, while stressing the importance of appreciating the sensitivity of our linguistic conclusions to this decision.\n\n\n5.3.4 Fixed and random factors in contrast\nFixed and random effects differ along a number of dimensions, which are shown in Figure 5.3. The grey lines indicate that each dimension spans a continuum with two endpoints that may be considered as describing prototypical features of each classification.1\n1 See Rabe-Hesketh and Skrondal (2021), p. 102-104 for a helpful comparison.\n\n\n\n\n\n\n\nFigure 5.3: Fixed vs. random factors.\n\n\n\n\n\nDimension (I) asks about the status of the observed factor levels (Searle, Casella, and McCulloch 1992, 7–16; Brown and Mosteller 1991, 194–99; Underwood 1997, 199; Rabe-Hesketh and Skrondal 2021, 103–4). For fixed effects, these very specific levels, which we have observed, are of direct interest, with no concern whatsoever for any other possible versions of the factor. For random effects, the observed levels are of no particular interest in themselves; they are only relevant insofar as they tell us something about the population they represent. Factors for which no additional levels exist would naturally sit at the fixed end of the dimension. For other factors, there may be additional levels, but we may have deliberately restricted our focus to the set of versions at hand, and chose to disregard other classes.\nDimension (II) is related to our discussion in Chapter 2, where we talked about the intended range of validity, or inference space, for our statistical conclusions (Eisenhart 1947, 20; Cornfield and Tukey 1977, 913; Anderson and McLean 1974, 57–59). If statistical uncertainty statements are to be applicable only to the observed set of levels, we would represent the factor as fixed. If, instead, we wish to extend our conclusions, on statistical grounds, to a larger population of factor levels, from which the versions at hand can be considered a representative set, we would designate this factor as random.\nDimension (III) concerns prediction. [; Rabe-Hesketh and Skrondal (2021), p. 103]\nDimension (IV) moves on to attributes of the factor in question. What matters here is to which extent the observed set of levels exhaust the population, i.e. whether the existing levels have been sampled completely or incompletely (Cornfield and Tukey (1977), p. 909; Wilk and Kempthorne (1955), p. 1153; Nelder (1956), p. 50). At the fixed end, there are no further levels that could (even theoretically) be of interest, simply because none exist: sampling is complete. At the random end, the number of observed levels is very small compared to the size of the population, which could in fact be infinite; samping is incomplete. There are obviously intermediate situations, and the vertical scale may be considered as indicating the share of the population that was observed, ranging from 0 (infinite population) to 1 (population exhausted).\nDimension (V) shifts the focus to research methodology; specifically, how the observed levels were selected (Eisenhart 1947, 19). At the fixed end of the continuum, the classes in our study are the result of a purposeful and determined selection procedure, with no chance element involved. The choice of levels is determined on substantive grounds, in light of our research objectives. For random factors, on the other hand, the set of levels is obtained via random sampling, a procedure that grants each level in the (potentially infinite) population the same probability of being selected. The choice of levels rests on a purely haphazard, chance process.\nDimension (VI) considers what would happen in a direct replication of the study, i.e. a repetition with data collected under similar circumstances (Eisenhart 1947, 20). For fixed effects, it would be typical for a prospective replication to include the same set of levels as the original study. For a random factor, on the other hand, direct replications would observe a new set of levels.\nNelder (1977: 50-51) distinguishes between two kinds of random term: (i) a component of error, where no interest attaches to individual values, only their mass behavior; and (ii) effects of interest which is nevertheless specifies in terms of a sample from a population.\nThere may be additional considerations when a model also includes level-1 predictors. (Rabe-Hesketh and Skrondal 2021, 176–78)\n\n\n5.3.5 Fixed and random factors: Prototypes\nWe can now think about prototypical exemplars for each category. Such representatives would be located at (or very near) the extreme ends of the vertical dimensions. Let us start with fixed effects. We will try to name factors from different research contexts:\n\nPlace of articulation\nWord class\n\nThe prototypical example of a random factor is the speaker. Individual speakers are almost never of direct substantive interest (I), and inferences therefore usually aim at a broader range of validity than the set of individuals observed (II). Informants could, in principle, be selected via techniques that may approach that of random sampling (III). They are usually a small subset of the population they represent (IV).\nLet us also consider some intermediate cases: GENRE.\n\n\n\n\n\n\n\n\n\nFigure 5.4: Examples of fixed and random factors.\n\n\n\n\nKeppel and Wickens (2004, 533) state that “fixed and random factors do not exist in the world, but are interpretations we impose when designing and analyzing a study”.\n\n\n5.3.6 Three functions of random factors\n\nTo specify the appropriate source of variation against which a systematic factor should be compared (Nelder 1956, 51).2\nTo extend the generality of a particular estimate on statistical grounds (Keppel and Wickens 2004, 533)\nTo obtain more stable estimates for the individual levels of the factor.\nAs a general shrinkage or regularization device.\n\n2 In the DoE literature this corresponds to the choice of the right error term, or the appropriate mean square (MS) which should appear in the denominator of the F-ratio.\n\n5.3.7 Special considerations in observational data settings\nIn experimental work, the researcher may build an appropriate design based on their research objectives. The intended scope of inference has clear implications for the design and analysis of the experiment (Lorenzen and Anderson 1993, 8–9)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical models: Integration of research objectives and data structure</span>"
    ]
  },
  {
    "objectID": "04_statistical_models.html#the-structural-component-fixed-vs.-random-factors",
    "href": "04_statistical_models.html#the-structural-component-fixed-vs.-random-factors",
    "title": "5  Statistical models: Integration of research objectives and data structure",
    "section": "5.4 The structural component: Fixed vs. random factors",
    "text": "5.4 The structural component: Fixed vs. random factors\n\n5.4.1 Clustering by source\nConsidering the delineation of fixed and random factors in the previous section, Speaker clearly classifies as a random factor. Only if we decide to deliberately restrict our inferences to the set of speakers observed does it make sense to treat Speaker as a fixed factor. In psychology, it is fairly routine practice to treat human subjects as a random factor. So-called within-subject designs correspond to our setting where a token-level predictor occurs in the presence of clustering by source. This token-level predictor is assessed against its consistency across subjects.\nIllustration with the (ING) data\nWe will use the (ING) data to illustrate the consequences of treating speaker as a fixed or random factor. We will also consider two speaker-level predictors, Date of Birth and Sex. We will start with a random-factor approach, to obtain broad-inference estimates. The resulting statistical inferences therefore extend to the sampled population, i.e. the population represented by the speakers in Forrest’s study. Thus, we are extending our statistical conclusions to unobserved speakers from this population.\n\n\n\n\n\n\n\n\nFigure 5.5: Speaker-level predictors: Broad- vs. narrow-inference estimates.\n\n\n\n\n\nThese kinds of regression plots go by different names, including predictive margins (e.g. Stata).\nRabe-Hesketh and Skrondal (2021), p. 156-157 rite that in Stata, a distinction is made between adjusted predictions, where you hold covariates at specific values, and predictive margins, where you evaluate covariates at their means.\nSpeaker as random s. fixed effect Speaker-level factor\n\n\n5.4.2 Clustering by lexical unit\nIn certain settings, we may chose not to consider the lexical items in our corpus study as a sample. In general, we have the option of treating words as if they were a sample from some population to which we wish to make inferences. Whether this makes sense depends on your inferential objectives. If your interest is in making inferences about, say, present-day British English, you may chose to restrict your attention to the set of lexemes that instantiate a certain construction or structure in this specific language (variety). This makes sense, of course. You would then treat words as fixed effects.\nYou could also conceptualize your inferences as extending to the systematic forces that produce certain languages or language varieties. Then, it makes sense to consider the set of lexemes that happens to have emerged to instantiate a certain construction in a particular language as a sample result of this process. Your inferential objectives are broader.\nWhich perspective you chose to take is up to you. There are statistical option matching each perspective. If your interest is restricted to the set of words in your data, you treat these words as a fixed effect. If you consider them as a sample from a certain system, they would be treated as random. Which of these conceptualizations makes more sense is a matter of debate. We will juxtapose these two perspectives and resulting differences in the statistical conclusions later on. Sometimes, we may have to resort to practicable solutions. If we are forced to choose a certain analysis strategy, it helps to have some idea about how this affects the statistical summaries we end up with.\nIn some cases, there may simply not be enough data points to reliably handle rare words. This may then affect our comparisons between groups of words. Sparse data bias can be a problem. Then, a random.effects approach may be more feasible.\n\n\n\n\n\n\n\n\nFigure 5.6: Word-level predictors: Broad- vs. narrow-inference estimates.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical models: Integration of research objectives and data structure</span>"
    ]
  },
  {
    "objectID": "04_statistical_models.html#the-fixed-vs.-random-effects-debate-in-language-research",
    "href": "04_statistical_models.html#the-fixed-vs.-random-effects-debate-in-language-research",
    "title": "5  Statistical models: Integration of research objectives and data structure",
    "section": "5.5 The fixed-vs.-random effects debate in language research",
    "text": "5.5 The fixed-vs.-random effects debate in language research\nIn data situations where Subject and Item are crossed - that is, where every subject encounters every item - the statistical analysis may proceed in different ways. The choice of which strategy is “appropriate” has received considerable attention in the psycholinguistic literature. We will first take a closer look at how the controversy has unfolded in this subfield of language research and then transfer the implications of the arguments raised to corpus-based observational work. It is important to clearly separate two aspects of this debate: (i) the conceptual form of the statistical analysis and (ii) the choice of statistical procedure for its implementation. Debates about the conceptual analysis form have been much more controversial, because they (must) take into account not only aspects of data structure but also the nature of the linguistic phenomenon studied as well as study design and the researcher’s objectives. It is also the more important aspect of the debate, since the statistical uncertainty attached to the results varies greatly depending on the general analysis strategy. The same conceptual form of analysis may then be implemented using different procedures, and the choice between them essentially boils down to purely statistical arguments (such as the Type I and II error rates, or the ability of the procedure to handle unbalanced and/or missing data). These issues can be settled more easily using, say, simulation studies that compare the performance of different procedures.\n\n5.5.1 The debate in psycholinguistics\nWe will divide our summary of the fixed-vs-random-effect debate in psycholinguistics according to the two aspects just mentioned - conceptual form of analysis and choice of statistical procedure. Before we go further, however, let us call into mind key features of data derived from psycholinguistic experimentation.\n\nThe data are balanced: Each subject and each item contributes the same number of tokens. (2a) The experimenter selects (or samples) subjects from a speaker population. (2b) The choice of subjects may be stratified (e.g. by age and gender). (3a) The experimenter selects (or samples) items from a language population. (3b) The choice of items may be stratified (e.g. by frequency and word class)\nRandomization may be applied to token-level predictors.\n\n\n5.5.1.1 Illustrative data layouts\nTo make sense of the controversy surrounding the decision of whether to treat SUBJECT and/or ITEM as fixed or random factors, we must understand the scientific contexts within which scholars have gone on record with regard to this issue. We will concentrate on two data situations, which are sufficiently general to allow for extrapolation to other settings. In both scenarios, Subject and Item are crossed. This means that every subject encounters every item. A further commonality between design (1) and (2) is that Subject is crossed with Treatment. If Treatment has two levels (A and B), every subject encounters both treatment levels. This means that Treatment varies within Subject, and in psyhcologinguistics this is usually referred to as a within-subjects design.\nThe difference between the two designs is the relation between Item and Treatment. In design (1), Item is nested within Treatment. As illustrated in …, this means that a particular item either appear under treatment A or treatment B. This is illustrated in Figure 5.7: While subjects (the rows) respond to both treatment A and B, items are separated by treatment. Items 1-4 appear with treatment A, item 5-8 with treatment B. Such a data layout is typical for situations where the factor of interest is an attribute of items (e.g. word class, concreteness, animacy, frequency class, etc.). Note how Subject is crossed with Treatment: Each row cuts across both treatments.\n\n\n\n\n\n\n\n\n\nFigure 5.7: Factor diagram for design 1, where Item is nested under Treatment\n\n\n\n\nIn design (2), on the other hand, Item is crossed with Treatment. This means that each item appears with both treatment A and treatment B. This data layout is shown in Figure 5.8. In this design, then, Treatment is both a within-subject as well as a within-item variable.\n\n\n\n\n\n\n\n\n\nFigure 5.8: Factor diagram for design 2, where Item and Treatment are crossed.\n\n\n\n\nFor illustration, we draw on constructed data sets from Raaijmakers, Schrijnemakers, and Gremmen (1999, Table 2, p. 418) for design (1) and Raaijmakers (2003, Table 6, p. 148) for design (2).\nData description\n\n\n5.5.1.2 The conceptual form of analysis\nThe discussion began in the domain of psycholinguistics, with a paper by Coleman (1964). This paper concerned psycholinguistic experimentation, where subjects responded to a set of items (“stimuli” or “materials”), and treatment factor varied within item. Coleman (1964) noted that psycholinguistic work in those days only consider generalizability of findings across subjects, but never considered the question of generalizability across items. He rightly argued that statistical inferences then only extend to the set of items used in the experiment. However, he arguably went too far when stating that “investigations of verbal behavior are scientifically pointless if their conclusions have to be restricted to the specific materials used in the experiment” (Coleman 1964, 19). Two aspects are critical here. First, in this area of research (psycholinguistic experimentation), generalization across items may indeed be the intention of the researcher. Further, Coleman (1964, 222–23) talks about situations where items “logically represent a sampling variable”.\n“double analysis” [Coleman1964, p. 223]\nAn influential paper by Clark (1973) led to the widespread recognition of the issue. The paper was concerned with experimental work and offered a detailed discussion of the statistical (re)analysis of two studies, where the predictor of interest was crossed with SUBJECT and (i) nested in ITEM or (ii) crossed with ITEM. Clark (1973) demonstrated that the choice of whether ITEM is treated as a fixed or random factor altered the binary interpretation of the resulting p-value, when judged against a .05 significance threshold. The paper also discusses alternative design and analysis strategies. As for the question of when a researcher should choose to represent ITEM as a random factor, Clark (1973) [pp. 348-349] gives a simple answer: “whenever the language stimuli used do not deplete the population from which they were drawn”. The question of whether items were sampled randomly is “in a sense, secondary”. Clark (1973) therefore emphasizes one specific dimension of the fixed-vs.-random distinction. Since he backgrounds the role of random sampling, he appropriately considers as the (immediate) target of inference the “language population sampled” (1973, 346), i.e. “the population defined by [the] non-random sampling procedure” (1973, 349). Clark (1973, 349) notes that if inferences do not legitimately extend to the sampled population, there is “no possibility of generalizing […] findings to the more inclusive population”. He notes that “the non-random sampling procedure causes difficulty only later when the investigator wants to determine exactly what population he can legitimately generalize his results to”. The paper also discusses the difficulties in psycholinguistic research of drawing ITEMS at random while at the same time balancing materials across relevant properties such as frequency, word class, etc.\n\nAlso discusses design issues: pay attention to number of SUBJECTS and ITEMS\nMentions peer review as an obstacle to more appropriate reporting\nMentions that ITEM is different from SUBJECT since individual items may be of direct interest (dimension I) (Clark 1973, 354) and are repeatable (dimension VI) (Clark 1973, 355, footnote 12)\n\nColeman (1964) and Clark (1973) assume that researchers intend to generalize findings to a language population.\nThe paper by Clark (1973) made a convincing case, and its forceful recommendation was picked up readily in the scholarly community. The problem with statistical dogmas of this kind is that they offer no room to the individual researcher for careful reflection. The dramatic changes in research practice quite naturally led to critical responses to the points advocated by Clark (1973). Thus, Wike and Church (1976) raised a number of important counterpoints. Their recommendation, however, was equally categorical, as their paper closes with the suggestion “that investigators continue using fixed factor designs”. A close reading of their paper is instructive, however. We will disregard the statistical criticism directed against Clark (1973)’s proposed method for statistically generalizing to two populations and focus on the more substantive aspects. They start by raising the question of what a random effect is, and refer to “the usual definitions”, which correspond to dimensions IV and V in Figure 5.3. They emphasize the mathematical definition of random factors, which considers them as random variables. The consequence of this was illustrated in SECTION_statistical_inference. They call Clark (1973)’s definition, which emphasized dimension IV, as “unorthodox” (Wike and Church 1976, 250). Their most important point is that the kinds of generalizations researcher intend to make cannot be made exclusively on statistical grounds, but require replication. They emphasize that statistical and extra-statistical generalization must be distinguished, and note that “generality is not achieved by using one or another statistical procedure in analyzing the results of a single experiment, it arises from the scientific community’s carrying out its relentless and converging mission” (Wike and Church 1976, 253).\nIn the discussion of Clark (1973), KeithSmith (1976, 262) rightly states that “it does seem useful […] to consider whether, had these materials been a sample, the main effect would still be large enough to justify confidence in its reality.” He closes by say that “no one should accept [Clark’s] dogma, nor that of his critics (i.e. Wike and Church 1976). Authors will continue to be responsible for defending the generality of their findings across subjects and materials, statistically where appropriate, logically always.”\nA close reading of Clark (1973) and Wike and Church (1976) shows that their points of view on permissive generalizations are quite similar. Wike and Church (1976) state that the random-effects model allows us to “generalize to larger populations for which the selected levels are typical or representative” (1976, 253), which corresponds to Clark (1973)’s “population defined by [the] non-random sampling procedure” (1973, 349). Wike and Church (1976) explicitly state that the delimitation of this population is “outside the realm of statistics” (1976, 253), and while Clark does not use the word extra- (or non-)statistical generalization, he does note the “difficulty” of delineating the sampled population.\nA reconciliatory and softened summary of the early exchange on the random-vs.-fixed debate may read as follows: Clark (1973) drew researcher’s attention to the fact that the substantive generalizations they were trying to attain often involved extrapolations across two populations, SUBJECTS and ITEMS. The statistical methods they routinely applied, however, only offered statistical inferences across one of these dimensions. Accordingly, he advocated the use of an analysis strategy that would bring the statistical inference more into line with the substantive inference goals. Wike and Church (1976), on the other hand, cautioned against equating the two types of inferences - i.e. that the choice of data-analytic strategy would solve the non-statistical problems involved in generalization.\nColeman (1979) suggests that the term “random effect” be replaced with the label “generalization effect”. He also makes the sensible suggestion that editorial policies should require that results for both types of analyses, the fixed-effect and the random-effect version, should be reported. He refers to Cornfield and Tukey (1977)’s two-brides metaphor, and the suggested practice then creates bridges of varying spans. This makes a lot of sense, and we will adopt the practice of reporting both kinds of statistical summaries. However, use of the label “generalization effect” should be avoided since it encourages misconceptions [cf. Raaijmakers (2003); see …].\nWickens and Keppel (1983) offer a balanced discussion of model choice, noting that “the appropriate statistical analysis of a set of data is not absolutely determined” (1983, 297). They draw attention to the relevance of the design of a study, noting that “many investigations are ill-designed to employ Clark’s suggestion”. They demonstrate how the statistical sensitivity (power) of a design varies across features related to study design (number of subjects, number of items), and the pattern of variation in the outcome (variation among subjects, variation among items). More resources should be allocated to the clustering variable that shows more variation across units. Further, they discuss the role of item variability and item-level variables. If these are used in the selection (or stratified sampling) of materials, then they must be included into the analysis, especially if items are not well balanced across relevant classifications. In such a case, Wickens and Keppel (1983) state that “lack of a truly random-sampling scheme for materials does not prevent the random-effects model from being the most accurate and leading to the best tests” (1983, 304).\nForster and Dickinson (1976) ran simulation studies and suggested that the question of whether the joint criterion (F1 x F2) or minF/quasi-F is the ideal analysis strategy depended on the structure of variation in the data: If there is large variation among items in the overall outcome score, and if the treatment effect varies considerably across subjects, minF/quasi-F are the measures of choice. If, on the other hand, there is little variability among items, and if the treatment effects is homogeneous across subject, the joint criterion seemed a better choice. It is of interest to note that Forster and Dickinson (1976) conclude by saying that the joint criterion is “at best a stop-gap procedure”, noting that “a far better solution would be to improve the procedure for estimating the degrees of freedom of minF`” (1976, 142).\nRaaijmakers, Schrijnemakers, and Gremmen (1999) note that “by the early 1980s the issue was more or less settled an researchers started to routinely perform both subject and item analyses” (1999, 419). The question of how exactly a statistical analysis is carries out had not been settled, since there are in fact a number of possibilities for including Item as a random effect into the analysis. Without going much into the details, there are basically three options. Let us consider two different study designs. In both designs, the factors Subject and Item are crossed.\n\nF1: By-item analysis\nF2: By-subject analysis\nF1 x F2: Joint criterion, require both F1 and F2 to be significant\nF´ (quasi-F)\nminF: an approximation to F´ (quasi-F)\n\nWhat to do in situations where the selection of items is stratified? Raaijmakers (2003) notes that is depends on the degree to which matching was successful.\nThe form of analysis that was employed, however, failed to fully address the statistical issues discussed by Coleman (1964) and Clark (1973).\nLorch and Myers (1990) discuss the analysis of design I (Item nested within Treatment) from a regression (rather than ANOVA) perspective, and compare three analytic approaches. These are (i) averaging data over subjects and then regressing the item means on the predictor variable; (ii) implicitly treating Item as a fixed effect, i.e. excluding it from the model; and (iii) the inclusion of both Subject and Item as random factors. They recommend analysis (iii) and describe how appropriate test may be obtained by following a certain sequence of steps. Following this sequence allows the analyst to obtain, at specific steps, the appropriate error terms for the different types of predictor variables. This laborious (and error-prone) procedure is largely equivalent to a mixed-effects regression model, which yields relevant error terms based on a single model including all variables. Interestingly, Baayen, Davidson, and Bates (2008) attribute to Lorch and Myers (1990) the “random regression” approach to data based on design (I). This strategy involves fitting separate regression models, one for each subject, and then performing simple analyses of the set coefficients (e.g. whether their mean differs statistically from 0). However, it semms that Lorch and Myers (1990) use this approach mainly as an illustration of the rationale underlying by-subject analyses.\nI would like to understand the stepwise regression procedure.\nBaayen, Davidson, and Bates (2008; see also Baayen 2008, 259–75) demonstrate the use of mixed-effects regression for a number of different designs. Using simulation, he showed that this procedure performed at least as well as traditional analyses in terms of Type I and Type II error rates, while at the same time producing more informative analyses.\nRaaijmakers (2003) deals with a number of issues, including misconceptions of what it means for a treatment effect to be statistically generalizable across items (or subjects), and what to do if the items used in an experiment exhaust the underlying language population. Here, Raaijmakers (2003) concedes that “it is indeed true that such cases violate the assumptions underlying the random effects model” [p. 148]. However, he asks researchers to consider what a valid replication of their study would or could look like. For instance, if the experiment were repeated using a different language. He notes that if it appears feasible that items could have been selected in a different way, “it is probably best (i.e., the most conservative), to treat the Item factor as random rather than fixed.” [p. 149].\nBarr et al. (2013) argue for random slopes\nBarr (2018) argues for generalizing over encounters, i.e. a random interaction between Subject and Item\nMatuschek et al. (2017) argue in favor of model selection to balance power and Type-I error rates\n\n\n5.5.1.3 The choice of statistical procedure\nAn early paper advocating the analysis of repeated-measures data using mixed-effects modeling is Quené and Bergh (2004), who emphasized two statistical advantages of this approach: its ability to handle missing data and sidestep the sphericity assumption.\nLocker, Hoffman, and Boviard (2007)\nBaayen, Davidson, and Bates (2008) then discussed the use of mixed-effects models with crossed random factors.\nQuené and Bergh (2004) argue that items should be considered as nested under subject. This form model assumes, with regard to a specific item, no commonalities among subjects - i.e. that each speaker responds differently the this item. An intermediate solution is to include an additional random intercept for the Subject-by-Item interaction (cf. Barr 2018).\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 Analysis of illustrative data\n\n5.5.2.1 Design (1)\nBy-subject analysis: F1\nAs explained by Baayen (2008, 263–64), the by-subject analysis first requires averaging over items. This step is illustrated in Figure 5.9, where items are greyed out. The analysis is therefore based on 8 scores, and each scores is an average over 4 items. Figure 5.10 shows the structure of the data. Each line represents a subject.\n\n\n\n\n\n\n\n\n\nFigure 5.9: By-subject analysis for design I: Factor diagram\n\n\n\n\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.10: Data structure for the by-subject analysis (design I).\n\n\n\n\n\n\n               df   SS   MS    F      p\ntreatment       1 2008 2008 7.41 0.0297\nerror(within)   7 1897  271            \nerror(subject)  7 6563  938            \n\n\nThe mixed-effects regression version of the by-subject analysis is the following. It yields the same value for F1.\n\n\n          df   SS   MS    F\ntreatment  1 2008 2008 7.41\n\n\nBy-item analysis: F2\nAs illustrated by Baayen (2008, 263–64), the by-item analysis first requires averaging over subjects. This step is illustrated in Figure 5.11, where subjects are backgrounded. The analysis is therefore based on 8 scores, and each scores is an average over 4 subjects. Figure 5.12 shows the structure of the data. Each point represents an item.\n\n\n\n\n\n\n\n\n\nFigure 5.11: By-item analysis for design I: Factor diagram\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.12: Data structure for the by-item analysis (design I).\n\n\n\n\n\n\n            df   SS   MS    F     p\ntreatment    1 1004 1004 2.17 0.191\nerror        6 2772  462           \n\n\nJoint criterion\nIf we were to use the joint criterion, we would declare the difference between treatments A and B statistically significant if both the by-subjects analysis (F1) and the by-item analysis (F2) yielded a statistically significant effect for Treatment. While the by-subjects p-value turned out to by smaller than the conventional .05 mark (p = .03), the by-item p-value failed to reach this threshold (p = .19).\nQuasi-F and minF\nClark (1973) proposed \\(minF^\\prime\\), a simple approximation to quasi-F (or pseudo-F), using the following formula:\n\\[\nminF^\\prime = \\frac{F1 \\times F2}{F1 + F2}\n\\]\nFor design (1), \\(minF^\\prime\\) is 1.678\n\n\nWarning in anova.lm(m): ANOVA F-tests on an essentially perfect fit are\nunreliable\n\n\nAnalysis of Variance Table\n\nResponse: RT\n                  Df Sum Sq Mean Sq F value Pr(&gt;F)\nsubject            7  26252    3750     NaN    NaN\ntreatment          1   8033    8033     NaN    NaN\nitem               6  22174    3696     NaN    NaN\nsubject:treatment  7   7587    1084     NaN    NaN\nsubject:item      42   4209     100     NaN    NaN\nResiduals          0      0     NaN               \n\n\nAnalysis using mixed-effects model\n\n\nlmer(formula = RT ~ treatment + (1 | item) + (treatment | subject), \n    data = design_1)\n            coef.est coef.se\n(Intercept) 563.31    12.38 \ntreatmentB  -22.41    17.10 \n\nError terms:\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 21.20         \n subject  (Intercept) 17.39         \n          treatmentB  22.18    0.08 \n Residual             10.01         \n---\nnumber of obs: 64, groups: item, 8; subject, 8\nAIC = 543.7, DIC = 558\ndeviance = 543.6 \n\n\n\n\nAnalysis of Variance Table\n          npar  SS  MS    F\ntreatment    1 172 172 1.72\n\n\n\n\n5.5.2.2 Design (2)\nBy-subject analysis (F1)\n\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.13: Data structure for the by-subject analysis: Design (2).\n\n\n\n\n\n\n               df   SS   MS    F        p\ntreatment       1 1864 1864 1157 5.59e-05\nerror(within)   3    5    2              \nerror(subject)  3    1    0              \n\n\nBy-item analysis (F2)\n\n\n`summarise()` has grouped output by 'item'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.14: Data structure for the by-item analysis: Design (2).\n\n\n\n\n\n\n              df   SS   MS    F      p\ntreatment      1 3729 3729 5.97 0.0446\nerror(within)  7 4373  625            \nerror(item)    7 3647  521            \n\n\nJoint criterion\nSince both the by-subjects analysis (p = .00) and the by-item analysis (p = .04) yielded a statistically significant effect for Treatment, the effect of Treatment would be declared statistically significant based on the joint criterion.\nQuasi-F and minF\nClark (1973) proposed \\(minF^\\prime\\), a simple approximation to quasi-F, using the following formula:\n\\[\nminF^\\prime = \\frac{F1 \\times F2}{F1 + F2}\n\\]\nFor design (2), \\(minF^\\prime\\) is 5.939.\nFor quasi-F, two different formulas exist. The first is due to Scatterthwaite (1946), the second is a modification proposed by Cochran (1951), which generally works better (cf. Cobb 1997, 597).\n\n\nWarning in anova.lm(m): ANOVA F-tests on an essentially perfect fit are\nunreliable\n\n\nAnalysis of Variance Table\n\nResponse: RT\n                       Df Sum Sq Mean Sq F value Pr(&gt;F)\nsubject                 3      4       1     NaN    NaN\nitem                    7  14589    2084     NaN    NaN\ntreatment               1  14915   14915     NaN    NaN\nsubject:treatment       3     39      13     NaN    NaN\nitem:treatment          7  17491    2499     NaN    NaN\nsubject:item:treatment 42   1467      35     NaN    NaN\nResiduals               0      0     NaN               \n\n\n\\[\nF^\\prime_{Scatterthwaite} = \\frac{MS_T}{MS_{T \\times I} + MS_{T \\times S} - MS_{T \\times I \\times S}}\n\\]\n\\[\n\\frac{14915}{2499 + 13 - 35}\n\\]\n\n\n[1] 6.02\n\n\n\\[\nF^\\prime_{Cochran} = \\frac{MS_T + MS_{T \\times I \\times S}}{MS_{T \\times I} + MS_{T \\times S}}\n\\]\n\\[\n\\frac{14915}{2499 + 13 - 35}\n\\]\n\n\n[1] 5.95\n\n\nDegrees of freedom Scatterthwaite:\nNumerator: df of Treatment\nDenominator:\n\\[\n\\frac{(MS_{T \\times S} + MS_{T \\times I} + MS_{T \\times S \\times I})^2}{\\frac{{MS_{T \\times S}}^2}{df_{T \\times S}} + \\frac{{MS_{T \\times I}}^2}{df_{T \\times I}} + \\frac{{MS_{T \\times S \\times I}}^2}{df_{T \\times S \\times I}}}\n\\]\n\n\n[1] 6.88\n\n\nDegrees of freedom Cochran:\nNumerator:\n\\[\n\\frac{(MS_{T} + MS_{T \\times S \\times I})^2}{\\frac{{MS_{T}}^2}{df_{T}} + \\frac{{MS_{T \\times S \\times I}}^2}{df_{T \\times S \\times I}}}\n\\]\nDenominator:\n\\[\n\\frac{(MS_{T \\times S} + MS_{T \\times I})^2}{\\frac{{MS_{T \\times S}}^2}{df_{T \\times S}} + \\frac{{MS_{T \\times I}}^2}{df_{T \\times I}}}\n\\]\nAnalysis using mixed-effects model\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nlmer(formula = RT ~ treatment + (1 | item) + (1 | subject), data = design_2)\n            coef.est coef.se\n(Intercept) 499.06     6.16 \ntreatmentB   30.53     4.65 \n\nError terms:\n Groups   Name        Std.Dev.\n item     (Intercept) 14.74   \n subject  (Intercept)  0.00   \n Residual             18.59   \n---\nnumber of obs: 64, groups: item, 8; subject, 4\nAIC = 567.8, DIC = 578\ndeviance = 568.0 \n\n\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance Table\n          npar    SS    MS    F\ntreatment    1 14915 14915 43.2\n\n\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance Table\n          npar  SS  MS    F\ntreatment    1 230 230 6.55\n\n\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance Table\n          npar    SS    MS    F\ntreatment    1 14915 14915 43.2\n\n\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nAnalysis of Variance Table\n          npar  SS  MS    F\ntreatment    1 230 230 6.55\n\n\n\nLeonard - Orchard 1996\nBaugh 2001\nQuene - van den Bergh 2003 [!]\nGorman 2009\nSmith 2009\nJanssen 2012\nGorman & Johnson 2013\nPark et al. 2020\nFrossard - Renaud 2020\nDeBruine - Barr 2021 [!]\n\n\n\n\n\nAnderson, Virgil E., and Robert A. McLean. 1974. Design of Experiments: A Realistic Approach. New York: Marcel Dekker.\n\n\nBaayen, R. Harald. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using R. Cambridge: Cambridge University Press.\n\n\nBaayen, R. Harald, Douglas J. Davidson, and Douglas M. Bates. 2008. “Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items.” Journal of Memory and Language 59 (4): 390–412. https://doi.org/10.1016/j.jml.2007.12.005.\n\n\nBarr, Dale J. 2018. “Generalizing over Encounters: Statistical and Theoretical Considerations.” In The Oxford Handbook of Psycholinguistics, edited by Shirley-Ann Rueschemeyer and M. Gareth Gaskell, 917–29. Oxford: Oxford University Press. https://doi.org/10.1093/oxfordhb/9780198786825.013.39.\n\n\nBarr, Dale J., Roger Levy, Christoph Scheepers, and Harry J. Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” Journal of Memory and Language 68 (3): 255–78. https://doi.org/10.1016/j.jml.2012.11.001.\n\n\nBrown, Constance, and Frederick Mosteller. 1991. “Variance Components.” In Fundamentals of Exploratory Analysis of Variance, edited by David C. Hoaglin, Frederick Mosteller, and John W. Tukey, 193–244. New York: Wiley.\n\n\nClark, Herbert H. 1973. “The Language-as-Fixed-Effect Fallacy: A Critique of Language Statistics in Psychological Research.” Journal of Verbal Learning and Verbal Behavior 12 (4): 335–59. https://doi.org/10.1016/S0022-5371(73)80014-3.\n\n\nCobb, George W. 1997. Introduction to Design and Analysis of Experiments. New York: Wiley.\n\n\nCochran, William G. 1951. “Testing a Linear Relation Among Variances.” Biometrics 7 (1): 17–32. https://doi.org/10.2307/3001601.\n\n\nColeman, Edmund B. 1964. “Generalizing to a Language Population.” Psychological Reports 14 (1): 219–26. https://doi.org/10.2466/pr0.1964.14.1.219.\n\n\n———. 1979. “Generalization Effects Vs Random Effects: Is \\(\\sigma_{TL}^{2}\\) a Source of Type 1 or Type 2 Error?” Journal of Verbal Learning and Verbal Behavior 18 (2): 243–56. https://doi.org/10.1016/S0022-5371(79)90145-2.\n\n\nCornfield, Jerome, and John W. Tukey. 1977. “Average Values of Mean Squares in Factorials.” Journal of the Royal Statistical Society A 50 (1): 48–76. https://doi.org/10.1214/aoms/1177728067.\n\n\nEisenhart, Chruchill. 1947. “The Asumptions Underlying the Analysis of Variance.” Biometrics 3 (1): 1–21. https://doi.org/10.2307/3001534.\n\n\nForster, Kenneth I., and Rod G. Dickinson. 1976. “More on the Language-as-Fixed-Effect Fallacy: Monte Carlo Estimates of Error Rates for F1, F2, F’, and Min F’.” Journal of Verbal Learning and Verbal Behavior 15 (2): 135–42. https://doi.org/10.1016/0022-5371(76)90014-1.\n\n\nGelman, Andrew. 2005. “Analysis of Variance: Why It Is More Important Than Ever.” The Annals of Statistics 33 (1): 1–53. https://doi.org/10.1214/009053604000001048.\n\n\nKeithSmith, J. E. 1976. “Discussion of Wike and Church’s Comments.” Journal of Verbal Learning and Verbal Behavior 15 (3): 262–63. https://doi.org/10.1016/0022-5371(76)90024-4.\n\n\nKeppel, Geoffrey, and Thomas D. Wickens. 2004. Design and Analysis: A Researcher’s Handbook. London: Pearson.\n\n\nLocker, Lawrence, Lesa Hoffman, and James A. Boviard. 2007. “On the Use of Multilevel Modeling as an Alternative to Items Analysis in Psycholinguistic Research.” Behavior Research Methods 39: 723–30. https://doi.org/10.3758/BF03192962.\n\n\nLorch, Robert F., and Jerome L. Myers. 1990. “Regression Analyses of Repeated Measures Data in Cognitive Research.” Journal of Experimental Psychology: Learning, Memory, and Cognition 16 (1): 149–57. https://doi.org/10.1037/0278-7393.16.1.149.\n\n\nLorenzen, Thomas J., and Virgil L. Anderson. 1993. Design of Experiments: A No-Name Approach. New York: Dekker.\n\n\nMatuschek, Hannes, Reinhold Kliegl, Shravan Vasishth, R. Harald Baayen, and Douglas Bates. 2017. “Balancing Type i Error and Power in Linear Mixed Models.” Journal of Memory and Language 94: 305–15. https://doi.org/10.1016/j.jml.2017.01.001.\n\n\nNelder, John A. 1956. “A Reformulation of Linear Models.” The Annals of Mathematical Statistics 27 (4): 907–49. https://doi.org/10.2307/2344517.\n\n\nQuené, Hugo, and Huub van den Bergh. 2004. “On Multi-Level Modeling of Data from Repeated-Measures Designs: A Tutorial.” Speech Communication 43 (1-2): 103–24. https://doi.org/10.1016/j.specom.2004.02.004.\n\n\nRaaijmakers, Jeroen G. W. 2003. “A Further Look at the ‘Language-as-Fixed-Effect Fallacy’.” Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale 57 (3): 141–51. https://doi.org/10.1037/h0087421.\n\n\nRaaijmakers, Jeroen G. W., Joseph M. C. Schrijnemakers, and Frans Gremmen. 1999. “How to Deal with \"The Language-as-Fixed-Effect Fallacy\": Common Misconceptions and Alternative Solutions.” Journal of Memory and Language 41 (3): 416–26. https://doi.org/10.1006/jmla.1999.2650.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.\n\n\nScatterthwaite, F. E. 1946. “An Approximate Distribution of Estimates of Variance Components.” Biometrics Bulletin 2 (6): 110–14. https://doi.org/10.2307/3002019.\n\n\nSearle, Shayle R., George Casella, and Charles E. McCulloch. 1992. Variance Components. Hoboken, NJ: Wiley.\n\n\nUnderwood, A. J. 1997. Experiments in Ecology. Cambridge: Cambridge University Press.\n\n\nWickens, Thomas D., and Geoffrey Keppel. 1983. “On the Choice of Design and of Test Statistic in the Analysis of Experiments with Sampled Materials.” Journal of Verbal Learning and Verbal Behavior 22 (3): 296–309. https://doi.org/10.1016/S0022-5371(83)90208-6.\n\n\nWike, Edward L., and James D. Church. 1976. “Comments on Clark’s \"The Language-as-Fixed-Effect Fallacy\".” Journal of Verbal Learning and Verbal Behavior 15 (3): 249–55. https://doi.org/10.1016/0022-5371(76)90023-2.\n\n\nWilk, MArtin B., and Oscar Kempthorne. 1955. “Fixed, Mixed, and Random Models.” Journal of the American Statistical Association 50 (272): 1144–67. https://doi.org/10.2307/2281212.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical models: Integration of research objectives and data structure</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html",
    "href": "05_model_specification.html",
    "title": "6  Model specification",
    "section": "",
    "text": "6.1 Model specification based on a plot plan\nThe model employed to analyze corpus data must be related to the data structure. Nesting relationships have clear implications for model specification. Crossing, on the other hand, opens up the possibility of interaction patterns among factors. Interactions between predictor variables and factors belonging to the structural component (i.e. Speaker or Item) may also have implications for model specification, and may shed light on linguistically relevant variability in the data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html#model-specification-based-on-a-plot-plan",
    "href": "05_model_specification.html#model-specification-based-on-a-plot-plan",
    "title": "6  Model specification",
    "section": "",
    "text": "6.1.1 A plot plan for the ING data\n\n\n\n\n\n\n\n\nFigure 6.1\n\n\n\n\n\nOnce the plot plan for a set of data is set up, with all structural and systematic components included, the nature of the different factors and their relationship can be directly translated into a model statement. An intermediate step is the transfer of the factors into the scheme shown in Figure Figure 6.1. The boxes in the middle tier feature the clustering factors (typically one or two) and the token-level predictors are listed at the bottom. The cluster-level predictors are written above their respective clustering factors. Figure Figure 6.2 shows the filled-in scheme for the (ING) data. Panel (a) gives a simplified version that ignores the clustering factor Item and the word-level predictors Word Class, Frequency, and Preceding Consonant. Panel (b) represents the full data layout.\n\n\n\n\n\n\n\n\nFigure 6.2\n\n\n\n\n\nTo start with, the model includes a term for each factor. All factors except the ones shown in boxes are fixed factors. The dotted lines extending from each box to its associated level-2 predictors represents a nesting relationship: Speakers are nested withing speaker-level predictors, and words are nested within word-level predictors. This nesting relationship is represented in the model if the Speaker and Item are specified as random factors.\nThe arrows in Figure Figure 6.2 show additional terms in the full model. Grey arrows denote crossing relationships involving a random factor. As we noted in Chapter Chapter 4, language-internal and -external clustering factors are crossed. Also, each clustering factor is crossed with any token-level predictor. Finally, each clustering factors is crossed with any level-2 predictors linked to another clustering factor. For instance, Speaker is crossed with any word-level predictor. Black arrows, on the other hand, indicate crossing relationships between fixed factors.\nIn principle, crossing relationship suggest the inclusion of interaction terms into the model. Interaction terms add complexity to a regression model, and it is in many cases unreasonable to represent all crossing relationship as interactions. However, the exclusion of interaction terms should proceed in an informed manner – we need to understand the consequences of a given reduction in model complexity. In Figure Figure 6.2 b, there are three different kinds of crossing relationship, which are shown with different types of arrows. These three types of crossing relationships require different treatment.\nCrossing relationships between fixed factors are shown using black arrows. Whether an interaction between fixed factors should be part of the model depends on the researcher’s aims and background knowledge. The data layout provides no guidance. Note, however, that crossing relationships between level-2 and level-1 predictors are shown using dashed black arrows. Here, the plot plan does offer some guidelines, to which we will return shortly.\nGrey arrows represent crossing relationships involving at least one random factor. Leaving out interaction terms of this type is appropriate only in certain fairly restricted situations. In general, if we decide to leave out one of these terms, it will be replaced by an additional modeling assumption, which can be considered as an additional requirement for valid statistical inferences. This means that the resulting inferences are meaningful only if these additional assumptions are approximately met by the data. All of these assumptions involve the idea of consistency – or constancy – of the pattern associated with a given factor. Let us first consider the crossing relationship between our clustering factors. Leaving out this interaction means that we are comfortable with the assumption that the systematic differences we observe between words will be consistent across speakers. Depending on the structure under study, this assumption will be more or less reasonable. If we are not comfortable with conditioning our inferences on the approximate truth of this assumption, we should include an interaction between Speaker and Item .\nNext, consider the crossing relationship between the clustering factors, Speaker and Item, with each token-level predictor. Absence of interaction here implies that the token-level predictor in question behaves consistently across clusters. An interaction between Speaker and Following Consonant, for instance, signals that speakers react differently to coarticulatory constraints. Some speakers may show a more pronounced sensitivity to the articulatory context, others may show virtually no differences in the rate of g-dropping in different contexts. Similar considerations apply to Item.\n\n\n6.1.2 Random intercepts and random slopes\nThe data layouts that call for an incorporation of random intercepts are different from those that suggest the addition of random slopes. The difference can be understood quite well when looking at our plot plan for corpus data. Random intercepts are needed if our data include predictors measured at the level of the subject or word. These are sometimes referred to as level-2 predictors. Random intercepts then basically inform the analysis about the appropriate sample size for these predictors. In general, then, if the data show clustering and there are cluster-level predictors, the model must include random intercepts.\nIn the literature on the design and analysis of experiments, this issue is treated under the notion of the experimental unit. Thus, a single experiment or study may include experimental units of different size. The design and execution of the experiment determine the size and number of experimental units associated with each factor in the analysis. For corpus data, the situation is simpler. We can rely on our understanding of the linguistic data setting. Instead of experimental unit, we use the term unit of analysis. Thus each predictor is tied to a certain unit of analysis. It is usually straightforward to identify the relevant unit of analysis for a predictor. [DoE: split plot designs. The name derives from its use in agriculture.]\n\nClusters are nested in the predictor.\nAnother setting: No predictors, but overall average.\n\nThe question of whether random slopes should be included in a model is relevant in different settings. This question comes up when there is a predictor that is crossed with subjects and/or words. Three types of crossing can occur:\n\nToken-level predictors may be crossed with subjects and words\nWord-level predictor can be cropssed with subjects\nSubject-level predictors may be crossed with word\n\nIn such settings, we could, in principle, break down the data by subject/word and look at the the predictor for each subject/word individually. If we have enough data to do so, this would give us an idea of whether the association between predictor and outcome is stable across subjects and/or words. It would allow us to answer the question of how much variation we observe among speakers or words with respect to this predictor. We could express the relationship between predictor and outcome for each subject with a parameter, and then we could look at the distribution of these parameters. Ideally, all parameters are very similar, indicating that the predictor of interest operates similarly across speakers or words. As the variation between speakers or words grows, the average over these parameters becomes less and less representative of the set of units.\nOur default attitude will be to consider the possibility that predictor parameters may vary across speakers and/or words. This means that we will always make an effort to inspect and understand this type of variability. Once we have looked at the distribution of parameters, we can decide how to proceed. Essentially, we have three options.\nWe could decide to ignore any variation that may exist between speakers and/or words. This simplifies our analysis and seems like a reasonable option if the variation among speakers is small. Our analysis then proceeds on the assumption that predictor parameters are constant across individuals/words, with observed variation being the result of sampling variation.\nA second option would be to represent between-speaker variability in the predictor parameters in the form of random slopes. These enable the model to take into account the variation in parameter values across individuals. Statistical uncertainty estimates are adjusted accordingly. The model then returns a parameter that expresses the amount of between-subject variability.\nA third option would be similar to the second one, but we add fixed rather than random slopes. This may be an option if our interest is focused on the specific units. This is probably more likely for words than for subjects.\nThe advice that is frequently offered in the statistical literature is to use a statistical test, or a model comparison, to decide whether there is heterogeneity in the parameter values and therefore whether this should be represented in the analysis. We should note that the significance criterion for such a test is usually more lenient, because rejecting the null hypothesis is usually not in the interest of the investigator (elaborate on this). Thus, p-values are set to .20 (Maxwell et al. 2017: 559) or .25 (Lawson 2015: 128). What appears to be problematic with this approach is that it relies exclusively on inferential criteria. If the statistical power of such test is low, this is problematic. We therefore prefer to address this issue descriptively, by appreciating the amount of variability that is there, relating it to the average predictor value across subjects, and then deciding how to proceed.\n\nIn experiments, randomization balances out the heterogeneity (Kish 1987: 13-14)\nHeterogeneity of treatment effects\nUnit-treatment additivity (Cox & Reid 2000)\n\n\n\n6.1.3 Nested variables are treated as random factors\nNested variables that constitute part of the structural component, i.e. our clustering variables Speaker and Item, are typically treated as random effects. While this modeling strategy seems to be fairly uncontroversial for handling the factor Speaker, there are situations where it can be argued that Item may equally well be handled as a fixed effect. We will elaborate on this modeling decision further below. For now, we treat both factors as random, which is the preferred strategy when clustering variables are nested, and interest is focused on level-2 predictors measured on speakers (e.g. Age and Sex) and words (e.g. Word Class and Frequency).\nBy specifying nested structural factors as random effects, we build into the model the appropriate relationship between (i) Speaker and any speaker-level predictors and (ii) Item and any word-level predictors. This means that we clarify the unit of analysis for level-2 predictors.\n\n\n6.1.4 Speaker-level predictors: Consequences of ignoring nesting\n\n\n6.1.5 Token-level predictors\nDue to this crossed relationship, the data provide information about systematic variability in g-dropping by Following Place of Articulation not just averaged across the 66 speakers, but also for each speaker individually. This is to say that we can assess, for each informant, how the share of /ɪŋ/-realizations varies over the four contexts. This will allow us to appraise the level of consistency across speakers. This consistency sheds light on the between-speaker stability of this articulatory constraint, and therefore nicely supplements the information that we obtain from averaging over the 66 speakers in our sample. It reveals to which extend the average pattern holds at the level of the individual speaker. The status of constraints that are relatively stable across speakers is different from those that fluctuate widely across speakers. These two perspectives, the aggregate and the person-specific level of description are combine features of the nomothetic (generalizing) and the idiographic (particularizing) approach to data analysis.\nLooking at the token counts in the lower part of Figure Figure 4.8, there are quite a few sparsely populated cells, especially for velar contexts. Such cells are not going to provide us with reliable speaker-specific estimates. For coronal and other contexts, however, we observe relatively comfortable token counts. Data sparsity, then, can be an obstacle for the assessment of speaker-specific behavior. In such cases where token counts are too small to yield reliable estimates, the evaluation of between-speaker consistency can rely on summary measures of pattern stability, for instance in the form of a standard deviation parameter.\nThe crossing relationship between Item and Following Place of Articulation also allows us to investigate the variation in the likelihood of g-dropping across different phonetic contexts at different levels. Apart from calculating overall averages, we could look into this constraint at the level of the individual words. This will shed light on the differences among the words in terms of their susceptibility to the phonetic context. Between-word variation again addresses the question of consistency with which the constraint operates. However, our linguistic curiosity is likely to take us to the idiographic level of analysis. Thus, if certain words show a stable pronunciation across the four contexts, we would like to know which ones. Why do they run counter to the general trend in that they are largely unaffected by the following place of articulation? We would try to make sense of this finding, and our linguistic intuitions and experience may suggest different possible explanation. Perhaps lexical frequency plays a role, which could lead to stable, entrenched pronunciation routines. Or perhaps high-frequency strings show stronger phonetic fusion and assimilation (e.g. going to). In any case, between-word variability is likely to pose further questions and can lead to linguistic insights.\nData sparsity, of course, again is the bottleneck at the idiographic, lexeme-specific level of analysis. Given the ubiquity of Zipfian frequency profiles, it is not unlikely that the number of tokens available per lexical item quickly levels off into the single-digit range. This is evident from Figure ?fig-ing-following-context-crossed-with-item. Note that we have picked those words that occur at least 20 times. More then 700 words are not shown. Now, even though we are looking at the top end of the frequency list, there are quite a few empty cells. Reliable estimates for the conditions of interest are likely to emerge only for the top-frequency forms. We can fall back on summary measures of between-word variability, and enrich these with a reasonable selection of words.\nBoth crossing relationships afford the opportunity to study between-cluster consistency of token-level predictors, and – subject to the condition of sufficient tokens counts – to advance to an idiographic analysis perspective. This allows us to appreciate whether, and to what extent, average patterns materialize within clusters.\n\n\n6.1.6 Token-level predictors: Bias\nWhenever a multilevel model includes token-level predictors, there is the danger that the estimates for these predictors are biased. Bias may arise when there is an association between the token-level variable and a clustering variable. An association reflect difference among clusters in the distributinon of token-level variables. We will deal with such situations in Chapter ?sec-within-between.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html#dealing-with-categorical-units",
    "href": "05_model_specification.html#dealing-with-categorical-units",
    "title": "6  Model specification",
    "section": "6.2 Dealing with categorical units",
    "text": "6.2 Dealing with categorical units\nThe plot plan also makes clear when categorical units should and should not be removed from the data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html#the-function-of-random-effects-in-data-analysis",
    "href": "05_model_specification.html#the-function-of-random-effects-in-data-analysis",
    "title": "6  Model specification",
    "section": "6.3 The function of random effects in data analysis",
    "text": "6.3 The function of random effects in data analysis\n\nStatistical tools for addressing questions of generality\nStatistical tools for studying patterns of variation\nIntermediate position between the idiographic and nomothetic approach\nStatistical tools for obtaining more accurate cluster-specific predictions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html#consequences-of-model-misspecification",
    "href": "05_model_specification.html#consequences-of-model-misspecification",
    "title": "6  Model specification",
    "section": "6.4 Consequences of model misspecification",
    "text": "6.4 Consequences of model misspecification\nAs we will demonstrate shortly, this has implications for the analysis of within-subjects (our token-level) factors. In other settings, the blocking factor is not routinely considered as random. The analysis of a token-level predictor then depends on whether the pattern we observe for the token-level predictor is consistent across blocks (i.e. subjects). This means that the difference(s) of interest is (are) stable across blocks. We usually consult the data to decide whether consistency among subjects is a reasonable assumption to make.1 If there is evidence of between-subject variation with regard to the difference of interest, the analysis proceeds along similar lines as when subject is treated as a random factor. The statistical uncertainty surrounding our key comparisons will then be wider.\n1 This is usually done via some preliminary test. The alpha-level for such tests is not the conventional .05, but usually set to .25 (cf. Bancroft (1964)) or .20. This threshold is given in various textbooks (e.g. Hinkelmann and Kempthorne (2008, 313); Lawson (2015, 128)); Maxwell, Delaney, and Kelley (2018, 559)).Such decisions are best based on substantive grounds. Anderson (2001, 178) notes, for instance, that differences among subjects with regard to the difference of interest “may generally be taken for granted”.\nIf we are dealing with a token-level predictor crossed with subjects. Note that for token-level predictors, we do not usually talk about subsampling\n\n6.4.1 Random intercepts and random slopes\n(Rabe-Hesketh and Skrondal 2021, 214) note that because the correlation between intercept and slope depends on how x is scaled, it does not make sense to set the correlation to zero by specifying uncorrelated intercepts and slopes.\nInteresting: (Rabe-Hesketh and Skrondal 2021, 235, 407) state that a model may include random slopes for a level-2 predictor to construct heteroskedastic random intercepts. (?)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "05_model_specification.html#connection-to-other-literatures",
    "href": "05_model_specification.html#connection-to-other-literatures",
    "title": "6  Model specification",
    "section": "6.5 Connection to other literatures",
    "text": "6.5 Connection to other literatures\n\n6.5.1 Connections to the DoE literature\nThe within-subject design corresponds to a randomized complete block design (RCBD). Usually, these are run with each speaker producing only a single replicate per token-level condition. In natural language use, we never obtain such balanced data situations. We routinely have, for most speakers, multiple tokens per token-level condition. Thus, the situation corresponds more closely to what is called a generalized randomized block design (GRBD). The way in which these design are analyzed depends on the nature of the blocking factor. In sciences studying human beings, it seems to be common to regard blocks (i.e. subjects) as random.2\n2 Apart from substantive considerations, which clearly point to subject as a random factor, there are very likely also practical reasons for this choice. Thus, in psychological studies, it is easy to have a comfortably high number of subjects. The number of levels of the blocking factor can therefore easily be as large as 30, say. In other disciplines, the number of blocks is more restricted. This is the case, for instance, in agricultural work. This may partly explain the readiness of psychological researchers to consider the blocking factor as random.\n\n6.5.2 Agriculture vs. language sciences: Mapping issues\nOne problem is the notion of a replicate for the common layout of a strip-plot or strip-plot design. In agricultural experiments, all whole-plot units are units are perfectly replicable. In language data, if we define the whole-units to be speakers or words, these are not replicable in the same way. A second replicate of the whole-unit factor gender would involve two new speakers. This is not problem, in principle. But if we are looking at strip-plot layouts, with words as a second type of whole-unit, a replicate would also involve a second set of new words. And this is not what is happening.\nIt therefore seems that we need to conceptualize our plot plan ina different way. The basic point of the notion of replicates (and top-level blocks) is to use the appropriate error term for comparisons of whole-plot factors. For natural language data, determine the unit of analysis for different factors does not seem to be too difficult. So what we can do instead in our plot plan is to merge the different speakers and words into one array, and then annotate our speaker-and word-level predictors at the margins.\nThe fact that we are not dealing with top-level blocks means that the “design” for speaker- and word-level predictors changes slightly. In the split-plot and strip-plot literature, these error terms are always based on a randomized complete block design, because the whole-plot factors are replicated within blocks. Therefore, the error term is the Block x Factor interaction, which is typical for this type of design. In our modified arrangement we are looking at the equivalent of a completely randomized design. The error term is therefore a slightly different one.\n\n\n\n\nAnderson, Norman H. 2001. Empirical Direction in Design and Analysis. Mahwah, NJ: Lawrence Erlbaum.\n\n\nBancroft, Theodore A. 1964. “Analysis and Inference for Incompletely Specified Models Involving the Use of Preliminary Test(s) of Significance.” Biometrics 20 (3): 427–42. https://doi.org/10.2307/2528486.\n\n\nHinkelmann, Klaus, and Oscar Kempthorne. 2008. Design and Analysis of Experiments, Volume 1: Introduction to Experimental Design. Hoboken, NJ: Wiley.\n\n\nLawson, John. 2015. Design and Analysis of Experiments with r. Boca Raton, FL: CRC Press.\n\n\nMaxwell, Scott E., Harold D. Delaney, and Ken Kelley. 2018. Designing Experiments and Analyzing Data: A Model Comparison Perspective. New York: Routledge.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model specification</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html",
    "href": "06_token_level_predictors.html",
    "title": "7  Token-level predictors",
    "section": "",
    "text": "7.1 Context and composition\nRecall that token-level variables are attributes of the specific context of the corpus hit. To measure them, we need to inspect the environment of a particular instance. In natural (observational) data settings, the token-level variables are free to vary over clusters. This is true for both clustering variables, Item and Speaker. For instance, words may differ in the types of contexts in which they occur - they may partake in different syntactic constructions, they may differ in collocational preferences, etc. It seems that in many cases we would in fact expect words to differ, perhaps systematically, in their co-textual surroundings, or milieu. This possibility, which often turns out to be a reality, forms the backbone of the present chapter.\nLet us start by introducing terminology. We will use the term context to refer to the immediate environment of a corpus hit, i.e. what we see when inspecting a concordance. Token-level variables are among the features that characterize the context. We now need a term for describing context-related differences between cluster, i.e. for describing the way in which, say, words vary in their surroundings. When describing such differences, we will talk about the composition of – or, equivalently, the makeup of the tokens belonging to – a particular word. We can therefore describe the composition of clusters with regard to different token-level variables.1 The composition of a word then reflects the within-word distribution of a given token-level predictor. In linguistic terms, it describes the co-textual milieu that is typical for this word in the corpus at hand. Put differently, the composition of a word for a specific token-level variable describes one particular aspect of the contexts to which this word is typically drawn. Of course, we may be interested in the composition of words along different lines, i.e. looking at different token-level variables.\nIf clusters differ in composition, level-1 variables will vary both within and between clusters. In natural language data, it is often the case that the distribution of token-level predictor values varies from cluster to cluster. Statistically, differences in composition can be described as an association between token-level variable and clustering variable. The strength of this association reflects the degree of heterogeneity among clusters, i.e. to which extent they differ in the kinds of contexts in which they surface.\nFor language-internal clustering factors such as Item, variation in co-textual surroundings is to some extent systematic. This means, based on our knowledge about language (use), we can anticipate certain compositional differences among words, or, once they have been observed, they usually make sense. It is also of substantive interest, since the typical environment in which a word appears may have an influence on the behavior of this particular word. For instance, if a word such as trying tends to occur almost exclusively in contexts that are, for coarticulatory reasons, adverse to ing, this may, over generations of speakers, lead to a diachronic trend towards in.2 In contrast to Item, differences in composition among speakers will typically be haphazard, and of little substantive interest. Thus, we may observe that the tokens produced by a particular speaker differ from other speakers in our sample. It would usually seem, however, that this is reflects random, rather than systematic, variation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#context-and-composition",
    "href": "06_token_level_predictors.html#context-and-composition",
    "title": "7  Token-level predictors",
    "section": "",
    "text": "1 Even though this term does not fit language data structures that weel, we nevertheless adopt it from the literature (cf. Diez Roux 2002). As a label, it is more transparent when considering human beings as tokens and higher-level units such as classrooms or school as clusters. Thus, it makes more sense to talk about the composition of class, or school, e.g. in terms of socio-economic characteristics of its students.\n\n2 See agent-based model.\nmicro-scale vs. macro-scale\nstudies discussing the possibility of contextual effects: Bybee 2002, Torres Cacoullos 199; cited in Barth & Kapatsinski 2014: 212-213",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#differences-in-composition-risks-and-opportunities",
    "href": "06_token_level_predictors.html#differences-in-composition-risks-and-opportunities",
    "title": "7  Token-level predictors",
    "section": "7.2 Differences in composition: Risks and opportunities",
    "text": "7.2 Differences in composition: Risks and opportunities\nAs we will discuss in the present chapter, situations where clusters differ in composition bear both risks and opportunities. On the one hand, we need to be cautious because a multilevel model may yield misleading patterns for a token-level predictor. As will be seen throughout the following sections, an uncarefully specified multilevel model will return an estimate for the token-level predictor that blends two types of comparison. Only if these two patterns are sufficiently similar is there no cause to worry. We therefore need to check whether the data suggest sufficient similarity, and whether we would expect different patterns on substantive grounds. We will see that, in cases where the patterns diverge, the course of action will depend on the underlying causal structure.3\n3 Are experimental data really free from this bias threat?On the other hand, the two types of comparisons, or patterns, that we have loosely referred to so far, open up new and linguistically meaningful dimensions of variation for theoretical description and empirical analysis. To this end, multilevel models are a critical tool, since it is difficult (if not impossible) to separate these two types of pattern using other data-analytic techniques. Again, however, the linguistic interpretation we may attach to these comparisons depends on the underlying causal structure.\nWe begin by illustrating the difference between these two types of patterns, first using synthetic data and then turning to the ING data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#within--and-between-comparisons-illustration-using-synthetic-data",
    "href": "06_token_level_predictors.html#within--and-between-comparisons-illustration-using-synthetic-data",
    "title": "7  Token-level predictors",
    "section": "7.3 Within- and between-comparisons: Illustration using synthetic data",
    "text": "7.3 Within- and between-comparisons: Illustration using synthetic data\nConsider, as an example, a hypothetical data set of 20 words. There is a binary token-level predictor, with one category favorable to ing and the other category unfavorable to ing.\n\n7.3.1 Composition of clusters\nThe composition of clusters - i.e. words - with respect to this binary predictor varies: The proportion of favorable contexts ranges from .08 to .94. The differences in composition are shown graphically in Figure 7.1. Panel (a) shows the composition of the 20 verbs using a bar chart. Words are ordered by the proportion of favorable contexts, which are shown with gray fill color. Most words are attracted to favorable contexts. Panel (b) shows a more compact display of this distribution: Each word is represented by a filled circle, which indicates its composition, i.e. its share of favorable contexts. Note that the y-axis label has changed accordingly. In what follows, we will usually prefer compact displays of this kind. Panel (b) also introduces two useful terms: The word-specific average for the token predictor will be referred to as the cluster mean. The average over all tokens in the data set, ignoring cluster membership, is called the overall mean. Here, the overall mean is identical to the grand mean, which is the average of the cluster means, disregarding the number of tokens per cluster.4\n4 The distinction between the overall and the grand mean is not usually made in the literature, but it is a useful distinction in observational, unbalanced data settings, since the two measures may differ.\n\n\n\n\n\n\n\nFigure 7.1\n\n\n\n\n\nWhenever clusters differ in composition, a token-level predictor varies at two levels. First, it varies within clusters: For a specific cluster, some tokens appear in favorable contexts, others in unfavorable contexts. Further, it varies between clusters: Favorable contexts may be overrepresented in some clusters relative to others. In other words, with regard to our token-level predictor, clusters differ in composition. Let us extend our terminology: When clusters differ in composition, as in Figure 7.1, the variation of the token-level predictor can be partitioned, or decomposed into two components, a within-cluster component and a between-cluster component (cf. Neuhaus and Kalbfleisch 1998).\nIt is informative to summarize and compare the two components. One way to go about this is to fit a variance components model (i.e. a multilevel model) to the token-level variable, with a variance component (i.e. random intercept) for Item (see Section data-structure-systematic-component). This gives us the following measures of variation, expressed on the proportion scale:5\n5 As a further measure of variation, the intraclass correlation is 0.18.\nOverall SD: 0.49\nWithin SD: 0.28\nBetween SD: 0.18\n\nThis allows us to compare the two components. A between SD close to zero would indicate that differences in composition may be negligible - which is not the case in our illustrative example. In data settings where we are dealing with two clustering factors, a further informative comparison is that of the two between SDs, one for each clustering variable. Far language data settings, we would expect the between SD for Item to be larger than that for Speaker.\nToken-level variables that vary both within and between clusters can be studied at these two levels. First, we can take a purely within-cluster perspective and look at the pattern the token-level predictor forms in a specific cluster (i.e. Speaker or Item). We can also take a between-cluster perspective and study a token-level predictor at level 2. We would then be interested in whether the observed differences between clusters in terms of the outcome proportion are in line with the observed within-cluster patterns. Put differently, we may examine whether within-cluster patterns extend in a direct manner to the higher level. In the illustrative example at hand, we would expect the raw outcome proportions for the individual words in our example to reflect differences in composition, with words that tend to occur in favorable contexts showing a higher proportion of ing, on average.\n\n\n7.3.2 Within-cluster differences\nLet us first consider the within-cluster perspective. To this end, we can compare, separately for each word, the share of ing in favorable vs. unfavorable contexts. In panel (a) in Figure 7.2, each word is represented by a line that connects the observed proportion of ing in these two contexts. Such a plot is sometimes referred to as a spaghetti plot. All lines, except for one, slope upwards, indicating a higher share of ing in favorable contexts - as expected. Panel (b) directly shows the word-specific differences between these two contexts; words do not appear in a specific order. We can see that (again with the exception of one word) all differences are positive, indicating a consistent pattern across words: Tokens in favorable contexts show higher shares of ing. The observed differences average at about .40. At the right margin of panel (b), the distribution of differences is summarized with a dot diagram. This diagram brings into view the variation among clusters in the within-differences.\n\n\n\n\n\n\n\n\nFigure 7.2\n\n\n\n\n\nFor the estimation of within-cluster differences using regression modeling, we have various options. Further below, we will see how to isolate within-cluster comparisons using multilevel modeling. Here, we note that there are alternative strategies. Thus, within-differences can be estimated with a fixed-effects regression model, where either (i) the clustering variable (i.e. Item) is added as a fixed predictor or, alternatively, (ii) all level-1 variables and the outcome are de-meaned, i.e. the regression is run on mean-centered deviation scores for these variables.6. These approaches implicitly control for level-2 predictors, both observed and unobserved, because characteristics of clusters are held constant in the comparison via (i) the fixed effects, or (ii) by subtracting cluster means prior to modeling. In a way, each cluster serves as their own control.\n6 On these approaches, see Rabe-Hesketh and Skrondal (2021), p. 162. This strategy is popular in disciplines where research interest is exclusively in within-cluster patterns. Examples are econometrics, applied research in public policy, and longitudinal and panel data modeling more generally (cf. Schnuck and Perales 2017, 95).\n\n7.3.3 Between-cluster differences\nNext, we consider the between-cluster perspective. Panel (a) in Figure 7.3 shows the 20 verbs in our data set. The y-axis represents the share of ing and the x-axis shows the composition of the word, i.e. the proportion of tokens that occur in favorable contexts. This is the same distribution that appeared in Figure 7.1. Thus, if the points were to be dropped onto the horizontal axis, they would form the dot diagram that we saw at the right margin in Figure 7.2. The trend line gives a summary of the pattern in the plot, i.e. it shows how the average share of ing varies across different compositions: Verbs that predominantly feature in favorable contexts tend to show a higher share of -ing. Given the within-cluster patterns that we just saw in Figure 7.2, this makes sense. Looked at from a different perspective, the token-level predictor accounts for some of the variability that we observe among the words in terms of the share of ing.7 It helps us understand the observed variation among words in terms of the outcome proportion a little better.8\n7 We can compare the variability among words before and after adjusting for the token-level predictor. This tells us how much of the raw variation may be attributed to differences in composition. To this end, we compute a ratio of two random-intercept variances: the one from a model including the token-level predictor divided by that not including the token-level predictor (see Bingenheimer and Raudenbush 2004, 61). Here, this ratio is 0.86, which means that 0.14 of the variation among words is accounted for by differences in composition.8 The distributional properties of a word can be considered part of its inherent characteristics.It is often informative to ask about the amount of variation among verbs that remains after we adjust for differences in composition, i.e. the observed imbalance in token-level predictors. This adjusted estimate is a better representation of what is idiosyncratic about a word, or, at a different level of description, the the amount of unexplained variability among words that is not accounted for by the predictors in our model. We can use the model to compute adjusted cluster means, which reflect, for each specific word, the expected share of ing if the proportion of favorable contexts were equal across all words. To arrive at this adjusted estimate, we need to take into account the within-cluster difference between favorable and unfavorable contexts.\nPanel (b) in Figure 7.3 illustrates the adjustment we are making. The filled circles are the same ones as in panel (a). They represent the observed share of ing plotted against the composition of the cluster. For our adjustment, we first overlay, for each word, a model-based regression line that covers its location in the scatter plot. On the model scale (here: logits), all regression lines have the same slope, which represents the observed within-cluster difference. On the data scale (here: proportions), we end up with a set of curves. Now, each word is sitting on a grey curve. This curve traces, for this specific word, the expected overall share of ing for the range of possible compositions. Thus, shifting a given point along its line, we can determine its model-implied share of ing for different proportions of favorable and unfavorable contexts.\nWe can set cluster composition to a fixed value across all clusters and thereby remove from the raw proportion of ing for each word the variation that may be linked to the within-cluster differences. We are adjusting for the observed imbalance. In order to do so, we need to pick a reference composition. There are different options. Here, we take the value representing a balanced distribution of favorable and unfavorable contexts (both 50%).9 The blue curve in the center represents the within-cluster difference of the token-level predictor. It connects values of about .25 (left-hand side) and .70 (right-hand side), producing an absolute difference corresponding to the one we saw in Figure 7.2. This line represents a regression slope and was determined with a logistic regression model. Note that this slope expresses differences on the logit scale, but we have instead plotted on the probability scale. As a result, curves closer to 0 or 1 become flatter.\n9 For this comparison, it would also make sense to use the grand mean.\n\n\n\n\n\n\n\nFigure 7.3\n\n\n\n\n\nThis leaves us with a set of adjusted cluster estimates. Panel (c) takes these adjusted estimates and plots them (again) against the composition of the cluster. After adjusting for the within-cluster differences, there is little systematic variation left – the slope of the regression line is close to zero. This indicates that the pattern in panel (a) reflects within-cluster differences only. Once these are subtracted out, the clusters no longer vary systematically across different compositions. The pattern in panel (a) is then referred to as reflecting a compositional effect.10 Compositional effects are those differences among clusters that are due to differences among clusters in the distribution of level-1 variables.\n10 This use of the term compositional effect follows Rabe-Hesketh and Skrondal (2021).11 The term contextual effect is used differently in the literature (see Diez Roux 2002). We follow Castellano, Rabe-Hesketh, and Skrondal (2014) and Rabe-Hesketh and Skrondal (2021), p. 167 and make a distinction between a contextual effect and a compositional effect. Note that some authors use these terms interchangeably (e.g. Raudenbush and Bryk 2002, 140).It could also be the case, however, that after adjusting for the within-cluster difference, the residual variation among words still shows systematic patterns when drawn against cluster composition. This would indicate that the token-level predictor produces different patterns at these levels, since the within-cluster pattern would differ from the between-cluster pattern. The discrepancy between the two is often referred to as a contextual effect.11\n\n\n7.3.4 Comparison of between-cluster and within-cluster differences\nWe can inspect the agreement between the within-cluster pattern and the between-cluster pattern by plotting them into the same display. That is, we directly compare the dashed curve from Figure 7.3 a to the blue curve in Figure 7.3 b. This comparison appears in Figure 7.4, where the between-pattern is shown as a dashed line and the within-pattern as a solid line. Note that the line for the between-pattern only covers the range of compositions supported by the data.\n\n\n\n\n\n\n\n\n\nFigure 7.4",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#within--and-between-comparisons-illustration-using-the-ing-data",
    "href": "06_token_level_predictors.html#within--and-between-comparisons-illustration-using-the-ing-data",
    "title": "7  Token-level predictors",
    "section": "7.4 Within- and between-comparisons: Illustration using the ING data",
    "text": "7.4 Within- and between-comparisons: Illustration using the ING data\nLet us consider the token-level predictor Following Context. To keep the following illustration simple, we will first reduce this variable to a binary distinction between contexts favorable to -ing (pause, velar), and contexts favorable to -in (coronal, other). In the next section, we will represent all four contexts.\n\n7.4.1 Composition of clusters\nLet us start by looking at the composition of clusters with regard to phonetic context. Recall that it only makes sense to distinguish level-specific patterns if there is variation in the composition of clusters. Thus, words might differ in their distribution across favorable and unfavorable contexts – that is, some words may occur more often in favorable contexts. For speakers, a similar situation may hold: Some speakers may happen to produce a higher number of tokens in favorable contexts than others. Whereas it is reasonable to consider variation among speakers as an unsystematic, erratic feature of the data, we would in fact expect systematic differences between words, perhaps because of their word class or their collocational preferences.\nFigure 7.5 shows the variation in composition among speakers and words. First, we note that, in general, the distribution of tokens in the data set is clearly inclined towards unfavorable contexts. For speakers, we observe variation in composition from close to 0 to about 30 percent of tokens in favorable contexts. For words, the variation is larger. Note that we are only showing items here that occur at least 10 times in the data. Further, we have varied the fill color of the data points to give a rough reflection of the word-specific token count. Compositions represented with lighter fill colors show an increasing susceptibility to sampling variation – that is, their composition may not have been estimated with great precision.\n\n\n\n\n\n\n\n\n\nFigure 7.5\n\n\n\n\n(ref:tab-ing-foll-cont-SDs) Token-level predictor : Variation between and within clusters.\n\n\n\n\n\nTo offer a more complete summary of these components of variation, let us consider the set of standard deviations. The overall SD is at 0.38, the within SD at 0.24. We have a between SD for each clustering variable: For Speaker, the between cluster variability is much smaller than for Item (0.05 vs. 0.13).\n\n\n7.4.2 Within-cluster differences\nSince we have two clustering factors (Speaker and Item), there are, in principle, two kinds of clusters that we can consider for within-cluster comparisons. Let us compute, for each cluster, a simple difference, between the share of ing in favorable vs. unfavorable contexts. This gives us, for each clustering variable, a distribution of within-differences. These distributions are shown in Figure 7.6. For words, we restrict our attention to types that occur at least 5 times in the corpus.\n\n\n\n\n\n\n\n\nFigure 7.6\n\n\n\n\n\nFor speakers, differences range between -.10 and +.50, with an average of about 0.17. We interpret this as the average within-word difference in proportion ing between favorable and unfavorable contexts. For words, the distribution of differences is wider, with an average of 0.06. What the shows are crude estimates of the within-cluster differences. The within-effect can be interpreted as the difference in the proportion of ing between contexts (coronal, other, pause, velar) for a given word.\n\n\n7.4.3 Between-cluster differences\nNext, let us plot the cluster-specific proportion of -ing against the cluster-specific percentage of tokens in favorable contexts. We would expect to see an association that is consistent with the within-cluster differences we observed above. We will use a bubble chart to show the data, to bring into view the different cluster sizes.\nIn Figure 7.7, each circle represent a cluster. The size of the circle is proportional to the number of tokens for a specific cluster. For Speaker, cluster sizes do not vary much (cf. Figure 4.1). For the clustering variable Item, however, the skewed distribution of token counts (cf. Figure 4.2) surfaces in “bubbles” of widely varying size.\nLet us consider the patterns in the plots. The dotted trend lines represent the model-based within-comparison. Looking at the distribution of points for speakers, it is difficult to make out a pattern in the plot. The between-pattern is summarized by a solid trend line, which seems to be dominated by a handful of speakers sitting at the extremes with respect to both the share of favorable contexts and the overall proportion of ing. Since the cluster compositions are probably the result of chance fluctuations, we should hesitate to “believe” the trend line. The fact that the trend is reversed compared to the within-pattern also raises doubts.\nFor words, on the other hand, the distribution of points seems to be summarized quite well by the solid trend line: Words that have a stronger tendency to occur in favorable contexts also have a higher share of -ing, on average. The directionality of this pattern is consistent with the observed within-cluster differences.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The following aesthetics were dropped during statistical transformation: size.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: size.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\nFigure 7.7\n\n\n\n\n\nFor Item, the between-trend is steeper than the within-trend. This indicates that the between-pattern is unlikely to be purely compositional. A multilevel regression model can represent these two curves with different coefficients, which allows us to compare numbers reflecting the steepness of the trend lines. In the present case, they are perceptibly different – the between-word difference is larger than the within-word difference.12 This means that an association between the composition of words and the outcome quantity persists once the estimates are adjusted for within-differences linked to the token-level predictor. This discrepancy may represent a contextual effect. As we will discuss further below, the interpretation of divergent between- and within-differences as contextual effects hinges on a number of causal assumptions. Importantly, however, the existence of a contextual effect also needs to make sense on linguistic grounds.\n12 This is also reflected in the amount by which the random intercept SD decreases: Including the token-level predictor into the model leads to only a minor decrease, since the variability among words when account is taken of composition almost remains unchanged. In other words, the residual variation among words that is independent of contextual make-up. As mentioned above, a useful measure is the ratio of the random intercept variance before/after adjusting for level-1 covariates (Bingenheimer and Raudenbush 2004: p. 61).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#interpretation-of-between-and-within-comparisons",
    "href": "06_token_level_predictors.html#interpretation-of-between-and-within-comparisons",
    "title": "7  Token-level predictors",
    "section": "7.5 Interpretation of between and within comparisons",
    "text": "7.5 Interpretation of between and within comparisons\nBefore we go about splitting between- and within patterns, and attaching different interpretations to each, we need to be convinced that differences in composition really exist among clusters. There are two sources of information we can rely on: our subject-matter knowledge and the information in the data. As for empirical hints about the reality of compositional variation, we can look at the amount of between-cluster variation, which is reflected in the between-SD, compared with the within-.cluster variation. We could also attempt an inferential assessment, asking whether a variance component representing between-cluster variability improves model fit. Ultimately, however, we should think about whether it makes sense for clusters to vary in their distribution of token-level predictors. In our case, it seems difficult to explain why speakers should vary systematically in terms of the kinds of contexts in which they use words ending in -ing. For words, however, it does not take much to convince ourselves that there are real differences among words. These linguistic assessments find support in the descriptive assessments of the between-cluster SDs: Variation among words is greater than among speakers (0.13 vs. 0.05), the latter being quite small. We will therefore focus on potential contextual differences for Item.\nThe interpretation of differences in within and between comparisons greatly depends on the underlying subject matter. It seems, however, that we would need a convincing story for why contextual differences should exist.\nThe story of trying and going…\nWe can formalize and check the coherence of our story using agent-based modeling.\nA look at the literature also shows that disciplines differ in the emphasis they attach to these two types of comparisons. Some consider within-comparisons as the primary target of inference. This is often the case in econometrics or public policy, where interest lies in isolating the effect of different potential interventions at the level of the individual (the level-1 unit in these settings). This is because relevant decisions are made by individuals. In other research traditions, contextual effects are of substantive interest. Examples are education and public health.\nInterpretation of compositional effects: We probably wouldn’t go as far as saying that, in cases where differences between words reduce to the compositional effect, the observed between-word differences are an artefact of differences in token-level composition. This is because the make-up is part of the word.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#model-specification-for-between-and-within-comparisons",
    "href": "06_token_level_predictors.html#model-specification-for-between-and-within-comparisons",
    "title": "7  Token-level predictors",
    "section": "7.6 Model specification for between and within comparisons",
    "text": "7.6 Model specification for between and within comparisons\nA multilevel model allows us to decompose the association between a token-level predictor and the outcome into level-specific components. To make this split, the model must include cluster means for token-level variables as an additional level-2 variable. This means that, prior to modeling, we need to construct derived variables that represent cluster composition. This will always be some sort of cluster-specific average. For a quantitative predictor, it is the average over all tokens. For a binary predictor, it depends on how the variable is coded. Since all variables are coded in some numeric form, we still take an averge (e.g. over indicator dummy variables 0 and 1, or over sum contrasts -1 and +1).13 We will refer to these derived variables as cluster means.\n13 The same is true for a categorical variable with \\(k\\) levels, where we average over each of the \\(k - 1\\) contrast variables.For token-level variables whose cluster-means are included in the model, the model will return information about the relationship at two levels. What complicates matters slightly is that we now have two options for how the token-level predictor itself should be coded. There are two options. While these options essentially produce identical analyses, and produce the same coefficients for within-cluster comparisons, they return different coefficients for between-cluster comparisons. Somewhat counter-intuitively, then, the way in which we code the token-level predictor affects the meaning and interpretation of the between-cluster comparison.\n\n7.6.1 Model specification: Two options\nLet us first look at these two options and then consider differences in meaning. First, we can simply include the token-level predictor in its original form, i.e. without making any changes to it. The second option is to include a cluster-mean centered version of it. Token-level predictor values are then represented as deviations from their cluster-specific means.14\n14 They are therefore also called mean-deviation variables, or de-meaned variables, or within-group deviation scores. The procedure is sometimes called within-group centering or centering within context.15 The label between-within model appears to be due to Sjölander et al. (2013); this form is also referred to as the hybrid model (Allison 2009, 23). The Mundlak model is named after (Mundlak 1978); it is also referred to as the correlated random effects model (Wooldridge 2010, 286; Cameron and Trivedi 2005, 786) or the including-the-group-means approach (Castellano, Rabe-Hesketh, and Skrondal 2014, 335).Along with its cluster means, a token-level predictor can therefore enter the model in its original form or its cluster-mean centered form. These formulations are referred to as the the Mundlak model (with the token-level predictor in its original form) between-within model (with the token-level predictor in its cluster-mean centered form).15 To reiterate, choosing between these two options has no effect on the within-cluster estimate; however, the meaning of the between-cluster coefficients will differ:\n\nBetween-within model: Including the cluster-mean centered version of the token-level predictor will produce an estimate of the between-cluster comparison. In Figure 7.7) above, this is the slope of the between-cluster trend line.\nMundlak model: Using the original form of the token-level predictor returns and estimate of the difference of the between-estimate and the within-estimate. In Figure 7.7), this is the differences between the two slopes. In that case, it signals how much steeper the between-trend is. Note that this differences corresponds to what is meant by a contextual effect. To obtain the between-cluster comparison, we have to add the two coefficients.16.\n\n16 See Raudenbush and Bryk (2002), p. 140; Rabe-Hesketh and Skrondal (2021), p. 171Here are the two model formulations in math notation: \\[\n\\begin{aligned}\ny_{ij} &= \\bar\\alpha + \\alpha_j + \\beta^{\\textrm{B}}\\bar{x}_j + \\beta^{\\textrm{W}}(x_{ij}-\\bar{x}_j) && \\text{Between-within model} \\\\\ny_{ij} &= \\bar\\alpha + \\alpha_j + \\beta^{\\textrm{C}}\\bar{x}_j + \\beta^{\\textrm{W}}x_{ij} && \\text{Mundlak model}\n\\end{aligned}\n\\]\nThese two formulations yield identical estimates for within-comparisons - hence the identical label \\(\\beta^{\\textrm{W}}\\). The interpretation of the other coefficient differs, however: The between-within model returns the between-estimator \\(\\beta^{\\textrm{B}}\\) and the Mundlak model returns the contextual difference \\(\\beta^{\\textrm{C}}\\) (i.e. \\(\\beta^{\\textrm{B}} - \\beta^{\\textrm{W}}\\)). In general, then, there is a simple relationship between these coefficients: \\(\\beta^{\\textrm{B}} = \\beta^{\\textrm{W}} + \\beta^{\\textrm{C}}\\), or, equivalently, \\(\\beta^{\\textrm{C}} = \\beta^{\\textrm{B}} - \\beta^{\\textrm{W}}\\).\nThe meaning signaled by \\(\\beta^{\\textrm{C}}\\) is the change in outcome expected for a level-1 (a token) moving from one level-2 unit (i.e. word) to another. In our case, how does the probability of ing change for a specific token in the same context (i.e. with the same following context) if it occurs in another word which differs from the original word in composition only. The specific context in which the token occurs stays the same. \\(\\beta^{\\textrm{B}}\\), on the other hand, gives the expected change in the probability of observing ing when changing from one word to another word that differs in composition, without holding constant the specific context in which a word occurs. \\(\\beta^{\\textrm{B}}\\) therefore includes the token-level within-effect.\n\n\n7.6.2 Choosing between the two specifications\nIf we understand which meaning is signaled by the coefficient attaching to the derived level-2 cluster-mean variable (i.e. \\(\\beta^{\\textrm{B}}\\) or \\(\\beta^{\\textrm{C}}\\)), there is little to choose between these two formulations. [Bell_etal2019, p. 1056] mention computational advantages of the between-within-specification.17 Which of the interpretations is more meaningful in substantive terms depends on the subject-matter context [see Bell_etal2019, p. 1056]. In some settings, it may not make sense to think of level-1 tokens as being able to change cluster membership. Then, the direct interpretation of \\(\\beta^{\\textrm{C}}\\) may not be directly meaningful. However, thinking of a token as being able to change cluster membership doesn’t seem too problematic for language data.\n17 This is because cluster means and cluster-mean centered scores are uncorrelated.On the other hand, it is sometimes argued that \\(\\beta^{\\textrm{B}}\\) offers a blend of two components that should be kept apart for interpretation. Thus, (Begg and Parides 2003, 2598–99) argue in favor of the Mundlak specification since they consider \\(\\beta^{\\textrm{B}}\\) in the between-within model to be vulnerable to misinterpretation. This is because it mixes two differences, the between- and the within-difference. The usual interpretation of regression coefficients as a change in the outcome associated with a one-unit increase in one specific predictor does not apply to \\(\\beta^{\\textrm{B}}\\), which signals the expected change in the outcome associated with a one-unit increase on both the within-cluster and the between-cluster level. Since we usually do not interpret coefficients but rather use model-based visualizations, we do not have a strong general position here.\nAnother advantage of the Mundlak formulation is that it returns \\(\\beta^{\\textrm{C}}\\) as an estimate of the difference of within- and between-patterns. On this scale, zero is an informative value, since it represents the absence of a contextual difference, i.e. the situation where between-cluster differences are due to differences in composition. The statistical uncertainty surrounding \\(\\beta^{\\textrm{C}}\\) permits the data to comment on the question whether there are contextual differences over and above compositional differences.18\n18 This statistical assessment is of course also possible with the between-within model, but here this information is in the difference between two coefficients and therefore does not appear in the regression table.A point that seems to have received less attention is the setting of cross-classified clustering structures, what may be particularly relevant for language data settings. The between-within specification would require us to derive cluster-mean centered variables with reference to two types of cluster membership. It is not immediately clear what such a centered variable should look like. The Mundlak model, in contrast, extends directly to cross-classified clustering factors: We can include the original token-level predictor along with two sets of cluster means – one for speakers, and one for words.\n\n\n7.6.3 Failure to partition between- and within-differences\nLet us compare the Mundlak and the between-within model to a multilevel model that does not include cluster means for the token-level predictor \\(x_{ij}\\): \\[\ny_{ij} = \\bar\\alpha + \\alpha_j + \\beta^*x_{ij}\n\\]\nIf we do not partition token-level predictors into a between-cluster and a within-cluster component, a multilevel model will return a comparison that blends both differences. Thus, a model that does not include cluster means will not distinguish between \\(\\beta^{\\textrm{W}}\\) and \\(\\beta^{\\textrm{C}}\\) (or \\(\\beta^{\\textrm{B}}\\)). Instead, we will obtain the coefficient \\(\\beta^*\\), which is intermediate between \\(\\beta^{\\textrm{W}}\\) and \\(\\beta^{\\textrm{B}}\\). More specifically, \\(\\beta^*\\) is a weighted average of for \\(\\beta^{\\textrm{W}}\\) and \\(\\beta^{\\textrm{B}}\\), with the weights proportional to the precision of each (as reflected in their standard error). The combined estimate is closer to the difference that is estimated with greater precision, which is the within-difference (See Rabe-Hesketh and Skrondal 2021, 164–65). Here is the relationship between \\(\\beta^*\\), \\(\\beta^{\\textrm{B}}\\), and \\(\\beta^{\\textrm{W}}\\) (see Raudenbush and Bryk 2002, 138–39): \\[\n\\begin{aligned}\n\\beta^* &= \\frac{w_{\\textrm{W}} \\beta^{\\textrm{W}} + w_{\\textrm{B}} \\beta^{\\textrm{B}}}{w_{\\textrm{W}} + w_{\\textrm{B}}} && \\text{Precision-weighted estimate for } \\beta^*  \\\\\nw_{\\textrm{W}} &= \\frac{1}{(SE_{\\beta^{\\textrm{W}}})^2} && \\text{Weight for within-component} \\\\\nw_{\\textrm{B}} &= \\frac{1}{(SE_{\\beta^{\\textrm{B}}})^2} && \\text{Weight for between-component} \\\\\nSE_{\\beta^*} &= \\sqrt{\\frac{1}{w_{\\textrm{W}} + w_{\\textrm{B}}}} && \\text{Standard error for } \\beta^*\n\\end{aligned}\n\\]\nBy blending relationships at two levels, \\(\\beta^*\\) usually has no direct interpretation (Raudenbush and Bryk 2002, 138). If the between- and within-differences were very similar, however, \\(\\beta^*\\) does reflect an interpretable comparison.19 To guard against inappropriately blending between- and within-comparisons, we can separate these comparisons in our multilevel model specification.20\n19 This estimator has the additional advantage of being more efficient, i.e. yielding higher precision than alternative approaches that isolate the between- and within-components. This is because it combines information from two sources. See (Bell, Fairbrother, and Jones 2019, 1057; Rabe-Hesketh and Skrondal 2021, 165).20 There is also a “test” for such situations, the Hausman test (Hausman 1978).\n\n7.6.4 Example: Following context\nFor a concrete example, consider the token-level predictor Following Context. (tab-ing-b-bW-bB?) reports estimates from three different models. A multilevel model without cluster means returns \\(\\beta^*\\), the conflated comparison. This difference is a weighted average of \\(\\beta^{\\textrm{W}}\\) and \\(\\beta^{\\textrm{B}}\\), the coefficients reported by the between-within model. \\(\\beta^*\\) is closer to \\(\\beta^{\\textrm{W}}\\), the estimate with the smaller standard error (i.e. greater precision).\n\n\n\n\n\nFigure 7.8 illustrates the relationship between the three estimates graphically. Each density curve represents a coefficients. The dispersion of the curve reföects the statistical uncertainty surrounding each estimate. We can see clearly that \\(\\beta^*\\) is drawn heavily towards the more precise estimate of \\(\\beta^{\\textrm{W}}\\).\n\n\n\n\n\n\n\n\n\nFigure 7.8: β* as the precision-weighted average over βW and βB.\n\n\n\n\n\n\n7.6.5 Simulation\nIn our illustrative data set, \\(\\beta^*\\) ended up being quite close to \\(\\beta^{\\textrm{W}}\\), in which case concerns about bias are somewhat alleviated. \\(\\beta^*\\) will usually be closer to \\(\\beta^{\\textrm{W}}\\), since this coefficient is typically estimated with higher precision. To form some intuition about the factors influencing the amount of bias in \\(\\beta^*\\), let us run a simulation study. The amount by which \\(\\beta^*\\) deviates from \\(\\beta^{\\textrm{W}}\\) depends on the relative precision of \\(\\beta^{\\textrm{W}}\\) and \\(\\beta^{\\textrm{B}}\\). We will vary three parameters: The number of tokens per cluster, the number of clusters, and the random intercept SD for the clusters.\nFigure 7.8 shows the results of our simulation. The limits of the vertical axis denote the locations of the coefficients \\(\\beta^{\\textrm{W}}\\) (bottom) and \\(\\beta^{\\textrm{B}}\\) (top). The intermediate value of \\(\\beta^*\\) for different simulated values is denoted by points. In both panels, the horizontal axis shows the most important parameter: the number of tokens per cluster. With only 2 tokens per cluster (at the left end of the scale), \\(\\beta^*\\) lies roughly half-way in between the coefficients. As the number of tokens per cluster increases, \\(\\beta^{\\textrm{W}}\\) is estimated with higher precision; as a result, \\(\\beta^*\\) approaches \\(\\beta^{\\textrm{W}}\\). Panel (a) shows an additional parameter: the number of clusters. With more clusters, the precision of \\(\\beta^{\\textrm{B}}\\) increases, and \\(\\beta^*\\) therefore moves toward \\(\\beta^{\\textrm{B}}\\). This influence only surfaces with lower token counts per cluster – at about 50 tokens per cluster, the number of clusters appears to be largely irrelevant under the conditions simulated here. Panel (b) shows the third parameter: the random intercept SD for clusters. As the variation among clusters decreases, \\(\\beta^{\\textrm{B}}\\) is estimated with higher precision and therefore exerts more pull on \\(\\beta^*\\). With our current simulation settings, this influence is only felt for large numbers of tokens per cluster.\n\n\n\n\n\n\n\n\nFigure 7.9",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#causal-assumptions-for-the-identification-of-contextual-effects",
    "href": "06_token_level_predictors.html#causal-assumptions-for-the-identification-of-contextual-effects",
    "title": "7  Token-level predictors",
    "section": "7.7 Causal assumptions for the identification of contextual effects",
    "text": "7.7 Causal assumptions for the identification of contextual effects\nThe interpretation of a difference in within- and between-coefficients as a contextual effect hinges on the absence of confounding. Unobserved confounders with an arrow pointing into X and Y can sit at level 2 (a word-level confounder) or at level 1 (a token-level confounder). These two scenarios are shown in Figure 7.10, where variables are arranged by the level at which they are measured: Level-1 variables appear at the bottom, level-2 variables at the top.\n\n\n\n\n\n\n\n\nFigure 7.10\n\n\n\n\n\n\n7.7.1 Token-level confounding\nThere is no easy fix for token-level confounding. It requires some form of instrumental-variables approach.\n\n\n7.7.2 Cluster-level confounding\nOne assumption is that there are no unobserved word-level confounders, i.e. word-level predictors that have a causal effect on both X (the token-level predictor) and Y. A situation that would distort our estimates is shown in Figure 7.10 a, where U is the unobserved word-level confounder. We will refer to this situation as word-level unobserved confounding or word-level omitted variable bias. In the econometrics literature, this situation is called level-2 endogeneity.21\n21 In that literature, endogeneity (i.e. unobserved confounding) is often described as a correlation between the predictor in question (here the token-level predictor) and an error term (here, the random intercept for word). The correlation arises because the error term contains the effects of unobserved confounders - thus, the word-level intercept reflects unobserved word-level predictors. This correlation is a consequence of the omitting a confounder. To make sense of the econometrics literature, we can establish the following mapping: level-2 endogeneity = correlation between predictor and random intercept = omitted variable bias due to unobserved level-2 confounder.22 Another viable option, which is commonly referred to as the fixed-effects approach, is to include the individual cluster as fixed (instead of random) intercepts. See Bell, Fairbrother, and Jones (2019); Rabe-Hesketh and Skrondal (2021), p. 162In such cases, the within-comparison can be consistently estimated by including mX, the cluster means of the token-level variable, as an additional predictor.22 The resulting DAG is shown in Figure 7.11. X denotes the token-level predictor, and mX the derived cluster means. These are shown on level 2, since they are cluster attributes. The dotted line connecting X and mX indicates the intrinsic link between the variables: mX is a derived variable, a summary measure of a token-level predictor.\n\n\n\n\n\n\n\n\nFigure 7.11\n\n\n\n\n\nAdjusting for mX effectively closes the backdoor path from U into X. This is not immediately clear from our knowledge about conditional independencies in DAGs. We need new heuristics for denoting derived cluster means in a DAG. Since the value of a level-2 confounder U is constant across all tokens in a certain cluster, the influence of U on X will surface at the level of the cluster. The derived cluster means mX therefore block the causal effect a level-2 confounder has on X. By adjusting for mX, we are therefore able to remove the confounding bias from the within-effect, i.e. the arrow pointing from X into Y. This means that within-effects are identified. As Figure 7.11 b illustrates, the bias is assigned to mX. Now, only the effect of mX on Y is confounded by U.\nFor word-level unobserved confounding, here are the requirements for consistent estimation of our comparisons: (i) for \\(\\beta^*\\), we need to adjust for U; (ii) for \\(\\beta^{\\textrm{W}}\\), we need to adjust for mX; and (iii) for \\(\\beta^{\\textrm{B}}\\), we need to adjust for U. If U is indeed unobserved, standard regression modeling can only arrive at consistent estimates of \\(\\beta^{\\textrm{W}}\\). All other parameters remain biased. Importantly, this bias propagates to the estimation of other cluster-level (i.e. level-2) predictors as well as cluster-specific parameters (e.g. random intercepts) and their associated hyperparameters (e.g. random intercept SD). These estimates remain biased even after adjusting for mX. This means that the between-comparison will still be confounded with the omitted level-2 predictors and therefore still be biased. (See Rabe-Hesketh and Skrondal 2021, 165).\nThe econometrics literature therefore cautions against using random effects when exogeneity assumptions are not met. Fixed-effects regression is recommended as an alternative approach.23 If there is level-2 endogeneity, a fixed-effects model can be used to arrive at consistent estimates of within-differences.24 For situations such as that depicted in Figure 7.11 a, fixed-effects regression (which includes the clustering variable as a factor) returns only \\(\\beta^{\\textrm{W}}\\), which is identical to \\(\\beta^{\\textrm{W}}\\) in the Mundlak or between-within model, i.e. a multilevel regression including cluster means. Since it is a fixed-effects regression, the biased parameters from the random-effects model are simply not estimated. \\[\n\\begin{aligned}\ny_{ij} &\\sim \\textrm{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_j^{\\scriptsize{\\textrm{FE}}} + \\beta^{\\textrm{W}}x_{ij} && \\text{Fixed-effects} \\\\[14pt]\ny_{ij} &\\sim \\textrm{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_j^{\\scriptsize{\\textrm{RE}}} + \\beta^*x_{ij} && \\text{Naive multilevel model} \\\\\n\\alpha_j^{\\scriptsize{\\textrm{RE}}} &\\sim \\textrm{Normal}(0, \\sigma_{\\alpha}) \\\\\n\\bar{\\alpha} &= \\gamma   + \\gamma^{\\textrm{G}} G_j \\\\[14pt]\ny_{ij} &\\sim \\textrm{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_j^{\\scriptsize{\\textrm{RE}}} + \\beta^{\\textrm{W}}(x_{ij} - \\bar{x}_j) && \\text{Mundlak model} \\\\\n\\alpha_j^{\\scriptsize{\\textrm{RE}}} &\\sim \\textrm{Normal}(\\bar{\\alpha}, \\sigma_{\\alpha}) \\\\\n\\bar{\\alpha} &= \\gamma   + \\beta^{\\textrm{C}} \\bar{x}_j   + \\gamma^{\\textrm{G}} G_j \\\\[14pt]\ny_{ij} &\\sim \\textrm{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_j^{\\scriptsize{\\textrm{RE}}} + \\beta^{\\textrm{W}}x_{ij} && \\text{Between-within model} \\\\\n\\alpha_j^{\\scriptsize{\\textrm{RE}}} &\\sim \\textrm{Normal}(\\bar{\\alpha}, \\sigma_{\\alpha}) \\\\\n\\bar{\\alpha} &= \\gamma   + \\beta^{\\textrm{B}}\\bar{x}_j   + \\gamma^{\\textrm{G}} G_j\n\\end{aligned}\n\\]\n23 It is sometimes said that fixed-effects and random effects differ in that random effects assume zero correlation between the random effects and observed level-2 predictors (i.e. level-2 exogeneity), while fixed effects permit correlation between fixed effects and observed predictors. A correlation between the random effects and observed covariates reflects omitted variable bias.24 There are two strategies: de-meaning and subject dummies (See Rabe-Hesketh and Skrondal 2021, 274–79).\n\n\n\n\nUsing a between-within (or Mundlak) model relaxes the assumption of level-2 exogeneity, i.e. that the random intercept is uncorrelated with the token-level predictor. This leads to consistent estimates for within-differences. (See Rabe-Hesketh and Skrondal 2021, 174). Yet, the estimates for cluster-level predictors and the random intercept SD are not consistently estimated (Ebbes, Böckenholt, and Wedel 2004, 166). This is because if the cluster means absorb causal information flowing from level-2 predictors. This means that the between-difference is contaminated by the absorbed information.\nIn situations where cluster-level unobserved confounding is suspected, the following advice can be given:\n\nA multilevel model without cluster means includes absolutely no precautions against bias.\nIf interest lies exclusively in within-effects, we can use either fixed-effects regression or a multilevel model including cluster means (i.e. a Mundlak or between-within model).\nIf we are also interested in parameters associated with level 2, we must understand that estimates from a multilevel model including cluster means are likely to be biased: This applies to (i) the random effects themselves, (ii) random effects hyper-parameters, and (iii) estimates for level-2 predictor, including the between-comparison and, alternatively, the contextual effect.\n\nIn short: Fixed-effects regression is save but limiting, and the naive use of multilevel models may lead to misleading interpretations.\nFor the ING data, we should therefore be asking whether the apparent contextual effect could reflect cluster-level confounding. Are there any word-level variables that have a causal effect on the response and on the token-level predictor. To rephrase, are there any unobserved word-level confounders that influence both the outcome and the composition of clusters. A candidate is Word Class: g-dropping is sensitive to word class, with verbs and gerunds showing a higher rate of g-dropping. The word class of a token will also have an influence on the Following Context, since word classes surface in different syntactic contexts, and there may be systematic differences in the kinds of words that tend to follow. If this is the case, we would be dealing with cluster-level confounding, or cluster-level omitted variable bias.25\n25 In the econometrics jargon, we would state that the level-2 exogeneity assumption is violated.\n\n7.7.3 Understanding bias\n\n\n\n\n\n\n\n\nFigure 7.12\n\n\n\n\n\nIn setting (a), where there is no contextual effect, \\(\\gamma\\), the coefficient for the cluster-level predictor \\(W\\), will be biased. The amount of bias in \\(\\gamma\\), denoted as \\(\\delta_\\gamma\\), depends on (Castellano, Rabe-Hesketh, and Skrondal 2014, 348):\n\nthe association between \\(\\bar{x}\\) and \\(U\\) (\\(\\rho_{\\bar{x}u}\\))\nthe association between \\(\\bar{x}\\) and \\(W\\) (\\(\\rho_{\\bar{x}w}\\))\nthe standard deviation of the random intercepts \\(U\\) (\\(\\sigma_u\\))\nthe standard deviation of \\(W\\) (\\(\\sigma_w\\))\n\nHere is the formula reported by Castellano, Rabe-Hesketh, and Skrondal (2014), p. 348:\n\\[\n\\delta_\\gamma = \\frac{-\\rho_{\\bar{x}u}\\rho_{\\bar{x}w}\\sigma_u}{\\sigma_w(1-\\rho^2_{\\bar{x}w})}\n\\]\n\n\n\n\n\n\n\n\n\nThe two-step estimator or the Hausmann-Taylor estimator produce an unbiased estimate for \\(\\gamma^{G}\\), the direct causal effect of \\(W\\) on \\(Y\\). An important aspect, however, is how the association between mX and W arises. Three possible underlying causal structures are shown in Figure 7.13. In panel (a) the association reflects a causal effect of \\(W\\) on \\(X\\), which then surfaces in \\(\\bar{x}\\). Panel (b) show a scenario where the association is due to a common cause U, which may be sitting at Level 1 or Level 2. Finally, panel (c) shows a direct causal effect of \\(X\\) on \\(W\\), which gives rise to the association between \\(\\bar{x}\\) and \\(W\\). In settings (a) and (b), \\(\\bar{x}\\) is a collider on the path from \\(U\\) to \\(W\\). Adjusting for \\(\\bar{x}\\) therefore opens a non-causal path that leads to bias in the direct causal effect of \\(W\\) on \\(Y\\). In these scenarios, we would like to obtain an unbiased estimate. In panel (c), however, adjusting for \\(\\bar{x}\\) actually closes a backdoor path. Without this adjustment, our estimate of the direct causal effect of \\(W\\) on \\(Y\\) is confounded. This leads me to conclude that only scenarios (a) and (b) call for techniques such as 2S and HT.\n\n\n\n\n\n\n\n\nFigure 7.13\n\n\n\n\n\n\n\n7.7.4 Simulation\nNext, we carry out a bias analysis. We vary the parameters of the model to see what effect each one has on the amount of bias in the results.\n\n\n7.7.5 Bias decreases with increasing cluster size\nAs cluster size increases, random intercept estimates approach their fixed counterparts. The bias in between-differences and other cluster-level predictors therefore decreases. As noted by (Rabe-Hesketh and Skrondal 2021, 178)\nLet us run a bias analysis here:\n\n\n7.7.6 Tools for consistent estimation\nUnder certain circumstances, there are strategies for obtaining consistent estimates for cluster-level coefficients. Thus, in cases where there is no contextual effect – i.e. where \\(\\beta^{\\textrm{C}} = 0\\) or, equivalently, \\(\\beta^{\\textrm{W}} = \\beta^{\\textrm{B}}\\) – we can obtain consistent estimates for cluster-level predictors (\\(\\gamma^{\\textrm{G}}\\)), cluster-specific intercepts (\\(\\alpha_j^{\\scriptsize{\\textrm{RE}}}\\)) and the random-intercept variance (\\(\\sigma_{\\alpha}\\)). More specifically, we can strip from these estimates the amount of bias that is due exclusively to Level-2 endogeneity of the Level-1 predictor.\nThe general strategy is the following. We first obtain a consistent estimate of \\(\\hat\\beta^{\\textrm{W}}\\) using one of the models discussed above. Recall that, apart from the naive multilevel model, all produce consistent estimates for \\(\\beta^{\\textrm{W}}\\). We then plug the estimate \\(\\hat\\beta^{\\textrm{W}}\\) into the following equation to get consistent estimates for \\(\\gamma^{\\textrm{G}}\\), :\n\\[\n\\begin{aligned}\ny_{ij} - (\\hat\\beta^{\\textrm{w}}x_{ij}) &\\sim \\textrm{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_j^{\\scriptsize{\\textrm{RE}}} \\\\\n\\alpha_j^{\\scriptsize{\\textrm{RE}}} &\\sim \\textrm{Normal}(\\bar{\\alpha}, \\sigma_{\\alpha}) \\\\\n\\bar{\\alpha} &= \\gamma + \\gamma^{\\textrm{G}} G_j\n\\end{aligned}\n\\] We can preserve the uncertainty in \\(\\hat\\beta^{\\textrm{W}}\\) with a Bayesian regression model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#additional-complexities",
    "href": "06_token_level_predictors.html#additional-complexities",
    "title": "7  Token-level predictors",
    "section": "7.8 Additional complexities",
    "text": "7.8 Additional complexities\n\n7.8.1 Correct specification of the between-pattern\nIn categorical regression models (e.g. logistic, multinomial, or count regression) the inclusion of cluster means may not reliably partition within- and between components in certain cases. For instance, if the association between cluster means and the outcome is non-linear but constrained by the model specification to linearity, the estimates for the within-cluster differences will be biased (Bell, Fairbrother, and Jones 2019, 1066; Palta and Seplaki 2003, 188; Schnuck and Perales 2017, 109). This is in contrast to ordinary linear regression. We should therefore be on the lookout for potential non-linearities during initial data analysis. As illustrated in Figure 7.14, we can add a flexible summary of the trend to our bubble chart, e.g. in the form of a smoother or spline. In Figure 7.14, where the grey trend line represents a fairly flexible spline with 3 knots, there is no indication of non-linearity.\n\n\n\n\n\n\n\n\n\nFigure 7.14\n\n\n\n\nThe between-cluster relationship may also show an interaction with other variables. It is usually best if interactions have a good linguistic footing, i.e. that there is reason to expect them on substantive grounds. For our illustrative case study, the data suggest an interaction between cluster means and frequency. Figure 7.15 divides words into three frequency bands using token counts of 5 and 20 as thresholds. We can see that the between-cluster trend is steeper among high-frequency words.\n\n\n\n\n\n\n\n\nFigure 7.15\n\n\n\n\n\n\n\n7.8.2 Population-averaged vs. cluster-specific estimates\n\nThe difficulty of comparing population-averaged differences (between-differences) and cluster-specific differences (within-differences). (Neuhaus and Kalbfleisch 1998, 641) comment on this, but it seems that they miss the point that it does not make sense to interpret a between-subject difference on a subject-specific scale.\n\n\n\n7.8.3 Crossed clustering structures\n\nUse of deviation variables impossible?\n\n\n\n7.8.4 Random slopes\n\nWhat happens when we specify random slopes on \\(\\beta^*\\)?\n\n\n\n7.8.5 Contextual effects for categorical predictors\nToken-level predictors can also be categorical, with 3 or more categories. This adds some complexity to the analysis since categorical predictors with \\(k\\) levels are represented in a regression model by \\(k - 1\\) contrasts. Binary and quantitative predictors can be represented in the model with one term. Cluster means then have to be included for each contrast, and we can distinguish within- and between-comparisons for each contrast. Graphically, we could therefore compare \\(k - 1\\) trend-lines, similar to our visualization of binary predictors. The disadvantage of this multi-panel approach is that the individual contrasts themselves show a specific fragment of the patterns formed by the categorical predictor. Depending on which kind of contrast coding is used, these fragments may not be directly informative. We therefore introduce another visualization technique, which allows us to draw a more comprehensive comparison of between- and within-patterns of categorical predictors.\nWe start by fitting a Mundlak (or, equivalently, a between-within) model to isolate the within-comparisons for the token-level predictor. We then use these coefficients to compute for each cluster, the predicted outcome based on the composition of the cluster. We first compute the predicted outcome for each category and then weight these based on their share among the tokens for a particular cluster. This produces a weighted average reflecting the compositional effect.\nWe are then ready to produce a plot similar to Figure 7.16: The horizontal axis shows the cluster-specific weighted average reflecting the compositional effect. In the present case, this is expressed on the log-odds (or logit) scale. The vertical axis represents the observed share of ing for each cluster. The dashed trend line denotes the expected trend among the clusters based on the compositional effect. It is the value on the x-axis expressed as a proportion.\nThe lower panel in Figure 7.16 shows the composition of the clusters. Thus, for each bubble in the top panel, the bottom panel shows four characters representing the share of each category: (c)oronal, o(t)her, (v)elar, (p)ause. Coronal contexts disfavor ing – words with high shares of pre-coronal tokens therefore appear towards the left end of the scale. A pause, on the other hand, favors ing. Words sitting at the right end of the scale therefore have a high share of pre-pausal tokens.\n\n\n\n\n\n\n\n\nFigure 7.16\n\n\n\n\n\nIf the data show only a compositional, and no contextual, effect, the dashed trend line will give a good approximation of the patterns formed by the bubbles representing the clusters. In Figure 7.16, we have added a solid trend line that gives a simple summary of the actual pattern formed by the bubbles in the graph. Similar to Figure 7.7 above, this allows us to obtain a visual comparison of within- and between-patterns in the data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#within--and-between-effects-and-fallacies",
    "href": "06_token_level_predictors.html#within--and-between-effects-and-fallacies",
    "title": "7  Token-level predictors",
    "section": "7.9 Within- and between-effects and fallacies",
    "text": "7.9 Within- and between-effects and fallacies\nThe distinction between within- and between-effects is related to some well-known fallacies in clustered data structures. Thus, the ecological fallacy describes a situation where we base our analysis on cluster means of a token-level predictor and then interpret the estimates as a within-cluster effect. The atomistic fallacy, on the other hand, refers to the situation where we ignore the clustering and interpret effects as between-effects.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#tools",
    "href": "06_token_level_predictors.html#tools",
    "title": "7  Token-level predictors",
    "section": "7.10 Tools",
    "text": "7.10 Tools\n\n7.10.1 Plotting the composition of clusters using a dot diagram\nTo graph the composition of clusters, we first use the package dplyr to determine the cluster means for the token-level predictor, which is here denoted by X:\n\ning_sample %&gt;% \n  group_by(word) %&gt;% \n  summarize(mX = mean(X))\n\nThis will return the following table:\n\n\n# A tibble: 1,024 × 2\n   word_2                    mX\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 going_verb            -0.845\n 2 doing_verb            -0.752\n 3 being_gerund          -0.752\n 4 working_verb          -0.811\n 5 trying_verb           -1    \n 6 being_verb            -0.759\n 7 interesting_adjective -0.333\n 8 coming_verb           -0.732\n 9 getting_verb          -0.744\n10 going_gerund          -0.882\n# ℹ 1,014 more rows\n\n\nWe can pass this table on to a plotting call to produce a dot diagram. Before plotting, we filter out words that occur fewer than 5 times in the corpus.\n\ning_sample %&gt;% \n  filter(frequency &gt;=5) %&gt;%     # exclude words that occur fewer than 5 times\n  group_by(word) %&gt;% \n  summarize(mX = mean(X)) %&gt;% \n  ggplot(aes(x=mX)) +\n  geom_dotplot()\n\n\n\n\n\n\n\n\n\n\n\n\n7.10.2 Exploring between-cluster trends using a bubble chart\nTo draw a bubble chart, we first use dplyr to calculate the necessary cluster-level statistics, i.e. cluster means for the token-level predictor (mX), overall token count in the data set (frequency) and the overall share of ing for the cluster (prop_ing).\n\ning_sample %&gt;% \n  group_by(word) %&gt;% \n  summarize(mX = mean(X),\n            prop_ing = sum(ing)/n(),\n            frequency = n())\n\nThis will return the following table:\n\n\n# A tibble: 1,024 × 4\n   word_2                    mX prop_ing frequency\n   &lt;fct&gt;                  &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;\n 1 going_verb            -0.845    0.345       386\n 2 doing_verb            -0.752    0.438       226\n 3 being_gerund          -0.752    0.578       161\n 4 working_verb          -0.811    0.497       159\n 5 trying_verb           -1        0.150       147\n 6 being_verb            -0.759    0.481       133\n 7 interesting_adjective -0.333    0.894       132\n 8 coming_verb           -0.732    0.354       127\n 9 getting_verb          -0.744    0.592       125\n10 going_gerund          -0.882    0.445       119\n# ℹ 1,014 more rows\n\n\nThis table is then passed on to a plotting call to produce a bubble chart. In the ggplot call, we use the argument size to make the circles proportional to the token count. In geom_point the argument alpha serves to make the bubbles transparent. geom_smooth adds the trend line to the display, with a few additional settings to instruct R that we are looking at proportions (bounded by 0 and 1) and specify the weights for the data points (more frequent token have greater weight).\n\ning_sample %&gt;% \n  group_by(word_2) %&gt;% \n  summarize(mX = mean(foll_cont_bin_c),\n            prop_ing = sum(ing)/n(),\n            frequency = n()) %&gt;% \n  ggplot(aes(x=mX, y=prop_ing, size=sqrt(frequency/pi))) +\n  geom_point(shape=1, alpha=.3) +\n  geom_smooth(method = \"glm\",\n              method.args = list(family = \"binomial\"),\n              aes(weight = frequency), se=F) +\n  scale_size_identity()\n\n\n\n\n\n\n\n\n\n\nFigure 7.17\n\n\n\n\n\n\n7.10.3 Recoding token-level predictors in R\nTo specify a between-within model and a Mundlak model, we need to add two new variables to our data frame: (i) cluster means mX and (ii) the cluster-mean centered version of the token-level predictor dX. We can do this using dplyr:\n\ning_sample %&gt;% \n  group_by(word) %&gt;% \n  mutate(mX = mean()) %&gt;% \n  ungroup() %&gt;% \n  mutate(dX = X - mX)\n\nThis will return the following table:\n\n\n# A tibble: 6,314 × 4\n   word_2                 X     mX     dX\n   &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 morning_noun          -1 -0.489 -0.511\n 2 engineering_gerund    -1 -0.125 -0.875\n 3 falling_verb          -1 -1      0    \n 4 walking_verb          -1 -0.895 -0.105\n 5 working_gerund        -1 -0.634 -0.366\n 6 trying_verb           -1 -1      0    \n 7 trying_verb           -1 -1      0    \n 8 trying_verb           -1 -1      0    \n 9 trying_verb           -1 -1      0    \n10 going_verb            -1 -0.845 -0.155\n# ℹ 6,304 more rows\n\n\n\n\n7.10.4 Model syntax\nSpecifying the models is then straightforward with these new variables:\nBetween-within model:\n\nglmer(ing ~ dX + mX + (1|word), data=ing_sample, family=binomial)\n\nMundlak model:\n\nglmer(ing ~ X + mX + (1|word), data=ing_sample, family=binomial)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "06_token_level_predictors.html#literature-on-between--and-within-comparisons",
    "href": "06_token_level_predictors.html#literature-on-between--and-within-comparisons",
    "title": "7  Token-level predictors",
    "section": "7.11 Literature on between- and within comparisons",
    "text": "7.11 Literature on between- and within comparisons\nRaudenbush & Willms 1995 How does this connect to Gelman & Hill (2007: 310-314) How does this connect to the BK plot (Wainer 2005, chapter 10)\n\n\n\n\nAllison, Paul D. 2009. Fixed Effects Regression Models. Thousand Oaks, CA: Sage.\n\n\nBegg, Melissa D., and Michael K. Parides. 2003. “Separation of Individual-Level and Cluster-Level Covariate Effects in Regression Analysis of Correlated Data.” Statistics in Medicine, no. 22: 2591–2602. https://doi.org/10.1002/sim.1524.\n\n\nBell, Andrew, Malcolm Fairbrother, and Kelvyn Jones. 2019. “Fixed and Random Effects Models: Making an Informed Choice.” Quality & Quantity 53: 1051–74. https://doi.org/10.1007/s11135-018-0802-x.\n\n\nBingenheimer, Jeffrey B., and Stephen W. Raudenbush. 2004. “Statistical and Substantive Inferences in Public Health: Issues in the Application of Multilevel Models.” Annual Review of Public Health, no. 25: 53–77. https://doi.org/10.1146/annurev.publhealth.25.050503.153925.\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\n\n\nCastellano, Katherine E., Sophia Rabe-Hesketh, and Anders Skrondal. 2014. “Composition, Context, and Endogeneity in School and Teacher Comparisons.” Journal of Educational and Behavioral Statistics 39 (5): 333–67. https://doi.org/10.3102/1076998614547576.\n\n\nDiez Roux, Ana M. 2002. “A Glossary for Multilevel Analysis.” Journal of Epidemiology and Community Health 56: 558–94. https://doi.org/10.1136/jech.56.8.588.\n\n\nDuncan, Craig, Kelvyn Jones, and Graham Moon. 1998. “Context, Composition and Heterogeneity: Using Multilevel Models in Health Research.” Social Science & Medicine 1 (46): 97–117. https://doi.org/10.1016/s0277-9536(97)00148-2.\n\n\nEbbes, Peter, Ulf Böckenholt, and Michel Wedel. 2004. “Regressor and Random-Effects Dependencies in Multilevel Models.” Statistica Neerlandica 58 (2): 161–78. https://doi.org/10.1046/j.0039-0402.2003.00254.x.\n\n\nHausman, Jerry A. 1978. “Specification Tests in Econometrics.” Econometrica 46: 1251–71. https://doi.org/10.2307/1913827.\n\n\nMundlak, Yair. 1978. “On the Pooling of Time Series and Cross Section Data.” Econometrica 46 (1): 69–85. https://doi.org/1913646.\n\n\nNeuhaus, J. M., and J. D. Kalbfleisch. 1998. “Between- and Within-Cluster Covariate Effects in the Analysis of Clustered Data.” Biometrics, no. 54: 638–45.\n\n\nPalta, Mari, and Chris Seplaki. 2003. “Causes, Problems and Benefits of Different Between and Within Effects in the Analysis of Clustered Data.” Health Services and Outcomes Research Methodology, no. 3: 177–93. https://doi.org/10.1023/A:1025893627073.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and Longitudinal Modeling Using Stata. College Station, TX: Stata Press.\n\n\nRaudenbush, Stephen W., and Anthony S. Bryk. 2002. Hierarchical Linear Models: Applications and Data Analysis Methods. Thousand Oaks, CA: Sage.\n\n\nSchnuck, Reinhard, and Francisco Perales. 2017. “Within- and Between-Cluster Effects in Generalized Linear Mixed Models: A Discussion of Approaches and the Xthybrid Command.” The Stata Journal 17 (1): 89–115. https://doi.org/10.1177/1536867X1701700106.\n\n\nSjölander, Arvid, Paul Lichtenstein, Henrik Larsson, and Yudi Pawitan. 2013. “Between–Within Models for Survival Analysis.” Statistics in Medicine 18 (32): 3067–76. https://doi.org/10.1002/sim.5767.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section and Panel Data. Cambridge, MA: MIT Press.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Token-level predictors</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html",
    "href": "07_foregrounding_variation.html",
    "title": "8  Foregrounding variation",
    "section": "",
    "text": "8.1 Generalizing and particularizing approaches to clustering units\nThe behavior of organisms can be studied from different perspectives, which have been classified along a continuum from idiographic to nomothetic. This distinction, which was introduced by Windelband (1998)1, can, for our present purposes, be understood as referring to different levels of analysis. In this chapter we will discuss the relevance of the nomothetic-idiographic cline for the study of language and the analysis of corpus data. Both levels of analysis have advantages and limitations. An awareness of existing trade-offs can serve to inform our approach to corpus data, enhance our data-analytic repertoire and lead to more nuanced empirical insights. We will start with a delineation of the two styles and then discuss how they can be integrated into corpus data analysis. Graphical and numerical tools that add idiographic insights will be discussed. The final section is concerned with the relevance of the idiographic perspective in language data analysis more generally.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#generalizing-and-particularizing-approaches-to-clustering-units",
    "href": "07_foregrounding_variation.html#generalizing-and-particularizing-approaches-to-clustering-units",
    "title": "8  Foregrounding variation",
    "section": "",
    "text": "1 The distinction between idiographic and nomothetic perspectives has had noticeable repercussions in the field of psychology, where it was popularized by (Allport 1937, 22; 1962). See Thomae (1999) for an overview and Holt (1962) for a critical perspective.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#the-two-orientations-in-contrast",
    "href": "07_foregrounding_variation.html#the-two-orientations-in-contrast",
    "title": "8  Foregrounding variation",
    "section": "8.2 The two orientations in contrast",
    "text": "8.2 The two orientations in contrast\nThe nomothetic research style is characterized by the pursuit of generalization. Its aim is to identify regularities across organisms, with a view, perhaps, to establishing general laws. As described by (Levy1970?), this branch of research activity includes “the development of generalizations of ever increasing scope, so greater and greater varieties of phenomena may be explained by them, and broader and broader reaching predictions and decision based upon them”. Accordingly, empirical work seeks to collect sufficiently large samples to establish a solid base for generalization. Data analysis involves the aggregation and averaging over individuals to derive broad-inference estimates that can be projected across classes of organisms, or organisms in general. Variation among units around these abstracted patterns is a minor concern at this level of analysis.\nThe idiographic orientation, on the other hand, shifts the main focus to the individual organism. This style of research is concerned with a comprehensive appreciation of each specific unit, offering a description of its patterns of behavior. The detailed profiles obtained by this strategy reveal consistencies and differences among units, showing stable configurations across units as well as the idiosyncratic features in each. Empirical studies therefore typically investigate fewer distinct organisms, but compile more extensive records of each. Hence, the method is sometimes referred to as the “small-n approach”.2 Data-analytic techniques involve the computation of summary measures at the level of the individual unit, whose integrity is preserved throughout all stages of analysis and interpretation.\n2 Here, n refers to the number of units. This should not be taken to suggest that idiographic studies require fewer resources or deal with data sets of smaller size in general (Mook 1982, 223).As originally conceived, the labels “idiographic” and “nomothetic” refer to different research methodologies or even scientific mentalities. Differences between the two styles therefore surface in all branches of research activity, extending from data analysis and research design to problem formulation, substantive priorities and philosophical outlook. Our focus will be on the relevance of these viewpoints for data analysis. We will see that the distinction becomes relevant when there is clustering in the data. As we have argued in Chapter Chapter 4, clustered data structures are typical for natural language data. We would therefore argue that the distinction between nomothetic and idiographic data-analytic philosophies is also of broader relevance for corpus data analysis.\nThe analysis of structured data layouts can put varying degrees of emphasis on the individual units (clusters) in the data. We might background them by only considering their average behavior. This would correspond to a nomothetic viewpoint, with the average smoothing over the underlying idiosyncracies to distill the information that is shared by the set of units. We will also refer to this empirical direction as the group-averaged, generalizing approach. The individual units submerge into the error term of the model, which collects the the residual variation not accounted for by the systematic component of the model.\nOn the other hand, we could also be interested in the underlying units as such, asking about the patterns the data form in each particular cluster. This would resemble, in some sense, the idiographic research style. Our attention could be focused on the specific unit, as we might consider each cluster an object of interest in its own right. We will refer to this kind of outlook as the unit-specific approach. A weaker form of unit-specificity would be show an interest not for each unit individually, but on the structure of the existing between-unit variability, that is, the degree of similarity or variability among units.\nLet us consider a hypothetical example. Figure Figure 8.1 shows how the same set of data could be visualized from these two angles. In our illustrative example, a predictor of interest has three levels (conditions); these are shown along the horizontal axis. There are five subjects, and every subject is observed in each condition. Panel (a) shows a nomothetically inspired, group-averaged perspective on the data. In each condition, we average over the five speakers. This condenses the information into a three scores, which represent, in some average sense, how the typical speaker behaves in these conditions.\nThe data are of higher resolution, however – we also have information about the shape of the pattern that the predictor forms in each subject. An idiographically-inspired, unit-specific perspective on the data would foreground the configurations observed for each speaker. The corresponding visualization is shown in Figure Figure 8.1 b, where the individual profiles denote within-speaker conditional averages. This reveals a richer perspective on the data: It shows the degree of consistency of the group-statistical profile across individuals. Thus, it is not concerned primarily with the most typical score for a particular condition, but with the regularity of this pattern across individual speakers. In Figure Figure 8.1 b, we are asking about whether the hockey-stick pattern materializes in each individual.\n\n\n\n\n\n\n\n\nFigure 8.1: Illustration of the nomothetic and idiographic approach to data analysis and interpretation.\n\n\n\n\n\nSince both perspectives offer meaningful insights, our aim should be to combine features of both approaches in corpus data analysis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#complementary-approaches-to-data-analysis",
    "href": "07_foregrounding_variation.html#complementary-approaches-to-data-analysis",
    "title": "8  Foregrounding variation",
    "section": "8.3 Complementary approaches to data analysis",
    "text": "8.3 Complementary approaches to data analysis\nData analysis using regression modeling is typically geared towards group-statistical, nomothetic description. Our interpretation usually centers on those regression coefficients that reflect averages and differences between averages (the “fixed effects”). The visualization of regression models also rests on predicted averages for certain constellations of predictor values. As we will demonstrate in the following, a useful strategy is to supplement and enrich aggregating approaches with empirical indications at the level of the individual units – about pattern variability and consistency. To encourage a more balanced emphasis on different levels of analysis, let us contrast drawbacks of the nomothetic view with benefits of the idiographic angle.\nWe should emphasize that group-statistical summaries do play an important role in data-based work. By averaging over a set of scores, we gain information by discarding numerical details (Stigler2016?). Thus, if averages provide good summaries, they facilitate comparisons across partitions of the data. We will distinguish two types of averaging: (i) average scores computed over a set of numbers and (ii) average patterns computed over a set of profiles. Figure Figure 8.1 shows examples of both kinds: (i) each conditional average in panel (a) is an average over a set of 5 scores; and (ii) the hockey-stick profile in panel (a) is an average over a set of 5 patterns.\nThe important point for the ensuing discussion is that any kind of averaging treats the data points, or patterns, as interchangeable (e.g. Cobb 1997, 155–57), i.e. sufficiently similar to yield a valid abstraction. Thus, average scores should do a good job of indicating what the typical observation looks like. In Figure Figure 8.1 a, for instance, the average for condition 3 does a slightly better job than that for condition 1. Average patterns, on the other hand, should provide a valid summary of the individual profiles that go into this average. In Figure Figure 8.1 b, we see some variability in the subject-specific configurations. In general, average scores and patterns lose their appeal (and validity) if a closer look at the underlying cases raises doubts about the similarity, or interchangeability of the underlying numbers or profiles.\nBefore computing an average over a set of scores or patterns, then, we should inspect the data for threats to the validity of our summary measures. Since averages can mask a great deal of underlying variability, they should routinely be accompanied by measures of spread (e.g. the standard deviation or median absolute deviation). The dispersion of scores around their average gives us some idea of how “typical” the average case really is. This routine textbook advice is more difficult to apply to average patterns, however – especially if the profiles of interest are formed over three or more conditions. Visual examinations of unit-level patterns, as illustrated in Figure Figure 8.1 b, then provide invaluable assistance. Side-by-side comparisons of the average pattern and its constituent profiles will show the descriptive adequacy of the average pattern. We will consider additional techniques for the appraisal of pattern consistency across units further below.\nIf an average pattern is forced onto an ensemble of erratic profiles, it does not describe the typical pattern in the underlying set; it is rather an elusive abstraction that may in fact represent only a fraction of the units.3 A breakdown of the focal profile by individual unit can therefore urge a more cautionary interpretation of group-statistical trends. In cases where the extent of commonalities among units, as reflected in the smoothed overlay, diminish against the backdrop of unit-to-unit variability, averages may fail to articulate meaningful and interesting data features.\n3 In some cases, average patterns can also distort the form of a relationship (Mook 1982, 193–94).4 As noted by (Mook 1982, 230), “the odds are that you would see no need to test the statistical significance of this relationship. It has met the ’traumatic interocular test” […]: It hits you between the eyes.”If, on the other hand, the profiles are, by and large, in good agreement with their balanced outline, this indicates that the group-statistical summary offers a good description of the typical pattern and does not obscure variation. We would then be more inclined to take the summary statement at face value. More importantly, however, observed regularities among units enable us to make a much stronger case for the factor of interest. By allowing – or forcing – the patterns to become directly visible in the individual units, we may be able to offer a simple and direct demonstration of the capacity of the focal predictor. Stable within-unit profiles offer a far more telling and convincing story than averaged summaries (Mook 1982, 228–29). Consistent configurations within individuals therefore add additional weight to the evidence and may in fact obviate the need for statistical significance testing.4\nIn fact, the demonstrability of between-unit consistency may be considered a more stringent criterion for judgments about the “significance” of an observed pattern. This is because, given a sufficiently large sample of units, “statistical significance” can be demonstrated for almost any kind of pattern. At some point, the size of the data will outweigh unit-to-unit heterogeneity. In terms of research strategy, larger samples are therefore not the primary key to establishing the validity and generality of statistical conclusions. If the averaged pattern reverberates steadily at the level of the individual units, however, this will add evidential force to our conclusions, providing more reassurance to the reader than any statistical testing procedure could.\nDemonstrations of consistency also provide leverage when considering the generality of research findings (see, e.g., Sidman 1960, 51, 83). If we find corresponding patterns in a set of units, this provides statistical grounds for extending conclusions to unobserved cases. Consistency among units therefore enhances generality, especially if regularities emerge across a diverse set or different classes of units. Judgments as to the range of validity of research findings, which ultimately rest on extra-statistical, subject-matter grounds, can at least in part be informed by statistical analysis.\nTo summarize, we have seen that the particularization of empirical insights that is characteristic of the idiographic research style attaches great importance to the integrity of the individual unit. Instead of averaging over cases, it offers a description of the behavior of each unit under study. This angle reveals a level of detail that is beyond the reach of group-statistical approaches. A great deal of analytical leverage can be gained by combining group-averaged and unit-specific levels of analysis. We have accentuated potential disadvantages of group-statistical methods to encourage a shift of emphasis to the variability among units. A complementary approach will assess the consistency among units to arrive at a proper understanding of what the data are saying. While inconsistency among units would qualify and challenge average summaries, consistent profiles produce a coherent picture that is more than the sum of its parts.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#graphical-techniques",
    "href": "07_foregrounding_variation.html#graphical-techniques",
    "title": "8  Foregrounding variation",
    "section": "8.4 Graphical techniques",
    "text": "8.4 Graphical techniques\n\nDot diagram brings all the data points into view\nUncover and interpret between-unit variation that is represented in the random-effects hyperparameters\nComparative measures: Extend focus to the level of the individual units, e.g. Probability of superiority (Grissom and Kim (2012), p. 149–174)\nicon arrays Akl et al. (2011)\n\nPresent information on the level of the individual unit\n\nconsistency refers to the share of units showing a particular pattern or configuration\nHow representative is the mean value of all the subjects in the population?\nThe number of subjects who actually fell at the mean value.\nIf there were a greater interest in the population distribution, we would see more data expressed not simply in terms of summary measures, but rather in terms of the number of subjects that yield each value of the quantity of interest.\n\n\n8.4.1 Techniques developed in the variationist paradigm\nBickerton (1971)’s criticism on the averaging triggered an increased focus on speaker-specific behavior, which led to a number of methodological advances. We will here discuss two exemplary studies, each with a methodological focus, which illustrate the degree of data-analytic sophistication that was developed early on in the variationist paradigm.\nThe first is G. Sankoff (1974), who used demonstrated how to look beyond aggregated results. The first graph type is concerned with the consistency of a linguistic factor across individuals. Figure 8.2 applies the graphing technique to the predictor LEXICAL CATEGORY, for which we observed the highest rate of g-dropping for verbs, followed by gerunds, nouns and then adjectives. The graph shows two things. First, the vertical spread of points reflects the variation in dropping rates across speakers within each lexical category. The range of dropping rates is marked visually using a rectangle, which extends from the lowest to the highest proportion of [in]-realizations. The mean rate for each column (i.e. each lexical category) is shown using a thicker crossbar. It is clear that each mean summarizes a highly variable distribution.\nThe second aspect highlighted in Figure 8.2 is the consistency of the verb &gt; gerund &gt; noun &gt; adjective cline across speakers. Each speaker appears as a profile, connecting their dropping rates for these four lexical classes. To avoid a clutter due to noise and sampling variability, Figure 8.2 only includes individuals with at least 5 tokens for each lexical class. The focus is then on whether the overall configuration emerges within each speaker, which appears to be roughly the case.\n\n\n\n\n\n\n\n\n\nFigure 8.2: Consistency of lexical category constraint across individuals with at least 5 tokens in each word class (15 speakers). This graph is modeled on G. Sankoff (1974, 33).\n\n\n\n\nAnother display type used in G. Sankoff (1974) is mimicked in Figure 8.3. Again, only 15 speakers are shown, each with at least 5 tokens per lexical category. Black bars indicate the number of [in] tokens, white bars the number of [ing] tokens produced by an individual in a specific category. Individuals are ordered by g-dropping rate: Speaker 404, at the top, almost categorically uses the [in] pronunciation; speaker 116, at the bottom, shows the reverse behavior. We note that the dropping rate decreases fairly regulary from left to right for these 15 speakers.\n\n\n\n\n\n\n\n\nFigure 8.3: Consistency of lexical category constraint across individuals with at least 5 tokens in each word class (15 speakers). This graph is modeled on G. Sankoff (1974, 36).\n\n\n\n\n\nThe final graphing technique used by G. Sankoff (1974) has a different focus: the coherence of social groupings. In the example she uses, speaker-specific rates are plotted against speaker age, and the visual demonstration reveals a patterning that is so clear that no trend line is required. Figure 8.4 spplies the technique to the predictor EDUCATION.\n\n\n\n\n\n\n\n\n\nFigure 8.4: Between-speaker variation within social categories. This graph is modeled on G. Sankoff (1974, 41).\n\n\n\n\nFurther, distinctly different graphical methods were proposed by Guy (1980), who distinguishes between qualitative and quantitative assessments. Qualitative checks focus on the rank order of factor levels and its consistency across individuals. A pairwise comparison chart is used to visually depict the consistency of ordering. A major obstacle is the data sparsity, and Guy (1980) demonstrates the use of graphical tools that include information about token counts and therefore the statistical precision of speaker-specific values.\nGuy (1980) proposed a number of graphical techniques for assessing the consistency of data patterns across individuals. The illustrative analysis used in that study was concerned with a variable rule analysis of /t,d/ deletion, focusing on a number of grammatical and phonological constraints. To examine the generality across individuals of the average pattern obtained for a specific predictor, Guy (1980) used two general approaches, which we will illustrate using our data on (ING). We have observed that the average pattern is that verbs show the highest rate of g-dropping, followed by nouns and then adjectives. In other words, the realization as [ing] is most likely in adjectives, and less likely in nouns and verbs.\nThe first approach used by Guy (1980) is qualitative. It looks at whether the cline adjective &gt; noun &gt; verb is stable across individuals. Since there are three levels of the factor, there are three ordered comparisons we can make for each speaker. The first step is to obtain, for each speaker, the rate of g-dropping for each lexical category.Gelman et al. (2008) The three rates can then be compared, to see whether the material produced by this individual conforms to the general pattern.\nGuy (1980) records the results of these comparisons in a pairwise comparison chart. This chart is a cross-table that lists all factors levels as column and row labels. It makes sense to arrange these levels according to the average pattern in the data, in decreasing order from left to right an from top to bottom. For each combination of factor levels (e.g. adjective - verb), there are two cells - one above the diagonal and one below the diagonal. If the within-speaker rank order of the two factor levels is consistent with the general pattern, a mark is entered into the cell below the diagonal, if the pattern is reversed in the data produced by this individual, the mark appears above the diagonal.\nFigure 8.5 shows a pairwise comparison chart for the predictor Lexical Category. When interpreting this graph, our focus is on the degree to which marks occupy the area below the diagonal. If the center of visual gravity is located firmly towards the lower left corner of the cross table, this reflects consistency across speakers.\n\n\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\nFigure 8.5, which uses dots as visual marks, deviates from the version that appears in Guy (1980) in one important regard: The original pairwise comparison use an additional mark if there is a draw between the factor levels - in our case, if the rate of g-dropping is the same in both contexts. This mark (a horizontal instead of a vertical bar) is then added to the cell below the diagonal. Since this adds to the visual weight below the diagonal, i.e. to the preferred outcome of the data examination, we prefer to exclude such draws from the chart.\nA key issue that arises when studying these speaker-level comparisons is data sparsity. Thus, a speaker may have produced only few tokens for a specific factor level, which adds uncertainty to the comparison and the location of the mark in the pairwise comparison chart. This issue is discussed at length in Guy (1980), where information about the number of tokens buttressing a specific comparison is incorporated into graphs. As a measure of data quantity, the token count for the factor level with the smaller number of occurrences is used. This allows us to see whether inconsistencies may be attributable to paucity of data and the resulting sampling variation .\nTo add information of data quantity to the graph, the cells in the pairwise comparison chart are rearranged into stacked rows (Figure 1.3 in Guy 1980, 19). Unfortunately, the resulting visual form loses the direct interpretation in terms of the center of viusal gravity. We therefore opt for a different strategy, which allows us to include token count information directly into the pairwise comparison chart: Fill color reflects token count, with darker greyshades signalling more comfortable sample sizes.\nThe second approach used by Guy (1980) is quantitative in nature. Instead of noting the rank order of factor levels, it records and compares the proportionate difference obtained for specific comparisons across speakers. These differences are graphed against the data quantity measure, the smaller number of tokens for the pair of factor levels. Figure 8.6 shows such a quantitative factor pair chart for the predictor Lexical Category. The black vertical line indicates the average over all speakers, and the scatter of points allows us to note how consistent speakers are. Again, data sparsity is a potential issue. Since smaller token counts will yield more variable difference estimates, the point cloud should assume the shape of a funnel, with points higher up in the graph being closer to the group average. For our data, we observe this pattern for the (adjective - verb) and the (adjective - noun) difference. For the (noun-verb) difference, however, there is a clear outlier.\n\n\n\n\n\n\n\n\nFigure 8.6: Quantitative factor pair chart\n\n\n\n\n\n\n\nWarning: Removed 33 rows containing missing values or values outside the scale range\n(`stat_bindot()`).\n\n\n\n\n\n\n\n\nFigure 8.7: Quantitative factor pair dot diagram\n\n\n\n\n\nFurther graphical techniques that use speaker-specific factor levels or differences between these appear in Guy and Boyd (1990) and Guy (1991). For instance, Guy and Boyd (1990, 8, 11) graph the factor levels for an internal predictor (morphological class) against an external variable, speaker age. They also graph the difference between two morphological classes against speaker age, as a reassurance.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nWarning: Removed 19 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#the-issue-of-data-sparsity",
    "href": "07_foregrounding_variation.html#the-issue-of-data-sparsity",
    "title": "8  Foregrounding variation",
    "section": "8.5 The issue of data sparsity",
    "text": "8.5 The issue of data sparsity\nThe individual-level analyses developed in the variable rule paradigm rely on separate analyses of speakers. This means that the data need to be divided up, and separate regressions (or Varbrul analyses) are then run on these partitions (Guy 1980; Guy and Boyd 1990). Alternatively, an interaction between Speaker and the within-speaker predictor can be included into the model (D. Sankoff and Labov 1979, 209). Both of these strategies may run into data sparsity issues.\nData sparsity issues lead to three problems:\n\nNoisy parameter estimates\nSeparation or quasi-separation (referred to as “knockouts” in the variationist paradigm)\nEmpty cells(^ Guy 1988, 126 refers to this as “non-orthogonality” of factors (or, in variable rule parlance, “factor groups”).)\n\nVarious work-arounds have been used in the literature (Guy 1980, 20).\n\nCombine data for speakers (Guy and Boyd 1990, 7)\nCombination of factor levels\nExclusion of speakers (Paolillo 2013)\n\nNone of these strategies may be considered satisfactory. The combination of data from different speakers (or items, as the case may be) appears the least problematic. Guy (2018) suggested that clustering units with fewer than 20 tokens should be combined in a “residual category”. As for the combination of factor levels, if there are theoretical reasons to distinguish between them, the informativity of the analysis may be impeded. As for the conflation of categories, Guy (1988, 132) notes that “in the absence of linguistically meaningful generalization about common properties that unite [these classes] […], this procedure would yield garbage”. Finally, the exclusion of speakers may also be undesirable. If individuals are excluded due to data sparseness, this reduces the sample size. If speakers are excluded because they show categorical behavior, either in general, or for a specific factor level, the exclusion of these individuals may distort the results for speaker-level predictors. For instance, Paolillo (2013, 97) removes from the analysis two speakers who showed categorical usage, due to “knockouts”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#idiographic-perspectives-on-language-data",
    "href": "07_foregrounding_variation.html#idiographic-perspectives-on-language-data",
    "title": "8  Foregrounding variation",
    "section": "8.6 Idiographic perspectives on language data",
    "text": "8.6 Idiographic perspectives on language data\nIt was stressed by Carver (1978) that Coleman (1964) and Clark (1973) represented a particular scientific mentality, which he refers to as that of “statistical significance testers”. This mentality is related to the generalizing, nomothetic approach (see Chapter …). Carver (1978) provides perspective on the critique that had been advanced by Miller and Coleman (1972) against his study on reading behavior (Carver 1972). Miller and Coleman (1972) reported a replication study that apparently failed to replicate Carver’s findings. Carver (1978) lays out how his reanalysis of Miller and Coleman (1972)’s data as well as several further successful replications by himself lent support to his original findings. We can learn from this episode that the nomothetic approach to language data analysis, and its associated test of significance are not the only reliable scientific strategies. Carver’s research style focussed on within-study replicative patterns as well as coherence with theory. Thus, Carver (1978) noted that the dependability of his earlier findings could be read from two forms of replication: The consistency of a function relationship across two groups of subjects, and the inherent replication offered by the individual data points in the curve, where the similarity between adjacent difficulty levels reflected consistency, or repeatability.(^Is this form of replication mentioned at all in Sidman 1960?)\nInterestingly, the issue of generality debate also appeared in the fixed-vs-random-effect- controversy. As Raaijmakers (2003) notes, a common misconception in the psycholinguistic community is that item analyses only return a significant result if the treatment effect is obtained for every item. He notes that “[t]he idea seems to be that a treatment effect should be present for each and every one of the subjects and items, otherwise it is not a real effect.” (2003, 146; see also Forster 2008). This is an idiographic interpretation, attached to a nomothetic analysis. In fact, traditional ANOVA-style analyses are ill-equipped to give a meaningful reflection of generality of an effect across subjects and/or items. In light of these misconceptions, and it appears questionable whether random effects should actually be referred to as “generalization effects”, as suggested by Coleman (1979).\n\ndiscussion in experimental syntax/generative grammar: Den Dikken et al. 2007\nstudies that have demonstrated stable variation across speakers, cited in Barth & Kapatsinsi 2014: 205-206\nFindings apply to individuals, the locus of the speaker’s mind as the locus of grammar\nthe findings do apply to live behaving organisms, rather than in a “by-and-large, on-the-average” way to the abstract “typical case”. After all, only individual behave. Groups do not. (Mook 1982, 238)\nCedergren and Sankoff (1974, 347) caution against generalizing across variable speaker-level grammars; check the fit of their model against the individual speakers performance; “categorical rule differences”\nBickerton (1971, 488) cautions against averaging over between-speaker variability\nreversals take a special place\n\nVariationist sociolinguistics has experienced its own nomothetic-idiographic debate in the 1970s. Thus, Bickerton (1971) criticized the nomothetic approach taken by Labov et al. (1968) and Labov (1969), which triggered methodological progress in variationist research towards a greater consideration of the idiographic perspective. A variety of examples are summarized by D. Sankoff and Labov (1979, 206–10). See Guy (1980) for an exemplary study.\n\n8.6.1 Individual differences: Substantive interest in variability\n\nIf variation in a set of data reflects real between-speaker variation, this variability has basic substantive interest\nintrinsic interest\nIf the source of variability is real individual differences, this variability has basic substantive interest. (Anderson 2001, 19)\nIn most theory-oriented research, individual differences are ignored. The general run of experimental process studies take for granted either that essentially all individuals in a given condition will exhibit the same directional effect or that any opposite effects by a minority of subjects would not trouble the face value of the means. This may usually be justified, but the massive unconcern with obtaining such justification is disturbing. (Anderson 2001, 19–20)\nthe intricacy of internal structure\n\n\n\n8.6.2 Idiographic approach to lexical units\n\na uniqueness that defies all formulation (Allport 1962, 408, quoting James)\nwe can give no ultimate account of it (Allport 1962, 408, quoting James)\nappreciation of the richness of lexical individuality\nresidual peculiarities\nwell-configurated prominent dispositions\ncapture the full richness\nthe internal organization\nA single exception may be buried in a standard deviation\nmore nuanced attention to\n\nImplications\n\nshift our field of activity\ncontinue to develop and sharpen procedures for investigating the behavior of individual subjects\nnomotheitc approach should be supplemented with idiographic level of description\naverages are taken as valid statistical summaries",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "07_foregrounding_variation.html#combining-nomothetic-and-idiographic-perspectives-using-mixed-effects-models",
    "href": "07_foregrounding_variation.html#combining-nomothetic-and-idiographic-perspectives-using-mixed-effects-models",
    "title": "8  Foregrounding variation",
    "section": "8.7 Combining nomothetic and idiographic perspectives using mixed-effects models",
    "text": "8.7 Combining nomothetic and idiographic perspectives using mixed-effects models\nThe danger of shrinkage/partial pooling\n\nBaayen, Davidson, and Bates (2008) suggest inspection of the BLUPs\nThis is what Forrest (2015) does\n\n\n\n\n\nAkl, Elie A., Andrew D. Oxman, Jeph Herrin, Gunn E. Vist, Irene Terrenato, Francesca Sperati, Cecilia Costiniuk, Diana Blank, and Holder Schünemann. 2011. “The Dynamic Interaction Between Lexical and Contextual Frequency: A Case Study of (ING).” Cochrane Database of Systematic Reviews, no. 3. https://doi.org/10.1002/14651858.CD006776.pub2.\n\n\nAllport, Gordon W. 1937. Personality: A Psychological Interpretation. New York: Henry Holt.\n\n\n———. 1962. “The General and the Unique in Psychological Science.” Journal of Personality 30 (3): 405–22. https://doi.org/10.1111/j.1467-6494.1962.tb02313.x.\n\n\nAnderson, Norman H. 2001. Empirical Direction in Design and Analysis. Mahwah, NJ: Lawrence Erlbaum.\n\n\nBaayen, R. Harald, Douglas J. Davidson, and Douglas M. Bates. 2008. “Mixed-Effects Modeling with Crossed Random Effects for Subjects and Items.” Journal of Memory and Language 59 (4): 390–412. https://doi.org/10.1016/j.jml.2007.12.005.\n\n\nBickerton, Derek. 1971. “Inherent Variability and Variable Rules.” Foundations of Language 7 (4): 457–92. http://www.jstor.org/stable/25000558.\n\n\nCarver, Ronald P. 1972. “Evidence for the Invalidity of the Miller-Coleman Readability Scale.” Journal of Reading Behavior 4 (3): 42–47. https://doi.org/10.1080/10862967109546999.\n\n\n———. 1978. “Sense and Nonsense about Generalizing to a Language Population.” Journal of Reading Behavior 10 (1): 25–33. https://doi.org/10.1080/10862967809547252.\n\n\nCedergren, Henrietta J., and David Sankoff. 1974. “Variable Rules: Performance as a Statistical Reflection of Competence.” Language 50 (2): 333–55. https://doi.org/10.2307/412441.\n\n\nClark, Herbert H. 1973. “The Language-as-Fixed-Effect Fallacy: A Critique of Language Statistics in Psychological Research.” Journal of Verbal Learning and Verbal Behavior 12 (4): 335–59. https://doi.org/10.1016/S0022-5371(73)80014-3.\n\n\nCobb, George W. 1997. Introduction to Design and Analysis of Experiments. New York: Wiley.\n\n\nColeman, Edmund B. 1964. “Generalizing to a Language Population.” Psychological Reports 14 (1): 219–26. https://doi.org/10.2466/pr0.1964.14.1.219.\n\n\n———. 1979. “Generalization Effects Vs Random Effects: Is \\(\\sigma_{TL}^{2}\\) a Source of Type 1 or Type 2 Error?” Journal of Verbal Learning and Verbal Behavior 18 (2): 243–56. https://doi.org/10.1016/S0022-5371(79)90145-2.\n\n\nForrest, Jon. 2015. “Community Rules and Speaker Behavior: Individual Adherence to Group Constraints on (ING).” Language Variation and Change 27 (3): 377–406. https://doi.org/10.1017/s0954394515000137.\n\n\nForster, Kenneth I. 2008. “What Is F2 Good For?” Journal of Memory and Language 59 (4): 389. https://doi.org/10.1016/j.jml.2008.08.002.\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4): 1360–83. https://doi.org/10.1214/08-AOAS191.\n\n\nGigerenzer, Gerd. 1996. “The Psychology of Good Judgment: Frequency Formats and Simple Algorithms.” Medical Decision Making 16 (3): 273–80. https://doi.org/10.1177/0272989X9601600312.\n\n\nGigerenzer, Gerd, and Ulrich Hoffrage. 1995. “How to Improve Bayesian Reasoning Without Instruction: Frequency Formats.” Psychological Review 102 (4): 684–704. https://doi.org/10.1037/0033-295X.102.4.684.\n\n\nGrissom, Robert J., and John J. Kim. 2012. Effect Sizes for Research: Univariate and Multivariate Applications. New York: Routledge.\n\n\nGuy, Gregory R. 1980. “Variation in the Group and the Individual: The Case of Final Stop Deletion.” In Locating Language in Time and Space, edited by William Labov, 1–36. New York: Academic Press.\n\n\n———. 1988. “Advanced VARBRUL Analysis.” In Linguistic Change and Contact: NWAV-XVI, edited by Kathleen Ferrara, Becky Brown, Keith Walters, and John Baugh, 124–36. Austion, TX: University of Texas, Department of Linguistics.\n\n\n———. 1991. “Explanation in Variable Phonology: An Exponential Model of Morphological Constraints.” Language Variation and Change 3 (1): 1–22. https://doi.org/10.1017/S0954394500000429.\n\n\n———. 2018. “LVC Guidelines for Reporting Quantitative Results.” http://gregoryrguy.com/wp-content/uploads/Guy-2018-Guidelines-for-reporting-quantitative-results-LVC-Nov-18-2018.pdf.\n\n\nGuy, Gregory R., and Sally Boyd. 1990. “The Development of a Morphological Class.” Language Variation and Change 2 (1): 1–18. https://doi.org/10.1017/S0954394500000235.\n\n\nHolt, Robert A. 1962. “Individuality and Generalization in the Psychology of Personality.” Journal of Personality 30 (3): 377–404. https://doi.org/10.1111/j.1467-6494.1962.tb02312.x.\n\n\nLabov, William. 1969. “Contraction, Deletion, and Inherent Variability of the English Copula.” Language 45 (4): 715–62. https://doi.org/10.2307/412333.\n\n\nLabov, William, Paul Cohen, Clarence Robins, and John Lewis. 1968. A Study of the Non-Standard English of Negro and Puerto Rican Speakers in New York City. New York: Columbia University.\n\n\nMiller, Gerald R., and Edmund B. Coleman. 1972. “The Measurement of Reading Speed and the Obligation to Generalize to a Population of Reading Materials.” Journal of Reading Behavior 4 (3): 48–56. https://doi.org/10.1080/10862967109547000.\n\n\nMook, Douglas G. 1982. Psychological Research: Strategy and Tactics. New York: Harper; Row.\n\n\nPaolillo, John C. 2013. “Individual Effects in Variation Analysis: Model, Software, and Research Design.” Language Variation and Change 25 (1): 89–118. https://doi.org/10.1017/S0954394512000270.\n\n\nRaaijmakers, Jeroen G. W. 2003. “A Further Look at the ‘Language-as-Fixed-Effect Fallacy’.” Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale 57 (3): 141–51. https://doi.org/10.1037/h0087421.\n\n\nSankoff, David, and William Labov. 1979. “On the Uses of Variable Rules.” Language in Society 8 (2): 189–222. https://doi.org/10.1017/S0047404500007430.\n\n\nSankoff, Gillian. 1974. “A Quantitative Paradigm for the Study of Communicative Competence.” In Explorations in the Ethnography of Speaking, edited by Richard Baumann and Joel Sherzer, 18–49. Cambridge: Cambridge University Press.\n\n\nSidman, Murray. 1960. Tactics of Scientific Research: Evaluating Experimental Data in Psychology. New York: Basic Books.\n\n\nThomae, Hans. 1999. “The Nomothetic-Idiographic Issue: Some Roots and Recent Trends.” International Journal of Group Tensions 20 (1/2): 187–215. https://doi.org/10.1023/a:1021891506378.\n\n\nWindelband, Wilhelm. 1894. “Geschichte Und Naturwissenschaft.” In Rektoratsreden Der Universität Strassburg, 193–208. Strassburg: Heitz und Mündel. https://doi.org/10.11588/diglit.20767.\n\n\n———. 1998. “History and Natural Science.” Theory & Psychology 8 (1): 5–22. https://doi.org/10.1177/0959354398081001.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Foregrounding variation</span>"
    ]
  },
  {
    "objectID": "08_modeling_tactics.html",
    "href": "08_modeling_tactics.html",
    "title": "9  Modeling tactics",
    "section": "",
    "text": "9.1 Using random effects to assess the generality of a pattern\nWe discussed the limited role of statistics for establishing the generality of a pattern we have found in our data. Whenever we infer beyond the set of speakers, conditions, and circumstances we have observed, conclusions rest in part on extra-statistical sources of information. Nevertheless, our data do offer information that allow us to form some judgement on the degree of tenacity with which we should cling to broad inferences. Broad inferences represent the nomothetic orientation in empirical work, and usually include statements about typical, or average cases. As we discussed in Chapter 8, the idiographic orientation offers an additional approach to data. In the following, we will demonstrate how we can use a mixed-effects regression model to pursue strategies at this level of analysis. These will provide supplementary insights that enable us to audit our inferences about the average case. We will see that this approach will uncover linguistically meaningful assessments of variability, and also add data-based clues to the generality of the average patterns we infer.\nI will use the term consistency",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling tactics</span>"
    ]
  },
  {
    "objectID": "08_modeling_tactics.html#using-random-effects-to-assess-the-generality-of-a-pattern",
    "href": "08_modeling_tactics.html#using-random-effects-to-assess-the-generality-of-a-pattern",
    "title": "9  Modeling tactics",
    "section": "",
    "text": "Cedergren and Sankoff (1974, 334) “it is difficult to escape the conclusion that those aspects of performance that are found to be thoroughly systematic in an individual and throughout a community are relfections of linguistic competence.”\n\n\n9.1.1 Consistency across speakers\nSidman (1960, 46) refers to this as subject generality.\n\n\n9.1.2 Residual variation among speakers\nThe residual variation among speakers is interesting in two respects. For one, it tells us about the amount of remaining between-speaker variation, i.e. the variation that is left unexplained (in a statistical sense) once the speaker-level predictors (if any) are taken into account. Further, it provides a relevant yardstick for assessing the explanatory power of the speaker-level predictors. If the remaining variation overwhelms that accounted for by cluster-level predictors, this can be interpreted either as indicating the predictive incapacity of the model and/or the large amount of unsystematic idiosyncratic behavior of the individual speakers.\nThe left-hand dot diagram in Figure 9.1 shows the fitted values for the speakers in the sample on the proportions scale. The social predictors apparently account for a considerable variation in g-dropping. The right-hand dot diagram in Figure 9.1 compares this systematic part of the model to the residual variation among speakers on the model (i.e. logit) scale. The residual variation is noticeably greater, which indicates that considerabl parts of the variation among speaker is left unaccounted for.\n\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\nTo get a sense of what the residual variation among speakers amounts to on the proportion scale, Figure 9.2 illustrates the model-based residual variation among speakers for two conditions, one with a predicted probability near .50 (where between-speaker variation is greatest on the proportion scale), and one closer to the bounds of the scale. Icons are used to give a sense of the variability among 100 hypothetical speakers. This technique is borrowed from Kay et al. (2016). The probability distribution that is represented by the random intercept standard deviation is visualized by discrete icons. Each icon depicts a 1 percent probability, i.e. roughly 1 percent in the sampled population of speakers.\n\n\n\n\n\n\n\n\nFigure 9.2\n\n\n\n\n\n\n\n9.1.3 Residual variation among words\nWe can do the same for the the other clustering variable, Item. Figure 9.3 shows the fitted values for the lexical items in the estimation sample.\n\n\n\n\n\n\n\n\nFigure 9.3\n\n\n\n\n\nFigure 9.4 compares the fitted values against the residuals on the model scale.\n\n\n\n\n\n\n\n\nFigure 9.4\n\n\n\n\n\n\n\n9.1.4 Token-level predictors: Consistency across speakers\nSensitivity analysis for prior of random effects parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the token-level predictor Following Context is crossed with Speaker, the data have higher resolution than is offered by the average profiles that we have extracted above. Thus, we have information about this predictor at the level of the individual speaker. We can assess the stability of the averaged pattern across the speakers in our sample. This type of variability is linguistically meaningful since it reveals the consistency with which a constraint operates across speakers. It will also add statistical indications that may inform questions about the generality of the pattern throughout the sampled and target population.\nIn principle, we could use a model that directly estimates the pattern for each speaker. While this approach has intuitive appeal, it only works if we have, for each speaker, enough tokens for each level of the token-level factor. Even though we have an appreciable number of tokens, overall, for each speaker, the cross-classification of Following Context and Speaker in Figure 4.8 showed that many speakers produced few pre-pausal and pre-velar tokens. We therefore need a more general approach to the assessment of between-speaker consistency with regard to language-internal factors.\nIn multilevel models, the variability among speakers is absorbed into the variational structure of the model. This part of our statistical model includes the batches of random intercepts and random slopes. Random intercepts express the variation among speakers in the overall rate of g-dropping and random slopes express the variation among speakers in the patterns observed for factors that can vary within a speaker. These are typically language-internal factors such as token-level and word-level predictor. Whil the behavior of each speaker informs the estimation of the variational part of the model, this variation is documented in the form of summary measures that aim to condense this information into a few numbers. There are two types of quantities involved: standard deviations and correlations. Together, they form a highly abstract report of the patterns of variation.\nIt is rarely the case that these summary measures of variational structure are directly interpretable. Even in simple settings where we have only a single random intercept standard deviation, this will still be on the logit scale. We therefore need to post-process these measures into interpretable quantities. To this end, we will use the following strategy. Recall that the random effects parameters (standard deviations and correlations) condense the data-based clues about between-speaker variability into a handful of numbers. We will use these parameters to simulate a set of, say, 500 imaginary speakers, who mirror the structure of between-speaker variation preserved by our model. We will use these fabricated individuals to extract quantities at the idiographic level of description and behavior. An essential part of this strategy is to preserve the integrity of the (fabricated) speaker for visualization and computation of summary measures.\nLet us now consider the token-level factor Following Context, which has 4 levels: coronal, other, pause, and velar. In our regression model, this predictor is represented by three terms. We used sum contrasts to represent the predictor, and the model therefore produces the following summary:\n\\[\n\\left[\\begin{array}{lll}\n  \\sigma_{1} & \\rho_{12}  & \\rho_{13} \\\\\n             & \\sigma_{2} & \\rho_{23} \\\\\n             &            & \\sigma_{3}\n\\end{array}\\right]\n\\]\nOur first step, then, is to reconstruct the standard deviations and correlations for (and among) the 4 levels. Technically, what we do is generate 500 imaginary speakers and then compute standard deviations and correlations from this set of fabricated speakers:\n\\[\n\\left[\\begin{array}{llll}\n  \\sigma_{\\textrm{coronal}} & \\rho_{\\textrm{other}}^{\\textrm{coronal}}  & \\rho_{\\textrm{pause}}^{\\textrm{coronal}} & \\rho_{\\textrm{velar}}^{\\textrm{coronal}} \\\\\n             & \\sigma_{\\textrm{other}} & \\rho_{\\textrm{pause}}^{\\textrm{other}} & \\rho_{\\textrm{velar}}^{\\textrm{other}} \\\\\n             &            & \\sigma_{\\textrm{pause}} & \\rho_{\\textrm{velar}}^{\\textrm{pause}} \\\\\n             &            &                          & \\sigma_{\\textrm{velar}}\n\\end{array}\\right]\n\\]\nThese standard deviations and correlations are more informative. For instance, we now have a standard deviation for each following context. This measure of spread is on the logit scale, so we need to be careful when interpreting it. We will return to this point shortly. Nevertheless, we now have summary figures for all contexts:\n\\[\n\\left[\\begin{array}{rrrr}\n  0.27 &  -.05 & -.01 & -.38\\\\\n             &  0.47 & +.12 & -.72\\\\\n             &            & 0.37  & -.62\\\\\n             &            &      & 0.72\n\\end{array}\\right]\n\\]\nSuch summaries are useful, but the suggested patterns of variability can be made more transparent through visualization. Figure 9.5 shows a set of 500 synthetic speakers, which are simulated based on the posterior medians of the random effects parameters. The think line that runs through the center represents the average pattern over the speakers in our sample. The thin lines represent the simulated individuals. Each imagined speaker adds a profile to the plot. While these are imagined individuals, their joint distribution mimics that of the speakers in our sample.\n\n\n\n\n\n\n\n\nFigure 9.5: Visualization of the consistency of the difference in the share of [ɪŋ] between velar and coronal contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.6: Visualization of the consistency of the difference in the share of [ɪŋ] between velar and coronal contexts using an icon array. The 90% posterior interval is given in brackets.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.7: Mapping the SDs to the proportion scale.\n\n\n\n\n\n\n\n9.1.5 Word-level predictors: Consistency across speakers\nSince word-level predictors are crossed with Speaker, we can likewise examine their consistency across speakers. We will start with the continuous predictor Frequency. Figure 9.8 shows the amount of between-speaker variation in the association between Frequency and the share of ing. Apparently, the relation between Frequency and the response variable is very consistent across individuals, While the magnitude of the pattern (i.e. the slope of the regression line) varies among individuals, the model expects very few speakers to have small (or even a reverse) frequency effect.\n\n\n\n\n\n\n\n\nFigure 9.8: Visualization of the consistency of the association between Frequency and the share of [ɪŋ].\n\n\n\n\n\nThe icon array in Figure 9.9 underscores this remarkable level of stability. On average, 100 out of 100 speakers are expected to show the same directionality.\n\n\n\n\n\n\n\n\n\nFigure 9.9: Visualization of the between-speaker consistency of the association between Frequency and the share of [ɪŋ] using an icon array. The 90% posterior interval is given in brackets.\n\n\n\n\nFigure 9.10 shows between-speaker variation for the predictor Lexcial Category. The cline across word classes is very stabe across individuals.\n\n\n\n\n\n\n\n\nFigure 9.10: Visualization of the random effects distribution for Lexical Category: Assessment of the consistency of the pattern across speakers.\n\n\n\n\n\nThe icon array in Figure 9.11 compares the rate of g-dropping in nouns and verbs. This contrast is also remarkably stable across speakers.\n\n\n\n\n\n\n\n\n\nFigure 9.11: Visualization of the consistency of the difference in the share of [ɪŋ] between nouns and verbs using an icon array. The 90% posterior interval is given in brackets.\n\n\n\n\nFinally, Figure 9.12 shows the consistency of variation as a function of the preceding sound.\n\n\n\n\n\n\n\n\nFigure 9.12: Visualization of the random effects distribution for Preceding Sound: Assessment of the consistency of the pattern across speakers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.13: Visualization of the consistency of the difference in the share of [ɪŋ] between preceding velar and coronal consonants using an icon array. The 90% posterior interval is given in brackets.\n\n\n\n\n\n\n\n\nCedergren, Henrietta J., and David Sankoff. 1974. “Variable Rules: Performance as a Statistical Reflection of Competence.” Language 50 (2): 333–55. https://doi.org/10.2307/412441.\n\n\nKay, Matthew, Tara Kola, Jessica R. Hullman, and Sean A. Munson. 2016. “When (Ish) Is My Bus? User-Centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems.” In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, 5092–5103. New York: Association for Computing Machinery. https://doi.org/10.1145/2858036.2858558.\n\n\nSidman, Murray. 1960. Tactics of Scientific Research: Evaluating Experimental Data in Psychology. New York: Basic Books.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modeling tactics</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html",
    "href": "09_model_predictions.html",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "",
    "text": "10.1 Model-based quantities: regression coefficients, predictions, estimates\nWe begin by delineating different types of quantities that are encountered in regression modeling. We will use the term regression coefficient to collectively refer to all coefficients (or parameters) that are part of a regression structure. This includes intercepts and slopes (both fixed and “random”) as well as the parameters that describe the (joint) distribution of the “random effects” in the model, i.e. variance (or standard deviation) and covariance (or correlation) parameters. This means that all the quantities that are typically listed in a regression table will be referred to as regression coefficients.\nWhile regression coefficients have a clear meaning, the information they express may not be of direct interest to the researcher, which means that they may not provide an answer to the question that is actually being asked of the data. In GLMMs, a further limitation of regression coefficients is that fact that they are defined on the model scale (e.g. the logit scale), which may be difficult to interpret.\nLet us call into mind the meaning expressed by two types of regression coefficient, which often assume a central role in model interpretation:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html#model-based-quantities-regression-coefficients-predictions-estimates",
    "href": "09_model_predictions.html#model-based-quantities-regression-coefficients-predictions-estimates",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "",
    "text": "The (fixed) intercept of a model is the predicted value on the model scale, for a situation where all predictors are set to 0. The meaning of the intercept therefore depends on how the predictor variables are scaled.\nA (fixed) slope, on the other hand, is tied to a specific predictor; it expresses a difference between two conditions. The first condition is the one denoted by the intercept, which provides the baseline. The second predicted value is for a new condition, which differs from the baseline in that the predictor of interest has been increased by 1 unit. The meaning of a (fixed) slope therefore also depends on the scaling of all variables in the model, especially the associated predictor. Critically, the difference applies to one particular point in the predictor space.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html#predicted-quantites-predictions-and-comparisons",
    "href": "09_model_predictions.html#predicted-quantites-predictions-and-comparisons",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "10.2 Predicted quantites: Predictions and comparisons",
    "text": "10.2 Predicted quantites: Predictions and comparisons\nThere are two fundamentally different types of predicted quantity. The first is a plain prediction, which is a specific value on the outcome scale. In the case of logistic regression, this could be an individual proportion (or percentage) for the outcome variable. Predictions can be made for a specific point in the predictor space, or they may be averaged over some variable. We will return to this point further below.\nThe second type of predicted quantity expresses a comparison between two predicted values. Comparisons help us understand the way in which a specific predictor relates to the outcome of interest. Comparisons may be expressed in two basic ways: in absolute terms (differences) or in relative terms (ratios). Like predictions, comparisons may be evaluated at a specific point in the predictor space, or they may be averaged over other variables in the model.\n\ndifference between the averages vs. average over the differences",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html#focal-vs.-peripheral-variables",
    "href": "09_model_predictions.html#focal-vs.-peripheral-variables",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "10.3 Focal vs. peripheral variables",
    "text": "10.3 Focal vs. peripheral variables\nIf a model includes multiple predictor variables, model interpretation usually proceeds step by step, considering each predictor in turn. This is because in a regression model with three or more predictor variables, it is usually difficult to appreciate patterns by jointly considering all predictors. Instead, model interpretation requires a compartmentalized approach. A distinction can then be made between focal and peripheral variables. This division does not imply differences in the linguistic relevance of variables, but is a necessary feature of a compartmentalized strategy. The specific predictor(s) whose association with the outcome is to be examined is/are referred to as the focal variable(s). The remaining variables, which remain backgrounded at this step of model interpretation, are the peripheral variables.\nIf a predictor does not enter into a statistical interaction with another variable, it will usually be considered in isolation, meaning there will only be one focal variable. If there is a detectable interaction between two (or more) variables, these will usually (though not necessarily) be considered together.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html#adjusting-for-a-peripheral-variable",
    "href": "09_model_predictions.html#adjusting-for-a-peripheral-variable",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "10.4 Adjusting for a peripheral variable",
    "text": "10.4 Adjusting for a peripheral variable\nSince peripheral variables are the ones we wish to background at a specific step in model interpretation, we must adjust for these features. In some sense, we must decide on a way of ignoring them when making the focal comparisons. Peripheral variables cannot be simply ignored, however. Instead, we must decide on how to handle them when forming predictions for the focal variables. There are two general strategies for this, to which we now turn.\n\n10.4.1 Conditioning vs. marginalizing\nOne option would be to hold a peripheral variable at a specific value. This means that we decide on a value that may be considered informative (or representative) and then set the predictor to this value. We will follow Lane & Nelder (198X) and refer to this as the conditional approach. For a quantitative predictor, this could be the mean, and for a binary variable (coded as 0 and 1), this could be the middle value (i.e. 0.5). The distinctive feature of the conditional approach is that we “ignore” a predictor by holding it at a specific value.\nThe second way in which we may background a predictor is by averaging over the predictions we obtain across all of its levels. Instead of fixing the binary variable Female (coded as 0 vs. 1) to 0.5, for instance, we could average over the predictions we obtain for male and for female respondents. We will follow Lane & Nelder (198X) and refer to this as the marginal approach.\nThus, if the binary indicator variable Female assumes the status of a peripheral variable, a prediction or comparison of interest can either be conditional on Female - setting the variable Female to the intermediate value - or marginal to Female - averaging over the prediction obtained for the two conditions (i.e. male and female respondents).\n\n\n10.4.2 Marginal approach: Simple vs. weighted averages\nWhen averaging over the levels of a peripheral variable, different weighting schemes may be used. The most straightforward one is the simple, unweighted average, where all levels have the same weight. In some settings, however, we may not want to assign equal weight to all levels (or values) of a peripheral variable. We may decide to weight them differently, based on their population frequency. This applies in particular to internal variables.\n\n\n10.4.3 Weighting schemes for quantitative variables\nDifferent weighting schemes may be applied when marginalizing over quantitative variables. There are two general approaches. We may decide to approximate the distribution of the variable in the sample. In this case, we may used quantiles of the observed distribution. Alternatively, we may prefer to assign equal weights across the range of the variable. In this case, we would select equal increments on the scale of the variable. This approach may make more sense for (quasi-)temporal features.\n\n\n10.4.4 Predictor space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.4.5 Handling peripheral variables\nWhen forming predictive margins, we usually proceed in a step-by-step fashion. A specific predictive model query focuses on one (or several) targeted variable(s). The remaining variables are then backgrounded and assume the status of peripheral variables. When forming predictive margins for targeted variables, we must decide how to handle the peripheral ones. Backgrounding techniques involve some form of adjustment. There are two general strategies, conditioning and marginalizing.\n\n10.4.5.1 Conditioning vs. marginalizing\nConditioning means that we set a peripheral variable to a (user-specified) value. For categorical features, this is often the mode, and for quantitative variables, the mean. If we set the peripheral variable B to the level B1 (or, if it is a quantitative variable, the value 40.1), the predictive margins are conditional on B = B1 (or B = 40.1).\nMarginalizing instead involves some form of averaging over the levels (or values) of the peripheral features. In contrast to conditioning, then, marginalizing aims for a broader representation of the predictor space. If we average over the levels of the peripheral variable B, the predictive margins are marginal to B. When marginalizing over a peripheral variable, we must decide on how to weight the levels (or values). We can form simple (i.e. equally-weighted) averages, or weighted averages. Weights may be chosen to reflect the distribution of a peripheral variable in the data, or we may use custom weights, based on some external source of information.\n\n\n10.4.5.2 Backgrounding a single peripheral variable\nLet us compare these backgrounding strategies in a simple setting where the peripheral variable B is categorical, with three levels. The bar chart at the top of ?fig-illustration-backgrounding-categorical shows the distribution of variable B in the data. Level B1 is the most frequent one, with 60% of the tokens. Below the bar chart, different backgrounding strategies and weighting schemes are shown. The first approach, conditioning on B = B1, holds this variable at the level B1 and ignores the other categories. In ?fig-illustration-backgrounding-categorical, the modal category, i.e. the one with the highest frequency, is chosen; of course, other choices are possible. In some sense, conditioning on B = B1 is a special case of weighted mean - one which assigns the weight 1 to the selected levels, and 0 to all others. This is illustrated by the singleton black square, which amasses the full weight.\n\n\n\n\n\n\n\n\n\npng \n  2 \n\n\nThree ways of marginalizing over C are shown at the bottom of ?fig-illustration-backgrounding-categorical. These differ in the size of the weights that are assigned to the three levels of the variable. As illustrated by the identically sized black squares, equal weights give the same importance to all categories. The second scheme, proportionate weights, instead weights the categories in proportion to their frequency in the data. The final option is to select custom weights, which may be based on external (e.g. census) data.\nWhen the peripheral variable is continuous, conditioning on D works in a similar way: The variable is held at a representative value such as its mean. As ?fig-illustration-backgrounding-quantitative illustrates, this means that the full weight is given to this value. Marginalizing, on the other hand, works differently for continuous variables. It requires selecting a number of points on the continuous scale to represent the variable’s distribution. In ?fig-illustration-backgrounding-quantitative, we have settled for 8 points to obtain a sufficiently detailed coverage of variable D. The first three weighting schemes rely on equally-spaced points, which here range from the 5th to the 95th percentile. A balanced representation of age groups is achieved with equal weights. To match the weighting scheme with the representation of age groups in the data, proportional weights may be assigned to the points. Finally, custom weights may be used. The final scheme uses quantiles (instead of equally-spaced steps) to determine the points informing the averaging procedure. The motivation is similar to that underlying proportional weighting, the aim being to attune predictive margins to the observed distribution of the data.\n\n\n[1] 13\n\n\n\n\n\n\n\n\n\n\n\npng \n  2 \n\n\n\n\n10.4.5.3 Backgrounding multiple peripheral variables\n\n\n\n\n\n\n\n\n\npng \n  2 \n\n\n\n\n[1] 13\n\n\n[1] 13\n\n\n[1] 38.9\n\n\n[1] 52.5\n\n\n[1] 43.9\n\n\n\n\n\n\n\n\n\n\n\npng \n  2",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "09_model_predictions.html#dealing-with-structural-variables",
    "href": "09_model_predictions.html#dealing-with-structural-variables",
    "title": "10  Model summary: Adjusted predictions and comparisons",
    "section": "10.5 Dealing with structural variables",
    "text": "10.5 Dealing with structural variables",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Model summary: Adjusted predictions and comparisons</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allison, Paul D. 2009. Fixed Effects Regression Models.\nThousand Oaks, CA: Sage.\n\n\nAllport, Gordon W. 1937. Personality: A Psychological\nInterpretation. New York: Henry Holt.\n\n\n———. 1962. “The General and the Unique in Psychological\nScience.” Journal of Personality 30 (3): 405–22. https://doi.org/10.1111/j.1467-6494.1962.tb02313.x.\n\n\nAnderson, Norman H. 2001. Empirical Direction in Design and\nAnalysis. Mahwah, NJ: Lawrence Erlbaum.\n\n\nAnderson, Virgil E., and Robert A. McLean. 1974. Design of\nExperiments: A Realistic Approach. New York: Marcel Dekker.\n\n\nArppe, Antti, Gaëtanelle Gilquin, Dylan Glynn, Martin Hilpert, and Arne\nZeschel. 2010. “Cognitive Corpus\nLinguistics: Five Points of Debate on Current\nTheory and Methodology.” Corpora 5 (1): 1–27. https://doi.org/10.3366/E1749503210000341.\n\n\nBaayen, R. Harald. 2008. Analyzing Linguistic Data: A\nPractical Introduction to Statistics Using R.\nCambridge: Cambridge University Press.\n\n\nBaayen, R. Harald, and Antti Arppe. 2011. “Statistical\nClassification and Principles of Human Learning.” In\nProceedings of Quantitative Investigations in Theoretical\nLinguistics 4 (QITL-4), 8–11. Berlin: Humboldt-Universität zu\nBerlin. https://doi.org/10.1145/2858036.2858558.\n\n\nBaayen, R. Harald, Douglas J. Davidson, and Douglas M. Bates. 2008.\n“Mixed-Effects Modeling with Crossed Random Effects for Subjects\nand Items.” Journal of Memory and Language 59 (4):\n390–412. https://doi.org/10.1016/j.jml.2007.12.005.\n\n\nBancroft, Theodore A. 1964. “Analysis and Inference for\nIncompletely Specified Models Involving the Use of Preliminary Test(s)\nof Significance.” Biometrics 20 (3): 427–42. https://doi.org/10.2307/2528486.\n\n\nBarr, Dale J. 2018. “Generalizing over Encounters:\nStatistical and Theoretical Considerations.” In\nThe Oxford Handbook of Psycholinguistics, edited by Shirley-Ann\nRueschemeyer and M. Gareth Gaskell, 917–29. Oxford: Oxford University\nPress. https://doi.org/10.1093/oxfordhb/9780198786825.013.39.\n\n\nBarr, Dale J., Roger Levy, Christoph Scheepers, and Harry J. Tily. 2013.\n“Random Effects Structure for Confirmatory Hypothesis Testing:\nKeep It Maximal.” Journal of Memory and\nLanguage 68 (3): 255–78. https://doi.org/10.1016/j.jml.2012.11.001.\n\n\nBegg, Melissa D., and Michael K. Parides. 2003. “Separation of\nIndividual-Level and Cluster-Level Covariate Effects in Regression\nAnalysis of Correlated Data.” Statistics in Medicine,\nno. 22: 2591–2602. https://doi.org/10.1002/sim.1524.\n\n\nBell, Andrew, Malcolm Fairbrother, and Kelvyn Jones. 2019. “Fixed\nand Random Effects Models: Making an Informed Choice.”\nQuality & Quantity 53: 1051–74. https://doi.org/10.1007/s11135-018-0802-x.\n\n\nBiber, Douglas, Stig Johansson, Geoffrey Leech, Susan Conrad, and Edward\nFinegan. 1999. Longman Grammar of Spoken and Written\nEnglish. Harlow: Pearson Education.\n\n\nBickerton, Derek. 1971. “Inherent Variability and Variable\nRules.” Foundations of Language 7 (4): 457–92. http://www.jstor.org/stable/25000558.\n\n\nBingenheimer, Jeffrey B., and Stephen W. Raudenbush. 2004.\n“Statistical and Substantive Inferences in Public Health:\nIssues in the Application of Multilevel Models.”\nAnnual Review of Public Health, no. 25: 53–77. https://doi.org/10.1146/annurev.publhealth.25.050503.153925.\n\n\nBottai, Matteo. 2014. “Lessons in Biostatistics:\nInferences and Conjectures about Average and Conditional\nTreatment Effects in Randomized Trials and Observational\nStudies.” Journal of Internal Medicine 276 (3): 229–37.\nhttps://doi.org/10.1111/joim.12283.\n\n\nBrown, Constance, and Frederick Mosteller. 1991. “Variance\nComponents.” In Fundamentals of Exploratory Analysis of\nVariance, edited by David C. Hoaglin, Frederick Mosteller, and John\nW. Tukey, 193–244. New York: Wiley.\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2005. Microeconometrics:\nMethods and Applications. Cambridge: Cambridge\nUniversity Press.\n\n\nCarver, Ronald P. 1972. “Evidence for the Invalidity of the\nMiller-Coleman Readability Scale.”\nJournal of Reading Behavior 4 (3): 42–47. https://doi.org/10.1080/10862967109546999.\n\n\n———. 1978. “Sense and Nonsense about Generalizing to a Language\nPopulation.” Journal of Reading Behavior 10 (1): 25–33.\nhttps://doi.org/10.1080/10862967809547252.\n\n\nCastellano, Katherine E., Sophia Rabe-Hesketh, and Anders Skrondal.\n2014. “Composition, Context, and Endogeneity in School and Teacher\nComparisons.” Journal of Educational and Behavioral\nStatistics 39 (5): 333–67. https://doi.org/10.3102/1076998614547576.\n\n\nCedergren, Henrietta J., and David Sankoff. 1974. “Variable Rules:\nPerformance as a Statistical Reflection of\nCompetence.” Language 50 (2): 333–55. https://doi.org/10.2307/412441.\n\n\nClark, Herbert H. 1973. “The Language-as-Fixed-Effect Fallacy:\nA Critique of Language Statistics in Psychological\nResearch.” Journal of Verbal Learning and Verbal\nBehavior 12 (4): 335–59. https://doi.org/10.1016/S0022-5371(73)80014-3.\n\n\nCobb, George W. 1997. Introduction to Design and Analysis of\nExperiments. New York: Wiley.\n\n\n———. 1998. Introduction to Design and Analysis of Experiments.\nNew York: Springer.\n\n\nCochran, William G. 1951. “Testing a Linear Relation Among\nVariances.” Biometrics 7 (1): 17–32. https://doi.org/10.2307/3001601.\n\n\n———. 1977. Sampling Techniques. New York: Wiley.\n\n\n———. 1983. Planning and Analysis of Observational Studies. New\nYork: Wiley.\n\n\nColeman, Edmund B. 1964. “Generalizing to a Language\nPopulation.” Psychological Reports 14 (1): 219–26. https://doi.org/10.2466/pr0.1964.14.1.219.\n\n\n———. 1979. “Generalization Effects Vs Random Effects:\nIs σTL2\na Source of Type 1 or Type 2 Error?”\nJournal of Verbal Learning and Verbal Behavior 18 (2): 243–56.\nhttps://doi.org/10.1016/S0022-5371(79)90145-2.\n\n\nCornfield, Jerome, and John W. Tukey. 1977. “Average Values of\nMean Squares in Factorials.” Journal of the Royal Statistical\nSociety A 50 (1): 48–76. https://doi.org/10.1214/aoms/1177728067.\n\n\nCox, David R. 1958. Planning of Experiments. New York: Wiley.\n\n\nDeming, W. Edwards. 1950. Some Theory of Sampling. New York:\nWiley.\n\n\n———. 1975. “On Probability as a Basis for Action.” The\nAmerican Statistician 29 (4): 146–52. https://doi.org/10.2307/2683482.\n\n\nDiez Roux, Ana M. 2002. “A Glossary for Multilevel\nAnalysis.” Journal of Epidemiology and Community Health\n56: 558–94. https://doi.org/10.1136/jech.56.8.588.\n\n\nDivjak, Dagmar, and Antti Arppe. 2013. “Extracting Prototypes from\nExemplars: What Can Corpus Data Tell Us about Concept\nRepresentation?” Cognitive Linguistics 24 (2): 221–74.\nhttps://doi.org/10.1515/cog-2013-0008.\n\n\nDivjak, Dagmar, Ewa Dąbrowska, and Antti Arppe. 2016. “Machine\nMeets Man: Evaluating the Psychological Reality of\nCorpus-Based Probabilistic Models.” Cognitive\nLinguistics 27 (1): 1–33. https://doi.org/10.1515/cog-2015-0101.\n\n\nDraper, David, James S. Hodges, Colin L. Mallows, and Daryl Pregibon.\n1993. “Exchangeability and Data Analysis.” Journal of\nthe Royal Statistical Society A 156 (1): 9–37. https://doi.org/10.2307/2982858.\n\n\nDuncan, Craig, Kelvyn Jones, and Graham Moon. 1998. “Context,\nComposition and Heterogeneity: Using Multilevel Models in\nHealth Research.” Social Science & Medicine 1 (46):\n97–117. https://doi.org/10.1016/s0277-9536(97)00148-2.\n\n\nEbbes, Peter, Ulf Böckenholt, and Michel Wedel. 2004. “Regressor\nand Random-Effects Dependencies in Multilevel Models.”\nStatistica Neerlandica 58 (2): 161–78. https://doi.org/10.1046/j.0039-0402.2003.00254.x.\n\n\nEisenhart, Chruchill. 1947. “The Asumptions Underlying the\nAnalysis of Variance.” Biometrics 3 (1): 1–21. https://doi.org/10.2307/3001534.\n\n\nFisher, Ronald A. 1956. Statistical Methods and Scientific\nInference. Edinburgh: Oliver & Boyd.\n\n\nForrest, Jon. 2015. “Community Rules and Speaker Behavior:\nIndividual Adherence to Group Constraints on (ING).” Language\nVariation and Change 27 (3): 377–406. https://doi.org/10.1017/s0954394515000137.\n\n\n———. 2017. “The Dynamic Interaction Between Lexical and Contextual\nFrequency: A Case Study of (ING).”\nLanguage Variation and Change 29 (2): 129–56. https://doi.org/10.1017/S0954394517000072.\n\n\nForster, Kenneth I. 2008. “What Is F2 Good\nFor?” Journal of Memory and Language 59 (4): 389. https://doi.org/10.1016/j.jml.2008.08.002.\n\n\nForster, Kenneth I., and Rod G. Dickinson. 1976. “More on the\nLanguage-as-Fixed-Effect Fallacy:\nMonte Carlo Estimates of Error Rates for F1,\nF2, F’, and Min F’.”\nJournal of Verbal Learning and Verbal Behavior 15 (2): 135–42.\nhttps://doi.org/10.1016/0022-5371(76)90014-1.\n\n\nGelman, Andrew. 2005. “Analysis of Variance: Why It Is More\nImportant Than Ever.” The Annals of Statistics 33 (1):\n1–53. https://doi.org/10.1214/009053604000001048.\n\n\nGigerenzer, Gerd, and Ulrich Hoffrage. 1995. “How to Improve\nBayesian Reasoning Without Instruction: Frequency\nFormats.” Psychological Review 102 (4): 684–704. https://doi.org/10.1037/0033-295X.102.4.684.\n\n\nGitlow, Howard, Shelly Gitlow, Alan Oppenheim, and Rosa Oppenheim. 1989.\nTools and Methods for the Improvement of Quality. Boston:\nIrwin.\n\n\nGrissom, Robert J., and John J. Kim. 2012. Effect Sizes for\nResearch: Univariate and Multivariate Applications.\nNew York: Routledge.\n\n\nGuy, Gregory R. 1980. “Variation in the Group and the Individual:\nThe Case of Final Stop Deletion.” In Locating\nLanguage in Time and Space, edited by William Labov, 1–36. New\nYork: Academic Press.\n\n\n———. 1988. “Advanced VARBRUL Analysis.” In\nLinguistic Change and Contact: NWAV-XVI, edited by Kathleen\nFerrara, Becky Brown, Keith Walters, and John Baugh, 124–36. Austion,\nTX: University of Texas, Department of Linguistics.\n\n\n———. 1991. “Explanation in Variable Phonology: An\nExponential Model of Morphological Constraints.” Language\nVariation and Change 3 (1): 1–22. https://doi.org/10.1017/S0954394500000429.\n\n\n———. 2018. “LVC Guidelines for Reporting Quantitative\nResults.” http://gregoryrguy.com/wp-content/uploads/Guy-2018-Guidelines-for-reporting-quantitative-results-LVC-Nov-18-2018.pdf.\n\n\nGuy, Gregory R., and Sally Boyd. 1990. “The Development of a\nMorphological Class.” Language Variation and Change 2\n(1): 1–18. https://doi.org/10.1017/S0954394500000235.\n\n\nHahn, Gerald J., and William Q. Meeker. 1993. “Assumptions for\nStatistical Inference.” The American Statistician 47\n(1): 1–11. https://doi.org/10.2307/2684774.\n\n\nHausman, Jerry A. 1978. “Specification Tests in\nEconometrics.” Econometrica 46: 1251–71. https://doi.org/10.2307/1913827.\n\n\nHinkelmann, Klaus, and Oscar Kempthorne. 2008. Design and Analysis\nof Experiments, Volume 1: Introduction to Experimental Design.\nHoboken, NJ: Wiley.\n\n\nHolt, Robert A. 1962. “Individuality and Generalization in the\nPsychology of Personality.” Journal of Personality 30\n(3): 377–404. https://doi.org/10.1111/j.1467-6494.1962.tb02312.x.\n\n\nJohnstone, David J. 1989. “On the Necessity for Random\nSampling.” The British Journal for the Philosophy of\nScience 40 (4): 443–57. https://www.jstor.org/stable/687735.\n\n\nKay, Matthew, Tara Kola, Jessica R. Hullman, and Sean A. Munson. 2016.\n“When (Ish) Is My Bus? User-Centered Visualizations of Uncertainty\nin Everyday, Mobile Predictive Systems.” In Proceedings of\nthe 2016 CHI Conference on Human Factors in Computing Systems,\n5092–5103. New York: Association for Computing Machinery. https://doi.org/10.1145/2858036.2858558.\n\n\nKeithSmith, J. E. 1976. “Discussion of Wike and Church’s\nComments.” Journal of Verbal Learning and Verbal\nBehavior 15 (3): 262–63. https://doi.org/10.1016/0022-5371(76)90024-4.\n\n\nKeppel, Geoffrey, and Thomas D. Wickens. 2004. Design and Analysis:\nA Researcher’s Handbook. London: Pearson.\n\n\nKish, Leslie. 1987. Statistical Design for Research. Hoboken,\nNJ: Wiley.\n\n\nKlavan, Jane, and Dagmar Divjak. 2016. “The Cognitive Plausibility\nof Statistical Classification Models: Comparing Textual and\nBehavioral Evidence.” Folia Linguistica 50 (2): 355–48.\nhttps://doi.org/10.1515/flin-2016-0014.\n\n\nLabov, William. 1969. “Contraction, Deletion, and Inherent\nVariability of the English Copula.” Language 45 (4):\n715–62. https://doi.org/10.2307/412333.\n\n\nLabov, William, Paul Cohen, Clarence Robins, and John Lewis. 1968. A\nStudy of the Non-Standard English of Negro and Puerto Rican Speakers in\nNew York City. New York: Columbia University.\n\n\nLawson, John. 2015. Design and Analysis of Experiments with r.\nBoca Raton, FL: CRC Press.\n\n\nLindley, Deniis V., and Melvin R. Novick. 1981. “The Role of\nExchangeability in Inference.” The Annals of Statistics\n9 (1): 45–58. https://doi.org/10.1214/aos/1176345331.\n\n\nLocker, Lawrence, Lesa Hoffman, and James A. Boviard. 2007. “On\nthe Use of Multilevel Modeling as an Alternative to Items Analysis in\nPsycholinguistic Research.” Behavior Research Methods\n39: 723–30. https://doi.org/10.3758/BF03192962.\n\n\nLohr, Sharon L. 2022. Sampling: Design and\nAnalysis. Boca Raton, FL: CRC Press.\n\n\nLorch, Robert F., and Jerome L. Myers. 1990. “Regression Analyses\nof Repeated Measures Data in Cognitive Research.” Journal of\nExperimental Psychology: Learning, Memory, and Cognition 16 (1):\n149–57. https://doi.org/10.1037/0278-7393.16.1.149.\n\n\nLorenzen, Thomas J., and Virgil L. Anderson. 1993. Design of\nExperiments: A No-Name Approach. New York: Dekker.\n\n\nMacKay, R. Jock, and R. Wayne Oldford. 2000. “Scientific Method,\nStatistical Method and the Speed of Light.” Statistical\nScience 15 (3): 254–78. https://doi.org/10.1214/ss/1009212817.\n\n\nMatuschek, Hannes, Reinhold Kliegl, Shravan Vasishth, R. Harald Baayen,\nand Douglas Bates. 2017. “Balancing Type i Error and\nPower in Linear Mixed Models.” Journal of Memory and\nLanguage 94: 305–15. https://doi.org/10.1016/j.jml.2017.01.001.\n\n\nMaxwell, Scott E., Harold D. Delaney, and Ken Kelley. 2018.\nDesigning Experiments and Analyzing Data: A Model Comparison\nPerspective. New York: Routledge.\n\n\nMcLean, Robert A., William L. Sanders, and Walter W. Stroup. 1991.\n“A Unified Approach to Mixed Linear Models.” The\nAmerican Statistician 45 (1): 54–64. https://doi.org/10.2307/2685241.\n\n\nMiller, Gerald R., and Edmund B. Coleman. 1972. “The Measurement\nof Reading Speed and the Obligation to Generalize to a Population of\nReading Materials.” Journal of Reading Behavior 4 (3):\n48–56. https://doi.org/10.1080/10862967109547000.\n\n\nMook, Douglas G. 1982. Psychological Research: Strategy\nand Tactics. New York: Harper; Row.\n\n\n———. 1983. “In Defense of External Invalidity.”\nAmerican Psychologist 38 (4): 379–87. https://doi.org/10.1037/0003-066X.38.4.379.\n\n\nMundlak, Yair. 1978. “On the Pooling of Time Series and Cross\nSection Data.” Econometrica 46 (1): 69–85. https://doi.org/1913646.\n\n\nNelder, John A. 1956. “A Reformulation of Linear Models.”\nThe Annals of Mathematical Statistics 27 (4): 907–49. https://doi.org/10.2307/2344517.\n\n\nNeuhaus, J. M., and J. D. Kalbfleisch. 1998. “Between- and\nWithin-Cluster Covariate Effects in the Analysis of Clustered\nData.” Biometrics, no. 54: 638–45.\n\n\nPalta, Mari, and Chris Seplaki. 2003. “Causes, Problems and\nBenefits of Different Between and Within Effects in the Analysis of\nClustered Data.” Health Services and Outcomes Research\nMethodology, no. 3: 177–93. https://doi.org/10.1023/A:1025893627073.\n\n\nPaolillo, John C. 2013. “Individual Effects in Variation Analysis:\nModel, Software, and Research Design.” Language\nVariation and Change 25 (1): 89–118. https://doi.org/10.1017/S0954394512000270.\n\n\nQuené, Hugo, and Huub van den Bergh. 2004. “On Multi-Level\nModeling of Data from Repeated-Measures Designs: A\nTutorial.” Speech Communication 43 (1-2): 103–24. https://doi.org/10.1016/j.specom.2004.02.004.\n\n\nQuirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik.\n1985. A Comprehensive Grammar of the English\nLanguage. London: Longman.\n\n\nRaaijmakers, Jeroen G. W. 2003. “A Further Look at the\n‘Language-as-Fixed-Effect Fallacy’.” Canadian\nJournal of Experimental Psychology / Revue Canadienne de Psychologie\nExpérimentale 57 (3): 141–51. https://doi.org/10.1037/h0087421.\n\n\nRaaijmakers, Jeroen G. W., Joseph M. C. Schrijnemakers, and Frans\nGremmen. 1999. “How to Deal with \"The\nLanguage-as-Fixed-Effect Fallacy\": Common Misconceptions\nand Alternative Solutions.” Journal of Memory and\nLanguage 41 (3): 416–26. https://doi.org/10.1006/jmla.1999.2650.\n\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2021. Multilevel and\nLongitudinal Modeling Using Stata. College Station,\nTX: Stata Press.\n\n\nRaudenbush, Stephen W., and Anthony S. Bryk. 2002. Hierarchical\nLinear Models: Applications and Data Analysis Methods.\nThousand Oaks, CA: Sage.\n\n\nRunyan, William M. 1982. Life Histories and Psychobiography:\nExplorations in Theory and Method. New York: Oxford\nUniversity Press.\n\n\nSankoff, David. 1978. “Probability and Linguistic\nVariation.” Synthese 37 (2): 217–38. https://www.jstor.org/stable/20115257.\n\n\nSankoff, David, and William Labov. 1979. “On the Uses of Variable\nRules.” Language in Society 8 (2): 189–222. https://doi.org/10.1017/S0047404500007430.\n\n\nSankoff, Gillian. 1974. “A Quantitative Paradigm for the Study of\nCommunicative Competence.” In Explorations in the Ethnography\nof Speaking, edited by Richard Baumann and Joel Sherzer, 18–49.\nCambridge: Cambridge University Press.\n\n\nScatterthwaite, F. E. 1946. “An Approximate Distribution of\nEstimates of Variance Components.” Biometrics Bulletin 2\n(6): 110–14. https://doi.org/10.2307/3002019.\n\n\nSchnuck, Reinhard, and Francisco Perales. 2017. “Within- and\nBetween-Cluster Effects in Generalized Linear Mixed Models:\nA Discussion of Approaches and the Xthybrid\nCommand.” The Stata Journal 17 (1): 89–115. https://doi.org/10.1177/1536867X1701700106.\n\n\nSearle, Shayle R., George Casella, and Charles E. McCulloch. 1992.\nVariance Components. Hoboken, NJ: Wiley.\n\n\nSidman, Murray. 1960. Tactics of Scientific Research:\nEvaluating Experimental Data in Psychology. New York:\nBasic Books.\n\n\nSjölander, Arvid, Paul Lichtenstein, Henrik Larsson, and Yudi Pawitan.\n2013. “Between–Within Models for Survival Analysis.”\nStatistics in Medicine 18 (32): 3067–76. https://doi.org/10.1002/sim.5767.\n\n\nSpiegelhalter, David. 2019. The Art of Statistics:\nLearning from Data. London: Penguin.\n\n\nStroup, Walter W. 2013. Generalized Linear Mixed Models:\nModern Concepts, Methods and Applications. Boca Raton:\nCRC Press.\n\n\nThomae, Hans. 1999. “The Nomothetic-Idiographic Issue:\nSome Roots and Recent Trends.” International\nJournal of Group Tensions 20 (1/2): 187–215. https://doi.org/10.1023/a:1021891506378.\n\n\nUnderwood, A. J. 1997. Experiments in Ecology. Cambridge:\nCambridge University Press.\n\n\nWallis, W. Allen, and Harry V. Roberts. 1956. Statistics:\nA New Approach. Glencoe, IL: The Free Press.\n\n\nWelham, S. J., S. J. Gezan, S. J. Clark, and A Mead. 2014.\nStatistical Methods in Biology: Design and Analysis of\nExperiments and Regression. Boca Raton: CRC Press.\n\n\nWells, John C. 2008. Longman Pronunciation Dictionary. Harlow:\nPearson Longman.\n\n\nWickens, Thomas D., and Geoffrey Keppel. 1983. “On the Choice of\nDesign and of Test Statistic in the Analysis of Experiments with Sampled\nMaterials.” Journal of Verbal Learning and Verbal\nBehavior 22 (3): 296–309. https://doi.org/10.1016/S0022-5371(83)90208-6.\n\n\nWike, Edward L., and James D. Church. 1976. “Comments on\nClark’s \"The Language-as-Fixed-Effect\nFallacy\".” Journal of Verbal Learning and Verbal\nBehavior 15 (3): 249–55. https://doi.org/10.1016/0022-5371(76)90023-2.\n\n\nWilk, MArtin B., and Oscar Kempthorne. 1955. “Fixed, Mixed, and\nRandom Models.” Journal of the American Statistical\nAssociation 50 (272): 1144–67. https://doi.org/10.2307/2281212.\n\n\nWindelband, Wilhelm. 1894. “Geschichte Und\nNaturwissenschaft.” In Rektoratsreden Der\nUniversität Strassburg, 193–208.\nStrassburg: Heitz und Mündel. https://doi.org/10.11588/diglit.20767.\n\n\nWooldridge, Jeffrey M. 2010. Econometric Analysis of Cross Section\nand Panel Data. Cambridge, MA: MIT Press.",
    "crumbs": [
      "References"
    ]
  }
]