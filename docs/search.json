[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Sönning",
    "section": "",
    "text": "Post-doc in English linguistics at the University of Bamberg\n\nUniversity of Bamberg\nDepartment of English Linguistics\n\n\n\nContact\n\nAddress: An der Universität 9, D-96047 Bamberg\nOffice: U9/00.10\nPhone: +49 (0)951/863-2267\nEmail: lukas[dot]soenning[at]uni-bamberg[dot]de\n ORCID 0000-0002-2705-395X\n OSF\n\n\n\nResearch interests\n\nStatistical analysis of language data\nCorpus linguistics\nLanguage variation and change\nData visualization\nGerman Learner English\nL2 phonology"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Lukas Sönning\nI am a post-doctoral researcher at the Chair of English Linguistics, University of Bamberg (Germany). Following my PhD project, which looked at phonological features in German Learner English, my interest shifted to statistical aspects of corpus-linguistic methodology. I have worked on topics such as keyness analysis, dispersion, and down-sampling, and my habilitation (post-doc) project concentrates on the linguistically grounded use of mixed-effects models in variationist corpus research. I have made an active effort to promote open-science practices and have a passion for data visualization. Currently, I am also involved in a DFG-funded project on the analysis of high-dimensional survey data drawn from the BSLVC (Bamberg Survey of Language Variation and Change).\n\nShort academic CV\n\n2012–present Research and teaching assistant at the University of Bamberg\n2020 awarded PhD\n2006–2012 Studies in English, Geography and Pedagogy at the University of Bamberg\n\n\n\nAwards\n\n2014 Best poster award, Olinco conference, Olomouc (“Vowel reduction in German Learner English: Developmental patterns”)\n2018 Best paper by an early career researcher, ICAME39, Tampere (John Sinclair bursary) (“Visual inference for corpus linguistics”)\n\n\n\nTeaching\nUniversity courses\n\nForming (new) words: The morphological architecture of English\nApplied data analysis for linguists\nInvestigating Lerner English\nSecond Language speech: Theory and practice\nMeasuring (your) foreign accent: The acoustic analysis of non-native speech\nEnglish phonetics & phonology\nEnglish grammar analysis\nTranslation English-German (intermediate and advanced level)\nRevision course for state exam candidates: Synchronic linguistics\nRevision course for state exam candidates: Translation English-German\n\nWorkshops\n\n2024 (FJUEL conference, Bamberg) Dynamic documents in R: Introduction to Quarto\n\nslides session 1 | slides session 2 | slides session 3\npractice 1 | practice 2 | practice 3\n\n2024 (University of Bamberg) RStudio crash course for PhD students | slides session 1 | slides session 2\n2023 (META-LING conference, Bamberg) Data publication using TROLLing  OSF\n2023 (University of Würzburg) Basics of data analysis using R and RStudio  OSF | slides part 1 | slides part 3\n2022 (University of Würzburg) Basic statistical methods for TEFL research  OSF\n2019 (FJUEL conference, Bayreuth): Using “statistics” to learn about language: What matters (and what doesn’t)  OSF\n2019 (BICLCE conference, Bamberg): The replication crisis in science: Challenges and chances for linguistics  OSF\n2018 (Uppsala University, Sweden) Statistical inference using estimation: Methods for corpus linguistics slides\n2014 (EmMeth conference, Bamberg): Data visualization with R\n2014 (FJUEL conference, Bamberg): Workshop on statistical methods\n\n\n\nTalks\n\n2024 The morpho-syntax of Scottish Standard English: Questionnaire-based insights. BICLCE 10, Alicante, Spain. (with Ole Schützler and Manfred Krug)\n2024 Down-sampling strategies in corpus phonology. BICLCE 10, Alicante, Spain.\n2024 Ordinal response scales: Psychometric grounding for design and analysis. BICLCE 10, Alicante, Spain.\n2024 Sensitivity of dispersion measures to distributional patterns and corpus design. ICAME45, Vigo, Spain. (with Jesse Egbert)\n2024 Regression and random forests: Synergies for variationist corpus research. ICAME45, Vigo, Spain. (with Jason Grafmiller and Raquel Romasanta)\n2023 Down-sampling from hierarchically structured corpus data. FJUEL 11, Erlangen, Germany.\n2023 Text-level measures of lexical dispersion: Robustness analysis. Corpus Linguistics 2023. Lancaster, UK.\n2023 Down-sampling from hierarchically structured corpus data: The case of third-person verb inflection in Early Modern English. Corpus Linguistics 2023. Lancaster, UK.\n2022 Seeing the wood for the trees: Predictive margins for random forests. ICAME 43, London, UK. (with Jason Grafmiller)\n2022 Keyword analysis: Progress through regression. ICAME 43, London, UK.\n2019 The English comparative alternation revisited: A fresh look at theory and data. ICAME 40, Neuchatel, Switzerland. (with Stefan Hartmann)\n2018 Frequency effects in the English comparative alternation: A reassessment. ISLE5, London, UK.\n2018 A normalization procedure for auditory vowel descriptions: Method and application. ISLE5, London, UK. (with Ole Schützler)\n2018 Drawing on principles of perception: The line plot. ICAME34, Tampere, Finland.\n2018 Visual inference for corpus data analysis: Dot plots of effect sizes with confidence intervals. ICAME34, Tampere, Finland.\n2018 A sociolinguistic study of actually in current spoken British English. ICAME34, Tampere, Finland.\n2017 (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. BICLCE 7, Vigo, Spain.\n2015 Developmental patterns in German Learner English: Vowel reduction and speech rhythm. Research seminar, University of Münster, Germany.\n2015 Methods for corpus data analysis: Dot plots of effect sizes with confidence intervals. Methods and Linguistic Theories (MaLT), Bamberg, Germany.\n2014 Vowel reduction in German Learner English: Developmental patterns. 47th Meeting of the Societas Linguistica Europaea (SLE 2014), Poznan, Poland.\n2014 Normalverteilungsannahme und Ausreißerwerte: Nachteile klassischer Statistik und robuste Alternativen. IV. Diskussionsforum Linguistik in Bayern. Munich, Germany.\n2014 [Poster] Vowel reduction in German Learner English: Developmental patterns. Olomouc Linguistics Colloquium (Olinco), Olomouc, Czeck Republic.\n2014 Vowel reduction in German Learner English. Research seminar, University of Münster, Germany.\n2014 The dot plot: A fine tool for data visualization. Advances in Visual Methods for Linguistics (AVML) 2014. Tübingen, Germany.\n2013 An acoustic analysis of unstressed vowels in German Learner English. Accents 2013, Łódź, Poland.\n2013 Vowel reduction in German Learner English. FJUEL 3, Regensburg, Germany.\n2013 Scrabble yourself to success: Methods in teaching transcription. Phonetics Teaching and Learning Conference (PTLC) 2013, London, UK."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Unpublished\n\nSönning, Lukas. (in press). Advancing our understanding of dispersion measures in corpus research. Corpora. doi: 10.3366/cor.2025.0326 | Accepted manuscript |  Data |  OSF\nSönning, Lukas. (in press). Count regression models for keyness analysis. Chapter for: Carolin Cholotta & Christine Renker (eds.), META-LING 2023 - Methodological Exploration and Technological Advances in Linguistics  Accepted manuscript |  OSF\nSönning, Lukas, Jason Grafmiller & Raquel Romasanta. (under review). Regression and random forests: Synergies for variationist corpus research.  Working paper |  OSF\nSönning, Lukas & Jesse Egbert. (under review). Sensitivity of dispersion measures to distributional patterns and corpus design.  Working paper |  OSF\nSönning, Lukas, Ole Schützler, Manfred Krug & Fabian Vetter. (in preparation). The morpho-syntax of Scottish Standard English: Questionnaire-based insights.  OSF\nSönning, Lukas. (in preparation). Down-sampling strategies in corpus phonology. Chapter for: Philipp Meer & Ulrike Gut (eds.) English corpus phonetics and phonology: Current approaches and future directions  OSF\nSönning, Lukas. (in preparation). Dispersion analysis. In Hilary Nesi & Petar Milin (eds.), International Encyclopedia of Language and Linguistics, 3rd ed. Amsterdam: Elsevier.\nSönning, Lukas. (in preparation). Case-control down-sampling in corpus-based research.\nSönning, Lukas. (in preparation). Random forests in corpus research: A systematic review.\nSönning, Lukas. (unpublished manuscript). Evaluation of text-level measures of lexical dispersion: Robustness and consistency.  Working paper |  Data |  OSF\n\n \n\n\nPublished\nMonograph\n\nSönning, Lukas. 2020. Phonological variation in German Learner English. University of Bamberg dissertation. doi: 10.20378/irb-49135 |  Open access |  Datasets |  OSF\n\nJournal articles\n\nSönning, Lukas. 2024. Ordinal response scales: Psychometric grounding for design and analysis. Research Methods in Applied Linguistics 3(3). 100156. doi: 10.1016/j.rmal.2024.100156 |  Open access |  Data |  OSF\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht & Paul Messer. 2024. Latent-variable modelling of ordinal outcomes in language data analysis. Journal of Quantitative Linguistics 31(2). 77–106. doi: 10.1080/09296174.2024.2329448 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Down-sampling from hierarchically structured corpus data. International Journal of Corpus Linguistics 29(4). 507–533. doi: 10.1075/ijcl.23079.son |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Evaluation of keyness metrics: Performance and reliability. Corpus Linguistics and Linguistic Theory 20(2). 263–288. doi: 10.1515/cllt-2022-0116 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Jason Grafmiller. 2024. Seeing the wood for the trees: Predictive margins for random forests. Corpus Linguistics and Linguistic Theory 20(1). 153–181. doi: 10.1515/cllt-2022-0083 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Valentin Werner. 2021. The replication crisis, scientific revolutions, and linguistics. Linguistics 59(5). 1179–1206. doi: 10.1515/ling-2019-0045 |  Open access\nSönning, Lukas. 2014. Unstressed vowels in German Learner English: An instrumental study. Research in Language 12(2). 163–173. doi: 10.2478/rela-2014-0001 |  Open access |  OSF\n\nEdited volumes\n\nSönning, Lukas & Ole Schützler (eds.). 2023. Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22\nSönning, Lukas & Valentin Werner. 2021. The replication crisis: Implications for linguistics. Special issue in Linguistics.  Open access\nChrist, Hanna, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.). 2016. A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium. Bamberg: University of Bamberg Press.  Open access\n\nBook chapters\n\nSönning, Lukas. 2023. Drawing on principles of perception: The line plot. In Lukas Sönning & Ole Schützler (eds.), Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22-2 |  Preprint |  OSF\nSönning, Lukas. 2023. (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. In Robert Fuchs (ed.), Speech rhythm in learner and second language varieties of English, 123–157. Singapore: Springer. doi: 10.1007/978-981-19-8940-7_6 |  Preprint |  Data |  OSF\nSönning, Lukas & Manfred Krug. 2022. Comparing study designs and down-sampling strategies in corpus analysis: The importance of speaker metadata in the BNCs of 1994 and 2014. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 127–159. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.006 |  Data |  OSF\nSönning, Lukas & Julia Schlüter. 2022. Comparing standard reference corpora and Google Books Ngrams: Strengths, limitations and synergies in the contrastive study of variable h- in British and American English. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 17–45. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.002 |  OSF\nKrug, Manfred & Lukas Sönning. 2018. Language change in Maltese English: The influence of age and parental languages. In: Patrizia Paggio & Albert Gatt (eds.), The languages of Malta, 247–270. Berlin: Language Science Press. doi: 10.5281/zenodo.1181801 |  Open access\n\nProceedings\n\nSönning, Lukas. 2016. The dot plot: A graphical tool for data analysis and presentation. In Hanna Christ, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.), A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium, 101–129. Bamberg: University of Bamberg Press.  Open access\nSönning, Lukas. 2014. Developmental patterns in the reduction of unstressed vowels by German learners of English. In Ludmila Veselovská & Markéta Janebová (eds.), Complex visibles out there: Proceedings of the Olomouc Linguistics Colloquium 2014: Language use and linguistic structure, vol. 4 Olomouc modern language series, 765–778. Olomouc: Palacký University.  Open access |  OSF\nSönning, Lukas. 2013. Scrabble yourself to success: Methods in teaching transcription. In Joanna Przedlacka, John Maidment & Michael Ashby (eds.), Proceedings of the Phonetics Teaching and Learning Conference, UCL, London, 8-10 August 2013. London: Phonetics Teaching and Learning Conference, 87–90.  Open access |  OSF |  Data\n\n \n\n\nDatasets\n\nSönning, Lukas. 2024. Background data for: Ordinal response scales: Psychometric grounding for design and analysis, https://doi.org/10.18710/0VLSLW, DataverseNO, V1\nKrug, Manfred, Fabian Vetter & Lukas Sönning. 2024. Background data for: Latent-variable modeling of ordinal outcomes in language data analysis. https://doi.org/10.18710/WI9TEH, DataverseNO, V1.\nSönning, Lukas. 2023. Background data (adapted from Jenset & McGillivray 2017) for: Down-sampling from hierarchically structured corpus data, https://doi.org/10.18710/5KCE4U, DataverseNO, V1.\nSönning, Lukas. 2023. Key verbs in academic writing: Dataset for “Evaluation of keyness metrics: Performance and reliability”, https://doi.org/10.18710/EUXSMW, DataverseNO, V1.\nSönning, Lukas. 2022. Speech rhythm in German Learner English: Dataset for “(Re-)viewing the acquisition of rhythm in the light of L2 phonological theories”, https://doi.org/10.18710/GTI2BR, DataverseNO, V1.\nSönning, Lukas & Manfred Krug. 2021. Actually in contemporary British speech: Data from the Spoken BNC corpora, https://doi.org/10.18710/A3SATC, DataverseNO, V1.\nSönning, Lukas. 2022. Dataset for “Scrabble yourself to success: Methods in teaching transcription”, https://doi.org/10.18710/2UJHHU, DataverseNO, V1.\n\n\nDissertation\n\nSönning, Lukas. 2021. The TRAP-DRESS contrast in German Learner English: Dataset for chapter 4 in “Phonological variation in German Learner English”, https://doi.org/10.18710/ATIRRV, DataverseNO, V1.\nSönning, Lukas. 2021. Clear vs. dark /l/ in German Learner English: Dataset for chapter 5 in “Phonological variation in German Learner English”, https://doi.org/10.18710/G6PJ5F, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. English /r/ in German Learner English: Dataset for chapter 6 in “Phonological variation in German Learner English”, https://doi.org/10.18710/YDKDFG, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. The labio-velar glide /w/ in German Learner English: Dataset for chapter 7 in “Phonological variation in German Learner English”, https://doi.org/10.18710/F1A34O, DataverseNO, V1.\nSönning, Lukas & Isabel Rank. 2021. The labiodental fricative /v/ in German Learner English: Dataset for chapter 8 in “Phonological variation in German Learner English”, https://doi.org/10.18710/B276ZX, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe & Christina Wunder. 2021. The voiced dental fricative in German Learner English: Dataset for chapter 9 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DYAGZG, DataverseNO, V1.\nSönning, Lukas & Graham Pascoe. 2021. Final voiced obstruents in German Learner English: Dataset for chapter 10 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DKIGE5, DataverseNO, V1."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Plot templates for Microsoft Excel\nExcel templates and instructions for some useful graph types can be found in the following  OSF project .\n\nDot diagram\ntemplate | instructions\n\n\n\n\n\nSimple dot plot\ntemplate | instructions\n\n\n\n\n\nGrouped dot plot\ntemplate | instructions\n\n\n\n\n\nBox plot\ntemplate | instructions\n\n\n\n\n\nVertical dot plot\ntemplate\n\n\n\n\n\nScatter plot\ntemplate | instructions\n\n\n\n\n\n\nSpeaker slides for workshop\n\nFJUEL workshop, Bamberg: speaker slides session 1 | speaker slides session 2 | speaker slides session 3\n\n\n\nExtended notes: Ordinal regression models\n\nSection 1: Background\nSection 2: Descriptive statistics\nSection 3: Ordered regression models\nSection 4: A latent-variable model\nSection 5: Methods of interpretation\nSection 6: R workbench"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Statistics for linguist(ic)s blog",
    "section": "",
    "text": "Modeling the interpretation of quantifiers using beta regression\n\n\n\n\n\n\nbeta regression\n\n\nregression\n\n\ndistributional modeling\n\n\n\nThis blog post shows how to use beta regression to model the proportional interpretation of the quantifiers few, some, many, and most. We consider variable-dispersion and mixed-effects structures as well as diagnostics for frequentist and Bayesian models.\n\n\n\n\n\nFeb 29, 2024\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent parameterizations of the negative binomial distribution\n\n\n\n\n\n\ncount data\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nnegative binomial distribution\n\n\n\nThis blog post discusses two different parameterizations of the negative binomial distribution and groups R packages (and functions) based on the version they implement.\n\n\n\n\n\nDec 13, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe negative binomial distribution: A visual explanation\n\n\n\n\n\n\ncount data\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nnegative binomial distribution\n\n\n\nThis blog post uses a visual approach to explain how the negative binomial distribution works.\n\n\n\n\n\nDec 12, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nA computational shortcut for the dispersion measure DA\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndispersion\n\n\n\nThis short blog post draws attention to the computational shortcut given in Wilcox (1973) for calculating the dispersion measure DA.\n\n\n\n\n\nDec 11, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe replication crisis: Implications for myself\n\n\n\n\n\n\nreplication crisis\n\n\nopen science\n\n\nreproducibility\n\n\n\nIn this blog post, I reflect on the ways in which learning about the replication crisis in science has affected my own work.\n\n\n\n\n\nNov 21, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nStructured down-sampling: Implementation in R\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\nThis blog post shows how to implement structured down-sampling in R.\n\n\n\n\n\nNov 18, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nTwo types of down-sampling in corpus-based work\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\nThis short blog post contrasts the different ways in which the term down-sampling is used in corpus-based work.\n\n\n\n\n\nNov 17, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\n‘Dispersion’ in corpus linguistics and statistics\n\n\n\n\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nterminology\n\n\n\nThis blog post clarifies the different ways in which the term dispersion is used in corpus linguistics and statistics.\n\n\n\n\n\nNov 16, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-18_dispersion_terminology/index.html",
    "href": "posts/2023-01-18_dispersion_terminology/index.html",
    "title": "‘Dispersion’ in corpus linguistics and statistics",
    "section": "",
    "text": "R setup\nlibrary(lattice)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nIn corpus linguistics, the term dispersion is used to describe the distribution of an item or structure in a corpus (see Gries 2008, 2020). For most dispersion measures, a corpus must first be divided into units (or parts). These units commonly reflect the design of the corpus – they can be text files, for instance, or text categories. A dispersion index then describes the distribution of an item across these units. There are two general classes of measures:\n\nthose measuring the pervasiveness of an item, which is reflected in the number of units that contain the item (Range and Text Dispersion, its proportional analog)\nthe much larger class of evenness measures, which express how evenly an item is distributed across the units (e.g. D, D2, S, DP, DA, DKL).\n\nMost dispersion measures range between 0 and 1, where 1 indicates a perfectly even distribution, or the maximal degree of pervasiveness (i.e. the item occurs in every unit).\nFrom a statistical viewpoint, the input for the calculation of evenness measures would be considered a count variable, since it records the number of events (occurrences of the item) that are observed during a certain period of observation. In corpus linguistics, the “period of observation” is “text time”, expressed as a word count.\nThere is an extensive literature on the use of regression models for count variables (e.g. Long 1997; Cameron and Trivedi 2013; Hilbe 2014), and such models have seen some successful applications to word frequency data (e.g. Mosteller and Wallace 1984; Church and Gale 1995); Winter and Bürkner (2021) provide an accessible introduction for linguists. In this literature, the term “dispersion” is also used, though with a different (apparently opposite) meaning.\nLet us first consider the corpus-linguistic (and lexicographic) sense, which can be best described visually, using a so-called “dispersion plot”. Figure 1 shows a dispersion plot for two corpora, A and B. The framed rectangles represent the sequence of words forming the corpus, and the spikes inside of these locate the occurrences of a specific item in the corpus. In corpus A, the item is spread out quite evenly. In corpus B, instances are more densely clustered, and there are large stretches where the item does not occur. In the corpus-linguistic sense, then, the dispersion of the item is greater in corpus A. The dispersion score for the item would be greater in Corpus A (i.e. closer to 1).\n\n\nR code: Figure 1\nset.seed(2000)\n\nn_tokens_A &lt;- c(3,5,4,4,3,4,4,5)\nn_tokens_B &lt;- c(5,0,1,9,0,1,0,3)\n\nn_texts &lt;- length(n_tokens_A)\n\nA_loc &lt;- rep(1:n_texts, n_tokens_A)+runif(sum(n_tokens_A))\nB_loc &lt;- rep((1:n_texts)[n_tokens_B!=0], n_tokens_B[n_tokens_B!=0])+runif(sum(n_tokens_B))\n\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,14), ylim=c(2.8,6),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=5.1, ybottom=4.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    panel.rect(xleft=1, xright=n_texts+1, ytop=5.1, ybottom=4.75, \n               border=\"grey50\", lwd=1)\n    \n\n        \n    panel.segments(x0=A_loc, x1=A_loc, y0=4.8, y1=5.05, lwd=.75)\n    panel.text(x=(1:n_texts)+.5, y=4.55, label=n_tokens_A, \n               col=\"grey50\", cex=.9)\n    \n    \n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=4.1, ybottom=3.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    \n    panel.rect(xleft=1, xright=n_texts+1, ytop=4.1, ybottom=3.75, \n               border=\"grey60\", lwd=1)\n    \n    panel.segments(x0=B_loc, x1=B_loc, y0=3.8, y1=4.05, lwd=.75)\n    \n    panel.text(x=(1:n_texts)+.5, y=3.55, label=n_tokens_B, \n               col=\"grey60\", cex=.9)\n    \n    panel.text(x=.4, y=c(4,5)-.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"Lower dispersion\\n\", \"Higher dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"\\n(more concentrated)\", \"\\n(more spread out)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.segments(x0=1, x1=2, y0=5.4, y1=5.4, lwd=.5, col=\"grey50\")\n    panel.segments(x0=1:2, x1=1:2, y0=5.4, y1=5.3, lwd=.5, col=\"grey50\")\n    \n    \n    panel.text(x=1.5, y=5.75, label=\"Text 1\", col=\"grey40\", cex=.8)\n    panel.text(x=7, y=2.75, label=\"Occurrences\\nof item in text\", col=\"grey40\", \n               cex=.9, lineheight=.85)\n    panel.segments(x0=5.9, x1=5.6, y0=3, y1=3.3, col=\"grey40\", lwd=.5)\n    })\n\n\n\n\n\nFigure 1: Dispersion in the corpus-linguistic sense: Distribution of word tokens in the corpus.\n\n\n\n\nNote how each corpus is divided into 8 texts, which are shown in Figure 1 using greyshading. The numbers below the dispersion plot for each corpus report the number of occurrences of the item in each text. For corpus A, they range between 3 and 5; for corpus B, between 0 and 9.\nFigure 2 shows a different representation of these data. Instead of looking at the corpus as a string of words, we consider the text-specific frequencies (sometimes called sub-frequencies) of the item. These indicate how often the item occurs in each document. Figure 2 shows these text-level token counts: Each text is represented by a dot, which marks how often the item appears in the text. In our hypothetical corpora, each text has the same length, which is why we can compare absolute counts. If texts differ in length, we would instead use normalized frequencies, i.e. occurrence rates such as “3.1 per thousand words”.\n\n\nR code: Figure 2\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,7.5), ylim=c(-.35,2.3),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.dotdiagram(1+(n_tokens_A/5), y_anchor=1, scale_y=.125, set_cex=1.3)\n    panel.dotdiagram(1+(n_tokens_B/5), y_anchor=0, scale_y=.125, set_cex=1.3)\n    panel.segments(x0=1, x1=3.2, y0=1, y1=1)\n    panel.segments(x0=1, x1=3.2, y0=0, y1=0)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=1, y1=.95)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=0, y1=-.05)\n    panel.text(x=1+c(0,5,10)/5, y=-.2, label=c(0,5,10), col=\"grey40\", cex=.8)\n    \n    panel.text(x=.6, y=c(0,1)+.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"Higher dispersion\\n\", \"Lower dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"\\n(more spread out)\", \"\\n(more concentrated)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.text(x=2, y=-.5, label=\"Occurrences of item\", cex=1, lineheight=.9)\n    panel.text(x=3, y=2.2, label=\"Each dot\\nrepresents a text\", cex=.9, \n               lineheight=.9, col=\"grey40\")\n    })\n\n\n\n\n\nFigure 2: Dispersion in the statistical sense: Distribution of text-level ocurrence rates.\n\n\n\n\nIf we compare the distribution of text-level occurrence rates in the two corpora, we note that while the texts in corpus A form a dense pile, the occurrence rates in corpus B are more widely spread out. At this level of description, then, it is the data from corpus B that show greater “dispersion”. In the statistical literature on count regression, the term dispersion is used in this sense, i.e. to refer to the variability of unit-specific (i.e. text-level) occurrence rates (e.g. Long 1997, 221; Gelman 2021, 264–68). An awareness of the different meanings of “dispersion” will prove helpful for corpus linguists (and lexicographers) when engaging with the statistical literature on count data modeling.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe term “dispersion” is used differently in corpus linguistics and statistics\nThe difference in meaning reflects a difference in perspective\nCorpus linguists picture the corpus as a sequence of words and understand the term as characterizing the spatial distribution of an item\nIn the statistical literature on count data modeling, the term describes the spread of a distribution of counts or occurrence rates\n\n\n\n\n\n\n\nReferences\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. Second edition. New York: Cambridge University Press.\n\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/S1351324900000139.\n\n\nGelman, Hill, Andrew. 2021. Regression and Other Stories. Cambridge: Cambridge University Press.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data. New York: Cambridge University Press.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage.\n\n\nMosteller, Frederick, and David L. Wallace. 1984. Applied Bayesian Inference: The Case of the Federalist Papers. New York: Springer.\n\n\nWinter, Bodo, and Paul‐Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11). https://doi.org/10.1111/lnc3.12439.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {“{Dispersion}” in Corpus Linguistics and Statistics},\n  date = {2023-11-16},\n  url = {https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “‘Dispersion’ in Corpus\nLinguistics and Statistics.” November 16, 2023. https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_two_types/index.html",
    "href": "posts/2023-11-17_downsampling_two_types/index.html",
    "title": "Two types of down-sampling in corpus-based work",
    "section": "",
    "text": "The data available from corpora are often too vast for certain types of linguistic analysis. Researchers are then forced to select a subset of the data, and this selection process can be referred to as “down-sampling”. Currently, the term is used to refer to two very different types of down-sizing.\nThe first deals with lists of occurrences extracted from a corpus and is used in studies that start out with a corpus query and a body of hits (often in the form of concordance lines). If the structure of interest is relatively frequent and/or the source corpus large, the researcher may need to reduce the number of data points studied. In particular, this will be necessary in variationist-type research, which often involves considerable manual work (e.g. disambiguation and annotation). In this form of down-sampling, the selection of elements usually proceeds (to some extent) at random, i.e. it involves a chance component. Simple techniques are implemented in corpus software, which allows users to extract from a list of hits a random sample. In CQPweb (Hardie 2012), for instance, this option is referred to as “thinning”. Depending on our research goals and the structure of our data, however, other strategies may be more efficient (e.g. structured down-sampling, see Sönning and Krug 2022).\nThe second type of down-sampling is concerned with the selection of texts for close reading. Here, the objective is to pick from a corpus those texts that are likely to be most informative for a thorough qualitative analysis. This method, which Gabrielatos et al. (2012) refer to as “targeted down-sampling”, uses surface-level features (such as the occurrence rate of certain forms) to detect relevant documents for a critical discourse analysis (see also Baker et al. 2008, 285). A procedure much in the same spirit is discussed in Anthony and Baker (2015), where prototypical exemplars, i.e. texts that are most representative of their corpus of origin, are selected based on keyword profiles.\nIt may therefore sometimes be helpful to distinguish the two types of down-sampling: We could call the first type “selection of concordance lines for annotation” and the second type “selection of texts for close reading”.\n\n\n\n\nReferences\n\nAnthony, Laurence, and Paul Baker. 2015. “ProtAnt: A Tool for Analysing the Prototypicality of Texts.” International Journal of Corpus Linguistics, August, 273–92. https://doi.org/10.1075/ijcl.20.3.01ant.\n\n\nBaker, Paul, Costas Gabrielatos, Majid KhosraviNik, Michał Krzyżanowski, Tony McEnery, and Ruth Wodak. 2008. “A Useful Methodological Synergy? Combining Critical Discourse Analysis and Corpus Linguistics to Examine Discourses of Refugees and Asylum Seekers in the UK Press.” Discourse &Amp; Society 19 (3): 273–306. https://doi.org/10.1177/0957926508088962.\n\n\nGabrielatos, Costas, Tony McEnery, Peter J. Diggle, and Paul Baker. 2012. “The Peaks and Troughs of Corpus-Based Contextual Analysis.” International Journal of Corpus Linguistics 17 (2): 151–75. https://doi.org/10.1075/ijcl.17.2.01gab.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Two Types of down-Sampling in Corpus-Based Work},\n  date = {2023-11-17},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Two Types of down-Sampling in Corpus-Based\nWork.” November 17, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_implementation/index.html",
    "href": "posts/2023-11-17_downsampling_implementation/index.html",
    "title": "Structured down-sampling: Implementation in R",
    "section": "",
    "text": "I recently consulted colleagues on how to down-sample their corpus data. Their study deals with modal auxiliaries in learner writing, and they are also interested in the semantics of modal verbs. This means that they have to manually annotate individual tokens of modals. In this blog post, I describe how we implemented structured down-sampling (Sönning and Krug 2022) in R. The data we use for illustration is a simplified subset of the originial list of corpus hits. We will concentrate on the modal verb can.\n\n\nR setup\nlibrary(tidyverse)\n\nd &lt;- read_tsv(\"./data/modals_data.tsv\")\n#d &lt;- read_tsv(\"./posts/2023-11-17_downsampling_implementation/data/modals_data.tsv\")\n\n\n\nThe data\nThe data include 300 tokens, which are grouped by Text (i.e. learner essay), and there are 162 texts where can occurs at least once. The distribution of tokens across texts is summarized in Figure 1: In most texts (n = 83), can occurs only once, 41 texts feature two occurrences, and so on.\n\n\nR code: Figure 1\nd |&gt; \n  group_by(text_id) |&gt; \n  tally() |&gt; \n  group_by(n) |&gt; \n  tally() |&gt; \n  ggplot(aes(x=n, y=nn)) + \n  geom_col(width = .7, fill=\"grey\") +\n  theme_classic() +\n  scale_x_continuous(breaks = 1:7) +\n  xlab(\"Number of occurrences\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\nFigure 1: Distribution of token counts across texts.\n\n\n\nA different arrangement of the data is shown in Figure 2, where texts are lined up from left to right. Each text is represented by a pile of dots, with each dot representing a can token. The text with the highest number of can tokens (n = 7) appears at the far left, and about half of the texts only have a single occurrence of can – these text are sitting in the right half of the graph.\n\n\nR code: Figure 2\nd |&gt;  \n  group_by(text_id) |&gt; \n  mutate(n_tokens = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x=reorder(text_id, -n_tokens))) + \n  geom_dotplot(dotsize = .13, stackratio=1.6) +\n  theme_void() +\n  labs(subtitle=\"Texts ranked by token count\",\n       caption = \"Each dot represents a token (can)\")\n\n\n\n\n\nFigure 2: Distribuition of tokens across texts.\n\n\n\n\n\n\nStructured down-sampling\nAs argued in Sönning and Krug (2022), structured down-sampling would be our preferred way of drawing a sub-sample from these data. In contrast to simple down-sampling (or thinning), where each token has the same probability of being selected, structured down-sampling aims for a balanced representation of texts in the sub-sample. Thus, we would aim for breadth of representation and only start selecting additional tokens from the same text if all texts are represented in our sub-sample. The statistical background for this strategy is discussed in Sönning and Krug (2022).\nLooking at Figure 2, this means that our selection of tokens would first consider the “bottom row” of dots in the graph, and then work upwards if necessary, i.e. sample one additional token (at random) from each text that contains two or more occurrences, and so on. It should be noted that, at some point, little more is learned by sampling yet further tokens from a specific text (see discussion in Sönning and Krug 2022, 147).\n\n\nImplementation in R\nOur first step is to add to the table a column that preserves the original order. This is important in case we want to return to the original arrangement at a later point. We will name the new column original_order.\n\nd$original_order &lt;- 1:nrow(d)\n\nThere may be settings where, due to resource constraints, we cannot pick a token from every single text. Or, similarly, where we cannot pick a second token from each text that contains at least two tokens. In such cases, a sensible default approach is to pick at random. Thus, if we were only able to analyze 100 tokens, but there are 162 texts in our data, we would like to pick texts at random. We therefore add another column where the sequence from 1 to N (the number of rows, i.e. tokens) is shuffled. This column will be called random_order. Further below, we will see how this helps us out.\n\nd$random_order &lt;- sample(\n  1:nrow(d), \n  nrow(d), \n  replace=F)\n\nThe next step is to add a column to the table which specifies the order in which tokens should be selected from a text. We will call the column ds_order (short for ‘down sampling order’). In texts with a single token, the token will receive the value 1, reflecting its priority in the down-sampling plan. For a text with two tokens, the numbers 1 and 2 are randomly assigned to the two tokens. For texts with three tokens, the numbers 1, 2 and 3 are shuffled, and so on. If we then sort the whole table according to the column ds_order, those tokens that are to be preferred, based on the rationale underlying structured down-sampling, appear at the top of the table.\nOur first step is to order the table by text_id, to make sure rows are grouped by Text.\n\nd &lt;- d[order(d$text_id),]\n\nWe then create a list of the texts in the data and sort it, so that it matches the way in which the table rows have just been ordered.\n\ntext_list &lt;- unique(d$text_id)\ntext_list &lt;- sort(text_list)\n\nWe now create the vector ds_order, which we will add to the table once it’s ready:\n\nds_order &lt;- NA\n\nThe following loop fills in the vector ds_order, text by text. It includes the following steps (marked in the script):\n\nProceed from text to text, from the first to the last in the text_list.\nFor text i, count the number of tokens in the text and store it as n_tokens.\nShuffle the sequence from 1 to n_tokens and store it as shuffled.\nAppend the shuffled sequence shuffled to the vector ds_order.\n\n\nfor(i in 1:length(text_list)){  # (1)\n  \n  n_tokens &lt;- sum(              # (2)\n    d$text_id == text_list[i])  # \n  \n  shuffled &lt;- sample(           # (3)\n    1:n_tokens,                 #\n    size = n_tokens,            #\n    replace = FALSE)            #\n  \n  ds_order &lt;- append(           # (4)\n    ds_order,                   #\n    shuffled)                   #\n}\n\nIf we look at the contents of ds_order, we note that it still has a leading NA:\n\nds_order\n\n  [1] NA  2  1  3  1  2  1  1  2  2  1  2  3  1  4  1  3  2  1  2  1  1  2  1  1\n [26]  1  1  1  3  4  1  2  1  1  2  3  1  1  2  3  1  1  2  1  1  1  1  2  1  4\n [51]  3  3  1  2  1  2  2  1  2  3  1  1  5  6  3  4  2  2  4  3  1  2  1  1  2\n [76]  4  3  1  2  1  2  1  4  3  1  1  1  1  2  1  1  1  1  3  2  1  1  2  2  4\n[101]  1  3  1  1  1  2  1  1  1  1  1  2  3  1  1  1  2  1  1  1  1  3  2  1  1\n[126]  1  2  2  3  1  1  1  2  1  3  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  3  2  1  2  1  1  1  2  1  1  2  3  1  1  1  2  1  1  2  1  1  1  2  1\n[176]  1  2  3  1  2  2  3  1  2  1  3  1  3  2  1  2  1  2  1  2  1  4  3  1  3\n[201]  2  1  2  1  3  1  1  2  2  1  1  3  2  1  1  1  1  2  2  1  1  1  1  2  2\n[226]  3  1  1  1  2  3  1  2  1  1  1  2  1  1  1  1  2  1  3  4  2  1  1  3  2\n[251]  4  3  2  1  1  2  1  2  1  1  3  5  2  4  1  1  2  1  1  2  1  2  1  1  2\n[276]  7  6  5  3  2  1  4  1  1  1  2  1  1  1  1  2  4  2  1  3  1  4  3  2  2\n[301]  1\n\n\nSo we get rid of it:\n\nds_order &lt;- ds_order[-1]\n\nWe can now add ds_order as a new column to our table:\n\nd$ds_order &lt;- ds_order\n\nThe final step is to order the rows of the table in a way that reflects our down-sampling priorities. We therefore primarily order the table based on ds_order. In addition, we order by the column random_order, which we created above. All tokens with the same priority level (e.g. all tokens with the value “1” in the column ds_order) will then be shuffled, ensuring that the order of tokens is random.\n\nd &lt;- d[order(d$ds_order, \n             d$random_order),]\n\nWe can now look at the result:\n\nhead(d)\n\n# A tibble: 6 x 7\n  text_id  left_context modal right_context original_order random_order ds_order\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;int&gt;\n1 text_19  music        can   just                     109            1        1\n2 text_158 you          can   only                     289            3        1\n3 text_123 and          cann~ distinguish              148            4        1\n4 text_69  How          can   this                      48            5        1\n5 text_88  music        can   also                      25            9        1\n6 text_156 One          can   not                      167           13        1\n\n\nNote that the strategy we have used, i.e. adding a column reflecting the priority of tokens for down-sampling, allows us to approach down-sampling in a flexible and adaptive way: Rather than actually selecting (or sampling) tokens (or rows) from the original data, we may now simply start analyzing from the top of the table. This way we remain flexibility when it comes to the choice of how many tokens to analyze.\n\n\n\n\n\nReferences\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Structured down-Sampling: {Implementation} in {R}},\n  date = {2023-11-18},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Structured down-Sampling: Implementation in\nR.” November 18, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "title": "The replication crisis: Implications for myself",
    "section": "",
    "text": "Since my research is almost exclusively quantitative, the methodological discourse surrounding the replication crisis has been directly relevant to my work. A recent invitation to take part in an online event by the International Society for the Linguistics of English (ISLE) on “Replication and Replicability” was an opportunity to reflect on the ways in which this “crisis” has affected how I do my job. In this blog post, I summarize these under three headings: (i) workflow and reproducibility, (2) open science, and (3) community discourse.\nI would like to start, however, with two preliminary remarks. For one, I consider the discussions, suggestions, and innovations that have arisen in the context of the credibility crisis in science as an opportunity – they should inspire us to improve the way(s) in which we do and communicate research. While there are some who point out that we actually don’t know whether there is a replication crisis in linguistics1, the suggested ways forward enable better science, so it is worth adopting them in any case.\nFurther, if we decide to change our research routines, we should be indulgent with ourselves: Many of the suggested improvements, especially concerning data analysis workflow, can be quite overwhelming at first. We should avoid setting our immediate aims too high – as I had to find out on numerous occasions, it is too easy to become frustrated. And this may also be something to keep in mind when making recommendations: The advice we give to others should be calibrated to the person across the table. Nothing is gained if a researcher with a genuine interest in adopting better practices ends up quitting in frustration."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "title": "The replication crisis: Implications for myself",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt seems that we are not particularly eager to find out (see, e.g., this preprint by Kristina Kobrock and Timo Roettger). It would be quite surprising, however, if linguist(ic)s were spared – after all, the same human factors are at work in language research as in neighboring disciplines such as psychology.↩︎"
  },
  {
    "objectID": "posts/2023-12-11_computation_DA/index.html",
    "href": "posts/2023-12-11_computation_DA/index.html",
    "title": "A computational shortcut for the dispersion measure DA",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by Burch, Egbert, and Biber (2017) as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to Wilcox (1973), a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While Wilcox (1973) focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as Burch, Egbert, and Biber (2017, 193) note, a measure equivalent to DP (Gries 2008) can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in Wilcox (1973) as the mean difference analog (MDA). Both Wilcox (1973) and Burch, Egbert, and Biber (2017) argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in Sönning (2023).\nGries (2020, 116) has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula Wilcox (1973) gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in Wilcox (1973).\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to Sönning (2023). We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1985)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.48 seconds to run. The computational formula is quicker – it completes the calculations in only 0 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 34.85 seconds to run; the shortcut procedure, on the other hand, is done after 0.01 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n1.48\n34.85\n0.6062\n0.6145\n\n\nComputational\n0.00\n0.01\n0.6064\n0.6145\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nSönning, Lukas. 2023. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” PsyArxiv Preprint. https://doi.org/10.31234/osf.io/ns4q9.\n\n\nWilcox, Allen R. 1973. “Indices of Qualitative Variation and Political Measurement.” The Western Political Quarterly 26 (2): 325–43. https://doi.org/10.2307/446831.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {A Computational Shortcut for the Dispersion Measure\n    {*D\\textasciitilde A\\textasciitilde*}},\n  date = {2023-12-11},\n  url = {https://lsoenning.github.io/posts/2023-12-11_computation_DA/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “A Computational Shortcut for the Dispersion\nMeasure *D~A~*.” December 11, 2023. https://lsoenning.github.io/posts/2023-12-11_computation_DA/."
  },
  {
    "objectID": "posts/2023-11-16_negative_binomial/index.html",
    "href": "posts/2023-11-16_negative_binomial/index.html",
    "title": "The negative binomial distribution: A visual explanation",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(gamlss)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe negative binomial distribution is a useful device for modeling word counts. A typical setting for its application in corpus linguistics is the modeling of word frequency data – for instance, if we wish to summarize (or compare) occurrence rates of an item in a corpus (or across sub-corpora). Each text then contributes information about the frequency of the item in the form of (i) a token count (the number of times the word occurs in the text) and (ii) a word count (the length of the text). Based on the token and word count we can calculate an occurrence rate (or normalized frequency) for each text, and these rates are then directly comparable across texts.\nFrom a statistical perspective, word frequency would be considered as a count variable, which is observed at the level of the text and can take on non-negative integer values (i.e. 0, 1, 2, 3, 4, …). The text length can be thought of as a period of observation (measured in text time, i.e. the number of running words), in which a tally is kept of the number of events (in this case the occurrence of the focal item). And this is the typical definition of a count variable.\nThis blog post takes a closer look at the negative binomial distribution – how it works and why it is a useful device for modeling word frequency data. It is helpful to start with a concrete example: the frequency of which in the Brown Corpus. To keep things simple, we will stick to this data setting, where texts have (nearly) the same length. Note, however, that the negative binomial distribution (like the Poisson) readily extends to situations where texts have different lengths.\n\nObserved and expected frequency distributions\nIf we count the number of occurrences of which in each text and then look at the distribution of token counts, we obtain what is referred to as a frequency distribution or a token distribution. The frequency distribution for which in the Brown Corpus, which consists of 500 texts, appears in Figure 1 a. It shows the distribution of token counts across texts: Each bar represents a specific token count, and the height of the bar is proportional to the number of texts that have this many instances of which. Token counts vary between 0 (n = 26 texts) and 40 (1 text), and the distribution is right-skewed, which is quite typical of count variables, since they have a lower bound at 0.\n\n\nLoad data\n# tdm &lt;- read_tsv(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/brown_tdm.tsv\")\n# \n# str(tdm)\n# \n# n_tokens &lt;- tdm[,which(colnames(tdm) == \"which\")]\n# \n# \n# saveRDS(n_tokens, \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\nn_tokens &lt;- readRDS(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\n\n\n\nDraw Figure 1\n# Poisson model\nm &lt;- glm(n_tokens$which ~ 1, family=\"poisson\")\npoisson_mean &lt;- exp(coef(m))\npoisson_density &lt;- dpois(0:40, lambda = poisson_mean)\n\n\nn_texts &lt;- as.integer(table(n_tokens))\ntoken_count &lt;- as.integer(names(table(n_tokens)))\n\np1 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 53), xlim=c(-1.5, NA),\n  xlab.top = \"(a)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"\",\n  ylab=\"Observed\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=30, y=40, label=\"Observed frequency distribution\", \n               col=\"grey30\", cex=.9)\n    })\n\np2 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 85), xlim=c(-1.5, NA),\n  xlab.top = \"(b)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"Number of instances of which\",\n  ylab=\"Expected\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, type=\"h\")\n    panel.text(x=30, y=60, label=\"Expected frequency distribution\\n(Poisson model)\",\n               col=\"grey30\", cex=.9)\n    })\n\n\n\n\n\n\n\nFigure 1: Which in the Brown Corpus: (a) observed frequency distribution and (b) expected frequency distribution based on the Poisson model.\n\n\n\n\nThe most basic probability distribution that is available for modeling count variables is the Poisson distribution. In general, we can check the fit of a distribution to the observed token counts by comparing the observed distribution (Figure 1 a) to the one expected under a Poisson model. The expected distribution appears in Figure 1 b. We note a mismatch with the observed data: Its tails are too thin – that is, the observed token counts are more widely spread out; counts of 0 are severely underpredicted (or underrepresented).\nIn fact, it is often the case that the Poisson distribution offers a poor fit to (language) data. This is because it rests on a simplistic assumption: It assumes that the expected frequency of which (or: the underlying probability of using which) is the same in every text. In our case, where we are dealing with texts of roughly 2,000 words in length, the expected number of instances of which, on average, is 7.1. Due to sampling variation, the actual number of instances per text will vary around this average. This sampling variation is accounted for in the Poisson distribution, giving it the (near-)bell-shaped appearance in Figure 1 b.\nIn linguistic terms, the model assumes that each text in the Brown Corpus, irrespective of genre or the idiosyncrasies of its author, has the same underlying probability of using which (i.e. about 7 in 2,000; or 3.5 per thousand words). Even for a function word such as which, this assumption seems difficult to defend. For instance, certain genres may use more postmodifying relative clauses, leading to a higher expected rate of which for texts in this category.\n\n\nPoisson mixture distributions\nTo offer a more adequate abstraction (or representation) of the observed token distribution, the assumption of equal rates across texts needs to be relaxed. We want the model to be able to represent variation among texts, and to record the amount of variation suggested by the data. On linguistic grounds, for instance, we would expect function words to vary less from text to text than lexical words, which are more sensitive to register and topic. The idea is to have an additional parameter in the model that acts like a standard deviation, essentially capturing (and measuring) the text-to-text variability in occurrence rates.\nIt is for this purpose that Poisson mixture distributions were invented. One such mixture distribution is the negative binomial distribution, which is also referred to as a Poisson-gamma mixture distribution. This is actually a more transparent label, as we will see shortly.\nThe idea behind Poisson mixtures is rather simple. Since the Poisson distribution on its own fails to adequately embrace high and low counts, its mean is allowed to vary. By allowing the Poisson mean to vary, i.e. shift up and down the count scale (or left and right in Figure 1), the probability distribution is more flexible, which allows it to accommodate the tails of the distribution.\nPoisson mixtures therefore include an additional dispersion parameter (similar to a standard deviation) and the Poisson mean is replaced by a distribution of Poisson means. Note that the way in which the term “dispersion” is used here differs from the sense it has acquired in lexicography and corpus linguistics (the difference is explained in this blog post).\n\n\nDraw Figure 2\nset.seed(1985)\n\ndelta_sample = rGA(20, mu=1, sigma=.1)\nlambda_plot = 7\nplot1 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    for(i in 1:length(delta_sample)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\ndelta_sample2 = rGA(20, mu=1, sigma=.25)\nlambda_plot = 7\nplot2 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    panel.text(x=7, y=.22, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_sample2)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\n\n\n\n\n\n\n\nFigure 2: Mixing Poisson distributions: Each panel shows a sample of 20 Poisson distributions whose means vary around the grand mean of 7. The variation among Poissons is greater in the top panel.\n\n\n\nPoisson mixtures can be thought of as consisting of multiple Poisson distributions with different individual means. This is illustrated in Figure 2. To be able to show multiple distributions in one graph, we now leave out the spikes and connect the dots – a single distribution therefore appears as a bell-shaped profile that looks like a pearl necklace. Each panel shows 20 Poisson distributions, and each of these 20 distributions has a different mean. The means vary around 7, the overall mean of the Poisson mixture.\nThe distributions in the upper panel are spread out more widely than in the lower panel, and it is the newly introduced dispersion parameter that expresses the amount of variation among Poisson means. This basic idea applies to all Poisson mixture distributions. They are called ‘mixture distributions’ because they mix two probability distributions: (i) the familiar Poisson distribution and (ii) an additional probability distribution which describes the variability in the Poisson means. Simplifying slightly, Poisson mixtures only differ in the probability distribution they employ to describe the distribution of the Poisson means.\n\n\nThe gamma distribution as a model of text-to-text variation\nThe negative binomial distribution, for instance, relies on the gamma distribution to describe the text-to-text variability in occurrence rates. It is therefore also called a Poisson-gamma mixture distribution. Figure 3 shows the two gamma distributions that were used to create Figure 2. The dashed curve, which shows greater spread, belongs to the upper panel.\n\n\nDraw Figure 3\nlambda_plot = 7\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20/7), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(a)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(b)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=expression(\"Number of instances of \"~italic(which)),\n  panel=function(x,y,...){\n    panel.segments(x0=7, x1=7, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing the variability of text-to-text occurrence rates.\n\n\n\nFigure 3 a shows the gamma distributions on their actual scale. These are spread out around a value of 1, because they indicate variability in Poisson means on a multiplicative scale. It makes sense to center the distribution around 1, since the overall occurrence rate (multiplied by 1) should be at the center. The x-axis therefore denotes factors, which means that variability between Poisson means is expressed as ratios. The dashed curve, for instance, ranges from roughly 0.5 to 1.5, which means that most Poisson means are found within ± 50% of the overall mean. Since this multiplicative factor cannot be smaller than 0, we need a probability distribution that is bounded at zero (like the gamma distribution).\nPanel (b) translates these distributions to the occurrence rate scale. To create this graph, the factors (i.e. the x-values) in panel (a) were simply multiplied by the overall mean of 7. Now we see that, for the dashed curve, most occurrence rates vary between 4 and 11 instances per text.\n\n\nNegative binomial distribution applied to which\nLet us now apply the negative binomial distribution to the data for which in the Brown Corpus. We first check the fit of this new model to the data. Figure 4 shows that it provides a much closer approximation to the observed token distribution. It accomodates low and high counts and there seems to be no systematic lack of fit.\n\n\nFit negative binomial model in R\nm &lt;- gamlss(n_tokens$which ~ 1, family=\"NBI\", trace = FALSE)\n\nnb_density &lt;- dNBI(\n  0:40, \n  mu = exp(coef(m, what = \"mu\")),\n  sigma = exp(coef(m, what = \"sigma\")))\n\n\n\n\nDraw Figure 4\nxyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 77), xlim=c(-1.5, NA),\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = expression(\"Number of instances of \"~italic(which)),\n  ylab=\"Number of texts\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=10, y=60, label=\"Poisson\", \n               col=\"grey30\", cex=.9, adj=0)\n    panel.text(x=20, y=12, label=\"Negative binomial\", \n               col=1, cex=.9, adj=0)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", type=\"l\")\n    \n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, type=\"l\")\n    })\n\n\n\n\n\nFigure 4: Which in the Brown Corpus: Observed token distribution compared against the Poisson and the negative binomial model.\n\n\n\n\nLet us consider the gamma distribution that describes the text-to-text variability in occurrence rates. Its density appears in Figure 5, which includes two x-axes: A multiplicative scale (bottom) and a scale showing the expected number of instances in a 2,000-word text (the average text length in Brown). The gamma distribution is centered at 1 (multiplicative scale) and 7.1 occurrences (number of instances of which).\n\n\nDraw Figure 5\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 21/7), ylim=c(0,1.4),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.polygon(x = c(seq(.01, 2.8, length=100), (seq(.01, 2.8, length=100))),\n                 y = c(dGA(seq(.01, 2.8, length=100), mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n                       rep(0,100)),\n                 col=\"lightgrey\", border=F)\n    panel.segments(x0=1, x1=1, y0=0, y1=1.4, col=1)\n    \n    panel.segments(\n      x0 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3,  lwd=2, col=1, lineend=\"butt\", alpha=.5)\n    \n    panel.segments(\n      x0 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3, lwd=.5, col=1, lineend=\"butt\", alpha=.5)\n                      \n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, \n                         sigma=exp(coef(m, what = \"sigma\"))),\n                 type=\"l\")\n    \n    panel.segments(x0=-.05, x1=21/7, y0=1.3, y1=1.3)\n    panel.segments(x0=seq(0,20,5)/exp(coef(m, what = \"mu\")), \n                   x1=seq(0,20,5)/exp(coef(m, what = \"mu\")), y0=1.3, y1=1.38)\n    \n    panel.text(x=seq(0,20,5)/exp(coef(m, what = \"mu\")), y=1.6,\n               label=seq(0,20,5), cex=.8)\n    \n    panel.text(x=1.5, y=2, label=expression(\"Number of instances of \"~italic(which)))\n    })\n\n\n\n\n\n\n\n\nFigure 5: The gamma distribution describing the variability of text-to-text occurrence rates of which in the Brown Corpus.\n\n\n\nThe gamma distribution represents a set of values, which specify the deviation of Poisson means from their overall mean in relative terms, as factors. For example, if the gamma distribution is restricted to the range [0.6; 1.7], the Poisson means will vary by a factor of 0.6 to 1.7 around their overall average. For a grand mean of 7, the Poisson means are then spread out between 7 \\(\\times\\) 0.6 = 4.2 and 7 \\(\\times\\) 1.7 = 11.9.\nThe grey vertical lines facilitate interpretation of the distribution: They show where the middle 50% of the texts (thick lines) and the middle 90% of the texts (thin lines) lie. Thus, half of the texts have an underlying expected number of occurrences between roughly 5 and 9; 90% of texts have expected counts between 2.5 and 14. This gives us a good idea of the underlying text-to-text variation in the Brown Corpus.\n\n\nGraphical derivation of the negative binomial distribution\nTo get a better understanding of the negative binomial distribution shown in Figure 4, let us now build one from scratch. Recall that the gamma distribution that is built into the negative binomial model provides us with a set of values with mean 1. We will refer to scores generated from this kind of gamma distribution as \\(\\delta\\) scores. To spread out the Poisson means, the overall mean is multiplied by the \\(\\delta\\) scores drawn from the gamma distribution. Since the \\(\\delta\\) scores are centered at 1, the overall mean is still 7. A gamma distribution that is spread out more widely produces more widely dispersed Poisson means.\nEssentially, then, a negative binomial distribution represents a batch of Poisson distributions whose individual means are spread out around the overall mean. This conceptual explanation of the negative binomial distribution illustrates the role of the gamma distribution and its auxiliary parameter \\(\\phi\\). We can translate this illustration into a simple simulation experiment. If we average over a large number of Poisson distributions produced by this procedure, we should arrive at the corresponding negative binomial distribution.\nThis is illustrated in Figure 6, which was constructed in the following way:\n\n\nDraw Figure 6\nset.seed(1985)\n\nset_nu = 2\ndelta_s = rGA(1000, mu=1, sigma=sqrt(1/set_nu))\n\nlambda_p = 7\npoisson_pool = matrix(NA, nrow=1000, ncol=21)\nfor (i in 1:1000){\n  poisson_pool[i,] = dpois(0:20, lambda=lambda_p*delta_s[i])\n}\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.45),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y){\n    panel.segments(x0=lambda_p, x1=lambda_p, y0=0, y1=.25, col=\"black\")\n    panel.text(x=7, y=.3, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_s)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", type=\"l\", alpha=.03)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", pch=16, cex=.4, alpha=.03)\n      }\n    # panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=\"white\", lwd=4)\n    # panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n    #              type=\"l\", col=\"white\", lwd=4)\n    panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n                 type=\"l\", col=\"white\", lwd=2)\n    panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=1, lty=\"22\", lineend=\"butt\")\n    })\n\nprint(p1, position=c(0,0,1,.95))\n\n\n\n\n\n\nFigure 6: Graphical derivation of the NB2 distribution: The blue curve shows the approximation based on averaging over 1,000 Poisson distributions whose means are random draws from a gamma distribution with \\(\\small{\\phi^{-1}}\\) = 0.25. The red curve shows the actual negative binomial distribution with \\(\\small{\\phi^{-1}}\\) = 0.25.\n\n\n\n\nSet the overall mean to 7, and the negative binomial dispersion parameter (here: the scale parameter) \\(\\phi^{-1}\\) to 0.5 (which is close to the value obtained for which in the Brown Corpus).\nTake 1,000 random draws from a gamma distribution defined by \\(\\mu\\) = 1 and \\(\\phi^{-1}\\) = 0.5. We refer to these draws as \\(\\delta\\) scores. The average of these scores is 1.\nMultiply 7 by these 1,000 \\(\\delta\\) scores. This produces 1,000 Poisson means, and hence 1,000 Poisson distributions.\nGraph these 1,000 Poisson distributions as pearl necklaces, adding transparency to avoid a cluttered display.\nDetermine the average probability for each count (0, 1, 2, etc.) by averaging over the 1,000 Poisson probabilities for each specific count. These averages should then resemble a negative binomial distribution with \\(\\mu\\) = 7 and \\(\\phi^{-1}\\) = 0.5.\n\nFigure Figure 6 shows the result of this simulation: The actual negative binomial distribution for these data is shown as a white trace, and the results of our simulation, i.e. average probability across the 1,000 simulated Poisson distributions, is shown as a dashed profile. The match is pretty good.\n\n\nDifferent parameterizations of the negative binomial distribution\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two ways. These different parameterizations have consequences for our interpretation of the negative binomial dispersion parameter returned by an analysis. This means that if we are interested in the dispersion parameter, we must know which parameterization our analysis is using. For an overview of which R packages/functions rely on which version of the negative binomial distribution, see this blog post\n\n\n\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {The Negative Binomial Distribution: {A} Visual Explanation},\n  date = {2023-12-12},\n  url = {https://lsoenning.github.io/posts/2023-11-16_negative_binomial/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “The Negative Binomial Distribution: A\nVisual Explanation.” December 12, 2023. https://lsoenning.github.io/posts/2023-11-16_negative_binomial/."
  },
  {
    "objectID": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "href": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "title": "Modeling the interpretation of quantifiers using beta regression",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(betareg)\nlibrary(lattice)\nlibrary(brms)\nlibrary(knitr)\nlibrary(tidybayes)\nlibrary(kableExtra)\nlibrary(sjPlot)\nlibrary(lmtest)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\ndirectory_path &lt;- \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/\"\n\n\nThe purpose of this blog post is to show how to model proportions using beta regression. This procedure is particularly suitable for outcome variables that assume values in the interval (0, 1) – but excluding 0 and 1 –, and which do not summarize underlying raw counts (e.g. .30 for 3 out of 10 “successes”), in which case they could be handled using logistic regression. Examples are proportions, rates, and other indices such as corpus-linguistic dispersion measures.\nOur illustrative data represent such a fractional variable: How speakers interpret the quantifiers few, some, many, and most on the percentage (or proportion) scale. Few, for instance, may be understood as referring to about 10% of the total. A critical feature of our data is that they do not include proportions of 0 and 1. And this is in fact a requirement of beta regression, which does not work if the outcome includes values of 0 and/or 1.\nWe will start by introducing our illustrative data, and then model them using different beta regression models.\n\nLinguistic research context\nThe need to model the perception of quantifiers arose in the context of the Bamberg Survey of Language Variation and Change (see Krug and Sell 2013), a large-scale survey on the use of various lexical and grammatical structures in varieties of English. The grammar part of the questionnaire is designed to permit estimates of the prevalence of a broad variety of (morpho-)syntactic features in two registers (speech vs. writing). To this end, respondents are asked to indicate, on a 6-point scale, how prevalent a specific feature is in their home country or region. Each sentence is presented in two modes (auditorily and in writing) and participants indicate how many speakers use this kind of structure by choosing from the following options: no-one, few, some, many, most, everyone.\nThe analysis and interpretation of these data can profit from the fact that these quantifiers can be understood as relative frequencies. When analyzing the data, we can assign sensible numeric scores to the ordered categories and then interpret the resulting quantities as approximate estimates of the prevalence of a specific feature. For more background on this approach and its measurement-theoretic and statistical drawbacks see Sönning (2024).\n\n\nExperimental work on the perception of quantifiers\nA few studies have looked at how speakers interpret the quantifiers few, some, many, and most (Newstead, Pollard, and Riezebos 1987; Borges and Sawyers 1974; Stateva et al. 2019; Tiel, Franke, and Sauerland 2021). Proportional estimates from the literature are collected in Figure 1. For more information on the corresponding studies, please refer to Sönning (2024).\n\n\n\n\n\n\nFigure 1: Literature summary: The perception of quantifiers, expressed as a percentage.\n\n\n\nWhile estimates for the individual items show some variation across studies (and experiments within studies), they allow us to roughly pin down the quantitative meaning of these expressions. If we take a weighted average across the studies, where the weight of the individual percentages is proportional to the number of subjects, we obtain the following means:\n\nfew (.11, or 11%)\nsome (.27, or 27%)\nmany (.67, or 67%)\nmost (.83, or 83%)\n\n\n\nIllustrative data: The perception of quantifiers\nI collected additional data on the perception of these expressions, using as participants university students and colleagues that took part in the English Linguistics research seminar at the University of Bamberg in the winter term of 2023. I obtained data from 20 individuals (around 2/3 being students) by handing out paper slips with the following instructions:\n\n\n\nSurvey task: Paper slips with instructions used for data collection\n\n\nParticipants were given two minutes to complete the task. I then collected the paper sheets and (later) entered the data into a spreadsheet (in wide format):\n\ndat &lt;- readxl::read_xlsx(paste0(directory_path, \"data/data_quantifiers.xlsx\"))\n\n\n\n\n\n\n\nsubject\nfew\nsome\nmany\nmost\n\n\n\n\nsubj_01\n5\n15.0\n30.0\n50.0\n\n\nsubj_02\n20\n35.0\n70.0\n80.0\n\n\nsubj_03\n10\n27.5\n65.0\n85.0\n\n\nsubj_04\n8\n20.0\n51.0\n80.0\n\n\nsubj_05\n12\n30.0\n50.0\n75.0\n\n\nsubj_06\n15\n35.0\n60.0\n90.0\n\n\nsubj_07\n25\n40.0\n67.5\n92.5\n\n\nsubj_08\n10\n30.0\n60.0\n80.0\n\n\nsubj_09\n15\n33.0\n67.0\n85.0\n\n\nsubj_10\n10\n33.0\n67.0\n90.0\n\n\nsubj_11\n10\n25.0\n50.0\n75.0\n\n\nsubj_12\n10\n40.0\n70.0\n90.0\n\n\nsubj_13\n7\n20.0\n60.0\n85.0\n\n\nsubj_14\n15\n40.0\n69.0\n90.0\n\n\nsubj_15\n15\n30.0\n75.0\n85.0\n\n\nsubj_16\n6\n14.0\n75.0\n91.0\n\n\nsubj_17\n5\n12.5\n65.0\n80.0\n\n\nsubj_18\n10\n25.0\n70.0\n90.0\n\n\nsubj_19\n15\n40.0\n75.0\n90.0\n\n\nsubj_20\n10\n20.0\n80.0\n90.0\n\n\n\n\n\n\n\n \nOur first step is to rearrange these data from wide to long form, and to translate percentages into proportions, as this is the scale on which beta regression operates. We also change the order of the quantifiers (from the default alphabetical arrangement) based on the relative frequency they express (see Figure 1).\n\nd &lt;- dat |&gt; \n  gather(\n    few:most, \n    key = quantifier, \n    value = percentage) |&gt; \n  mutate(\n    proportion = percentage/100\n  ) \n\nd$quantifier &lt;- factor(\n  d$quantifier, \n  levels = c(\"few\", \"some\", \"many\", \"most\"),\n  ordered = TRUE)\n\n\n\n\n\n\n\nFigure 2: Ratings collected from 20 informants.\n\n\n\nLet us start by looking at the distribution of the responses for each quantifier using a dot diagram. Figure 2 shows that there is some variation among subjects with regard to the perceived meaning of these expressions. For many and most, in particular, there is one unusually low estimate.\n\n\n\n\n\n\nFigure 3: Line plot linking the ratings provided by the same informant.\n\n\n\nTo check whether these outliers are responses from the same individual, we look at the data using a line plot that links observations from the same subject. In Figure 3, each profile represent a subject. The two unusually low responses for many and most are indeed due to the same individual, who also provided relatively (though not unusually) low ratings for few and some.\nApart from the fact that this respondent provides relatively low estimates for all quantifiers, there is no immediate reason why their data should be excluded from the analysis. We will therefore start by using the full data set and then rely on model diagnostics to see whether the data and model suggest that this informant be excluded from the analysis.\n\n\nBeta regression\nWe now look at how to use beta regression to model these data. First, however, let us be clear about the purpose of our analysis – i.e. the kind of information we wish to extract from the data. Our goals are descriptive and we will use beta regression to summarize the data. The following quantitative features of the data will be of interest:\n\nAveraging over the speakers in our sample, what is the typical proportional interpretation of each quantifier?\nSeeing that we are dealing with a rather small sample of speakers, what is the statistical uncertainty surrounding these typical values?\nHow much do speakers vary in their interpretation of the individual expressions, i.e. how dispersed are their perceptions around the typical interpretation?\n\nBeta regression can address these questions by providing (i) typical values (average proportions) for each quantifier (e.g. .12, or 12%, for few), (ii) a confidence (or posterior) interval for these estimates (e.g. 95% CI [.10; .14]), and (iii) a distributional summary of the dot diagrams we saw in Figure 2. To this end, the beta distribution serves as an abstraction of the observed distribution of responses and it allows us to make statements about, say, the share of speakers who interpret few as denoting a relative frequency of .05 or less. This kind of information is essential when we are interested in the stability (or consistency) of interpretations across speakers.\nOur main focus will be on how to run beta regression using R. We therefore only include a few essentials on the beta distribution and beta regression; for more background on both, please refer to this excellent blog post (Heiss 2021a).\n\n\nThe beta distribution\nThe beta distribution is a probability distribution bounded between (but excluding) 0 and 1. It is defined by two shape parameters, \\(a\\) and \\(b\\). Figure 4 shows two beta distributions with the same mean but different spreads. Note how, for each distribution, \\(a\\) is .30 (or 30%) of the sum of \\(a\\) and \\(b\\). In fact, \\(a/(a+b)\\) denotes the mean of the distribution. Further, we note that the spread of the distribution is related to the sum of \\(a\\) and \\(b\\): The greater the sum, the “tighter”, or more peaked, the beta distribution.\n\n\n\n\n\nFigure 4: The shape parameters \\(a\\) and \\(b\\) define the beta distribution.\n\n\n\n\nAs explained in much more detail by Heiss (2021a) in his blog post, the beta distribution can alternatively be written down using its mean \\(\\mu\\) and precision \\(\\phi\\). We have already encountered the mean, which is \\(a/(a+b)\\). The precision is simply \\((a+b)\\), and it is reflected in the spread of the distribution (see Figure 4): The greater the precision, the tighter the distribution of proportions about their mean. We can go back and forth between the two parameterization of the beta distribution as follows:\n\\[\\begin{align}\n\\mu &= a/(a+b) \\\\\n\\phi &= a+b \\\\\n\\\\\n\na &= \\mu\\phi \\\\\nb &= (1-\\mu)\\phi\n\\end{align}\\]\nWe need to be able to switch between these parameterizations. The reason is that regression models work with \\(\\mu\\) and \\(\\phi\\). The parameter \\(\\mu\\) answers questions (i) and (ii). To be able to draw a beta distribution, however, and use it to summarize variation in the perception of a specific quantifier in a population of speakers, we need the shape parameters \\(a\\) and \\(b\\). We therefore use code provided by Heiss (2021a) to write a function that allow us to switch from the \\(\\mu\\)-\\(\\phi\\) to the \\(a\\)-\\(b\\) parameterization:\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\n\n\nOverview of regression models\nWe will run a series of regression models, which differ in structure. We will start with frequentist models, using the {betareg} package (Cribari-Neto and Zeileis 2010) and then move on to Bayesian regression with the {brms} package (Bürkner 2017) to be able to incorporate by-subject random intercepts into our model.\nThe following table gives an overview of the series of models we are about to fit. Each model allows the outcome variable to vary by quantifier and therefore returns an estimate of \\(\\mu\\) for each expression. Model I assumes that quantifiers are interpreted with the same precision \\(\\phi\\), so it estimates only one \\(\\phi\\) parameter. This means that each quantifier has the same stability of interpretation in the populations of speakers represented by our sample. In contrast, Model II allows the precision to vary across quantifiers. This means that the model provides leeway for expressions to differ in the level of stability (or consistency) with which they are interpreted. A separate \\(\\phi\\) parameter is therefore estimated for each. This type of model has been referred to as a variable dispersion beta regression model (see Cribari-Neto and Zeileis 2010). Finally, for Model III we will switch to the {brms} package (and Bayesian inference), to be able to include random effects into our model.\n\n\n\n\n\nModel\nConstant precision\nRandom intercepts\nPackage\n\n\n\n\nI\nYes\nNo\nbetareg\n\n\nII\nNo\nNo\nbetareg\n\n\nIII\nYes\nYes\nbrms\n\n\n\n\n\n\n\n\n\nModel I\nWe start with a frequentist model (using {betareg}) that allows \\(\\mu\\) to vary across quantifiers but assumes a constant precision \\(\\phi\\). We model \\(\\mu\\) on the log-odds (i.e. logit) scale. We also drop the intercept from the model to directly obtain logits for each quantifier:\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d, \n  link = \"logit\")\n\nprint(m1)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier, data = d, link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9542         -0.9348          0.5649          1.6346  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n23.61  \n\n\nBefore we turn to the estimates returned by the model, let’s look at a number of diagnostic plots, which are shown in Figure 5:\n\nWhen graphing residuals against fitted values, we usually look for indications of heteroskedasticity, i.e. whether the spread of residuals increases with the fitted values. This is not the case for our data and model. However, the residuals surrounding the fitted values for many and most each include a large negative residual, i.e. a response that is unusually low for these quantifiers.\nThe quantile-quantile plot checks for normality of residuals. We have added a dot diagram at the right margin, which gives a more intuitive impression of the shape of the distribution and potential outliers. Both arrangements indicate that the residuals are negatively skewed (rather than normal), with two unusually large negative deviations.\nCook’s distance expresses the influence individual observations have on the regression coefficients. There are are two data points that appear to be particularly influential (number 41 and 61).\nSince the factor Subject is so far not included in the analysis, our model does not know about the fact that responses are clustered. This may induce non-independence and lead to correlated errors, because we would expect responses by the same subject to be similar. The graph shows residuals grouped by Subject. If responses were independent, the residuals should not correlate within the factor Subject. This means that residuals should not vary systematically across subjects. If correlated errors (and hence non-independence in the data) were indeed no concern, the 20 sets (of four residuals each) would look like they represent random draws from the dot diagram shown in panel (b). However, we observe that responses by the same subject in fact tend to be alike. Subjects 1, 4, and 11, for instance, have consistently negative residuals, indicating that they gave responses that were, overall, systematically lower compared to those of the other subjects.\n\n\n\nDraw figure\nm_diagnostics &lt;- d\nm_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3.6, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .1, .2, .3)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\nFigure 5: Diagnostic plots for Model I.\n\n\n\n\nOur diagnostic plots reveal two things:\n\nThere are two unusual (and influential) data points\nThe residuals are not independent but correlated with the factor Subject.\n\nIt turns out that the two unusual and influential data points are from subject 1. These are the responses for many and most, which already stood out above in Figure 3. We will remove subject 1 from the data. As for the non-independence of errors, Model III will include by-subjects random intercepts, to account for the correlation of residuals with the factor Subject.\nLet us remove subject 1, refit Model I, and again draw diagnostic plots:\n\nd_19 &lt;- subset(d, subject != \"subj_01\")\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\n\n\nDraw figure\nm_diagnostics &lt;- d_19\nm_diagnostics$subj_index &lt;- as.numeric(factor(d_19$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4.2),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .05)),\n                   x=list(at=c(1, 20, 40, 60))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 77))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:19, x1=1:19, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\nFigure 6: Diagnostic plots for Model I, after exclusion of subject 1.\n\n\n\n\nFigure 6 looks much better:\n\nThere are no clear outliers when looking at residuals by quantifier.\nThe residuals are more nearly normally distributed.\nThere are no influential data points.\n\nAs panel (d) shows, however, errors are still correlated with Subject. We will address this issue further below, in Model III.\nLet us now use our model to address our descriptive objectives. To answer question (i), we need to consider the \\(\\mu\\) coefficients. These are on the logit scale, so we need to back-transform them to proportions using the function plogis():\n\nmu_m1 &lt;- plogis(coef(m1)[1:4])\nphi_m1 &lt;- coef(m1)[5]\n\nround(mu_m1, 2)\n\n quantifierfew quantifiersome quantifiermany quantifiermost \n          0.12           0.29           0.66           0.85 \n\n\nTo answer question (ii), we obtain 95% confidence intervals for these using the function confint(), keeping in mind that we again need to translate logits back into proportions. We collect estimates in Table 1.\n\n\nConstruct table of estimates\nci_mu_m1 &lt;- plogis(confint(m1)[1:4,])\n\n\nm1_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m1), \n  paste0(\"[\", numformat(ci_mu_m1[,1]), \"; \", numformat(ci_mu_m1[,2]), \"]\"))\n\nrownames(m1_estimates) &lt;- NULL\ncolnames(m1_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm1_estimates |&gt; kable()\n\n\n\n\n\nTable 1: Typical values and 95% confidence intervals based on Model I.\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .15]\n\n\nsome\n.29\n[.25; .32]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.82; .88]\n\n\n\n\n\n\n\nThe following code uses the {sjPlot} package to print a similar table (output not shown):\n\ntab_model(m1, transform = \"plogis\")\n\nTo be able to make statements about the stability of interpretations across speakers, we need the shape parameters of the beta distributions. Consider, as an example, the quantifier few. The estimates returned by our model are the average proportion \\(\\mu =\\) 0.12 and the precision \\(\\phi =\\) 35.3. We can use the function defined above to translate these into the shape parameters \\(a\\) and \\(b\\):\n\nshape1_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape1\nshape2_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape2\n\nThese shape parameters define the beta distribution that describes the spread of responses, i.e. the variation among subjects in the relative-frequency interpretation of few. The percentages reported in Table 1 are averages over subjects. To obtain information on the distribution of responses around these typical values, we need to look at the associated beta density, which appears in Figure 7. This distribution is a model-based estimate of the variability in the responses across individuals in the population of speakers represented by the subjects in our sample. It shows us how stable (or consistent) the interpretation of few is across speakers.\n\n\n\n\n\n\nFigure 7: The beta density for few, based on model I.\n\n\n\nWe can summarize the information provided by such beta densities using informative quantiles; these can be located with the function qbeta(). A quantile is an x-value (here: a proportion/percentage) below which a certain portion of the probability mass lies. The .25 quantile, for instance, marks the x-value that cuts off the lower tail of the distribution that contains 25% of the mass.\nIn Figure 8, two intervals are marked using grey shading. The darker shade denotes the interval covering the central 50% of the subjects. It extends from the .25 quantile to the .75 quantile. The lighter shades denote the region where the middle 80% of the subjects are found. These quantiles tell us something about the speaker-to-speaker variability in the interpretation of few. While the population average is estimated at .12 (or 12%), speakers vary around this value: the central 50% of the speakers give estimates between .07 and .16, the central 80% of the population are found between .05 and .21. We will refer to these as coverage intervals.\n\n\n\n\n\nFigure 8: The beta distribution for few, including informative quantiles and coverage intervals; based on model I.\n\n\n\n\nTable 2 reports these quantiles, i.e. the central 50% and 80% coverage intervals for all quantifiers.\n\n\nConstruct table with quantiles\nbeta_quantiles_m1 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[1], shape2=shape2_m1[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[2], shape2=shape2_m1[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[3], shape2=shape2_m1[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[4], shape2=shape2_m1[4])))\n\nbeta_quantiles_m1 &lt;- cbind(c(\"few\", \"some\", \"many\", \"most\"),\n                        beta_quantiles_m1)\n\ncolnames(beta_quantiles_m1) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m1 |&gt; kable()\n\n\n\n\n\nTable 2: 50% and 80% coverage intervals, based on Model I.\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.16\n.20\n\n\nsome\n.19\n.23\n.34\n.39\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.77\n.82\n.90\n.92\n\n\n\n\n\n\n\nWe can now draw a graph based on Model I that gives a visual summary of the data: Figure 9 shows the typical values for each quantifier (average proportions), along with their 95% confidence interval and the associated beta density. Since Model I assumes constant precision across quantifiers, the visual spread of the beta distributions is constrained to be constant across these. The differences in spread that are evident from Figure 9 reflect boundary effects, as the variability of proportions is naturally constrained near the scale endpoints.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proporiton\", ylab=\"Density               \",\n  xlab.top=\"Model I\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n      type=\"l\")\n    panel.points(x=mu_m1, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=13, y1=13)\n    panel.text(x=mu_m1, y=11, label=numformat(mu_m1))\n    panel.text(x=mu_m1, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\nFigure 9: Visual summary of Model I: Typical values, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\nModel II\nNext we fit a model that allows the precision \\(\\phi\\) to vary across quantifiers; \\(\\phi\\) is now modeled on the natural log scale. We again drop the intercept from the model to directly obtain logits (\\(\\mu\\)) and natural logs (\\(\\phi\\)) for each quantifier:\n\nm2 &lt;- betareg(\n  proportion ~ -1 + quantifier | -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\nprint(m2)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier | -1 + quantifier, data = d_19, \n    link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9912         -0.9013          0.6445          1.7692  \n\nPhi coefficients (precision model with log link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n         3.903           3.251           3.464           3.781  \n\n\nThe precision parameters (Phi coefficients in the printed output) are quite similar across quantifiers. Let us therefore formally compare models I and II to see whether the added complexity is needed to adequately describe the observed data. As illustrated by Cribari-Neto and Zeileis (2010), the function lrtest(), which compares nested models using a likelihood-ratio test, may be used to this end:\n\nlrtest(m1, m2)\n\nLikelihood ratio test\n\nModel 1: proportion ~ -1 + quantifier\nModel 2: proportion ~ -1 + quantifier | -1 + quantifier\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)\n1   5 100.77                     \n2   8 102.05  3 2.5591     0.4647\n\n\nWe note that there is little evidence from the data that the quantifiers are interpreted with different precision. Instead, the variability of responses around the typical value for each quantifier appears to be rather stable (on the logit scale) across expressions.\nFor illustrative purposes, we nevertheless stick to this model to extract relevant quantities. We start by back-transforming the logit-scaled coefficients and confidence intervals for \\(\\mu\\) using the function plogis() and collect the estimates and their confidence intervals in Table 3. To summarize the beta distributions, we present informative quantiles in Table 4.\n\n\nConstruct table of estimates\nmu_m2 &lt;- plogis(coef(m2)[1:4])\nci_mu_m2 &lt;- plogis(confint(m2)[1:4,])\nphi_m2 &lt;- exp(coef(m2)[5:8])\n\nshape1_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape1\nshape2_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape2\n\nm2_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m2), \n  paste0(\"[\", numformat(ci_mu_m2[,1]), \"; \", numformat(ci_mu_m2[,2]), \"]\"))\n\nrownames(m2_estimates) &lt;- NULL\ncolnames(m2_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm2_estimates |&gt; kable()\n\n\n\n\n\nTable 3: Typical values and 95% confidence intervals based on Model II.\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .14]\n\n\nsome\n.29\n[.25; .33]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.83; .88]\n\n\n\n\n\n\n\n\n\nConstruct table of quantiles\nbeta_quantiles_m2 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[1], shape2=shape2_m2[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[2], shape2=shape2_m2[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[3], shape2=shape2_m2[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[4], shape2=shape2_m2[4])))\n\nbeta_quantiles_m2 &lt;- cbind(\n  c(\"few\", \"some\", \"many\", \"most\"),\n  beta_quantiles_m2)\n\ncolnames(beta_quantiles_m2) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m2 |&gt; kable()\n\n\n\n\n\nTable 4: 50% and 80% coverage intervals, based on Model II.\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.07\n.09\n.15\n.18\n\n\nsome\n.18\n.23\n.35\n.41\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.78\n.82\n.89\n.92\n\n\n\n\n\n\n\nA visual summary of Model II appears in Figure 10. We observe that the beta density for few is more peaked than in Figure 9. This information can also be read from the printed regression table, which shows that few has the highest precision. This is also reflected in the coverage intervals. Whereas in Model I, the 80% coverage intervals extended from .06 to .20, Model II suggests a slightly narrower 80% coverage interval, ranging from .07 to .18.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model II\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n      type=\"l\")\n    panel.points(x=mu_m2, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=13, y1=13)\n    panel.text(x=mu_m2, y=11, label=numformat(mu_m2))\n    panel.text(x=mu_m2, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\nFigure 10: Visual summary of Model II: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\nModel III\nTo address the issue of correlated errors (i.e. the non-independence of observations), we now add by-speaker random intercepts to our model. Since this feature is not implemented in the {betareg} package, we switch to Bayesian regression using the {brms} package. As we will now be explicitly modeling Speaker as a source of variation, let us give subject 1 a second chance. After all, the primary reason for excluding them from our analysis were the unusually low values given for many and most. We will then again look at diagnostic plots to see whether the data points from this speaker also appear problematic in the context of Model III.\nAs can be seen from the following R code, using {brms} requires a change in model syntax. We will rely on the default priors.\n\nm3 &lt;- brm(\n  proportion ~ -1 + quantifier + (1|subject),\n  data = d,\n  family = Beta(),\n  chains = 4, iter = 3000, warmup = 1000, cores = 4,\n  backend = \"cmdstanr\",\n  file = paste0(directory_path, \"m3\")\n)\n\nm3 &lt;- readRDS(paste0(directory_path, \"m3.rds\"))\n\nLet us first draw some diagnostic plots. Since we are dealing with a Bayesian regression model, some of the quantities we use for plotting are different. Their meaning and interpretation, however, is very similar:\n\nInstead of weighted residuals, we look at Pearson residuals.\nWe will use Pareto k values as influence indicators (for more information on this, see this paper and this video tutorial). For these, thresholds have been proposed to signal “good”, “ok”, “bad”, and “very bad” Pareto k values.\nSince correlated errors (within subject) should no longer be a concern, we will instead group influence measures (i.e. Pareto k values) by subject, to see if there are any influential individuals.\n\nThe diagnostic plots appear in Figure 11. We note the following:\n\nThere appear to be no outliers when looking at residuals by quantifier. This is because the random intercept for subject 1 now accomodates their unusually low responses.\nThe distribution of the Pearson residuals looks OK.\nThere is only one “bad” data point.\nThis “bad” observation stems from subject 16, whose responses sit less well with the model, on average. It turns out that the responses of subject 16 were drawn to the extremes: They gave relatively low responses to few (6%) and some (14%), but quite high responses to many (75%) and most (91%). This tendency towards the extremes is not captures by the random intercept for subject 16, since this parameter only represents their average response (46.5%), which is not particularly unusual.\n\nOverall, then, we are happy with Model III and welcome subject 1 back into the dataset.\n\n\nDraw figure\nloo_m3 &lt;- loo(m3)\nloo_m3_pointwise &lt;- data.frame(loo_m3$pointwise)\n\nm3_diagnostics &lt;- d\nm3_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm3_diagnostics$fitted_with_random_intercepts &lt;- qlogis(fitted(m3)[,1])\nm3_diagnostics$residual &lt;- residuals(m3, type = \"pearson\")[,1]\nm3_diagnostics$influence_pareto_k &lt;- loo_m3_pointwise$influence_pareto_k\nm3_diagnostics &lt;- m3_diagnostics |&gt; \n  group_by(quantifier) |&gt; \n  mutate(\n    fitted_quantifier = mean(fitted_with_random_intercepts))\n\n\np1 &lt;- xyplot(residual ~ fitted_quantifier, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m3_diagnostics) - 1/2)/nrow(m3_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m3_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m3_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-2, ybottom=-3,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(influence_pareto_k ~ 1:nrow(m3_diagnostics), data=m3_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.text(x=-5, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n       })\n\nsubj_pareto_k &lt;- m3_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_pareto_k = min(influence_pareto_k),\n    max_pareto_k = max(influence_pareto_k)\n  )\n\np4 &lt;- xyplot(influence_pareto_k ~ subj_index, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(d) Influence by subject\\n\",\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.text(x=-1.35, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, \n                        y0=subj_pareto_k$min_pareto_k, \n                        y1=subj_pareto_k$max_pareto_k)\n       })\n\n\n\n\n\n\n\nFigure 11: Diagnostic plots for Model III.\n\n\n\n\nWe now use Model III to address questions (i) and (ii). As described in more detail in the Marginal Effects Zoo book and in another fantastic blog post by Heiss (2021b), we use the {tidybayes} package to process the posterior distribution.\nWe use the model to generate posterior predictions and then summarize these. The kind of prediction we are interested in at the moment (for questions i and ii) are typical values, i.e. averages. These are sometimes referred to as expected values. We therefore use the function epred_draws(), which uses the full posterior distribution to generate expected predictions (epred) for conditions, i.e. (constellations of) predictor values. We need to specify the conditions of interest using the argument newdata. The additional argument re_formula = NA tells epred_draws() to disregard the by-subject random intercepts in this predictive task. After all, we are presently only concerned with the average across subjects.\n\nm3_epred &lt;- m3 |&gt;  \n  epred_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\nWe then summarize the posterior distribution of these expected values using the median and the .025 and .975 quantiles, which provide uncertainty estimtes similar to the 95% CIs reported for the other models.\n\nmu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    median = median(.epred)\n  )\n\nci_mu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    ci_lower = quantile(.epred, .025),\n    ci_upper = quantile(.epred, .975)\n  )\n\nWe collect these point and interval estimates for the expected value (i.e. proportions) of each quantifier in Table 5.\n\n\nConstruct table of estimates\nm3_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m3$median), \n  paste0(\"[\", numformat(ci_mu_m3$ci_lower), \"; \", numformat(ci_mu_m3$ci_upper), \"]\"))\n\nm3_estimates |&gt; kable()\n\n\n\n\n\nTable 5: Typical values and 95% posterior intervals based on Model III.\n\n\nQuantifier\n\n\n\n\n\n\nfew\n.11\n[.09; .14]\n\n\nsome\n.27\n[.23; .32]\n\n\nmany\n.64\n[.59; .69]\n\n\nmost\n.84\n[.81; .87]\n\n\n\n\n\n\n\nOur next task is to obtain estimates for the variability of subjects’ perceptions around these typical values. To this end, we also use posterior predictions from the model. We are no longer interested in the expected (or typical) value, however, but in the specific responses given by the speakers. We therefore use the function predicted_draws() which retains this variability in the distribution of responses.\nWe must make sure, however, that the variability among subjects, which is captured by the random intercepts in our model, is worked into these predictions. Thus, in a model with random intercepts, the extraction of information about the beta distribution describing the variability of interpretations across subjects requires some care. This is because between-subjects variation is now absorbed by this model component. If we fail to actively incorporate this source of variation into our model-based predictions, these do not answer question iii, which asks about the variation across subjects in the interpretation of quantifiers.\nFor purposes of illustration, let us first (inappropriately) disregard the by-speaker random effects. This means that we use the expected predictions generated above to construct beta distributions. This yields Figure 12, which does not show beta densities with the intended meaning. Recall that our question (iii) concerned variability across speakers. In Model III, between-subjects variation is captured by the random intercept SD (or variance) in our model, and this parameter is not built into the graph. As a result, the beta distribution are not spread out enough.\n\nm3_pred_wrong &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=.5, y=7, label=\"Warning: Beta densities do not\\nhave the intended meaning\", \n               col=\"darkred\", alpha=.7, cex=.9, lineheight=.9)\n    })\n\n\n\n\n\n\n\nFigure 12: Wrong visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\n\n\n\nFigure 13: Distribution of by-speaker averages on the proportion scale.\n\n\n\nBefore we go further, it is perhaps worth considering what kind of information the densities in Figure 12 do show. They show the “residual” variation in perceived interpretation for each quantifier, after removing the model component that describes the between-speaker variability in the overall average response. Thus, the by-speaker random intercepts capture the extent to which subjects vary in their average response. To get a better understanding of the kind of variability represented by the random intercepts, Figure 13 shows the distribution of speaker averages. Most speakers vary between roughly .40 and .55. It is this kind of between-speaker variation that is captured by the random intercepts in the model.\n\n\nConstruct wrong table of quantiles\nbeta_quantiles_m3_wrong &lt;- m3_pred_wrong |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3_wrong |&gt; kable()\n\n\n\n\n\nTable 6: 50% and 80% coverage intervals, based on Model III (random intercept variation not included).\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.14\n.18\n\n\nsome\n.19\n.23\n.32\n.36\n\n\nmany\n.55\n.60\n.69\n.73\n\n\nmost\n.77\n.81\n.88\n.91\n\n\n\n\n\n\n\nAfter removing this source of variation, responses will still vary, due to other sources of variation. One such source is measurement error – speakers’ response to our short survey may fluctuate (perhaps erratically) over time. If we handed the same paper slip to our 20 respondents one year later, they would probably give (slightly) different responses. Further, speakers may not only differ in their average response (as captured by the random intercepts), but also in their perception of the individual quantifiers. For instance, a speaker may have a systematically lower perception of the quantifier few compared to other speakers. In statistical terms, this would be referred to as a subject-by-quantifier interaction.\nTable 6 shows 50% and 80% coverage intervals for the beta distributions that do not contain the between-speaker variation.\nTo get beta densities with the intended meaning, we need to factor in the random effect. This means that we form model-based predictions that also incorporate (or represent) between-subjects variability. This is sometimes referred to as “integrating out” the random effects. As explained in more detail in the Marginal Effects Zoo book and in this blog post by Heiss (2021b), this is done by sampling from a normal distribution representing the spread of random intercepts and factoring these deviations into the posterior predicted distribution.\nTable 7 shows 50% and 80% coverage intervals for the beta distributions that now contain the between-speaker variation. A visual summary of Model III with the appropriate densities appears in Figure 14.\n\n\nConstruct table of quantiles\nm3_pred &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier),\n      subject = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n\nbeta_quantiles_m3 &lt;- m3_pred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3 |&gt; kable()\n\n\n\n\n\nTable 7: 50% and 80% coverage intervals, based on Model III (random intercept variation included).\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.05\n.07\n.16\n.21\n\n\nsome\n.15\n.20\n.35\n.43\n\n\nmany\n.48\n.56\n.72\n.79\n\n\nmost\n.72\n.79\n.89\n.93\n\n\n\n\n\n\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\nFigure 14: Visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\nComparison of estimates\nLet us finally compare the quantities of interest across our models. Figure 15 (a) shows estimates for the typical relative-frequency interpretation of the expressions. Difference between models are minor, but Model III returns slightly wider uncertainty intervals. This is mainly because the model has been informed that the sample size is 20 (rather than 80), but also because the model has (re)incorporated subject 1.\nIn panel (b), the 50% and 80% coverage intervals are shown. Two comparisons are particularly informative:\n\nComparing models I and II, we note the effect of allowing precision to vary across quantifiers. For few, Model II returns tighter coverage intervals, since this expression is estimated to show a higher level of precision, i.e. a more stable interpretation across subjects. For many, the situation is the other way around. The differences between models I and II are minor, however, which is consistent with the results of the likelihood-ratio test reported above.\nA comparison of the grey and red coverage intervals for Model III is also revealing: The red intervals are the ones that do not incorporate between-subject variability (as captured by the random intercepts). They therefore fail to answer question iii. The grey bands, on the other hand, do factor this source of variation into the predictions, and they therefore show us what we were looking for.\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2, 4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(a) Typical values + uncertainty\\n\",\n  panel=function(x,y){\n    panel.points(x=mu_m1, y=3, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=3, y1=3)\n\n    panel.points(x=mu_m2, y=2, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=2, y1=2)\n\n    panel.points(x=mu_m3$median, y=1, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=1, y1=1)\n\n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=-.3, y=3:1, label=c(\"Model I\", \"Model II\", \"Model III\"), adj=0)\n    })\n\n\nbeta_quantiles_m3 &lt;- data.frame(beta_quantiles_m3)\nbeta_quantiles_m3_wrong &lt;- data.frame(beta_quantiles_m3_wrong)\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2,4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(b) 50% and 80% coverage intervals\\n\",\n  panel=function(x,y){\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,2]), x1=as.numeric(beta_quantiles_m1[,5]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,3]), x1=as.numeric(beta_quantiles_m1[,4]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m2[,2]), x1=as.numeric(beta_quantiles_m2[,5]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m2[,3]), x1=as.numeric(beta_quantiles_m2[,4]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3[,2]), x1=as.numeric(beta_quantiles_m3[,5]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m3[,3]), x1=as.numeric(beta_quantiles_m3[,4]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,2]), x1=as.numeric(beta_quantiles_m3_wrong[,5]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,3]), x1=as.numeric(beta_quantiles_m3_wrong[,4]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    \n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n\n    })\n\n\n\n\n\n\n\nFigure 15: Comparison of quantities across models: (a) typical values and their uncertainty, and (b) coverage intervals.\n\n\n\n\n\n\nConclusion\nIn this blog post, we have looked at how to model proportional data using beta regression. Our focus was on the typical (average) interpretation speakers assign to quantifiers and its statistical uncertainty. Apart from this, we considered how to use and summarize beta densities to obtain information about the variability of interpretations (or, more specifically, responses) across subjects to quantify the stability of interpretations in the population represented by our sample of informants. We also looked at how to use model diagnostics for a frequentist and a mixed-effects Bayesian regression model to detect problematic data points and/or speakers.\n\n\n\n\n\nReferences\n\nBorges, Marilyn A., and Barbara K. Sawyers. 1974. “Common Verbal Quantifiers: Usage and Interpretation.” Journal of Experimental Psychology 102 (2): 335–38. https://doi.org/10.1037/h0036023.\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCribari-Neto, Francisco, and Achim Zeileis. 2010. “Beta Regression in R.” Journal of Statistical Software 34 (2): 1–24. https://doi.org/10.18637/jss.v034.i02.\n\n\nHeiss, Andrew. 2021a. “A Guide to Modeling Proportions with Bayesian Beta and Zero-Inflated Beta Regression Models.” November 8, 2021. https://doi.org/10.59350/7p1a4-0tw75.\n\n\n———. 2021b. “A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel Bayesian Models.” November 10, 2021. https://doi.org/10.59350/wbn93-edb02.\n\n\nKrug, Manfred, and Katrin Sell. 2013. “Designing and Conducting Interviews and Questionnaires.” In Research Methods in Language Variation and Change, edited by Manfred Krug and Julia Schlüter, 69–98. Cambridge University Press.\n\n\nNewstead, Stephen E., Paul Pollard, and D. Riezebos. 1987. “The Effect of Set Size on the Interpretation of Quantifiers Used in Rating Scales.” Applied Ergonomics 18 (3): 178–82. https://doi.org/10.1016/0003-6870(87)90001-9.\n\n\nSönning, Lukas. 2024. “Ordinal Rating Scales: Psychometric Grounding for Design and Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/jhv6b.\n\n\nStateva, Penka, Arthur Stepanov, Viviane Déprez, Ludivine Emma Dupuy, and Anne Colette Reboul. 2019. “Cross-Linguistic Variation in the Meaning of Quantifiers: Implications for Pragmatic Enrichment.” Frontiers in Psychology 10: 957. https://doi.org/10.3389/fpsyg.2019.00957.\n\n\nTiel, Bob van, Michael Franke, and Uli Sauerland. 2021. “Probabilistic Pragmatics Explains Gradience and Focality in Natural Language Quantification.” Proceedings of the National Academy of Sciences 118 (9). https://doi.org/10.1073/pnas.2005453118.\n\nCitationBibTeX citation:@online{sönning2024,\n  author = {Sönning, Lukas},\n  title = {Modeling the Interpretation of Quantifiers Using Beta\n    Regression},\n  date = {2024-02-29},\n  url = {https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2024. “Modeling the Interpretation of Quantifiers\nUsing Beta Regression.” February 29, 2024. https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/."
  },
  {
    "objectID": "posts/2024-01-07_modified_lobanov/index.html",
    "href": "posts/2024-01-07_modified_lobanov/index.html",
    "title": "Vowel normalization using modified Lobanov",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by Burch, Egbert, and Biber (2017) as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to Wilcox (1973), a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While Wilcox (1973) focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as Burch, Egbert, and Biber (2017, 193) note, a measure equivalent to DP (Gries 2008) can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in Wilcox (1973) as the mean difference analog (MDA). Both Wilcox (1973) and Burch, Egbert, and Biber (2017) argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in Sönning (2023).\nGries (2020, 116) has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula Wilcox (1973) gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in Wilcox (1973).\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to Sönning (2023). We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 0.86 seconds to run. The computational formula is quicker – it completes the calculations in only 0 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 20.64 seconds to run; the shortcut procedure, on the other hand, is done after 0 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n0.86\n20.64\n0.6003\n0.6139\n\n\nComputational\n0.00\n0.00\n0.6005\n0.6140\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nSönning, Lukas. 2023. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” PsyArxiv Preprint. https://doi.org/10.31234/osf.io/ns4q9.\n\n\nWilcox, Allen R. 1973. “Indices of Qualitative Variation and Political Measurement.” The Western Political Quarterly 26 (2): 325–43. https://doi.org/10.2307/446831.\n\nCitationBibTeX citation:@online{sönning2024,\n  author = {Sönning, Lukas},\n  title = {Vowel Normalization Using Modified {Lobanov}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2024. “Vowel Normalization Using Modified Lobanov\n.” January 7, 2024. https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/."
  },
  {
    "objectID": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "href": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "title": "Vowel normalization using modified Lobanov",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by @Burch_etal2017 as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to @Wilcox1973, a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While @Wilcox1973 focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as @Burch_etal2017 [p. 193] note, a measure equivalent to DP [@Gries2008] can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in @Wilcox1973 as the mean difference analog (MDA). Both @Wilcox1973 and @Burch_etal2017 argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in @Soenning2023.\n@Gries2020 [p. 116] has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula @Wilcox1973 gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in @Wilcox1973.\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to @Soenning2023. We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.58 seconds to run. The computational formula is quicker – it completes the calculations in only 0.01 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 35.81 seconds to run; the shortcut procedure, on the other hand, is done after 0.02 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n1.58\n35.81\n0.6003\n0.6139\n\n\nComputational\n0.01\n0.02\n0.6005\n0.6140\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {Vowel Normalization Using Modified {Lobanov}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Vowel Normalization Using Modified Lobanov .” 2024.\nJanuary 7, 2024. https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/."
  }
]