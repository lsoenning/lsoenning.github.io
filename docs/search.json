[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Sönning",
    "section": "",
    "text": "Post-doc in English linguistics at the University of Bamberg\n\nUniversity of Bamberg\nDepartment of English Linguistics\n\n\n\nContact\n\nAddress: An der Universität 9, D-96047 Bamberg\nOffice: U9/00.10\nPhone: +49 (0)951/863-2267\nEmail: lukas[dot]soenning[at]uni-bamberg[dot]de\n ORCID 0000-0002-2705-395X\n Google scholar\n Github\n OSF\n\n\n\nResearch interests\n\nStatistical analysis of language data\nCorpus linguistics\nLanguage variation and change\nData visualization\nGerman Learner English\nL2 phonology"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Lukas Sönning\nI am a post-doctoral researcher at the Chair of English Linguistics, University of Bamberg (Germany). Following my PhD project, which looked at phonological features in German Learner English, my interest shifted to statistical aspects of corpus-linguistic methodology. I have worked on topics such as keyness analysis, dispersion, and down-sampling, and my habilitation (post-doc) project concentrates on the linguistically grounded use of mixed-effects models in variationist corpus research. I have made an active effort to promote open-science practices and have a passion for data visualization. Currently, I am also involved in a DFG-funded project on the analysis of high-dimensional survey data drawn from the BSLVC (Bamberg Survey of Language Variation and Change).\n\nShort academic CV\n\n2012–present Research and teaching assistant at the University of Bamberg\n2020 awarded PhD\n2006–2012 Studies in English, Geography and Pedagogy at the University of Bamberg\n\n\n\nAwards\n\n2014 Best poster award, Olinco conference, Olomouc (“Vowel reduction in German Learner English: Developmental patterns”)\n2018 Best paper by an early career researcher, ICAME39, Tampere (John Sinclair bursary) (“Visual inference for corpus linguistics”)\n\n\n\nTeaching\nUniversity courses\n\nForming (new) words: The morphological architecture of English\nApplied data analysis for linguists\nInvestigating Learner English\nSecond language speech: Theory and practice\nMeasuring (your) foreign accent: The acoustic analysis of non-native speech\nEnglish phonetics & phonology\nEnglish grammar analysis\nTranslation English-German (intermediate and advanced level)\nRevision course for state exam candidates: Synchronic linguistics\nRevision course for state exam candidates: Translation English-German\n\nWorkshops\n\n2025 (University of Cologne) Dispersion analysis in corpus-based research: Purpose, problems, practice  OSF | slides part 1 | slides part 2 | slides part 3\n2025 (TU Dresden) Deskriptive und inferentielle Statistik für Korpusdaten  OSF | slides Teil 1 | slides Teil 2 | slides Teil 3\n2025 (Ghent University, Belgium) Random Forests in language data analysis: Tutorial  OSF | slides\n2024 (Ghent University, Belgium) Tutorial: Latent-variable modeling of ordinal data  OSF | slides part I | slides part II | slides part III\n2024 (FJUEL conference, Bamberg) Dynamic documents in R: Introduction to Quarto\n\nslides session 1 | slides session 2 | slides session 3\npractice 1 | practice 2 | practice 3\n\n2024 (University of Bamberg) RStudio crash course for PhD students | slides session 1 | slides session 2\n2023 (META-LING conference, Bamberg) Data publication using TROLLing  OSF\n2023 (University of Würzburg) Basics of data analysis using R and RStudio  OSF | slides part 1 | slides part 3\n2022 (University of Würzburg) Basic statistical methods for TEFL research  OSF\n2019 (FJUEL conference, Bayreuth): Using “statistics” to learn about language: What matters (and what doesn’t)  OSF\n2019 (BICLCE conference, Bamberg): The replication crisis in science: Challenges and chances for linguistics  OSF\n2018 (Uppsala University, Sweden) Statistical inference using estimation: Methods for corpus linguistics slides\n2014 (EmMeth conference, Bamberg): Data visualization with R\n2014 (FJUEL conference, Bamberg): Workshop on statistical methods\n\n\n\nTalks\n\n2025 Case-control down-sampling in corpus research. CL2025, Birmingham, UK.\n2025 Raising the bar: An analysis of bar chart usage in corpus linguistics. CL2025, Birmingham, UK.\n2025 Per corpora et diagrammata ad astra: Data visualization in corpus linguistics. Plenary at ICAME 46, Vilnius, Lithuania.\n2024 The morpho-syntax of Scottish Standard English: Questionnaire-based insights. BICLCE 10, Alicante, Spain. (with Ole Schützler and Manfred Krug)\n2024 Down-sampling strategies in corpus phonology. BICLCE 10, Alicante, Spain.\n2024 Ordinal response scales: Psychometric grounding for design and analysis. BICLCE 10, Alicante, Spain.\n2024 Sensitivity of dispersion measures to distributional patterns and corpus design. ICAME45, Vigo, Spain. (with Jesse Egbert)\n2024 Regression and random forests: Synergies for variationist corpus research. ICAME45, Vigo, Spain. (with Jason Grafmiller and Raquel Romasanta)\n2023 Down-sampling from hierarchically structured corpus data. FJUEL 11, Erlangen, Germany.\n2023 Text-level measures of lexical dispersion: Robustness analysis. CL2023. Lancaster, UK.\n2023 Down-sampling from hierarchically structured corpus data: The case of third-person verb inflection in Early Modern English. CL2023. Lancaster, UK.\n2022 Seeing the wood for the trees: Predictive margins for random forests. ICAME 43, London, UK. (with Jason Grafmiller)\n2022 Keyword analysis: Progress through regression. ICAME 43, London, UK.\n2019 The English comparative alternation revisited: A fresh look at theory and data. ICAME 40, Neuchatel, Switzerland. (with Stefan Hartmann)\n2018 Frequency effects in the English comparative alternation: A reassessment. ISLE5, London, UK.\n2018 A normalization procedure for auditory vowel descriptions: Method and application. ISLE5, London, UK. (with Ole Schützler)\n2018 Drawing on principles of perception: The line plot. ICAME34, Tampere, Finland.\n2018 Visual inference for corpus data analysis: Dot plots of effect sizes with confidence intervals. ICAME34, Tampere, Finland.\n2018 A sociolinguistic study of actually in current spoken British English. ICAME34, Tampere, Finland.\n2017 (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. BICLCE 7, Vigo, Spain.\n2015 Developmental patterns in German Learner English: Vowel reduction and speech rhythm. Research seminar, University of Münster, Germany.\n2015 Methods for corpus data analysis: Dot plots of effect sizes with confidence intervals. Methods and Linguistic Theories (MaLT), Bamberg, Germany.\n2014 Vowel reduction in German Learner English: Developmental patterns. 47th Meeting of the Societas Linguistica Europaea (SLE 2014), Poznan, Poland.\n2014 Normalverteilungsannahme und Ausreißerwerte: Nachteile klassischer Statistik und robuste Alternativen. IV. Diskussionsforum Linguistik in Bayern. Munich, Germany.\n2014 [Poster] Vowel reduction in German Learner English: Developmental patterns. Olomouc Linguistics Colloquium (Olinco), Olomouc, Czeck Republic.\n2014 Vowel reduction in German Learner English. Research seminar, University of Münster, Germany.\n2014 The dot plot: A fine tool for data visualization. Advances in Visual Methods for Linguistics (AVML) 2014. Tübingen, Germany.\n2013 An acoustic analysis of unstressed vowels in German Learner English. Accents 2013, Łódź, Poland.\n2013 Vowel reduction in German Learner English. FJUEL 3, Regensburg, Germany.\n2013 Scrabble yourself to success: Methods in teaching transcription. Phonetics Teaching and Learning Conference (PTLC) 2013, London, UK."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Unpublished\n\nSönning, Lukas. (forthcoming). Case-control down-sampling in corpus research. Corpus Linguistics and Linguistic Theory Submitted manuscript | Data |  OSF\nSönning, Lukas & Jesse Egbert. (forthcoming). Sensitivity of dispersion measures to distributional patterns and corpus design. International Journal of Corpus Linguistics Submitted manuscript |  OSF\nSönning, Lukas. (forthcoming). An analysis of bar chart usage in corpus data visualization. ICAME Journal Submitted manuscript | Data |  OSF\nSönning, Lukas & Timo Roettger. (forthcoming). Reproducibility, replication, and preregistration. In Stefan Hartmann & Laura J. Speed (eds.), Handbook of language and cognition. Bloomsbury. Submitted manuscript |  OSF\nSönning, Lukas. (forthcoming). Down-sampling strategies in corpus phonology. Chapter for: Philipp Meer & Ulrike Gut (eds.) English corpus phonetics and phonology: Current approaches and future directions Submitted manuscript | Data |  OSF\nSönning, Lukas. (forthcoming). Count regression models for keyness analysis. Chapter for: Carolin Cholotta & Christine Renker (eds.), META-LING 2023 - Methodological Exploration and Technological Advances in Linguistics Accepted manuscript |  OSF\nSönning, Lukas. (forthcoming). Juggling internal and external factors when modeling natural language data.\nSönning, Lukas. (under review). Dispersion analysis. In Hilary Nesi & Petar Milin (eds.), International Encyclopedia of Language and Linguistics, 3rd ed. Amsterdam: Elsevier. Submitted manuscript |  OSF\nSchützler, Ole, Lukas Sönning, Fabian Vetter & Manfred Krug. (under review). The morpho-syntax of Standard Scottish English: Questionnaire-based insights. Submitted manuscript |  OSF\nMühlbauer, Michael & Lukas Sönning. (under review). Ordinal random forests in language data analysis. Submitted manuscript | Data\nSönning, Lukas. (under review). Random forests in corpus research: A systematic review. Submitted manuscript | Data |  OSF\nSönning, Lukas. (in preparation). Distributional latent-variable modeling of ordinal outcomes in language data analysis.\nSönning, Lukas. (unpublished manuscript). Evaluation of text-level measures of lexical dispersion: Robustness and consistency.  Working paper |  Data |  OSF\nSönning, Lukas, Jason Grafmiller & Raquel Romasanta. (unpublished manuscript). Regression and random forests: Synergies for variationist corpus research. Working paper |  OSF\n\n \n\n\nPublished\nMonograph\n\nSönning, Lukas. 2020. Phonological variation in German Learner English. University of Bamberg dissertation. doi: 10.20378/irb-49135 |  Open access |  Datasets |  OSF\n\nJournal articles\n\nSönning, Lukas. 2025. Advancing our understanding of dispersion measures in corpus research. Corpora 20(1). 3–35. doi: 10.3366/cor.2025.0326 |  Open access |  Data |  OSF\nSönning, Lukas. 2024. Ordinal response scales: Psychometric grounding for design and analysis. Research Methods in Applied Linguistics 3(3). 100156. doi: 10.1016/j.rmal.2024.100156 |  Open access |  Data |  OSF\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht & Paul Messer. 2024. Latent-variable modelling of ordinal outcomes in language data analysis. Journal of Quantitative Linguistics 31(2). 77–106. doi: 10.1080/09296174.2024.2329448 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Down-sampling from hierarchically structured corpus data. International Journal of Corpus Linguistics 29(4). 507–533. doi: 10.1075/ijcl.23079.son |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Evaluation of keyness metrics: Performance and reliability. Corpus Linguistics and Linguistic Theory 20(2). 263–288. doi: 10.1515/cllt-2022-0116 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Jason Grafmiller. 2024. Seeing the wood for the trees: Predictive margins for random forests. Corpus Linguistics and Linguistic Theory 20(1). 153–181. doi: 10.1515/cllt-2022-0083 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Valentin Werner. 2021. The replication crisis, scientific revolutions, and linguistics. Linguistics 59(5). 1179–1206. doi: 10.1515/ling-2019-0045 |  Open access\nSönning, Lukas. 2014. Unstressed vowels in German Learner English: An instrumental study. Research in Language 12(2). 163–173. doi: 10.2478/rela-2014-0001 |  Open access |  OSF\n\nEdited volumes\n\nSönning, Lukas & Ole Schützler (eds.). 2023. Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22\nSönning, Lukas & Valentin Werner. 2021. The replication crisis: Implications for linguistics. Special issue in Linguistics.  Open access\nChrist, Hanna, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.). 2016. A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium. Bamberg: University of Bamberg Press.  Open access\n\nBook chapters\n\nSönning, Lukas. 2023. Drawing on principles of perception: The line plot. In Lukas Sönning & Ole Schützler (eds.), Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22-2 |  Preprint |  OSF\nSönning, Lukas. 2023. (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. In Robert Fuchs (ed.), Speech rhythm in learner and second language varieties of English, 123–157. Singapore: Springer. doi: 10.1007/978-981-19-8940-7_6 |  Preprint |  Data |  OSF\nSönning, Lukas & Manfred Krug. 2022. Comparing study designs and down-sampling strategies in corpus analysis: The importance of speaker metadata in the BNCs of 1994 and 2014. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 127–159. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.006 |  Published version |  Data |  OSF\nSönning, Lukas & Julia Schlüter. 2022. Comparing standard reference corpora and Google Books Ngrams: Strengths, limitations and synergies in the contrastive study of variable h- in British and American English. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 17–45. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.002 |  Published version |  OSF\nKrug, Manfred & Lukas Sönning. 2018. Language change in Maltese English: The influence of age and parental languages. In: Patrizia Paggio & Albert Gatt (eds.), The languages of Malta, 247–270. Berlin: Language Science Press. doi: 10.5281/zenodo.1181801 |  Open access\n\nProceedings\n\nSönning, Lukas. 2016. The dot plot: A graphical tool for data analysis and presentation. In Hanna Christ, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.), A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium, 101–129. Bamberg: University of Bamberg Press.  Open access\nSönning, Lukas. 2014. Developmental patterns in the reduction of unstressed vowels by German learners of English. In Ludmila Veselovská & Markéta Janebová (eds.), Complex visibles out there: Proceedings of the Olomouc Linguistics Colloquium 2014: Language use and linguistic structure, vol. 4 Olomouc modern language series, 765–778. Olomouc: Palacký University.  Open access |  OSF\nSönning, Lukas. 2013. Scrabble yourself to success: Methods in teaching transcription. In Joanna Przedlacka, John Maidment & Michael Ashby (eds.), Proceedings of the Phonetics Teaching and Learning Conference, UCL, London, 8-10 August 2013. London: Phonetics Teaching and Learning Conference, 87–90.  Open access |  OSF |  Data\n\nSoftware\n\nSönning, Lukas. 2025. tlda: Tools for language data analysis. R package version 0.1.0. doi: 10.32614/CRAN.package.tlda |  Github\n\nVignette on dispersion analysis using the tlda package\n\n\n \n\n\nDatasets\n\nSönning, Lukas. 2025. Background data for: An analysis of bar chart usage in corpus data visualization. https://doi.org/10.18710/ILJY4U, DataverseNO, V1.\nSönning, Lukas. 2025. Background data for: Down-sampling strategies in corpus phonology. https://doi.org/10.5281/zenodo.17877929. Zenodo, V1.\nKrug, Manfred, Fabian Vetter & Lukas Sönning. 2025. Background data for: Ordinal random forests in language data analysis. https://doi.org/10.5281/zenodo.18020809. Zenodo, V2.\nSönning, Lukas. 2025. Background data for: Random forests in corpus research: A systematic review. https://doi.org/10.5281/zenodo.17755086. Zenodo, V1.\nSönning, Lukas. 2025. Background data for: Case-control down-sampling in corpus research. https://doi.org/10.5281/zenodo.17877420. Zenodo, V2.\nBraun, Albert & Lukas Sönning. 2025. The comparative and superlative alternation in 1960s and 1970s written British English: Data from Braun (1982). https://doi.org/10.18710/4Z7OJG, DataverseNO, V1.\nSönning, Lukas. 2025. Biber et al.’s (2016) set of 150 BNC items for the analysis of dispersion measures: Dataset for “Evaluation of text-level measures of lexical dispersion. https://doi.org/10.18710/ATCQZW, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Advancing our understanding of dispersion measures in corpus research. https://doi.org/10.18710/FVHTFM, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Some obstacles to replication in corpus linguistics. https://doi.org/10.18710/7LNWJX, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Ordinal response scales: Psychometric grounding for design and analysis, https://doi.org/10.18710/0VLSLW, DataverseNO, V1.\nKrug, Manfred, Fabian Vetter & Lukas Sönning. 2024. Background data for: Latent-variable modeling of ordinal outcomes in language data analysis. https://doi.org/10.18710/WI9TEH, DataverseNO, V1.\nSönning, Lukas. 2023. Background data (adapted from Jenset & McGillivray 2017) for: Down-sampling from hierarchically structured corpus data, https://doi.org/10.18710/5KCE4U, DataverseNO, V1.\nSönning, Lukas. 2023. Key verbs in academic writing: Dataset for “Evaluation of keyness metrics: Performance and reliability”, https://doi.org/10.18710/EUXSMW, DataverseNO, V1.\nSönning, Lukas. 2022. Speech rhythm in German Learner English: Dataset for “(Re-)viewing the acquisition of rhythm in the light of L2 phonological theories”, https://doi.org/10.18710/GTI2BR, DataverseNO, V1.\nSönning, Lukas & Manfred Krug. 2021. Actually in contemporary British speech: Data from the Spoken BNC corpora, https://doi.org/10.18710/A3SATC, DataverseNO, V1.\nSönning, Lukas. 2022. Dataset for “Scrabble yourself to success: Methods in teaching transcription”, https://doi.org/10.18710/2UJHHU, DataverseNO, V1.\n\n\nDissertation\n\nSönning, Lukas. 2021. The TRAP-DRESS contrast in German Learner English: Dataset for chapter 4 in “Phonological variation in German Learner English”, https://doi.org/10.18710/ATIRRV, DataverseNO, V1.\nSönning, Lukas. 2021. Clear vs. dark /l/ in German Learner English: Dataset for chapter 5 in “Phonological variation in German Learner English”, https://doi.org/10.18710/G6PJ5F, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. English /r/ in German Learner English: Dataset for chapter 6 in “Phonological variation in German Learner English”, https://doi.org/10.18710/YDKDFG, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. The labio-velar glide /w/ in German Learner English: Dataset for chapter 7 in “Phonological variation in German Learner English”, https://doi.org/10.18710/F1A34O, DataverseNO, V1.\nSönning, Lukas & Isabel Rank. 2021. The labiodental fricative /v/ in German Learner English: Dataset for chapter 8 in “Phonological variation in German Learner English”, https://doi.org/10.18710/B276ZX, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe & Christina Wunder. 2021. The voiced dental fricative in German Learner English: Dataset for chapter 9 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DYAGZG, DataverseNO, V1.\nSönning, Lukas & Graham Pascoe. 2021. Final voiced obstruents in German Learner English: Dataset for chapter 10 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DKIGE5, DataverseNO, V1."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Plot templates for Microsoft Excel\nExcel templates and instructions for some useful graph types can be found in the following  OSF project .\n\nDot diagram\ntemplate | instructions\n\n\n\n\n\nSimple dot plot\ntemplate | instructions\n\n\n\n\n\nGrouped dot plot\ntemplate | instructions\n\n\n\n\n\nBox plot\ntemplate | instructions\n\n\n\n\n\nVertical dot plot\ntemplate\n\n\n\n\n\nScatter plot\ntemplate | instructions\n\n\n\n\n\n\nSpeaker slides for workshop\n\nFJUEL workshop, Bamberg: speaker slides session 1 | speaker slides session 2 | speaker slides session 3\n\n\n\nExtended notes: Ordinal regression models\n\nSection 1: Background\nSection 2: Descriptive statistics\nSection 3: Ordered regression models\nSection 4: A latent-variable model\nSection 5: Methods of interpretation\nSection 6: R workbench"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Statistics for linguist(ic)s blog",
    "section": "",
    "text": "Issues in random-forest modeling: Interaction predictors\n\n\n\n\n\n\ncorpus linguistics\n\n\nbinary data\n\n\nrandom forests\n\n\npartial dependence plots\n\n\n\nIn this blog post, I demonstrate that the inclusion of manually specified interaction predictors into a random-forest model yields biased (or uninterpretable) partial dependence plots.\n\n\n\n\n\nJan 12, 2026\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nIssues in random-forest modeling: Treatment of clustering variables\n\n\n\n\n\n\ncorpus linguistics\n\n\nclustered data\n\n\nbias\n\n\nbinary data\n\n\nrandom forests\n\n\n\nCorpus data often show a hierarchical structure, where observations are grouped by text/speaker or item. In this blog post, I comment on the practice of including clustering variables as predictors into a random-forest model. I demonstrate that this undermines the predictive utility of associated cluster-level predictors.\n\n\n\n\n\nJan 10, 2026\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nCustom scoring systems in ordinal data analysis: A tribute to Sharoff (2018)\n\n\n\n\n\n\nordinal data\n\n\n\nThis blog post discusses the issue of representing an ordinal response scale using numeric scores. It shows how psychometric research may suggest deviations from the near-universal use of equally spaced integers.\n\n\n\n\n\nNov 30, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDrawing grouped dot plots in R\n\n\n\n\n\n\ndata visualization\n\n\ndot plot\n\n\ncorpus linguistics\n\n\nfrequency data\n\n\nbinary data\n\n\n\nIn this blog post, I describe how to draw grouped dot plots as an alternative to grouped bar charts in R using the {ggplot2} package .\n\n\n\n\n\nNov 30, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDrawing panel charts in R\n\n\n\n\n\n\ndata visualization\n\n\ndot plot\n\n\ncorpus linguistics\n\n\nfrequency data\n\n\nbinary data\n\n\n\nIn this blog post, I describe how to draw panel charts as an alternative to stacked bar charts in R, using the {ggplot2} package.\n\n\n\n\n\nNov 29, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping dispersion measures\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis blog post discusses bootstrapping for dispersion measures and illustrates how two particularly useful variants, simple and stratified bootstrapping, may be applied with the {tlda} package in R.\n\n\n\n\n\nNov 18, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDispersion: Levels of analysis\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis blog post argues that there are three basic levels at which dispersion analysis isolates a particular distributional feature and thereby returns clear and interpretable scores. In corpus-based work, these levels are often compounded, and dispersion scores therefore represent a mix of differnet types of distributional information.\n\n\n\n\n\nNov 17, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nTukey’s folded power transformation in R\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\ndata visualization\n\n\ntlda\n\n\nbinary data\n\n\n\nThis blog post demonstrates a new functionality of the {tlda} package: The implementation of Tukey’s folded power transformation for proportions (and percentages), including its use in data visualization with {ggplot2}.\n\n\n\n\n\nNov 14, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDrawing spike graphs to examine dispersion across text files\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\ndata visualization\n\n\n\nThis blog post describes how to draw spike graphs that visualize the dispersion of an item across the text files in a corpus. These graphs are enriched with information about corpus design (and structure).\n\n\n\n\n\nNov 12, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDrawing structured dispersion plots in R\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\ndata visualization\n\n\n\nThis short note describes how to draw dispersion plots that include information about the design (and structure) of a corpus in R.\n\n\n\n\n\nNov 11, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nNelson’s (2025) Poisson-based dispersion measure\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis blog post discusses the dispersion measure MB, which was recently proposed by Nelson (2025). The motivation behind MB – to build an index that tolerates sampling variation – is highly commendable. To extend the applicability of this measure, I describe how it can be applied to linguistically grounded corpus parts (e.g. genres/regsiters, text files), which may also differ in length. I then study the behavior of MB and DMB and observe that both seem to show an overall bias toward evenness.\n\n\n\n\n\nOct 30, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nLyne’s (1985) graphical technique for the evaluation of dispersion measures\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\ndata visualization\n\n\n\nIn this blog post, I revisit the graphical strategy used by Lyne (1985) to study the behavior of the dispersion measures D, D2, and S, and then apply it to a number of other parts-based indices that have been proposed more recently.\n\n\n\n\n\nOct 27, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nColor-coded dendrograms using the R function A2Rplot()\n\n\n\n\n\n\ndata visualization\n\n\n\nIn this blog post, I illustrate how to use Romain Francois’ R function A2Rplot() to draw dendrograms with visually distinct clusters.\n\n\n\n\n\nJun 16, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nExporting R graphics: A basic workflow\n\n\n\n\n\n\ndata visualization\n\n\n\nIn this blog post, I describe my workflow for exporting and polishing graphs drawn in R.\n\n\n\n\n\nMay 20, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered frequency data II: Texts of disproportionate length\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nbias\n\n\nimbalance\n\n\nnegative binomial\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered count data. It describes how they handle the non-independence among observations and what kind of estimates they return. The focus is on a situation where texts have very different lengths.\n\n\n\n\n\nMay 15, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered frequency data I: Texts of similar length\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nnegative binomial\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered count data. It describes how they handle the non-independence among observations and what kind of estimates they return. The focus is on a situation where texts have roughly the same length.\n\n\n\n\n\nMay 14, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency estimates based on random-intercept Poisson models\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nnegative binomial\n\n\n\nClustered count data can be modeled using a Poisson regression model including random intercepts. This blog post describes how this model represents the data and the different kinds of frequency estimates it produces.\n\n\n\n\n\nMay 13, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered binomial data\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nbinary data\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered binomial data. It describes how they handle the non-independence among observations and what kind of estimates they return.\n\n\n\n\n\nMay 9, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nImbalance across predictor levels affects data summaries\n\n\nObstacles to replication in corpus linguistics\n\n\n\ncorpus linguistics\n\n\nreplication crisis\n\n\nregression\n\n\nbias\n\n\nimbalance\n\n\n\nThis blog post is part of a small series on obstacles to replication in corpus linguistics. It deals with problems that can arise if the observations drawn from a corpus are unbalanced across relevant subgroups in the data. I show how simple and comparative data summaries can vary depending on whether we (unintentionally) calculate weighted averages, or adjust estimates for imbalances by taking a simple average across subgroups. As these are two different estimands, the choice affects the comparability of studies – including an original study and its direct replication.\n\n\n\n\n\nMay 4, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nClustering in the data affects statistical uncertainty intervals\n\n\nObstacles to replication in corpus linguistics\n\n\n\ncorpus linguistics\n\n\nreplication crisis\n\n\nregression\n\n\nclustered data\n\n\n\nThis blog post is part of a small series on obstacles to replication in corpus linguistics. It deals with a prevalent issue in corpus data analysis: the non-independence of data points that results from clustered (or hierarchical) data layouts. I show how an inadequate analysis can produce unduly narrow expectations of a replication study.\n\n\n\n\n\nMay 2, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nUnbalanced distributions and their consequences: Speakers in the Spoken BNC2014\n\n\n\n\n\n\ncorpus linguistics\n\n\nclustered data\n\n\nnegative binomial\n\n\nclustered data\n\n\nimbalance\n\n\n\nThis blog post illustrates how the disproportionate representation of speakers in a corpus can lead to distorted results if the source of data points (i.e. the speaker ID) is not taken into account in the analysis.\n\n\n\n\n\nApr 29, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling the interpretation of quantifiers using beta regression\n\n\n\n\n\n\nregression\n\n\ndistributional modeling\n\n\n\nThis blog post shows how to use beta regression to model the proportional interpretation of the quantifiers few, some, many, and most. We consider variable-dispersion and mixed-effects structures as well as diagnostics for frequentist and Bayesian models.\n\n\n\n\n\nFeb 29, 2024\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent parameterizations of the negative binomial distribution\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\nnegative binomial\n\n\n\nThis blog post discusses two different parameterizations of the negative binomial distribution and groups R packages (and functions) based on the version they implement.\n\n\n\n\n\nDec 13, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe negative binomial distribution: A visual explanation\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\nnegative binomial\n\n\n\nThis blog post uses a visual approach to explain how the negative binomial distribution works.\n\n\n\n\n\nDec 12, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nA computational shortcut for the dispersion measure DA\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis short blog post draws attention to the computational shortcut given in Wilcox (1973) for calculating the dispersion measure DA.\n\n\n\n\n\nDec 11, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe replication crisis: Implications for myself\n\n\n\n\n\n\nreplication crisis\n\n\nopen science\n\n\n\nIn this blog post, I reflect on the ways in which learning about the replication crisis in science has affected my own work.\n\n\n\n\n\nNov 21, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nStructured down-sampling: Implementation in R\n\n\n\n\n\n\ncorpus linguistics\n\n\ndown-sampling\n\n\n\nThis blog post shows how to implement structured down-sampling in R.\n\n\n\n\n\nNov 18, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nTwo types of down-sampling in corpus-based work\n\n\n\n\n\n\ncorpus linguistics\n\n\ndown-sampling\n\n\n\nThis short blog post contrasts the different ways in which the term down-sampling is used in corpus-based work.\n\n\n\n\n\nNov 17, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\n‘Dispersion’ in corpus linguistics and statistics\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis blog post clarifies the different ways in which the term dispersion is used in corpus linguistics and statistics.\n\n\n\n\n\nNov 16, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-18_dispersion_terminology/index.html",
    "href": "posts/2023-01-18_dispersion_terminology/index.html",
    "title": "‘Dispersion’ in corpus linguistics and statistics",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(uls)\n\n\nIn corpus linguistics, the term dispersion is used to describe the distribution of an item or structure in a corpus (see Gries 2008, 2020). For most dispersion measures, a corpus must first be divided into units (or parts). These units commonly reflect the design of the corpus – they can be text files, for instance, or text categories. A dispersion index then describes the distribution of an item across these units. There are two general classes of measures:\n\nthose measuring the pervasiveness of an item, which is reflected in the number of units that contain the item (Range and Text Dispersion, its proportional analog)\nthe much larger class of evenness measures, which express how evenly an item is distributed across the units (e.g. D, D2, S, DP, DA, DKL).\n\nMost dispersion measures range between 0 and 1, where 1 indicates a perfectly even distribution, or the maximal degree of pervasiveness (i.e. the item occurs in every unit).\nFrom a statistical viewpoint, the input for the calculation of evenness measures would be considered a count variable, since it records the number of events (occurrences of the item) that are observed during a certain period of observation. In corpus linguistics, the “period of observation” is “text time”, expressed as a word count.\nThere is an extensive literature on the use of regression models for count variables (e.g. Long 1997; Cameron and Trivedi 2013; Hilbe 2014), and such models have seen some successful applications to word frequency data (e.g. Mosteller and Wallace 1984; Church and Gale 1995); Winter and Bürkner (2021) provide an accessible introduction for linguists. In this literature, the term “dispersion” is also used, though with a different (apparently opposite) meaning.\nLet us first consider the corpus-linguistic (and lexicographic) sense, which can be best described visually, using a so-called “dispersion plot”. Figure 1 shows a dispersion plot for two corpora, A and B. The framed rectangles represent the sequence of words forming the corpus, and the spikes inside of these locate the occurrences of a specific item in the corpus. In corpus A, the item is spread out quite evenly. In corpus B, instances are more densely clustered, and there are large stretches where the item does not occur. In the corpus-linguistic sense, then, the dispersion of the item is greater in corpus A. The dispersion score for the item would be greater in Corpus A (i.e. closer to 1).\n\n\nR code: Figure 1\nset.seed(2000)\n\nn_tokens_A &lt;- c(3,5,4,4,3,4,4,5)\nn_tokens_B &lt;- c(5,0,1,9,0,1,0,3)\n\nn_texts &lt;- length(n_tokens_A)\n\nA_loc &lt;- rep(1:n_texts, n_tokens_A)+runif(sum(n_tokens_A))\nB_loc &lt;- rep((1:n_texts)[n_tokens_B!=0], n_tokens_B[n_tokens_B!=0])+runif(sum(n_tokens_B))\n\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,14), ylim=c(2.8,6),\n  par.settings=lattice_ls, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=5.1, ybottom=4.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    panel.rect(xleft=1, xright=n_texts+1, ytop=5.1, ybottom=4.75, \n               border=\"grey50\", lwd=1)\n    \n\n        \n    panel.segments(x0=A_loc, x1=A_loc, y0=4.8, y1=5.05, lwd=.75)\n    panel.text(x=(1:n_texts)+.5, y=4.55, label=n_tokens_A, \n               col=\"grey50\", cex=.9)\n    \n    \n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=4.1, ybottom=3.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    \n    panel.rect(xleft=1, xright=n_texts+1, ytop=4.1, ybottom=3.75, \n               border=\"grey60\", lwd=1)\n    \n    panel.segments(x0=B_loc, x1=B_loc, y0=3.8, y1=4.05, lwd=.75)\n    \n    panel.text(x=(1:n_texts)+.5, y=3.55, label=n_tokens_B, \n               col=\"grey60\", cex=.9)\n    \n    panel.text(x=.4, y=c(4,5)-.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"Lower dispersion\\n\", \"Higher dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"\\n(more concentrated)\", \"\\n(more spread out)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.segments(x0=1, x1=2, y0=5.4, y1=5.4, lwd=.5, col=\"grey50\")\n    panel.segments(x0=1:2, x1=1:2, y0=5.4, y1=5.3, lwd=.5, col=\"grey50\")\n    \n    \n    panel.text(x=1.5, y=5.75, label=\"Text 1\", col=\"grey40\", cex=.8)\n    panel.text(x=7, y=2.75, label=\"Occurrences\\nof item in text\", col=\"grey40\", \n               cex=.9, lineheight=.85)\n    panel.segments(x0=5.9, x1=5.6, y0=3, y1=3.3, col=\"grey40\", lwd=.5)\n    })\n\n\n\n\n\n\n\n\nFigure 1: Dispersion in the corpus-linguistic sense: Distribution of word tokens in the corpus.\n\n\n\n\n\nNote how each corpus is divided into 8 texts, which are shown in Figure 1 using greyshading. The numbers below the dispersion plot for each corpus report the number of occurrences of the item in each text. For corpus A, they range between 3 and 5; for corpus B, between 0 and 9.\nFigure 2 shows a different representation of these data. Instead of looking at the corpus as a string of words, we consider the text-specific frequencies (sometimes called sub-frequencies) of the item. These indicate how often the item occurs in each document. Figure 2 shows these text-level token counts: Each text is represented by a dot, which marks how often the item appears in the text. In our hypothetical corpora, each text has the same length, which is why we can compare absolute counts. If texts differ in length, we would instead use normalized frequencies, i.e. occurrence rates such as “3.1 per thousand words”.\n\n\nR code: Figure 2\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,7.5), ylim=c(-.35,2.3),\n  par.settings=lattice_ls, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.dotdiagram(1+(n_tokens_A/5), y_anchor=1, scale_y=.125, set_cex=1.3)\n    panel.dotdiagram(1+(n_tokens_B/5), y_anchor=0, scale_y=.125, set_cex=1.3)\n    panel.segments(x0=1, x1=3.2, y0=1, y1=1)\n    panel.segments(x0=1, x1=3.2, y0=0, y1=0)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=1, y1=.95)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=0, y1=-.05)\n    panel.text(x=1+c(0,5,10)/5, y=-.2, label=c(0,5,10), col=\"grey40\", cex=.8)\n    \n    panel.text(x=.6, y=c(0,1)+.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"Higher dispersion\\n\", \"Lower dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"\\n(more spread out)\", \"\\n(more concentrated)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.text(x=2, y=-.5, label=\"Occurrences of item\", cex=1, lineheight=.9)\n    panel.text(x=3, y=2.2, label=\"Each dot\\nrepresents a text\", cex=.9, \n               lineheight=.9, col=\"grey40\")\n    })\n\n\n\n\n\n\n\n\nFigure 2: Dispersion in the statistical sense: Distribution of text-level ocurrence rates.\n\n\n\n\n\nIf we compare the distribution of text-level occurrence rates in the two corpora, we note that while the texts in corpus A form a dense pile, the occurrence rates in corpus B are more widely spread out. At this level of description, then, it is the data from corpus B that show greater “dispersion”. In the statistical literature on count regression, the term dispersion is used in this sense, i.e. to refer to the variability of unit-specific (i.e. text-level) occurrence rates (e.g. Long 1997, 221; Gelman 2021, 264–68). An awareness of the different meanings of “dispersion” will prove helpful for corpus linguists (and lexicographers) when engaging with the statistical literature on count data modeling.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe term “dispersion” is used differently in corpus linguistics and statistics\nThe difference in meaning reflects a difference in perspective\nCorpus linguists picture the corpus as a sequence of words and understand the term as characterizing the spatial distribution of an item\nIn the statistical literature on count data modeling, the term describes the spread of a distribution of counts or occurrence rates\n\n\n\n\n\n\n\nReferences\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. Second edition. New York: Cambridge University Press.\n\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/S1351324900000139.\n\n\nGelman, Hill, Andrew. 2021. Regression and Other Stories. Cambridge: Cambridge University Press.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data. New York: Cambridge University Press.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage.\n\n\nMosteller, Frederick, and David L. Wallace. 1984. Applied Bayesian Inference: The Case of the Federalist Papers. New York: Springer.\n\n\nWinter, Bodo, and Paul‐Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11). https://doi.org/10.1111/lnc3.12439.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {“{Dispersion}” in Corpus Linguistics and Statistics},\n  date = {2023-11-16},\n  url = {https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “‘Dispersion’ in Corpus\nLinguistics and Statistics.” November 16, 2023. https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_two_types/index.html",
    "href": "posts/2023-11-17_downsampling_two_types/index.html",
    "title": "Two types of down-sampling in corpus-based work",
    "section": "",
    "text": "The data available from corpora are often too vast for certain types of linguistic analysis. Researchers are then forced to select a subset of the data, and this selection process can be referred to as “down-sampling”. Currently, the term is used to refer to two very different types of down-sizing.\nThe first deals with lists of occurrences extracted from a corpus and is used in studies that start out with a corpus query and a body of hits (often in the form of concordance lines). If the structure of interest is relatively frequent and/or the source corpus large, the researcher may need to reduce the number of data points studied. In particular, this will be necessary in variationist-type research, which often involves considerable manual work (e.g. disambiguation and annotation). In this form of down-sampling, the selection of elements usually proceeds (to some extent) at random, i.e. it involves a chance component. Simple techniques are implemented in corpus software, which allows users to extract from a list of hits a random sample. In CQPweb (Hardie 2012), for instance, this option is referred to as “thinning”. Depending on our research goals and the structure of our data, however, other strategies may be more efficient (e.g. structured down-sampling, see Sönning and Krug 2022).\nThe second type of down-sampling is concerned with the selection of texts for close reading. Here, the objective is to pick from a corpus those texts that are likely to be most informative for a thorough qualitative analysis. This method, which Gabrielatos et al. (2012) refer to as “targeted down-sampling”, uses surface-level features (such as the occurrence rate of certain forms) to detect relevant documents for a critical discourse analysis (see also Baker et al. 2008, 285). A procedure much in the same spirit is discussed in Anthony and Baker (2015), where prototypical exemplars, i.e. texts that are most representative of their corpus of origin, are selected based on keyword profiles.\nIt may therefore sometimes be helpful to distinguish the two types of down-sampling: We could call the first type “selection of concordance lines for annotation” and the second type “selection of texts for close reading”.\n\n\n\n\nReferences\n\nAnthony, Laurence, and Paul Baker. 2015. “ProtAnt: A Tool for Analysing the Prototypicality of Texts.” International Journal of Corpus Linguistics, August, 273–92. https://doi.org/10.1075/ijcl.20.3.01ant.\n\n\nBaker, Paul, Costas Gabrielatos, Majid KhosraviNik, Michał Krzyżanowski, Tony McEnery, and Ruth Wodak. 2008. “A Useful Methodological Synergy? Combining Critical Discourse Analysis and Corpus Linguistics to Examine Discourses of Refugees and Asylum Seekers in the UK Press.” Discourse &Amp; Society 19 (3): 273–306. https://doi.org/10.1177/0957926508088962.\n\n\nGabrielatos, Costas, Tony McEnery, Peter J. Diggle, and Paul Baker. 2012. “The Peaks and Troughs of Corpus-Based Contextual Analysis.” International Journal of Corpus Linguistics 17 (2): 151–75. https://doi.org/10.1075/ijcl.17.2.01gab.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Two Types of down-Sampling in Corpus-Based Work},\n  date = {2023-11-17},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Two Types of down-Sampling in Corpus-Based\nWork.” November 17, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_implementation/index.html",
    "href": "posts/2023-11-17_downsampling_implementation/index.html",
    "title": "Structured down-sampling: Implementation in R",
    "section": "",
    "text": "I recently consulted colleagues on how to down-sample their corpus data. Their study deals with modal auxiliaries in learner writing, and they are also interested in the semantics of modal verbs. This means that they have to manually annotate individual tokens of modals. In this blog post, I describe how we implemented structured down-sampling (Sönning and Krug 2022) in R. The data we use for illustration is a simplified subset of the originial list of corpus hits. We will concentrate on the modal verb can.\n\n\nR setup\nlibrary(tidyverse)\n\nd &lt;- read_tsv(\"./data/modals_data.tsv\")\n\n\n\nThe data\nThe data include 300 tokens, which are grouped by Text (i.e. learner essay), and there are 162 texts where can occurs at least once. The distribution of tokens across texts is summarized in Figure 1: In most texts (n = 83), can occurs only once, 41 texts feature two occurrences, and so on.\n\n\nR code: Figure 1\nd |&gt; \n  group_by(text_id) |&gt; \n  tally() |&gt; \n  group_by(n) |&gt; \n  tally() |&gt; \n  ggplot(aes(x=n, y=nn)) + \n  geom_col(width = .7, fill=\"grey\") +\n  theme_classic() +\n  scale_x_continuous(breaks = 1:7) +\n  xlab(\"Number of occurrences\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of token counts across texts.\n\n\n\n\nA different arrangement of the data is shown in Figure 2, where texts are lined up from left to right. Each text is represented by a pile of dots, with each dot representing a can token. The text with the highest number of can tokens (n = 7) appears at the far left, and about half of the texts only have a single occurrence of can – these text are sitting in the right half of the graph.\n\n\nR code: Figure 2\nd |&gt;  \n  group_by(text_id) |&gt; \n  mutate(n_tokens = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x=reorder(text_id, -n_tokens))) + \n  geom_dotplot(dotsize = .13, stackratio=1.6) +\n  theme_void() +\n  labs(subtitle=\"Texts ranked by token count\",\n       caption = \"Each dot represents a token (can)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribuition of tokens across texts.\n\n\n\n\n\n\n\nStructured down-sampling\nAs argued in Sönning and Krug (2022), structured down-sampling would be our preferred way of drawing a sub-sample from these data. In contrast to simple down-sampling (or thinning), where each token has the same probability of being selected, structured down-sampling aims for a balanced representation of texts in the sub-sample. Thus, we would aim for breadth of representation and only start selecting additional tokens from the same text if all texts are represented in our sub-sample. The statistical background for this strategy is discussed in Sönning and Krug (2022).\nLooking at Figure 2, this means that our selection of tokens would first consider the “bottom row” of dots in the graph, and then work upwards if necessary, i.e. sample one additional token (at random) from each text that contains two or more occurrences, and so on. It should be noted that, at some point, little more is learned by sampling yet further tokens from a specific text (see discussion in Sönning and Krug 2022, 147).\n\n\nImplementation in R\nOur first step is to add to the table a column that preserves the original order. This is important in case we want to return to the original arrangement at a later point. We will name the new column original_order.\n\nd$original_order &lt;- 1:nrow(d)\n\nThere may be settings where, due to resource constraints, we cannot pick a token from every single text. Or, similarly, where we cannot pick a second token from each text that contains at least two tokens. In such cases, a sensible default approach is to pick at random. Thus, if we were only able to analyze 100 tokens, but there are 162 texts in our data, we would like to pick texts at random. We therefore add another column where the sequence from 1 to N (the number of rows, i.e. tokens) is shuffled. This column will be called random_order. Further below, we will see how this helps us out.\n\nd$random_order &lt;- sample(\n  1:nrow(d), \n  nrow(d), \n  replace=F)\n\nThe next step is to add a column to the table which specifies the order in which tokens should be selected from a text. We will call the column ds_order (short for ‘down sampling order’). In texts with a single token, the token will receive the value 1, reflecting its priority in the down-sampling plan. For a text with two tokens, the numbers 1 and 2 are randomly assigned to the two tokens. For texts with three tokens, the numbers 1, 2 and 3 are shuffled, and so on. If we then sort the whole table according to the column ds_order, those tokens that are to be preferred, based on the rationale underlying structured down-sampling, appear at the top of the table.\nOur first step is to order the table by text_id, to make sure rows are grouped by Text.\n\nd &lt;- d[order(d$text_id),]\n\nWe then create a list of the texts in the data and sort it, so that it matches the way in which the table rows have just been ordered.\n\ntext_list &lt;- unique(d$text_id)\ntext_list &lt;- sort(text_list)\n\nWe now create the vector ds_order, which we will add to the table once it’s ready:\n\nds_order &lt;- NA\n\nThe following loop fills in the vector ds_order, text by text. It includes the following steps (marked in the script):\n\nProceed from text to text, from the first to the last in the text_list.\nFor text i, count the number of tokens in the text and store it as n_tokens.\nShuffle the sequence from 1 to n_tokens and store it as shuffled.\nAppend the shuffled sequence shuffled to the vector ds_order.\n\n\nfor(i in 1:length(text_list)){  # (1)\n  \n  n_tokens &lt;- sum(              # (2)\n    d$text_id == text_list[i])  # \n  \n  shuffled &lt;- sample(           # (3)\n    1:n_tokens,                 #\n    size = n_tokens,            #\n    replace = FALSE)            #\n  \n  ds_order &lt;- append(           # (4)\n    ds_order,                   #\n    shuffled)                   #\n}\n\nIf we look at the contents of ds_order, we note that it still has a leading NA:\n\nds_order\n\n  [1] NA  2  1  3  1  1  2  1  2  2  1  3  1  2  4  3  1  2  1  1  2  2  1  1  1\n [26]  1  1  1  3  2  1  4  1  1  3  1  2  1  2  3  1  2  1  1  1  1  1  1  2  3\n [51]  4  3  1  2  2  1  2  1  3  2  1  5  3  1  4  6  2  4  1  3  2  1  2  1  4\n [76]  1  2  3  2  1  1  2  3  4  1  1  1  2  1  1  1  1  1  2  3  1  1  2  4  2\n[101]  3  1  1  1  2  1  1  1  1  1  1  3  1  2  1  1  1  2  1  1  1  3  2  1  1\n[126]  1  2  2  3  1  1  1  3  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  2  3  1  1  2  1  1  2  1  2  3  1  1  1  1  2  1  1  2  1  1  1  1  2\n[176]  2  1  2  1  3  3  2  1  3  2  1  1  1  3  2  1  2  1  2  1  4  2  3  1  3\n[201]  1  2  1  2  3  1  1  2  2  1  1  1  2  3  1  1  2  1  2  1  1  1  1  2  1\n[226]  2  3  1  2  3  1  1  2  1  1  2  1  1  1  1  1  3  2  4  1  1  2  1  2  3\n[251]  4  1  2  3  2  1  1  2  1  1  1  2  3  5  4  2  1  1  1  2  1  2  1  2  1\n[276]  1  3  2  5  6  7  4  1  1  2  1  1  1  1  1  2  3  4  1  2  2  3  4  1  2\n[301]  1\n\n\nSo we get rid of it:\n\nds_order &lt;- ds_order[-1]\n\nWe can now add ds_order as a new column to our table:\n\nd$ds_order &lt;- ds_order\n\nThe final step is to order the rows of the table in a way that reflects our down-sampling priorities. We therefore primarily order the table based on ds_order. In addition, we order by the column random_order, which we created above. All tokens with the same priority level (e.g. all tokens with the value “1” in the column ds_order) will then be shuffled, ensuring that the order of tokens is random.\n\nd &lt;- d[order(d$ds_order, \n             d$random_order),]\n\nWe can now look at the result:\n\nhead(d)\n\n# A tibble: 6 × 7\n  text_id  left_context modal right_context original_order random_order ds_order\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;int&gt;\n1 text_39  I            can'  t                        154            3        1\n2 text_41  still        can   see                      200            4        1\n3 text_5   you          can   call                      37            5        1\n4 text_27  you          can   observe                  254            6        1\n5 text_97  it           cann… be                       265            7        1\n6 text_152 you          can   pose                     213            8        1\n\n\nNote that the strategy we have used, i.e. adding a column reflecting the priority of tokens for down-sampling, allows us to approach down-sampling in a flexible and adaptive way: Rather than actually selecting (or sampling) tokens (or rows) from the original data, we may now simply start analyzing from the top of the table. This way we remain flexibility when it comes to the choice of how many tokens to analyze.\n\n\n\n\n\nReferences\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Structured down-Sampling: {Implementation} in {R}},\n  date = {2023-11-18},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Structured down-Sampling: Implementation in\nR.” November 18, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "title": "The replication crisis: Implications for myself",
    "section": "",
    "text": "Since my research is almost exclusively quantitative, the methodological discourse surrounding the replication crisis has been directly relevant to my work. A recent invitation to take part in an online event by the International Society for the Linguistics of English (ISLE) on “Replication and Replicability” was an opportunity to reflect on the ways in which this “crisis” has affected how I do my job. In this blog post, I summarize these under three headings: (i) workflow and reproducibility, (2) open science, and (3) community discourse.\nI would like to start, however, with two preliminary remarks. For one, I consider the discussions, suggestions, and innovations that have arisen in the context of the credibility crisis in science as an opportunity – they should inspire us to improve the way(s) in which we do and communicate research. While there are some who point out that we actually don’t know whether there is a replication crisis in linguistics1, the suggested ways forward enable better science, so it is worth adopting them in any case.\nFurther, if we decide to change our research routines, we should be indulgent with ourselves: Many of the suggested improvements, especially concerning data analysis workflow, can be quite overwhelming at first. We should avoid setting our immediate aims too high – as I had to find out on numerous occasions, it is too easy to become frustrated. And this may also be something to keep in mind when making recommendations: The advice we give to others should be calibrated to the person across the table. Nothing is gained if a researcher with a genuine interest in adopting better practices ends up quitting in frustration."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "title": "The replication crisis: Implications for myself",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt seems that we are not particularly eager to find out (see, e.g., this preprint by Kristina Kobrock and Timo Roettger). It would be quite surprising, however, if linguist(ic)s were spared – after all, the same human factors are at work in language research as in neighboring disciplines such as psychology.↩︎"
  },
  {
    "objectID": "posts/2023-12-11_computation_DA/index.html",
    "href": "posts/2023-12-11_computation_DA/index.html",
    "title": "A computational shortcut for the dispersion measure DA",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(uls)        # pak::pak(\"lsoenning/uls\")\n\n\nThe dispersion measure DA was proposed by Burch, Egbert, and Biber (2017) as a way of quantifying how evenly an item is distributed across corpus parts. The authors attribute this measure to Wilcox (1973), a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While Wilcox (1973) focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as Burch, Egbert, and Biber (2017, 193) note, a measure equivalent to DP (Gries 2008) can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in Wilcox (1973) as the mean difference analog (MDA). Both Wilcox (1973) and Burch, Egbert, and Biber (2017) argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in Sönning (2023).\nGries (2020, 116) has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between corpus parts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each corpus part. These occurrence rates can then be compared, to see how evenly the item is distributed. The basic formula for DA requires pairwise comparisons between all corpus parts. If there are 10, the number of pairwise comparisons is 45; for 20 corpus parts, this number climbs to 190. In general, if there are n corpus parts, the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: If we measure dispersion across 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, which has around 90,000 text files, there are more than 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula Wilcox (1973) gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed. Note that both versions of DA are now available in the function disp_DA() in the {tlda} package (Sönning 2025).\nFor the prpose of the following evaluation, we start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in Wilcox (1973).\n\nThese functions also work if corpus parts differ in length. They take two arguments:\n\nsubfreq: A vector of length n, giving the subfrequencies, i.e. the number occurrences of the item in each of the n corpus parts\npartsize: A vector of length n, giving the length of each corpus part (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to Sönning (2023). We first define the basic formula :\n\nDA_basic &lt;- function(subfreq, partsize){\n  \n    R_i &lt;- subfreq / partsize\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(subfreq)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(subfreq, partsize){\n  \n    R_i &lt;- subfreq / partsize\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(subfreq)\n\n    DA &lt;- (2 * (sum(sort(r_i, decreasing=TRUE) * 1:k) - 1)) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (subfreq_4000) and the \n# larger corpus (subfreq_20000)\n\nset.seed(1)\n\nsubfreq_4000 &lt;- rpois(n = 4000, lambda = 2)\nsubfreq_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\npartsize_4000 &lt;- rep(2000, length(subfreq_4000))\npartsize_20000  &lt;- rep(2000, length(subfreq_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\n\nDA_basic_4000 &lt;- DA_basic(\n  subfreq = subfreq_4000, \n  partsize = partsize_4000)\n\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\n\nDA_quick_4000 &lt;- DA_quick(\n  subfreq = subfreq_4000, \n  partsize = partsize_4000)\n\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\n\nDA_basic_20000 &lt;- DA_basic(\n  subfreq = subfreq_20000, \n  partsize = partsize_20000)\n\ntime_basic_20000 &lt;- toc()\n\n\ntic()\n\nDA_quick_20000 &lt;- DA_quick(\n  subfreq = subfreq_20000, \n  partsize = partsize_20000)\n\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.74 seconds to run. The computational formula is quicker – it completes the calculations in only 0 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 39.33 seconds to run; the shortcut procedure, on the other hand, is done after 0.02 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 parts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 parts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 parts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 parts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 parts\n20,000 parts\n4,000 parts\n20,000 parts\n\n\n\n\nBasic\n1.74\n39.33\n0.6003\n0.6139\n\n\nComputational\n0.00\n0.02\n0.6003\n0.6139\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nSönning, Lukas. 2023. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” PsyArxiv Preprint. https://doi.org/10.31234/osf.io/ns4q9.\n\n\n———. 2025. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\nWilcox, Allen R. 1973. “Indices of Qualitative Variation and Political Measurement.” The Western Political Quarterly 26 (2): 325–43. https://doi.org/10.2307/446831.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {A Computational Shortcut for the Dispersion Measure\n    {*D\\textasciitilde A\\textasciitilde*}},\n  date = {2023-12-11},\n  url = {https://lsoenning.github.io/posts/2023-12-11_computation_DA/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “A Computational Shortcut for the Dispersion\nMeasure *D~A~*.” December 11, 2023. https://lsoenning.github.io/posts/2023-12-11_computation_DA/."
  },
  {
    "objectID": "posts/2023-11-16_negative_binomial/index.html",
    "href": "posts/2023-11-16_negative_binomial/index.html",
    "title": "The negative binomial distribution: A visual explanation",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(gamlss)\nlibrary(uls)\n\n\nThe negative binomial distribution is a useful device for modeling word counts. A typical setting for its application in corpus linguistics is the modeling of word frequency data – for instance, if we wish to summarize (or compare) occurrence rates of an item in a corpus (or across sub-corpora). Each text then contributes information about the frequency of the item in the form of (i) a token count (the number of times the word occurs in the text) and (ii) a word count (the length of the text). Based on the token and word count we can calculate an occurrence rate (or normalized frequency) for each text, and these rates are then directly comparable across texts.\nFrom a statistical perspective, word frequency would be considered as a count variable, which is observed at the level of the text and can take on non-negative integer values (i.e. 0, 1, 2, 3, 4, …). The text length can be thought of as a period of observation (measured in text time, i.e. the number of running words), in which a tally is kept of the number of events (in this case the occurrence of the focal item). And this is the typical definition of a count variable.\nThis blog post takes a closer look at the negative binomial distribution – how it works and why it is a useful device for modeling word frequency data. It is helpful to start with a concrete example: the frequency of which in the Brown Corpus. To keep things simple, we will stick to this data setting, where texts have (nearly) the same length. Note, however, that the negative binomial distribution (like the Poisson) readily extends to situations where texts have different lengths.\n\nObserved and expected frequency distributions\nIf we count the number of occurrences of which in each text and then look at the distribution of token counts, we obtain what is referred to as a frequency distribution or a token distribution. The frequency distribution for which in the Brown Corpus, which consists of 500 texts, appears in Figure 1 a. It shows the distribution of token counts across texts: Each bar represents a specific token count, and the height of the bar is proportional to the number of texts that have this many instances of which. Token counts vary between 0 (n = 26 texts) and 40 (1 text), and the distribution is right-skewed, which is quite typical of count variables, since they have a lower bound at 0.\n\n\nLoad data\n# tdm &lt;- read_tsv(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/brown_tdm.tsv\")\n# \n# str(tdm)\n# \n# n_tokens &lt;- tdm[,which(colnames(tdm) == \"which\")]\n# \n# \n# saveRDS(n_tokens, \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\nn_tokens &lt;- readRDS(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\n\n\n\nDraw Figure 1\n# Poisson model\nm &lt;- glm(n_tokens$which ~ 1, family=\"poisson\")\npoisson_mean &lt;- exp(coef(m))\npoisson_density &lt;- dpois(0:40, lambda = poisson_mean)\n\n\nn_texts &lt;- as.integer(table(n_tokens))\ntoken_count &lt;- as.integer(names(table(n_tokens)))\n\np1 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=lattice_ls, axis=axis_L, ylim=c(0, 53), xlim=c(-1.5, NA),\n  xlab.top = \"(a)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"\",\n  ylab=\"Observed\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=30, y=40, label=\"Observed frequency distribution\", \n               col=\"grey30\", cex=.9)\n    })\n\np2 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=lattice_ls, axis=axis_L, ylim=c(0, 85), xlim=c(-1.5, NA),\n  xlab.top = \"(b)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"Number of instances of which\",\n  ylab=\"Expected\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, type=\"h\")\n    panel.text(x=30, y=60, label=\"Expected frequency distribution\\n(Poisson model)\",\n               col=\"grey30\", cex=.9)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 1: Which in the Brown Corpus: (a) observed frequency distribution and (b) expected frequency distribution based on the Poisson model.\n\n\n\n\n\nThe most basic probability distribution that is available for modeling count variables is the Poisson distribution. In general, we can check the fit of a distribution to the observed token counts by comparing the observed distribution (Figure 1 a) to the one expected under a Poisson model. The expected distribution appears in Figure 1 b. We note a mismatch with the observed data: Its tails are too thin – that is, the observed token counts are more widely spread out; counts of 0 are severely underpredicted (or underrepresented).\nIn fact, it is often the case that the Poisson distribution offers a poor fit to (language) data. This is because it rests on a simplistic assumption: It assumes that the expected frequency of which (or: the underlying probability of using which) is the same in every text. In our case, where we are dealing with texts of roughly 2,000 words in length, the expected number of instances of which, on average, is 7.1. Due to sampling variation, the actual number of instances per text will vary around this average. This sampling variation is accounted for in the Poisson distribution, giving it the (near-)bell-shaped appearance in Figure 1 b.\nIn linguistic terms, the model assumes that each text in the Brown Corpus, irrespective of genre or the idiosyncrasies of its author, has the same underlying probability of using which (i.e. about 7 in 2,000; or 3.5 per thousand words). Even for a function word such as which, this assumption seems difficult to defend. For instance, certain genres may use more postmodifying relative clauses, leading to a higher expected rate of which for texts in this category.\n\n\nPoisson mixture distributions\nTo offer a more adequate abstraction (or representation) of the observed token distribution, the assumption of equal rates across texts needs to be relaxed. We want the model to be able to represent variation among texts, and to record the amount of variation suggested by the data. On linguistic grounds, for instance, we would expect function words to vary less from text to text than lexical words, which are more sensitive to register and topic. The idea is to have an additional parameter in the model that acts like a standard deviation, essentially capturing (and measuring) the text-to-text variability in occurrence rates.\nIt is for this purpose that Poisson mixture distributions were invented. One such mixture distribution is the negative binomial distribution, which is also referred to as a Poisson-gamma mixture distribution. This is actually a more transparent label, as we will see shortly.\nThe idea behind Poisson mixtures is rather simple. Since the Poisson distribution on its own fails to adequately embrace high and low counts, its mean is allowed to vary. By allowing the Poisson mean to vary, i.e. shift up and down the count scale (or left and right in Figure 1), the probability distribution is more flexible, which allows it to accommodate the tails of the distribution.\nPoisson mixtures therefore include an additional dispersion parameter (similar to a standard deviation) and the Poisson mean is replaced by a distribution of Poisson means. Note that the way in which the term “dispersion” is used here differs from the sense it has acquired in lexicography and corpus linguistics (the difference is explained in this blog post).\n\n\nDraw Figure 2\nset.seed(1985)\n\ndelta_sample = rGA(20, mu=1, sigma=.1)\nlambda_plot = 7\nplot1 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    for(i in 1:length(delta_sample)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\ndelta_sample2 = rGA(20, mu=1, sigma=.25)\nlambda_plot = 7\nplot2 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    panel.text(x=7, y=.22, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_sample2)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Mixing Poisson distributions: Each panel shows a sample of 20 Poisson distributions whose means vary around the grand mean of 7. The variation among Poissons is greater in the top panel.\n\n\n\n\nPoisson mixtures can be thought of as consisting of multiple Poisson distributions with different individual means. This is illustrated in Figure 2. To be able to show multiple distributions in one graph, we now leave out the spikes and connect the dots – a single distribution therefore appears as a bell-shaped profile that looks like a pearl necklace. Each panel shows 20 Poisson distributions, and each of these 20 distributions has a different mean. The means vary around 7, the overall mean of the Poisson mixture.\nThe distributions in the upper panel are spread out more widely than in the lower panel, and it is the newly introduced dispersion parameter that expresses the amount of variation among Poisson means. This basic idea applies to all Poisson mixture distributions. They are called ‘mixture distributions’ because they mix two probability distributions: (i) the familiar Poisson distribution and (ii) an additional probability distribution which describes the variability in the Poisson means. Simplifying slightly, Poisson mixtures only differ in the probability distribution they employ to describe the distribution of the Poisson means.\n\n\nThe gamma distribution as a model of text-to-text variation\nThe negative binomial distribution, for instance, relies on the gamma distribution to describe the text-to-text variability in occurrence rates. It is therefore also called a Poisson-gamma mixture distribution. Figure 3 shows the two gamma distributions that were used to create Figure 2. The dashed curve, which shows greater spread, belongs to the upper panel.\n\n\nDraw Figure 3\nlambda_plot = 7\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20/7), ylim=c(0,4.5),\n  par.settings=lattice_ls, axis=axis_L,\n  xlab.top=\"(a)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,4.5),\n  par.settings=lattice_ls, axis=axis_L,\n  xlab.top=\"(b)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=expression(\"Number of instances of \"~italic(which)),\n  panel=function(x,y,...){\n    panel.segments(x0=7, x1=7, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing the variability of text-to-text occurrence rates.\n\n\n\n\nFigure 3 a shows the gamma distributions on their actual scale. These are spread out around a value of 1, because they indicate variability in Poisson means on a multiplicative scale. It makes sense to center the distribution around 1, since the overall occurrence rate (multiplied by 1) should be at the center. The x-axis therefore denotes factors, which means that variability between Poisson means is expressed as ratios. The dashed curve, for instance, ranges from roughly 0.5 to 1.5, which means that most Poisson means are found within ± 50% of the overall mean. Since this multiplicative factor cannot be smaller than 0, we need a probability distribution that is bounded at zero (like the gamma distribution).\nPanel (b) translates these distributions to the occurrence rate scale. To create this graph, the factors (i.e. the x-values) in panel (a) were simply multiplied by the overall mean of 7. Now we see that, for the dashed curve, most occurrence rates vary between 4 and 11 instances per text.\n\n\nNegative binomial distribution applied to which\nLet us now apply the negative binomial distribution to the data for which in the Brown Corpus. We first check the fit of this new model to the data. Figure 4 shows that it provides a much closer approximation to the observed token distribution. It accomodates low and high counts and there seems to be no systematic lack of fit.\n\n\nFit negative binomial model in R\nm &lt;- gamlss(n_tokens$which ~ 1, family=\"NBI\", trace = FALSE)\n\nnb_density &lt;- dNBI(\n  0:40, \n  mu = exp(coef(m, what = \"mu\")),\n  sigma = exp(coef(m, what = \"sigma\")))\n\n\n\n\nDraw Figure 4\nxyplot(\n  n_texts ~ token_count,\n  par.settings=lattice_ls, axis=axis_L, ylim=c(0, 77), xlim=c(-1.5, NA),\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = expression(\"Number of instances of \"~italic(which)),\n  ylab=\"Number of texts\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=10, y=60, label=\"Poisson\", \n               col=\"grey30\", cex=.9, adj=0)\n    panel.text(x=20, y=12, label=\"Negative binomial\", \n               col=1, cex=.9, adj=0)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", type=\"l\")\n    \n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 4: Which in the Brown Corpus: Observed token distribution compared against the Poisson and the negative binomial model.\n\n\n\n\n\nLet us consider the gamma distribution that describes the text-to-text variability in occurrence rates. Its density appears in Figure 5, which includes two x-axes: A multiplicative scale (bottom) and a scale showing the expected number of instances in a 2,000-word text (the average text length in Brown). The gamma distribution is centered at 1 (multiplicative scale) and 7.1 occurrences (number of instances of which).\n\n\nDraw Figure 5\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 21/7), ylim=c(0,1.4),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.polygon(x = c(seq(.01, 2.8, length=100), (seq(.01, 2.8, length=100))),\n                 y = c(dGA(seq(.01, 2.8, length=100), mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n                       rep(0,100)),\n                 col=\"lightgrey\", border=F)\n    panel.segments(x0=1, x1=1, y0=0, y1=1.4, col=1)\n    \n    panel.segments(\n      x0 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3,  lwd=2, col=1, lineend=\"butt\", alpha=.5)\n    \n    panel.segments(\n      x0 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3, lwd=.5, col=1, lineend=\"butt\", alpha=.5)\n                      \n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, \n                         sigma=exp(coef(m, what = \"sigma\"))),\n                 type=\"l\")\n    \n    panel.segments(x0=-.05, x1=21/7, y0=1.3, y1=1.3)\n    panel.segments(x0=seq(0,20,5)/exp(coef(m, what = \"mu\")), \n                   x1=seq(0,20,5)/exp(coef(m, what = \"mu\")), y0=1.3, y1=1.38)\n    \n    panel.text(x=seq(0,20,5)/exp(coef(m, what = \"mu\")), y=1.6,\n               label=seq(0,20,5), cex=.8)\n    \n    panel.text(x=1.5, y=2, label=expression(\"Number of instances of \"~italic(which)))\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The gamma distribution describing the variability of text-to-text occurrence rates of which in the Brown Corpus.\n\n\n\n\nThe gamma distribution represents a set of values, which specify the deviation of Poisson means from their overall mean in relative terms, as factors. For example, if the gamma distribution is restricted to the range [0.6; 1.7], the Poisson means will vary by a factor of 0.6 to 1.7 around their overall average. For a grand mean of 7, the Poisson means are then spread out between 7 \\(\\times\\) 0.6 = 4.2 and 7 \\(\\times\\) 1.7 = 11.9.\nThe grey vertical lines facilitate interpretation of the distribution: They show where the middle 50% of the texts (thick lines) and the middle 90% of the texts (thin lines) lie. Thus, half of the texts have an underlying expected number of occurrences between roughly 5 and 9; 90% of texts have expected counts between 2.5 and 14. This gives us a good idea of the underlying text-to-text variation in the Brown Corpus.\n\n\nGraphical derivation of the negative binomial distribution\nTo get a better understanding of the negative binomial distribution shown in Figure 4, let us now build one from scratch. Recall that the gamma distribution that is built into the negative binomial model provides us with a set of values with mean 1. We will refer to scores generated from this kind of gamma distribution as \\(\\delta\\) scores. To spread out the Poisson means, the overall mean is multiplied by the \\(\\delta\\) scores drawn from the gamma distribution. Since the \\(\\delta\\) scores are centered at 1, the overall mean is still 7. A gamma distribution that is spread out more widely produces more widely dispersed Poisson means.\nEssentially, then, a negative binomial distribution represents a batch of Poisson distributions whose individual means are spread out around the overall mean. This conceptual explanation of the negative binomial distribution illustrates the role of the gamma distribution and its auxiliary parameter \\(\\phi\\). We can translate this illustration into a simple simulation experiment. If we average over a large number of Poisson distributions produced by this procedure, we should arrive at the corresponding negative binomial distribution.\nThis is illustrated in Figure 6, which was constructed in the following way:\n\n\nDraw Figure 6\nset.seed(1985)\n\nset_nu = 2\ndelta_s = rGA(1000, mu=1, sigma=sqrt(1/set_nu))\n\nlambda_p = 7\npoisson_pool = matrix(NA, nrow=1000, ncol=21)\nfor (i in 1:1000){\n  poisson_pool[i,] = dpois(0:20, lambda=lambda_p*delta_s[i])\n}\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.45),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y){\n    panel.segments(x0=lambda_p, x1=lambda_p, y0=0, y1=.25, col=\"black\")\n    panel.text(x=7, y=.3, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_s)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", type=\"l\", alpha=.03)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", pch=16, cex=.4, alpha=.03)\n      }\n    # panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=\"white\", lwd=4)\n    # panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n    #              type=\"l\", col=\"white\", lwd=4)\n    panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n                 type=\"l\", col=\"white\", lwd=2)\n    panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=1, lty=\"22\", lineend=\"butt\")\n    })\n\nprint(p1, position=c(0,0,1,.95))\n\n\n\n\n\n\n\n\n\nFigure 6: Graphical derivation of the NB2 distribution: The blue curve shows the approximation based on averaging over 1,000 Poisson distributions whose means are random draws from a gamma distribution with \\(\\small{\\phi^{-1}}\\) = 0.25. The red curve shows the actual negative binomial distribution with \\(\\small{\\phi^{-1}}\\) = 0.25.\n\n\n\n\n\nSet the overall mean to 7, and the negative binomial dispersion parameter (here: the scale parameter) \\(\\phi^{-1}\\) to 0.5 (which is close to the value obtained for which in the Brown Corpus).\nTake 1,000 random draws from a gamma distribution defined by \\(\\mu\\) = 1 and \\(\\phi^{-1}\\) = 0.5. We refer to these draws as \\(\\delta\\) scores. The average of these scores is 1.\nMultiply 7 by these 1,000 \\(\\delta\\) scores. This produces 1,000 Poisson means, and hence 1,000 Poisson distributions.\nGraph these 1,000 Poisson distributions as pearl necklaces, adding transparency to avoid a cluttered display.\nDetermine the average probability for each count (0, 1, 2, etc.) by averaging over the 1,000 Poisson probabilities for each specific count. These averages should then resemble a negative binomial distribution with \\(\\mu\\) = 7 and \\(\\phi^{-1}\\) = 0.5.\n\nFigure Figure 6 shows the result of this simulation: The actual negative binomial distribution for these data is shown as a white trace, and the results of our simulation, i.e. average probability across the 1,000 simulated Poisson distributions, is shown as a dashed profile. The match is pretty good.\n\n\nDifferent parameterizations of the negative binomial distribution\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two ways. These different parameterizations have consequences for our interpretation of the negative binomial dispersion parameter returned by an analysis. This means that if we are interested in the dispersion parameter, we must know which parameterization our analysis is using. For an overview of which R packages/functions rely on which version of the negative binomial distribution, see this blog post\n\n\n\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {The Negative Binomial Distribution: {A} Visual Explanation},\n  date = {2023-12-12},\n  url = {https://lsoenning.github.io/posts/2023-11-16_negative_binomial/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “The Negative Binomial Distribution: A\nVisual Explanation.” December 12, 2023. https://lsoenning.github.io/posts/2023-11-16_negative_binomial/."
  },
  {
    "objectID": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "href": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "title": "Modeling the interpretation of quantifiers using beta regression",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(betareg)\nlibrary(lattice)\nlibrary(brms)\nlibrary(knitr)\nlibrary(tidybayes)\nlibrary(kableExtra)\nlibrary(sjPlot)\nlibrary(lmtest)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\ndirectory_path &lt;- \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/\"\n\n\nThe purpose of this blog post is to show how to model proportions using beta regression. This procedure is particularly suitable for outcome variables that assume values in the interval (0, 1) – but excluding 0 and 1 –, and which do not summarize underlying raw counts (e.g. .30 for 3 out of 10 “successes”), in which case they could be handled using logistic regression. Examples are proportions, rates, and other indices such as corpus-linguistic dispersion measures.\nOur illustrative data represent such a fractional variable: How speakers interpret the quantifiers few, some, many, and most on the percentage (or proportion) scale. Few, for instance, may be understood as referring to about 10% of the total. A critical feature of our data is that they do not include proportions of 0 and 1. And this is in fact a requirement of beta regression, which does not work if the outcome includes values of 0 and/or 1.\nWe will start by introducing our illustrative data, and then model them using different beta regression models.\n\nLinguistic research context\nThe need to model the perception of quantifiers arose in the context of the Bamberg Survey of Language Variation and Change (see Krug and Sell 2013), a large-scale survey on the use of various lexical and grammatical structures in varieties of English. The grammar part of the questionnaire is designed to permit estimates of the prevalence of a broad variety of (morpho-)syntactic features in two registers (speech vs. writing). To this end, respondents are asked to indicate, on a 6-point scale, how prevalent a specific feature is in their home country or region. Each sentence is presented in two modes (auditorily and in writing) and participants indicate how many speakers use this kind of structure by choosing from the following options: no-one, few, some, many, most, everyone.\nThe analysis and interpretation of these data can profit from the fact that these quantifiers can be understood as relative frequencies. When analyzing the data, we can assign sensible numeric scores to the ordered categories and then interpret the resulting quantities as approximate estimates of the prevalence of a specific feature. For more background on this approach and its measurement-theoretic and statistical drawbacks see Sönning (2024).\n\n\nExperimental work on the perception of quantifiers\nA few studies have looked at how speakers interpret the quantifiers few, some, many, and most (Newstead, Pollard, and Riezebos 1987; Borges and Sawyers 1974; Stateva et al. 2019; Tiel, Franke, and Sauerland 2021). Proportional estimates from the literature are collected in Figure 1. For more information on the corresponding studies, please refer to Sönning (2024).\n\n\n\n\n\n\n\n\n\nFigure 1: Literature summary: The perception of quantifiers, expressed as a percentage.\n\n\n\n\nWhile estimates for the individual items show some variation across studies (and experiments within studies), they allow us to roughly pin down the quantitative meaning of these expressions. If we take a weighted average across the studies, where the weight of the individual percentages is proportional to the number of subjects, we obtain the following means:\n\nfew (.11, or 11%)\nsome (.27, or 27%)\nmany (.67, or 67%)\nmost (.83, or 83%)\n\n\n\nIllustrative data: The perception of quantifiers\nI collected additional data on the perception of these expressions, using as participants university students and colleagues that took part in the English Linguistics research seminar at the University of Bamberg in the winter term of 2023. I obtained data from 20 individuals (around 2/3 being students) by handing out paper slips with the following instructions:\n\n\n\nSurvey task: Paper slips with instructions used for data collection\n\n\nParticipants were given two minutes to complete the task. I then collected the paper sheets and (later) entered the data into a spreadsheet (in wide format):\n\ndat &lt;- readxl::read_xlsx(paste0(directory_path, \"data/data_quantifiers.xlsx\"))\n\n\n\n\n\n\n\nsubject\nfew\nsome\nmany\nmost\n\n\n\n\nsubj_01\n5\n15.0\n30.0\n50.0\n\n\nsubj_02\n20\n35.0\n70.0\n80.0\n\n\nsubj_03\n10\n27.5\n65.0\n85.0\n\n\nsubj_04\n8\n20.0\n51.0\n80.0\n\n\nsubj_05\n12\n30.0\n50.0\n75.0\n\n\nsubj_06\n15\n35.0\n60.0\n90.0\n\n\nsubj_07\n25\n40.0\n67.5\n92.5\n\n\nsubj_08\n10\n30.0\n60.0\n80.0\n\n\nsubj_09\n15\n33.0\n67.0\n85.0\n\n\nsubj_10\n10\n33.0\n67.0\n90.0\n\n\nsubj_11\n10\n25.0\n50.0\n75.0\n\n\nsubj_12\n10\n40.0\n70.0\n90.0\n\n\nsubj_13\n7\n20.0\n60.0\n85.0\n\n\nsubj_14\n15\n40.0\n69.0\n90.0\n\n\nsubj_15\n15\n30.0\n75.0\n85.0\n\n\nsubj_16\n6\n14.0\n75.0\n91.0\n\n\nsubj_17\n5\n12.5\n65.0\n80.0\n\n\nsubj_18\n10\n25.0\n70.0\n90.0\n\n\nsubj_19\n15\n40.0\n75.0\n90.0\n\n\nsubj_20\n10\n20.0\n80.0\n90.0\n\n\n\n\n\n\n\n \nOur first step is to rearrange these data from wide to long form, and to translate percentages into proportions, as this is the scale on which beta regression operates. We also change the order of the quantifiers (from the default alphabetical arrangement) based on the relative frequency they express (see Figure 1).\n\nd &lt;- dat |&gt; \n  gather(\n    few:most, \n    key = quantifier, \n    value = percentage) |&gt; \n  mutate(\n    proportion = percentage/100\n  ) \n\nd$quantifier &lt;- factor(\n  d$quantifier, \n  levels = c(\"few\", \"some\", \"many\", \"most\"),\n  ordered = TRUE)\n\n\n\n\n\n\n\n\n\n\nFigure 2: Ratings collected from 20 informants.\n\n\n\n\nLet us start by looking at the distribution of the responses for each quantifier using a dot diagram. Figure 2 shows that there is some variation among subjects with regard to the perceived meaning of these expressions. For many and most, in particular, there is one unusually low estimate.\n\n\n\n\n\n\n\n\n\nFigure 3: Line plot linking the ratings provided by the same informant.\n\n\n\n\nTo check whether these outliers are responses from the same individual, we look at the data using a line plot that links observations from the same subject. In Figure 3, each profile represent a subject. The two unusually low responses for many and most are indeed due to the same individual, who also provided relatively (though not unusually) low ratings for few and some.\nApart from the fact that this respondent provides relatively low estimates for all quantifiers, there is no immediate reason why their data should be excluded from the analysis. We will therefore start by using the full data set and then rely on model diagnostics to see whether the data and model suggest that this informant be excluded from the analysis.\n\n\nBeta regression\nWe now look at how to use beta regression to model these data. First, however, let us be clear about the purpose of our analysis – i.e. the kind of information we wish to extract from the data. Our goals are descriptive and we will use beta regression to summarize the data. The following quantitative features of the data will be of interest:\n\nAveraging over the speakers in our sample, what is the typical proportional interpretation of each quantifier?\nSeeing that we are dealing with a rather small sample of speakers, what is the statistical uncertainty surrounding these typical values?\nHow much do speakers vary in their interpretation of the individual expressions, i.e. how dispersed are their perceptions around the typical interpretation?\n\nBeta regression can address these questions by providing (i) typical values (average proportions) for each quantifier (e.g. .12, or 12%, for few), (ii) a confidence (or posterior) interval for these estimates (e.g. 95% CI [.10; .14]), and (iii) a distributional summary of the dot diagrams we saw in Figure 2. To this end, the beta distribution serves as an abstraction of the observed distribution of responses and it allows us to make statements about, say, the share of speakers who interpret few as denoting a relative frequency of .05 or less. This kind of information is essential when we are interested in the stability (or consistency) of interpretations across speakers.\nOur main focus will be on how to run beta regression using R. We therefore only include a few essentials on the beta distribution and beta regression; for more background on both, please refer to this excellent blog post (Heiss 2021a).\n\n\nThe beta distribution\nThe beta distribution is a probability distribution bounded between (but excluding) 0 and 1. It is defined by two shape parameters, \\(a\\) and \\(b\\). Figure 4 shows two beta distributions with the same mean but different spreads. Note how, for each distribution, \\(a\\) is .30 (or 30%) of the sum of \\(a\\) and \\(b\\). In fact, \\(a/(a+b)\\) denotes the mean of the distribution. Further, we note that the spread of the distribution is related to the sum of \\(a\\) and \\(b\\): The greater the sum, the “tighter”, or more peaked, the beta distribution.\n\n\n\n\n\n\n\n\nFigure 4: The shape parameters \\(a\\) and \\(b\\) define the beta distribution.\n\n\n\n\n\nAs explained in much more detail by Heiss (2021a) in his blog post, the beta distribution can alternatively be written down using its mean \\(\\mu\\) and precision \\(\\phi\\). We have already encountered the mean, which is \\(a/(a+b)\\). The precision is simply \\((a+b)\\), and it is reflected in the spread of the distribution (see Figure 4): The greater the precision, the tighter the distribution of proportions about their mean. We can go back and forth between the two parameterization of the beta distribution as follows:\n\\[\\begin{align}\n\\mu &= a/(a+b) \\\\\n\\phi &= a+b \\\\\n\\\\\n\na &= \\mu\\phi \\\\\nb &= (1-\\mu)\\phi\n\\end{align}\\]\nWe need to be able to switch between these parameterizations. The reason is that regression models work with \\(\\mu\\) and \\(\\phi\\). The parameter \\(\\mu\\) answers questions (i) and (ii). To be able to draw a beta distribution, however, and use it to summarize variation in the perception of a specific quantifier in a population of speakers, we need the shape parameters \\(a\\) and \\(b\\). We therefore use code provided by Heiss (2021a) to write a function that allow us to switch from the \\(\\mu\\)-\\(\\phi\\) to the \\(a\\)-\\(b\\) parameterization:\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\n\n\nOverview of regression models\nWe will run a series of regression models, which differ in structure. We will start with frequentist models, using the {betareg} package (Cribari-Neto and Zeileis 2010) and then move on to Bayesian regression with the {brms} package (Bürkner 2017) to be able to incorporate by-subject random intercepts into our model.\nThe following table gives an overview of the series of models we are about to fit. Each model allows the outcome variable to vary by quantifier and therefore returns an estimate of \\(\\mu\\) for each expression. Model I assumes that quantifiers are interpreted with the same precision \\(\\phi\\), so it estimates only one \\(\\phi\\) parameter. This means that each quantifier has the same stability of interpretation in the populations of speakers represented by our sample. In contrast, Model II allows the precision to vary across quantifiers. This means that the model provides leeway for expressions to differ in the level of stability (or consistency) with which they are interpreted. A separate \\(\\phi\\) parameter is therefore estimated for each. This type of model has been referred to as a variable dispersion beta regression model (see Cribari-Neto and Zeileis 2010). Finally, for Model III we will switch to the {brms} package (and Bayesian inference), to be able to include random effects into our model.\n\n\n\n\n\nModel\nConstant precision\nRandom intercepts\nPackage\n\n\n\n\nI\nYes\nNo\nbetareg\n\n\nII\nNo\nNo\nbetareg\n\n\nIII\nYes\nYes\nbrms\n\n\n\n\n\n\n\nModel I\nWe start with a frequentist model (using {betareg}) that allows \\(\\mu\\) to vary across quantifiers but assumes a constant precision \\(\\phi\\). We model \\(\\mu\\) on the log-odds (i.e. logit) scale. We also drop the intercept from the model to directly obtain logits for each quantifier:\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d, \n  link = \"logit\")\n\nprint(m1)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier, data = d, link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9542         -0.9348          0.5649          1.6346  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n23.61  \n\n\nBefore we turn to the estimates returned by the model, let’s look at a number of diagnostic plots, which are shown in Figure 5:\n\nWhen graphing residuals against fitted values, we usually look for indications of heteroskedasticity, i.e. whether the spread of residuals increases with the fitted values. This is not the case for our data and model. However, the residuals surrounding the fitted values for many and most each include a large negative residual, i.e. a response that is unusually low for these quantifiers.\nThe quantile-quantile plot checks for normality of residuals. We have added a dot diagram at the right margin, which gives a more intuitive impression of the shape of the distribution and potential outliers. Both arrangements indicate that the residuals are negatively skewed (rather than normal), with two unusually large negative deviations.\nCook’s distance expresses the influence individual observations have on the regression coefficients. There are are two data points that appear to be particularly influential (number 41 and 61).\nSince the factor Subject is so far not included in the analysis, our model does not know about the fact that responses are clustered. This may induce non-independence and lead to correlated errors, because we would expect responses by the same subject to be similar. The graph shows residuals grouped by Subject. If responses were independent, the residuals should not correlate within the factor Subject. This means that residuals should not vary systematically across subjects. If correlated errors (and hence non-independence in the data) were indeed no concern, the 20 sets (of four residuals each) would look like they represent random draws from the dot diagram shown in panel (b). However, we observe that responses by the same subject in fact tend to be alike. Subjects 1, 4, and 11, for instance, have consistently negative residuals, indicating that they gave responses that were, overall, systematically lower compared to those of the other subjects.\n\n\n\nDraw figure\nm_diagnostics &lt;- d\nm_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3.6, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .1, .2, .3)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 5: Diagnostic plots for Model I.\n\n\n\n\n\nOur diagnostic plots reveal two things:\n\nThere are two unusual (and influential) data points\nThe residuals are not independent but correlated with the factor Subject.\n\nIt turns out that the two unusual and influential data points are from subject 1. These are the responses for many and most, which already stood out above in Figure 3. We will remove subject 1 from the data. As for the non-independence of errors, Model III will include by-subjects random intercepts, to account for the correlation of residuals with the factor Subject.\nLet us remove subject 1, refit Model I, and again draw diagnostic plots:\n\nd_19 &lt;- subset(d, subject != \"subj_01\")\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\n\n\nDraw figure\nm_diagnostics &lt;- d_19\nm_diagnostics$subj_index &lt;- as.numeric(factor(d_19$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4.2),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .05)),\n                   x=list(at=c(1, 20, 40, 60))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 77))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:19, x1=1:19, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 6: Diagnostic plots for Model I, after exclusion of subject 1.\n\n\n\n\n\nFigure 6 looks much better:\n\nThere are no clear outliers when looking at residuals by quantifier.\nThe residuals are more nearly normally distributed.\nThere are no influential data points.\n\nAs panel (d) shows, however, errors are still correlated with Subject. We will address this issue further below, in Model III.\nLet us now use our model to address our descriptive objectives. To answer question (i), we need to consider the \\(\\mu\\) coefficients. These are on the logit scale, so we need to back-transform them to proportions using the function plogis():\n\nmu_m1 &lt;- plogis(coef(m1)[1:4])\nphi_m1 &lt;- coef(m1)[5]\n\nround(mu_m1, 2)\n\n quantifierfew quantifiersome quantifiermany quantifiermost \n          0.12           0.29           0.66           0.85 \n\n\nTo answer question (ii), we obtain 95% confidence intervals for these using the function confint(), keeping in mind that we again need to translate logits back into proportions. We collect estimates in Table 1.\n\n\nConstruct table of estimates\nci_mu_m1 &lt;- plogis(confint(m1)[1:4,])\n\n\nm1_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m1), \n  paste0(\"[\", numformat(ci_mu_m1[,1]), \"; \", numformat(ci_mu_m1[,2]), \"]\"))\n\nrownames(m1_estimates) &lt;- NULL\ncolnames(m1_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm1_estimates |&gt; kable()\n\n\n\n\nTable 1: Typical values and 95% confidence intervals based on Model I.\n\n\n\n\n\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .15]\n\n\nsome\n.29\n[.25; .32]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.82; .88]\n\n\n\n\n\n\n\nThe following code uses the {sjPlot} package to print a similar table (output not shown):\n\ntab_model(m1, transform = \"plogis\")\n\nTo be able to make statements about the stability of interpretations across speakers, we need the shape parameters of the beta distributions. Consider, as an example, the quantifier few. The estimates returned by our model are the average proportion \\(\\mu =\\) 0.12 and the precision \\(\\phi =\\) 35.3. We can use the function defined above to translate these into the shape parameters \\(a\\) and \\(b\\):\n\nshape1_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape1\nshape2_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape2\n\nThese shape parameters define the beta distribution that describes the spread of responses, i.e. the variation among subjects in the relative-frequency interpretation of few. The percentages reported in Table 1 are averages over subjects. To obtain information on the distribution of responses around these typical values, we need to look at the associated beta density, which appears in Figure 7. This distribution is a model-based estimate of the variability in the responses across individuals in the population of speakers represented by the subjects in our sample. It shows us how stable (or consistent) the interpretation of few is across speakers.\n\n\n\n\n\n\n\n\n\nFigure 7: The beta density for few, based on model I.\n\n\n\n\nWe can summarize the information provided by such beta densities using informative quantiles; these can be located with the function qbeta(). A quantile is an x-value (here: a proportion/percentage) below which a certain portion of the probability mass lies. The .25 quantile, for instance, marks the x-value that cuts off the lower tail of the distribution that contains 25% of the mass.\nIn Figure 8, two intervals are marked using grey shading. The darker shade denotes the interval covering the central 50% of the subjects. It extends from the .25 quantile to the .75 quantile. The lighter shades denote the region where the middle 80% of the subjects are found. These quantiles tell us something about the speaker-to-speaker variability in the interpretation of few. While the population average is estimated at .12 (or 12%), speakers vary around this value: the central 50% of the speakers give estimates between .07 and .16, the central 80% of the population are found between .05 and .21. We will refer to these as coverage intervals.\n\n\n\n\n\n\n\n\nFigure 8: The beta distribution for few, including informative quantiles and coverage intervals; based on model I.\n\n\n\n\n\nTable 2 reports these quantiles, i.e. the central 50% and 80% coverage intervals for all quantifiers.\n\n\nConstruct table with quantiles\nbeta_quantiles_m1 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[1], shape2=shape2_m1[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[2], shape2=shape2_m1[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[3], shape2=shape2_m1[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[4], shape2=shape2_m1[4])))\n\nbeta_quantiles_m1 &lt;- cbind(c(\"few\", \"some\", \"many\", \"most\"),\n                        beta_quantiles_m1)\n\ncolnames(beta_quantiles_m1) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m1 |&gt; kable()\n\n\n\n\nTable 2: 50% and 80% coverage intervals, based on Model I.\n\n\n\n\n\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.16\n.20\n\n\nsome\n.19\n.23\n.34\n.39\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.77\n.82\n.90\n.92\n\n\n\n\n\n\n\nWe can now draw a graph based on Model I that gives a visual summary of the data: Figure 9 shows the typical values for each quantifier (average proportions), along with their 95% confidence interval and the associated beta density. Since Model I assumes constant precision across quantifiers, the visual spread of the beta distributions is constrained to be constant across these. The differences in spread that are evident from Figure 9 reflect boundary effects, as the variability of proportions is naturally constrained near the scale endpoints.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proporiton\", ylab=\"Density               \",\n  xlab.top=\"Model I\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n      type=\"l\")\n    panel.points(x=mu_m1, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=13, y1=13)\n    panel.text(x=mu_m1, y=11, label=numformat(mu_m1))\n    panel.text(x=mu_m1, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 9: Visual summary of Model I: Typical values, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nModel II\nNext we fit a model that allows the precision \\(\\phi\\) to vary across quantifiers; \\(\\phi\\) is now modeled on the natural log scale. We again drop the intercept from the model to directly obtain logits (\\(\\mu\\)) and natural logs (\\(\\phi\\)) for each quantifier:\n\nm2 &lt;- betareg(\n  proportion ~ -1 + quantifier | -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\nprint(m2)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier | -1 + quantifier, data = d_19, \n    link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9912         -0.9013          0.6445          1.7692  \n\nPhi coefficients (precision model with log link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n         3.903           3.251           3.464           3.781  \n\n\nThe precision parameters (Phi coefficients in the printed output) are quite similar across quantifiers. Let us therefore formally compare models I and II to see whether the added complexity is needed to adequately describe the observed data. As illustrated by Cribari-Neto and Zeileis (2010), the function lrtest(), which compares nested models using a likelihood-ratio test, may be used to this end:\n\nlrtest(m1, m2)\n\nLikelihood ratio test\n\nModel 1: proportion ~ -1 + quantifier\nModel 2: proportion ~ -1 + quantifier | -1 + quantifier\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)\n1   5 100.77                     \n2   8 102.05  3 2.5591     0.4647\n\n\nWe note that there is little evidence from the data that the quantifiers are interpreted with different precision. Instead, the variability of responses around the typical value for each quantifier appears to be rather stable (on the logit scale) across expressions.\nFor illustrative purposes, we nevertheless stick to this model to extract relevant quantities. We start by back-transforming the logit-scaled coefficients and confidence intervals for \\(\\mu\\) using the function plogis() and collect the estimates and their confidence intervals in Table 3. To summarize the beta distributions, we present informative quantiles in Table 4.\n\n\nConstruct table of estimates\nmu_m2 &lt;- plogis(coef(m2)[1:4])\nci_mu_m2 &lt;- plogis(confint(m2)[1:4,])\nphi_m2 &lt;- exp(coef(m2)[5:8])\n\nshape1_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape1\nshape2_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape2\n\nm2_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m2), \n  paste0(\"[\", numformat(ci_mu_m2[,1]), \"; \", numformat(ci_mu_m2[,2]), \"]\"))\n\nrownames(m2_estimates) &lt;- NULL\ncolnames(m2_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm2_estimates |&gt; kable()\n\n\n\n\nTable 3: Typical values and 95% confidence intervals based on Model II.\n\n\n\n\n\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .14]\n\n\nsome\n.29\n[.25; .33]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.83; .88]\n\n\n\n\n\n\n\n\n\nConstruct table of quantiles\nbeta_quantiles_m2 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[1], shape2=shape2_m2[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[2], shape2=shape2_m2[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[3], shape2=shape2_m2[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[4], shape2=shape2_m2[4])))\n\nbeta_quantiles_m2 &lt;- cbind(\n  c(\"few\", \"some\", \"many\", \"most\"),\n  beta_quantiles_m2)\n\ncolnames(beta_quantiles_m2) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m2 |&gt; kable()\n\n\n\n\nTable 4: 50% and 80% coverage intervals, based on Model II.\n\n\n\n\n\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.07\n.09\n.15\n.18\n\n\nsome\n.18\n.23\n.35\n.41\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.78\n.82\n.89\n.92\n\n\n\n\n\n\n\nA visual summary of Model II appears in Figure 10. We observe that the beta density for few is more peaked than in Figure 9. This information can also be read from the printed regression table, which shows that few has the highest precision. This is also reflected in the coverage intervals. Whereas in Model I, the 80% coverage intervals extended from .06 to .20, Model II suggests a slightly narrower 80% coverage interval, ranging from .07 to .18.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model II\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n      type=\"l\")\n    panel.points(x=mu_m2, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=13, y1=13)\n    panel.text(x=mu_m2, y=11, label=numformat(mu_m2))\n    panel.text(x=mu_m2, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 10: Visual summary of Model II: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nModel III\nTo address the issue of correlated errors (i.e. the non-independence of observations), we now add by-speaker random intercepts to our model. Since this feature is not implemented in the {betareg} package, we switch to Bayesian regression using the {brms} package. As we will now be explicitly modeling Speaker as a source of variation, let us give subject 1 a second chance. After all, the primary reason for excluding them from our analysis were the unusually low values given for many and most. We will then again look at diagnostic plots to see whether the data points from this speaker also appear problematic in the context of Model III.\nAs can be seen from the following R code, using {brms} requires a change in model syntax. We will rely on the default priors.\n\nm3 &lt;- brm(\n  proportion ~ -1 + quantifier + (1|subject),\n  data = d,\n  family = Beta(),\n  chains = 4, iter = 3000, warmup = 1000, cores = 4,\n  backend = \"cmdstanr\",\n  file = paste0(directory_path, \"m3\")\n)\n\nm3 &lt;- readRDS(paste0(directory_path, \"m3.rds\"))\n\nLet us first draw some diagnostic plots. Since we are dealing with a Bayesian regression model, some of the quantities we use for plotting are different. Their meaning and interpretation, however, is very similar:\n\nInstead of weighted residuals, we look at Pearson residuals.\nWe will use Pareto k values as influence indicators (for more information on this, see this paper and this video tutorial). For these, thresholds have been proposed to signal “good”, “ok”, “bad”, and “very bad” Pareto k values.\nSince correlated errors (within subject) should no longer be a concern, we will instead group influence measures (i.e. Pareto k values) by subject, to see if there are any influential individuals.\n\nThe diagnostic plots appear in Figure 11. We note the following:\n\nThere appear to be no outliers when looking at residuals by quantifier. This is because the random intercept for subject 1 now accomodates their unusually low responses.\nThe distribution of the Pearson residuals looks OK.\nThere is only one “bad” data point.\nThis “bad” observation stems from subject 16, whose responses sit less well with the model, on average. It turns out that the responses of subject 16 were drawn to the extremes: They gave relatively low responses to few (6%) and some (14%), but quite high responses to many (75%) and most (91%). This tendency towards the extremes is not captures by the random intercept for subject 16, since this parameter only represents their average response (46.5%), which is not particularly unusual.\n\nOverall, then, we are happy with Model III and welcome subject 1 back into the dataset.\n\n\nDraw figure\nloo_m3 &lt;- loo(m3)\nloo_m3_pointwise &lt;- data.frame(loo_m3$pointwise)\n\nm3_diagnostics &lt;- d\nm3_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm3_diagnostics$fitted_with_random_intercepts &lt;- qlogis(fitted(m3)[,1])\nm3_diagnostics$residual &lt;- residuals(m3, type = \"pearson\")[,1]\nm3_diagnostics$influence_pareto_k &lt;- loo_m3_pointwise$influence_pareto_k\nm3_diagnostics &lt;- m3_diagnostics |&gt; \n  group_by(quantifier) |&gt; \n  mutate(\n    fitted_quantifier = mean(fitted_with_random_intercepts))\n\n\np1 &lt;- xyplot(residual ~ fitted_quantifier, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m3_diagnostics) - 1/2)/nrow(m3_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m3_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m3_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-2, ybottom=-3,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(influence_pareto_k ~ 1:nrow(m3_diagnostics), data=m3_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.text(x=-5, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n       })\n\nsubj_pareto_k &lt;- m3_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_pareto_k = min(influence_pareto_k),\n    max_pareto_k = max(influence_pareto_k)\n  )\n\np4 &lt;- xyplot(influence_pareto_k ~ subj_index, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(d) Influence by subject\\n\",\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.text(x=-1.35, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, \n                        y0=subj_pareto_k$min_pareto_k, \n                        y1=subj_pareto_k$max_pareto_k)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 11: Diagnostic plots for Model III.\n\n\n\n\n\nWe now use Model III to address questions (i) and (ii). As described in more detail in the Marginal Effects Zoo book and in another fantastic blog post by Heiss (2021b), we use the {tidybayes} package to process the posterior distribution.\nWe use the model to generate posterior predictions and then summarize these. The kind of prediction we are interested in at the moment (for questions i and ii) are typical values, i.e. averages. These are sometimes referred to as expected values. We therefore use the function epred_draws(), which uses the full posterior distribution to generate expected predictions (epred) for conditions, i.e. (constellations of) predictor values. We need to specify the conditions of interest using the argument newdata. The additional argument re_formula = NA tells epred_draws() to disregard the by-subject random intercepts in this predictive task. After all, we are presently only concerned with the average across subjects.\n\nm3_epred &lt;- m3 |&gt;  \n  epred_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\nWe then summarize the posterior distribution of these expected values using the median and the .025 and .975 quantiles, which provide uncertainty estimtes similar to the 95% CIs reported for the other models.\n\nmu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    median = median(.epred)\n  )\n\nci_mu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    ci_lower = quantile(.epred, .025),\n    ci_upper = quantile(.epred, .975)\n  )\n\nWe collect these point and interval estimates for the expected value (i.e. proportions) of each quantifier in Table 5.\n\n\nConstruct table of estimates\nm3_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m3$median), \n  paste0(\"[\", numformat(ci_mu_m3$ci_lower), \"; \", numformat(ci_mu_m3$ci_upper), \"]\"))\n\nm3_estimates |&gt; kable()\n\n\n\n\nTable 5: Typical values and 95% posterior intervals based on Model III.\n\n\n\n\n\n\n\nQuantifier\n\n\n\n\n\n\nfew\n.11\n[.09; .14]\n\n\nsome\n.27\n[.23; .32]\n\n\nmany\n.64\n[.59; .69]\n\n\nmost\n.84\n[.81; .87]\n\n\n\n\n\n\n\nOur next task is to obtain estimates for the variability of subjects’ perceptions around these typical values. To this end, we also use posterior predictions from the model. We are no longer interested in the expected (or typical) value, however, but in the specific responses given by the speakers. We therefore use the function predicted_draws() which retains this variability in the distribution of responses.\nWe must make sure, however, that the variability among subjects, which is captured by the random intercepts in our model, is worked into these predictions. Thus, in a model with random intercepts, the extraction of information about the beta distribution describing the variability of interpretations across subjects requires some care. This is because between-subjects variation is now absorbed by this model component. If we fail to actively incorporate this source of variation into our model-based predictions, these do not answer question iii, which asks about the variation across subjects in the interpretation of quantifiers.\nFor purposes of illustration, let us first (inappropriately) disregard the by-speaker random effects. This means that we use the expected predictions generated above to construct beta distributions. This yields Figure 12, which does not show beta densities with the intended meaning. Recall that our question (iii) concerned variability across speakers. In Model III, between-subjects variation is captured by the random intercept SD (or variance) in our model, and this parameter is not built into the graph. As a result, the beta distribution are not spread out enough.\n\nm3_pred_wrong &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=.5, y=7, label=\"Warning: Beta densities do not\\nhave the intended meaning\", \n               col=\"darkred\", alpha=.7, cex=.9, lineheight=.9)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 12: Wrong visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Distribution of by-speaker averages on the proportion scale.\n\n\n\n\nBefore we go further, it is perhaps worth considering what kind of information the densities in Figure 12 do show. They show the “residual” variation in perceived interpretation for each quantifier, after removing the model component that describes the between-speaker variability in the overall average response. Thus, the by-speaker random intercepts capture the extent to which subjects vary in their average response. To get a better understanding of the kind of variability represented by the random intercepts, Figure 13 shows the distribution of speaker averages. Most speakers vary between roughly .40 and .55. It is this kind of between-speaker variation that is captured by the random intercepts in the model.\n\n\nConstruct wrong table of quantiles\nbeta_quantiles_m3_wrong &lt;- m3_pred_wrong |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3_wrong |&gt; kable()\n\n\n\n\nTable 6: 50% and 80% coverage intervals, based on Model III (random intercept variation not included).\n\n\n\n\n\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.14\n.18\n\n\nsome\n.19\n.23\n.32\n.36\n\n\nmany\n.55\n.59\n.69\n.73\n\n\nmost\n.77\n.81\n.88\n.91\n\n\n\n\n\n\n\nAfter removing this source of variation, responses will still vary, due to other sources of variation. One such source is measurement error – speakers’ response to our short survey may fluctuate (perhaps erratically) over time. If we handed the same paper slip to our 20 respondents one year later, they would probably give (slightly) different responses. Further, speakers may not only differ in their average response (as captured by the random intercepts), but also in their perception of the individual quantifiers. For instance, a speaker may have a systematically lower perception of the quantifier few compared to other speakers. In statistical terms, this would be referred to as a subject-by-quantifier interaction.\nTable 6 shows 50% and 80% coverage intervals for the beta distributions that do not contain the between-speaker variation.\nTo get beta densities with the intended meaning, we need to factor in the random effect. This means that we form model-based predictions that also incorporate (or represent) between-subjects variability. This is sometimes referred to as “integrating out” the random effects. As explained in more detail in the Marginal Effects Zoo book and in this blog post by Heiss (2021b), this is done by sampling from a normal distribution representing the spread of random intercepts and factoring these deviations into the posterior predicted distribution.\nTable 7 shows 50% and 80% coverage intervals for the beta distributions that now contain the between-speaker variation. A visual summary of Model III with the appropriate densities appears in Figure 14.\n\n\nConstruct table of quantiles\nm3_pred &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier),\n      subject = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n\nbeta_quantiles_m3 &lt;- m3_pred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3 |&gt; kable()\n\n\n\n\nTable 7: 50% and 80% coverage intervals, based on Model III (random intercept variation included).\n\n\n\n\n\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.05\n.07\n.16\n.21\n\n\nsome\n.15\n.20\n.35\n.43\n\n\nmany\n.48\n.56\n.72\n.79\n\n\nmost\n.72\n.79\n.89\n.93\n\n\n\n\n\n\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 14: Visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nComparison of estimates\nLet us finally compare the quantities of interest across our models. Figure 15 (a) shows estimates for the typical relative-frequency interpretation of the expressions. Difference between models are minor, but Model III returns slightly wider uncertainty intervals. This is mainly because the model has been informed that the sample size is 20 (rather than 80), but also because the model has (re)incorporated subject 1.\nIn panel (b), the 50% and 80% coverage intervals are shown. Two comparisons are particularly informative:\n\nComparing models I and II, we note the effect of allowing precision to vary across quantifiers. For few, Model II returns tighter coverage intervals, since this expression is estimated to show a higher level of precision, i.e. a more stable interpretation across subjects. For many, the situation is the other way around. The differences between models I and II are minor, however, which is consistent with the results of the likelihood-ratio test reported above.\nA comparison of the grey and red coverage intervals for Model III is also revealing: The red intervals are the ones that do not incorporate between-subject variability (as captured by the random intercepts). They therefore fail to answer question iii. The grey bands, on the other hand, do factor this source of variation into the predictions, and they therefore show us what we were looking for.\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2, 4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(a) Typical values + uncertainty\\n\",\n  panel=function(x,y){\n    panel.points(x=mu_m1, y=3, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=3, y1=3)\n\n    panel.points(x=mu_m2, y=2, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=2, y1=2)\n\n    panel.points(x=mu_m3$median, y=1, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=1, y1=1)\n\n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=-.3, y=3:1, label=c(\"Model I\", \"Model II\", \"Model III\"), adj=0)\n    })\n\n\nbeta_quantiles_m3 &lt;- data.frame(beta_quantiles_m3)\nbeta_quantiles_m3_wrong &lt;- data.frame(beta_quantiles_m3_wrong)\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2,4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(b) 50% and 80% coverage intervals\\n\",\n  panel=function(x,y){\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,2]), x1=as.numeric(beta_quantiles_m1[,5]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,3]), x1=as.numeric(beta_quantiles_m1[,4]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m2[,2]), x1=as.numeric(beta_quantiles_m2[,5]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m2[,3]), x1=as.numeric(beta_quantiles_m2[,4]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3[,2]), x1=as.numeric(beta_quantiles_m3[,5]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m3[,3]), x1=as.numeric(beta_quantiles_m3[,4]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,2]), x1=as.numeric(beta_quantiles_m3_wrong[,5]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,3]), x1=as.numeric(beta_quantiles_m3_wrong[,4]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    \n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 15: Comparison of quantities across models: (a) typical values and their uncertainty, and (b) coverage intervals.\n\n\n\n\n\n\n\nConclusion\nIn this blog post, we have looked at how to model proportional data using beta regression. Our focus was on the typical (average) interpretation speakers assign to quantifiers and its statistical uncertainty. Apart from this, we considered how to use and summarize beta densities to obtain information about the variability of interpretations (or, more specifically, responses) across subjects to quantify the stability of interpretations in the population represented by our sample of informants. We also looked at how to use model diagnostics for a frequentist and a mixed-effects Bayesian regression model to detect problematic data points and/or speakers.\n\n\n\n\n\nReferences\n\nBorges, Marilyn A., and Barbara K. Sawyers. 1974. “Common Verbal Quantifiers: Usage and Interpretation.” Journal of Experimental Psychology 102 (2): 335–38. https://doi.org/10.1037/h0036023.\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCribari-Neto, Francisco, and Achim Zeileis. 2010. “Beta Regression in R.” Journal of Statistical Software 34 (2): 1–24. https://doi.org/10.18637/jss.v034.i02.\n\n\nHeiss, Andrew. 2021a. “A Guide to Modeling Proportions with Bayesian Beta and Zero-Inflated Beta Regression Models.” November 8, 2021. https://doi.org/10.59350/7p1a4-0tw75.\n\n\n———. 2021b. “A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel Bayesian Models.” November 10, 2021. https://doi.org/10.59350/wbn93-edb02.\n\n\nKrug, Manfred, and Katrin Sell. 2013. “Designing and Conducting Interviews and Questionnaires.” In Research Methods in Language Variation and Change, edited by Manfred Krug and Julia Schlüter, 69–98. Cambridge University Press.\n\n\nNewstead, Stephen E., Paul Pollard, and D. Riezebos. 1987. “The Effect of Set Size on the Interpretation of Quantifiers Used in Rating Scales.” Applied Ergonomics 18 (3): 178–82. https://doi.org/10.1016/0003-6870(87)90001-9.\n\n\nSönning, Lukas. 2024. “Ordinal Rating Scales: Psychometric Grounding for Design and Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/jhv6b.\n\n\nStateva, Penka, Arthur Stepanov, Viviane Déprez, Ludivine Emma Dupuy, and Anne Colette Reboul. 2019. “Cross-Linguistic Variation in the Meaning of Quantifiers: Implications for Pragmatic Enrichment.” Frontiers in Psychology 10: 957. https://doi.org/10.3389/fpsyg.2019.00957.\n\n\nTiel, Bob van, Michael Franke, and Uli Sauerland. 2021. “Probabilistic Pragmatics Explains Gradience and Focality in Natural Language Quantification.” Proceedings of the National Academy of Sciences 118 (9). https://doi.org/10.1073/pnas.2005453118.\n\nCitationBibTeX citation:@online{sönning2024,\n  author = {Sönning, Lukas},\n  title = {Modeling the Interpretation of Quantifiers Using Beta\n    Regression},\n  date = {2024-02-29},\n  url = {https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2024. “Modeling the Interpretation of Quantifiers\nUsing Beta Regression.” February 29, 2024. https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/."
  },
  {
    "objectID": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "href": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "title": "Vowel normalization using modified Lobanov",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by @Burch_etal2017 as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to @Wilcox1973, a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While @Wilcox1973 focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as @Burch_etal2017 [p. 193] note, a measure equivalent to DP [@Gries2008] can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in @Wilcox1973 as the mean difference analog (MDA). Both @Wilcox1973 and @Burch_etal2017 argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in @Soenning2023.\n@Gries2020 [p. 116] has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula @Wilcox1973 gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in @Wilcox1973.\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to @Soenning2023. We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.58 seconds to run. The computational formula is quicker – it completes the calculations in only 0.01 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 35.81 seconds to run; the shortcut procedure, on the other hand, is done after 0.02 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n1.58\n35.81\n0.6003\n0.6139\n\n\nComputational\n0.01\n0.02\n0.6005\n0.6140\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {Vowel Normalization Using Modified {Lobanov}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Vowel Normalization Using Modified Lobanov .” 2024.\nJanuary 7, 2024. https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/."
  },
  {
    "objectID": "posts_draft/2025-04-28_disproportionate_representation_speaker/index.html",
    "href": "posts_draft/2025-04-28_disproportionate_representation_speaker/index.html",
    "title": "Distributional disproportions and their consequences: Speakers in the Spoken BNC2014",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(knitr)\nlibrary(kableExtra)\n\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative dataset records the distribution of actually in the Spoken BNC2014 [@Love_etal2017], which was analyzed in [@Soenning_Krug2022]. The question ist whether and how the usage rate of actually in conversational speech varies by Age and Sex.\n\nData preparation\nFor more information on the dataset, please refer to [@Krug_Soenning2021]. We start by downloading the data directly from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nThis dataset includes 668 speakers in total. For each individual, the data frame includes:\n\nan ID (speaker)\nthe number of times they used actually (count)\ntheir age in year, if provided in the metadata (Exact_age)\nthe age range (Age_range)\nself-reported gender (Gender)\nthe total number of words contributed to the corpus by the speaker (total), and\na lightly aggregated version of Age range (age_bins)\n\n\nstr(dat)\n\n'data.frame':   668 obs. of  7 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ Exact_age: int  32 NA NA NA NA NA NA 66 NA NA ...\n $ Age_range: chr  \"30-39\" \"19-29\" \"19-29\" \"30-39\" ...\n $ Gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ age_bins : chr  \"30-39\" \"20-29\" \"20-29\" \"30-39\" ...\n\n\n\nsubset(dat, is.na(age_bins))\n\n    speaker count Exact_age Age_range Gender total age_bins\n362   S0371     3        NA   Unknown   Male  6071     &lt;NA&gt;\n376   S0386     8        NA   Unknown   Male  3034     &lt;NA&gt;\n588   S0608     2        NA   Unknown   Male  1277     &lt;NA&gt;\n589   S0609     5        NA   Unknown Female  1738     &lt;NA&gt;\n592   S0612    17        NA   Unknown Female 10599     &lt;NA&gt;\n623   S0643     2        NA   Unknown   Male  2461     &lt;NA&gt;\n\n\nIn line with [@Soenning_Krug2022], we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age is missing. This leaves us with n = 656 individuals.\n\nd &lt;- dat |&gt; \n  filter(total &gt; 100,\n         Age_range != \"Unknown\")\n\nThen we add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as per million words:\n\nd$rate_pmw &lt;- (d$count / d$total) * 1000000\n\nFinally, we reverse the order of the levels of the categorical variable Age_range, so that they are listed in decreasing order. This is because we will interpret differences between age groups (i.e. differences in apparent time) as indicating differences in real time. The new variable is called age_group:\n\nd &lt;- d |&gt; \n  mutate(\n    age_group = factor(\n      Age_range, \n      levels = rev(sort(unique(Age_range))),\n      ordered = TRUE)) \n\nWe reduce the data frame to the variables we need for analysis:\n\nd &lt;- d |&gt; select(speaker, Gender, age_group, count, total, rate_pmw) |&gt; \n  rename(gender = Gender)\n\nInspect the data frame:\n\nstr(d)\n\n'data.frame':   656 obs. of  6 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ age_group: Ord.factor w/ 10 levels \"90-99\"&lt;\"80-89\"&lt;..: 7 8 8 7 2 2 8 4 8 7 ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ rate_pmw : num  2000 2460 4226 275 2070 ...\n\n\n\n\nData structure\nLet us start by looking at how speakers are distributed across the sociolinguistic categories:\n\nftable(gender ~ age_group, d)\n\n          gender Female Male\nage_group                   \n90-99                 1    3\n80-89                 8   11\n70-79                16   17\n60-69                29   36\n50-59                43   34\n40-49                43   32\n30-39                49   40\n19-29               148   99\n11-18                17   23\n0-10                  4    3\n\n\nNext, we consider the distribution of speaker word counts, i.e. the number of words they contributed to the corpus. In the Spoken BNC2014, the total number of word tokens contributed by each speaker varies markedly across individuals. The following dot diagram shows the skew in this distribution: The word count ranges from 19 to 362,107 and 81% of the speakers contribute fewer than 20,000 words to the corpus.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + \n  geom_dotplot(binwidth = 1500, stackratio = .75, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\nWhat is quite interesting is that this distribution is nicely symmetric on the log scale.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + geom_dotplot(binwidth = .04, method = \"histodot\") +\n  scale_x_log10(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n    theme_dotplot() +\n    xlab(\"Number of words per speaker (log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 2: Log-scaled distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\n\n\n\nFrequency of actually: Data summary\nTo obtain the corpus frequency of actually, we divide the total number of actually-tokens in the corpus by the corpus size. We multiply this rate by 1,000,000 to obtain a normalized frequency of per million words:\n\nn_tokens &lt;- sum(d$count)\ncorpus_size &lt;- sum(d$total)\n\nround((n_tokens / corpus_size) * 1000000)\n\n[1] 1537\n\n\nThis is very similar to the (normalized) corpus frequency reported in CQPweb [@Hardie2012]:\n\n\n\nCorpus frequency results in CQPweb\n\n\nDue to the skewed word count distribution across speaker, however, this corpus frequency is problematic. It turns out that the top 20 speakers (in terms of overall word count) account for 31% of the corpus size – together, they contribute around 350,000 words to the corpus. The corpus frequency is therefore potentially biased into the direction of the language use of these individuals.\n\nsum(d$total &gt; 110000)\n\n[1] 20\n\nsum(d$total[d$total &gt; 110000]) / corpus_size\n\n[1] 0.3081076\n\nsum(d$total[d$total &gt; 200000]) / corpus_size\n\n[1] 0.117284\n\n\nWe can consider each socio-demographic subgroup (i.e. age-by-gender combination) as a subcorpus and likewise calculate the (sub-)corpus frequency of actually. This means that we divide the total number of actually-tokens in the subcorpus by its size. We can visualize the resulting set of normalized subcorpus frequencies:\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = round((sum(count)/sum(total))*1e6)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_point() +\n  geom_line(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Subcorpus frequency of\\nactually (pmw, log-scaled)\")\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\nWarning in geom_line(bg = \"white\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 3: Subcorpus frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nAnother way of estimating the average rate of actually in each subgroup is to consider the speaker-specific normalized frequencies (i.e. he variable rate_pmw) and average over these. @Egbert_Burch2023 [p. 105] refer to these two ways of measuring frequency as corpus frequency and mean text frequency. In the present context, we slightly adapt these labels to subcorpus frequency and mean speaker frequency.\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = mean(rate_pmw)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_point() +\n  geom_line(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Mean speaker frequency of\\nactually (pmw, log-scaled)\")\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\nWarning in geom_line(bg = \"white\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 4: Mean speaker frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nThe frequency estimates differ considerably. Especially for 70-to-79-year-olds, the two methods yield very different usage rates. To understand why this is the case, we need to zoom in on the individual speakers. It helps to draw what is sometimes referred to as a bubble chart, where each speaker is drawn as a circle, and the size of the circles is proportional to the speaker-specific word count. The figure below arranges speakers by Gender (left panel: female speakers, right panel: male speakers) and by Age group within each panel. The y-axis shows the speaker-specific usage rate of actually, and the size of the circles reflects how many word the speaker contributed to the corpus. Note that the y-axis is log-scaled. To be able to include normalized frequencies of 0, we opted for a full scale break that is signaled by the grey horizontal lines.\n\n\nDraw Figure\nd |&gt; \n  mutate(\n    rate_pmw_0_start = ifelse(rate_pmw == 0, 60, rate_pmw)) |&gt; \n  ggplot(\n  aes(x = age_group, \n      y = rate_pmw_0_start, \n      size = count)) + \n  geom_jitter(shape = 1, alpha = .5, width = .2) +\n  facet_grid(. ~ gender) +\n  scale_y_log10(breaks = c(60, 100, 1000, 10000),\n                label = c(0, 100, 1000, 10000)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_size_area() +\n  geom_hline(yintercept = 80, col = \"grey\") +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Normalized frequency of actually\\n(per million words, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 5: Log-scaled normalized speaker frequency of actually by Age group and Gender, with symbol size reflecting the total number of words the speaker contributed to the corpus.\n\n\n\n\n\nWe note that the subgroup of female speakers aged 70 to 79 includes a speaker with both a disproportionately high rate of actually and a disproportionately high word count. This speaker exert considerable influence on the subcorpus frequency, which is upwardly biased as a result. In the subgroup of male speakers aged 70 to 79, the speaker with the greatest word count (i.e. the largest circle) uses actually at a relatively low rate, which likewise distorts the subcorpus frequency of this group.\n\n\nFrequency of actually: Statistical modeling\nLet us also consider how usage rate estimates can be formed using regression modeling. Using a statistical model allows us to obtain uncertainty intervals (e.g. 95% CIs) for the subgroup estimates.\nThe variable we are dealing with is a count variable. This is because it consists of non-negative integers that express the number of events (here: occurrences of actually) in a certain period of observation (here: text time, i.e. the number of running words). We therefore turn to the family of count regression models.\nThe most basic version of this family is the Poisson model. It turns out that it returns the same frequency estimates as the subcorpus frequencies. This is because it ignores the fact that each subgroups consists of different speakers, who in turn (may) show different usage rates of actually. More specifically, is assumes that all speakers in a specific subgroup have the same underlying usage rate of actually. The assumed absence of individual variation (or inter-speaker differences) appears implausible on linguistic grounds and for the data at hand, Figure 5 shows that this assumption is indeed not tenable for the data at hand.\nFor a point of reference, we nevertheless start by fitting a Poisson model to the data. Since the total word count speakers differs across speakers (see Figure 1), a count regression model must include what is referred to as an offset. The idea is the same as when we calculate normalized frequencies. To be able to compare usage rates across speakers in the first place, these must be expressed in relative terms, i.e. divided by thetotal number of words produced by the speaker.\nWe can fit a Poisson model using the base R function glm(), where the code chunk offset(log(total)) represents the offset.\n\nm_poi &lt;- glm(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d, \n    family = poisson())\n\nWe skip regression tables and directly proceed to the calculation of model-based estimates. To this end, we use the predictions() function in the very helpful {marginaleffects} package [@ArelBundock_etal2024]. We request estimates (i.e. predictions) for all combinations of Age (age_group) and Gender (gender), which means that we are not averaging over any predictor variables in the model. We specify these conditions (i.e. combinations of Age and Gender) using the datagrid() function. It creates a data frame of all predictor combinations of interest. Note that we must also supply a word count (total) to datagrid(); for model-based predictions, this variable controls the type of rate (or normalized frequency) returned by the predictions() function. We will stick to ‘per million words’ and therefore add total = 1e6 to the datagrid() function. If you fail to specify a value for total, the predictions() function will press ahead and use the in-sample mean of this variable (17282.57) – clearly, this is not what we want.\nTwo further argument require some explanation. Count regression models are generalized linear models that do not model the data on the original data scale (as is the case in ordinary linear regression). Instead, a link scale is used, similar to logistic regression models. For count regression models, the counts (or in our case: rates) are modeled on the log scale. For interpretation, however, we prefer the orginial data scale – in our case, normalized frequencies on the per-million-word scale. The predictions() function therefore includes an argumetn type, which allows us to specify the scale on which predictions should be returned. Specifying type = 'response' asks for predictions on the data scale (i.e. normalized frequencies). Unfortunately, this will also return uncertainty intervals that are computed on the data scale, i.e. after back-transformation. As you can check for yourself, this does not work well in the represent case, as some of the lower CI limits are negative (which is impossible). We therefore ask for predictions on the model scale (i.e. natural logarithms) using type = 'link' and then backtransform these to the data scale using transform = exp.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nHere is the (shortened) content of the table:\n\n\n# A tibble: 20 × 6\n   estimate conf.low conf.high age_group gender   total\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1    1604.   1529.      1683. 30-39     Female 1000000\n 2    1316.   1229.      1409. 30-39     Male   1000000\n 3    1641.   1596.      1689. 19-29     Female 1000000\n 4    1679.   1609.      1751. 19-29     Male   1000000\n 5     326.    189.       561. 80-89     Female 1000000\n 6    1364.   1131.      1646. 80-89     Male   1000000\n 7    1438.   1330.      1556. 60-69     Female 1000000\n 8     985.    910.      1065. 60-69     Male   1000000\n 9    3478.   3232.      3742. 70-79     Female 1000000\n10     589.    516.       673. 70-79     Male   1000000\n11    1876.   1803.      1952. 40-49     Female 1000000\n12    1397.   1276.      1529. 40-49     Male   1000000\n13    1605.   1514.      1701. 50-59     Female 1000000\n14     965.    879.      1059. 50-59     Male   1000000\n15     167.     41.7      667. 90-99     Female 1000000\n16    1042.    832.      1305. 90-99     Male   1000000\n17    1810.   1666.      1968. 11-18     Female 1000000\n18    1007.    912.      1112. 11-18     Male   1000000\n19    1320.   1090.      1598. 0-10      Female 1000000\n20    1668.   1382.      2015. 0-10      Male   1000000\n\n\nWe go ahead and graph these estimates. The result is virtually identical to the descriptive Figure 3. The error bars represent 95% CIs, and we note that the Poisson model return very confident estimates for most subgroups.\n\npreds_poi |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.5, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nPoisson model can be extended to account for the structure of the data, and for the fact that the usage rate of actually may very well vary from speaker to speaker, even within the same socio-demographic subgroup. The result is sometimes referred to as a Poisson mixture model, and it includes an extra parameter that captures the amount of observed between-speaker variation. A frequently used Poisson mixture distribution is the negative binomial distribution. For a visual explanation of this distribution, see this blog post. What matters for the present case study is the fact that this type of model knows about and adequately represents the structure of the data.\nNegative binomial model\n\nm_nb &lt;- MASS::glm.nb(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d)\n\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\n\npreds_nb |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.75, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {Distributional Disproportions and Their Consequences:\n    {Speakers} in the {Spoken} {BNC2014}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Distributional Disproportions and Their Consequences: Speakers in\nthe Spoken BNC2014.” 2024. January 7, 2024. https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/."
  },
  {
    "objectID": "posts/2025-04-28_disproportionate_representation_speaker/index.html",
    "href": "posts/2025-04-28_disproportionate_representation_speaker/index.html",
    "title": "Unbalanced distributions and their consequences: Speakers in the Spoken BNC2014",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative dataset records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). The question of main interest is whether and how the usage rate of actually in conversational speech varies by Age and Gender.\n\nData preparation\nFor more information on the dataset, please refer to Sönning and Krug (2021). We start by downloading the data from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nThis dataset includes 668 speakers and the following speaker variables:\n\nan ID (speaker)\nthe number of times they used actually (count)\ntheir age in years, if provided in the metadata (Exact_age)\nthe age range (Age_range)\nself-reported gender (Gender)\nthe total number of words contributed to the corpus by the speaker (total), and\na slightly aggregated version of age range (age_bins)\n\n\nstr(dat)\n\n'data.frame':   668 obs. of  7 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ Exact_age: int  32 NA NA NA NA NA NA 66 NA NA ...\n $ Age_range: chr  \"30-39\" \"19-29\" \"19-29\" \"30-39\" ...\n $ Gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ age_bins : chr  \"30-39\" \"20-29\" \"20-29\" \"30-39\" ...\n\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age is missing. This leaves us with n = 656 individuals.\n\nd &lt;- dat |&gt; \n  filter(total &gt; 100,\n         Age_range != \"Unknown\")\n\nThen we add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per million words’:\n\nd$rate_pmw &lt;- (d$count / d$total) * 1000000\n\nFinally, we reverse the order of the levels of the categorical variable Age_range, so that they are listed in decreasing order. This is because we will interpret differences between age groups (i.e. differences in apparent time) as indicating differences in real time. The new variable is called age_group:\n\nd &lt;- d |&gt; \n  mutate(\n    age_group = factor(\n      Age_range, \n      levels = rev(sort(unique(Age_range))),\n      ordered = TRUE)) \n\nWe reduce the data frame to the variables we need for analysis:\n\nd &lt;- d |&gt; dplyr::select(speaker, Gender, age_group, count, total, rate_pmw) |&gt; \n  dplyr::rename(gender = Gender)\n\nInspect the data frame:\n\nstr(d)\n\n'data.frame':   656 obs. of  6 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ age_group: Ord.factor w/ 10 levels \"90-99\"&lt;\"80-89\"&lt;..: 7 8 8 7 2 2 8 4 8 7 ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ rate_pmw : num  2000 2460 4226 275 2070 ...\n\n\n\n\nData structure\nLet us start by looking at how speakers are distributed across the sociolinguistic categories. The table below shows that the youngest and oldest cohorts are rather sparsely populated.\n\nftable(gender ~ age_group, d)\n\n          gender Female Male\nage_group                   \n90-99                 1    3\n80-89                 8   11\n70-79                16   17\n60-69                29   36\n50-59                43   34\n40-49                43   32\n30-39                49   40\n19-29               148   99\n11-18                17   23\n0-10                  4    3\n\n\nNext, we consider the distribution of speaker word counts, i.e. the number of words they contributed to the corpus. In the Spoken BNC2014, this count varies markedly across individuals. The following dot diagram shows the skewed distribution: The word count ranges from 117 to 362,107 (after removing speakers with fewer than 100 words) and 81% of the speakers contribute fewer than 20,000 words to the corpus.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + \n  geom_dotplot(binwidth = 1600, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\nWhat is quite interesting is that this distribution is perfectly symmetric on the log scale.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + geom_dotplot(binwidth = .041, method = \"histodot\") +\n  scale_x_log10(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n    theme_dotplot() +\n    xlab(\"Number of words per speaker (log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 2: Log-scaled distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\n\n\n\nFrequency of actually: Data summary\nTo obtain the corpus frequency of actually, we divide the total number of actually-tokens in the corpus by the corpus size. We multiply this rate by 1,000,000 to obtain a normalized frequency of ‘per million words’:\n\nn_tokens &lt;- sum(d$count)\ncorpus_size &lt;- sum(d$total)\n\nround((n_tokens / corpus_size) * 1000000)\n\n[1] 1537\n\n\nThis is very similar to the (normalized) corpus frequency reported in CQPweb (Hardie 2012):\n\n\n\n\n\nDue to the skewed word count distribution across speakers, however, this corpus frequency is potentially problematic. It turns out that the top 20 speakers (in terms of overall word count) together make up 31% of the corpus – their word count adds to around 350,000. The corpus frequency is therefore potentially biased toward the language use of these individuals.\nWe can consider each socio-demographic subgroup (i.e. age-by-gender combination) as a subcorpus and likewise calculate the (sub-)corpus frequency of actually. This means that we divide the total number of actually-tokens in the subcorpus by its size. We can visualize the resulting set of normalized subcorpus frequencies:\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = round((sum(count)/sum(total))*1e6)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Subcorpus frequency of\\nactually (pmw, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 3: Subcorpus frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nAnother way of estimating the average rate of actually in each subgroup is to consider the speaker-specific normalized frequencies (i.e. the variable rate_pmw) and average over these. Egbert and Burch (2023, 105) refer to these two types of frequency estimates as corpus frequency and mean text frequency. In the present context, we slightly adapt these labels to subcorpus frequency and mean speaker frequency.\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = mean(rate_pmw)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Mean speaker frequency of\\nactually (pmw, log-scaled)\")\n\nggsave(\"fig_actually_mean_speaker_frequency.pdf\")\n\n\n\n\n\n\n\n\nFigure 4: Mean speaker frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nThe frequency estimates in Figure 3 and Figure 4 differ considerably. Especially for 70-to-79-year-olds, the two methods yield very different usage rates. To understand why this is the case, we need to zoom in on the individual speakers. It helps to draw what is sometimes referred to as a bubble chart, where each data point (here: speaker) appears as a circle, and the size of the circles is proportional to some quantity (here: the speaker-specific word count). The figure below arranges speakers by Gender (female speakers on the left) and by Age, within each panel. The y-axis shows the speaker-specific usage rate of actually, and the size of the circles reflects how many words a person contributed to the corpus. Note that the y-axis is log-scaled. To be able to include normalized frequencies of 0 (for which the logarithm is not defined), we opt for a full scale break that is signaled by the grey horizontal line.\n\n\nDraw Figure\nd |&gt; \n  mutate(\n    rate_pmw_0_start = ifelse(rate_pmw == 0, 60, rate_pmw)) |&gt; \n  ggplot(\n  aes(x = age_group, \n      y = rate_pmw_0_start, \n      size = total)) + \n  geom_jitter(shape = 1, alpha = .5, width = .2) +\n  facet_grid(. ~ gender) +\n  scale_y_log10(breaks = c(60, 100, 1000, 10000),\n                label = c(0, 100, 1000, 10000)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_size_area() +\n  geom_hline(yintercept = 80, col = \"grey\") +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Normalized frequency of actually\\n(per million words, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 5: Log-scaled normalized speaker frequency of actually by Age group and Gender, with symbol size reflecting the total number of words the speaker contributed to the corpus.\n\n\n\n\n\nWe note that the subgroup of male speakers aged 70 to 79 includes an exceptionally large bubble, which represents a speaker with both (i) a disproportionately low rate of actually and (ii) a disproportionately high word count. This speaker exerts considerable influence on the subcorpus frequency, which is downwardly biased as a result. In the subgroup of female speakers aged 70 to 79, the two largest bubbles also show the highest rate of actually in this subgroup, which likewise distorts the subcorpus frequency of this group.\nTo summarize, corpus frequencies can be quite misleading if the size of the units in the corpus (i.e. texts or speakers) varies appreciably. A simple reassurance check compares the corpus frequency (or subcorpus frequency) with the mean text (or speaker) frequency. If these methods yield different results, we must decide which kind of frequency estimate is more appropriate for the research task at hand. In the current setting, there is no reason why a particular individual should receive greater weight when estimating a population quantity such as the usage rate of actually among 70-to-79-year-old female speakers of British English. In other words, the imbalance of word counts across speakers is a nuisance that must be adjusted for in the summary and analysis of these data.\n\n\nFrequency of actually: Statistical modeling\nLet us also consider how frequency estimates can be obtained using regression modeling. Using a statistical model allows us to construct uncertainty intervals (e.g. 95% CIs) around the subgroup estimates.\nThe variable we are dealing with is a count variable. This is because it consists of non-negative integers that express the number of events (here: occurrences of actually) in a certain period of observation (here: text time, i.e. the number of running words). We therefore turn to the family of count regression models.\nThe most basic version of this family is the Poisson model. It turns out that it produces the same frequency estimates as the subcorpus frequencies we reported above. This is because it ignores the fact that each subgroup consists of different speakers, who in turn (may) show different usage rates of actually. More specifically, is assumes that all speakers in a specific subgroup have the same underlying usage rate of actually. The assumed absence of individual variation (or inter-speaker differences) appears implausible on linguistic grounds and for the data at hand, Figure 5 shows that this assumption is indeed not tenable.\nFor a point of reference, we nevertheless start by fitting a Poisson model to the data. Since the total word count differs across speakers (see Figure 1), a count regression model must include what is referred to as an offset. The idea is the same as when we calculate normalized frequencies. To be able to compare usage rates across speakers in the first place, these must be expressed in relative terms, i.e. divided by the total number of words produced by the speaker.\nWe can fit a Poisson model using the base R function glm(), where the code chunk offset(log(total)) represents the offset.\n\nm_poi &lt;- glm(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d, \n    family = poisson())\n\nWe skip regression tables and directly proceed to the calculation of model-based estimates. To this end, we use the predictions() function in the very helpful {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). We request estimates (i.e. predictions) for all combinations of Age (age_group) and Gender (gender), which means that we are not averaging over any predictor variables in the model. We specify these conditions (i.e. combinations of Age and Gender) using the datagrid() function. It creates a data frame of all predictor combinations of interest. Note that we must also supply a word count (total) to datagrid(); for model-based predictions, this variable controls the type of rate (or normalized frequency) returned by the predictions() function. We will stick to ‘per million words’ and therefore add total = 1e6 to the datagrid() function. If you fail to specify a value for total, the predictions() function will press ahead and use the in-sample mean of this variable (i.e. 17,283) – clearly, this is not what we want.\nTwo further arguments in the code box below require some explanation. Count regression models are generalized linear models that do not model the data on the original data scale (as is the case in ordinary linear regression). Instead, a link scale is used, similar to logistic regression models. For count regression models, the counts (or in our case: rates) are modeled on the log scale. For interpretation, however, we prefer the original data scale – in our case, normalized frequencies on the per-million-word scale.\nThe predictions() function therefore includes an argument type, which allows us to specify the scale on which predictions should be returned. Specifying type = 'response' asks for predictions on the data scale (i.e. normalized frequencies). Unfortunately, this will also return uncertainty intervals that are computed using the data-scale standard error. As you can check for yourself, this does not work well in the represent case, as some of the lower CI limits are then negative (which is impossible). We therefore ask for predictions on the model scale (i.e. natural logarithms) using type = 'link' and then back-transform these to the data scale using transform = exp.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nHere is the (shortened) content of the output:\n\n\n# A tibble: 20 × 6\n   estimate conf.low conf.high age_group gender   total\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1    1604.   1529.      1683. 30-39     Female 1000000\n 2    1316.   1229.      1409. 30-39     Male   1000000\n 3    1641.   1596.      1689. 19-29     Female 1000000\n 4    1679.   1609.      1751. 19-29     Male   1000000\n 5     326.    189.       561. 80-89     Female 1000000\n 6    1364.   1131.      1646. 80-89     Male   1000000\n 7    1438.   1330.      1556. 60-69     Female 1000000\n 8     985.    910.      1065. 60-69     Male   1000000\n 9    3478.   3232.      3742. 70-79     Female 1000000\n10     589.    516.       673. 70-79     Male   1000000\n11    1876.   1803.      1952. 40-49     Female 1000000\n12    1397.   1276.      1529. 40-49     Male   1000000\n13    1605.   1514.      1701. 50-59     Female 1000000\n14     965.    879.      1059. 50-59     Male   1000000\n15     167.     41.7      667. 90-99     Female 1000000\n16    1042.    832.      1305. 90-99     Male   1000000\n17    1810.   1666.      1968. 11-18     Female 1000000\n18    1007.    912.      1112. 11-18     Male   1000000\n19    1320.   1090.      1598. 0-10      Female 1000000\n20    1668.   1382.      2015. 0-10      Male   1000000\n\n\nWe go ahead and graph these estimates. The result is virtually identical to the descriptive Figure 3. The error bars represent 95% CIs, and we note that the Poisson model returns very confident estimates for most subgroups.\n\n\nDraw Figure\npreds_poi |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.5, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nFigure 6: Log-scaled usage rate estimates based on a Poisson regression model.\n\n\n\n\n\nThe Poisson model can be extended to account for the structure of the data, and for the fact that the usage rate of actually may very well vary from speaker to speaker, even within the same socio-demographic subgroup. Such extensions are sometimes referred to as Poisson mixture models, which include an extra parameter that captures the amount of observed between-speaker variation. A frequently used Poisson mixture is the negative binomial distribution. For a visual explanation, see this blog post. What matters for the present case study is the fact that this type of model knows about and adequately represents the structure of the data.\nWe can fit a negative binomial regression model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d)\n\nPredictions can be calculated using {marginaleffects} in the same way as for the Poisson model:\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nThen we can graph the estimates based on this model, including statistical uncertainty intervals:\n\n\nDraw Figure\npreds_nb |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.75, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nFigure 7: Log-scaled usage rate estimates based on a negative binomial regression model.\n\n\n\n\n\nThis yields a pattern that is very similar to Figure 4, i.e. which reflects mean speaker frequency (rather than subcorpus frequency). The subgroup estimates also form more regular patterns, rather than zig-zag profiles as in Figure 6. This appears much more plausible from a linguistic perspective. Finally, the statistical uncertaintiy intervals are appropriately wide, especially for the oldest cohorts. As our cross-tabulation above showed, very few speakers are found in these cells, which necessarily leads to imprecise estimates.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nIf the word count distribution across the speakers or texts in a corpus varies, the corpus frequency of a structure may be biased toward its occurrence rate in overrepresented units (i.e. texts or speakers).\nA simple double-check is to compare the corpus frequency to the mean text (or speaker) frequency.\nIn count regression modeling, the Poisson model produces corpus frequency estimates, while the negative binomial model yields scores that are similar to mean text frequency estimates.\nThe negative binomial model also gives more reasonable statistical uncertainty intervals.\n\n\n\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Unbalanced Distributions and Their Consequences: {Speakers}\n    in the {Spoken} {BNC2014}},\n  date = {2025-04-29},\n  url = {https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Unbalanced Distributions and Their\nConsequences: Speakers in the Spoken BNC2014.” April 29, 2025. https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/."
  },
  {
    "objectID": "posts/2025-05-01_clustering_uncertainty_intervals/index.html",
    "href": "posts/2025-05-01_clustering_uncertainty_intervals/index.html",
    "title": "Clustering in the data affects statistical uncertainty intervals",
    "section": "",
    "text": "The data points drawn from a corpus often come in groups: There are typically multiple observations from the same text (or speaker). If this kind of data layout is not represented in the analysis, this will often yield spuriously precise estimates of the quantities of interest. This blog post describes how this can interfere with replication efforts in corpus linguistics.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(corpora)         # to calculate a log-likelihood score\nlibrary(kableExtra)      # for drawing html tables\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")\n\n\n\nTerminology\nBefore we get started, let me briefly clarify how a number of terms are used in this blog post:\n\nOriginal study: The study whose results are subject to a replication effort\nReplication study: A study that uses new data and repeats the original work in the closest possible way, i.e. using the same research design and methods; this type of replication is often referred to as a direct/close/exact/literal replication\nOriginal/replication estimate: For the quantity of interest (sometimes referred to as an effect size), the point estimate returned by the original/replication study\n\n \n\n\nInterpretation of confidence intervals from a replication perspective\nThe key point of this blog post is that an inadequate analysis of corpus data can produce unrealistic expectations of a replication study. This may lead to erroneous judgments as to the compatibility of the original and the replication estimate, and therefore incorrect conclusions about the success of a replication study.\nWe therefore start by considering what kind of information confidence intervals (CIs) give us about consistency and replication. It is important to clarify this because misconceptions about the meaning and interpretation of CIs are widespread.\n\nInterpreting CI overlap\nA common misconception when comparing 95% confidence intervals of two independent groups is to assume that if the error bars overlap, the difference between the groups is “not statistically significant”. A nice paper by Cumming (2009) discusses this issue. It turns out that even if the overlap is moderate, the p-value for the comparison would be p &lt; .05. For CIs that do not overlap, it would be p &lt; .01.\nIn the figure below, the upper pair of estimates illustrates a situation where the arms of two CIs overlap. Moderate overlap means that the amount of overlap is less than half of the average arm length. This means that you need to mentally approximate the average length of the overlapping arms. This is easy in the example below, because they have the same length. Overlapping amounts to roughly half of the average arm length.\n\n\n\n\n\nWhile this insight is particularly relevant for comparisons made within a study, it also useful in the context of replication work. If we compare the CIs from an original and a replication study, we may see overlapping CIs. In some sense, the misconception then work in the opposite way: It invites lax judgments of replication success. Thus, when the original and replication CIs overlap, we may be tempted to conclude that the replication result is statistically compatible with the original one. As we have just seen however, this interpretation requires overlap by at least the average arm length.\n\n\nReplication information provided by CIs\nAnother misconception about CIs relevant to the present discussion concerns the information they provide about replication estimates. As Cumming, Williams, and Fidler (2004) noted, many researchers think that, given a 95% CI, the probability that the replication estimate will fall within the limits of this interval is also 95%. They refer to this probability as the average probability of capture (APC): The probability that a CI will capture a future replication estimate.\nIt turns out that the APC is smaller than the confidence level. In their appendix, Cumming, Williams, and Fidler (2004) provide the mathematical background. To develop our intuition, we consider the APC for a number of frequently used confidence levels. The mapping in the table below shows that the average probability of capture for a 95% CI, for instance, is 83%.\n\n\n\n\n\nConfidence level\nAverage probability of capture\n\n\n\n\n68.3% (± 1 standard error)\n52%\n\n\n90%\n76%\n\n\n95%\n83%\n\n\n99%\n93%\n\n\n\n\n\nWe keep the overlap and the replication interpretation of confidence intervals in mind as we turn to our illustrative analysis task.\n \n\n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur linguistic focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work.\nFurther, questions about diachronic trends in the frequency of modals have generated discussions about replicability in corpus linguistics. Based on a comparison of the Brown and Frown corpus, Leech (2003) concluded that the frequency of English modal verbs declined in the latter half of the 20th century. This finding was challenged by Millar (2009), which in turn prompted a response by Leech (2011). McEnery and Brezina (2022) also used data on English modals as a case study for discussing and illustrating key ideas about replication in corpus linguistics.\nEnglish modal verbs therefore have a special place in the corpus-linguistic discourse on replication and replicability. I therefore decided to set up a dedicated TROLLing post (Sönning 2024), which includes frequency information on the English modals from the Brown Family of corpora. Perhaps this resource may be of value in future discussion on the topic. An excerpt from this dataset is used in the current series of blog posts, which concentrate on statistical issues that may get in the way of replication attempts in corpus work.\nWe will concentrate on a subset of these data: the modal verb should in Brown and Frown, i.e. written American English. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\n \n\nData\nWe start by downloading the data directly from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe dataset we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nNext, we extract the data for should in Brown and Frown and prepare them for analysis.\n\n\nLoad and prepare data\nd_modals &lt;- subset(dat, corpus %in% c(\"Brown\", \"Frown\"))\n\nd_modals$time_period &lt;- factor(d_modals$time_period)\nd_modals$genre &lt;- factor(d_modals$genre)\n\ncontrasts(d_modals$genre) &lt;- contr.sum(4)\ncontrasts(d_modals$time_period) &lt;- contr.sum(2)\n\nshould_data &lt;- subset(d_modals, modal==\"should\")\nshould_Brown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Brown\")\nshould_Frown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Frown\")\nshould_learned &lt;- subset(d_modals, modal==\"should\" & genre==\"learned\")\n\n\nBrown and Frown each consist of 500 texts, which are sampled from four different genres. The following table shows the word count and number of texts for each genre in Brown:\n\n\n\n\n\nGenre\nWords\nTexts\n\n\n\n\nFiction\n295,779 (25.8%)\n126 (25.2%)\n\n\nGeneral prose\n470,726 (41.0%)\n206 (41.2%)\n\n\nLearned\n180,649 (15.7%)\n80 (16.0%)\n\n\nPress\n201,300 (17.5%)\n88 (17.6%)\n\n\n\n\n\n\n\n \n\n\nCrude answers to our research questions\nWe may obtain crude (but quick) answers to our questions as follows. To measure the frequency of should in Brown, we divide its corpus frequency by the size of the corpus. We can do the same for Frown. We will multiply these rates by 1,000, to get normalized frequencies ‘per thousand words’.\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nAnd here they are, rounded to two decimal places:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\nFor Brown, we get a rate of 0.79 per thousand words, and for Frown the rate is 0.68 per thousand words.\nFor a quick answer to the second question, we divide the rate in Frown by that in Brown, which gives us a rate ratio of 0.86. This tells us that the frequency of should in the 1990s was only 86% as large as that in the 1960s:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\nTo see whether this frequency difference is “statistically significant”, a likelihood-ratio statistic may be computed. This score is based on a simple 2x2 table, which contains the frequency of should in each corpus, and the number of words in each corpus. We use the function keyness() in the R package {corpora} (Evert 2023) to calculate a likelihood-ratio test:\n\nkeyness(f1 = sum(should_Brown$n_tokens),\n        n1 = sum(should_Brown$n_words),\n        f2 = sum(should_Frown$n_tokens),\n        n2 = sum(should_Frown$n_words), \n        measure = \"G2\")\n\n[1] 9.255016\n\n\nThis returns a log-likelihood score of 9.3, which is close to the one reported by Leech (2003, 228) and indicates a “statistically significant” difference in normalized frequency between the two corpora.\nThese crude ways of assessing and testing frequencies and their differences in corpora are straightforward to carry out and therefore provide quick answers to our questions. We now look at how these estimates (and p-values) may be misleading, or may not answer the question we really had in mind.\nBefore we go further, however, I would like to should note that the following elaborations are not meant to discredit the work done by Geoffrey Leech in the early 2000s. In fact, Leech (2003) provides a balanced assessment of frequency changes in the English modal system. The diachronic patterns he observed were remarkably consistent across the 11 modal verbs, which strengthened the conclusion he drew. Further, his reference to log-likelihood scores for time differences explicitly noted that “too much should not be made of significance tests in comparative corpus studies” (2003, 228).\n \n\n\n\nClustering in the data affects statistical uncertainty intervals\nInterest in corpus-based work often centers on the frequency of a structure in language use, which is usually expressed as a normalized frequency (or occurrence rate), expressed, say, as ‘per million words’. Since any corpus is a sample of language use from a domain or language variety of interest, these normalized frequencies are sample statistics, which in turn often serve as estimates of population parameters.\n\nData structure\nThe Brown Corpus, for instance, contains a sample of written American English from the early 1960s, based on a purposefully compiled list of genres and sub-genres. If we look at the frequency of should in the Brown Corpus, it is unlikely that our linguistic interest is limited to the 500 texts (or text excerpts) in the corpus Rather, we would consider this as a sample from the population of interest – written American English in the 1960s.\nWhen extrapolating to this underlying language variety, our sample size is 500 (the number of texts in the corpus) rather than 1 million (the number of word tokens in Brown). In the sampling literature, the 500 texts would be considered the primary sampling units, and the 1 million word tokens in Brown are therefore clustered, or structured hierarchically. They are grouped by text file.\n\n\nData description\nLet’s take a look at the distribution of should in the Brown Corpus, by considering its occurrence rate at the level of the individual texts. This means that we first calculate normalized frequencies at the text level and then look at how they are distributed.\nFigure 1 shows the distribution of these text-level occurrence rates using a dot diagram. Each dot in the figure represents a text file. Since there are many texts with a frequency of 0 (n = 174, or 35%), the dot diagram is flipped: The y-axis shows the normalized frequency (expressed as ‘per thousand words’) and the dots form horizontal piles. We note that there are very few texts in which should occurs with a frequency greater than 5 per thousand words. Most texts (n = 382, or 76%) show at most 1 instance (i.e. a rate of roughly 2 ptw or lower).\n\n\nDraw Figure\nd_modals |&gt; filter(\n  corpus == \"Brown\",\n  modal == \"should\") |&gt; \n  mutate(rate_ptw = n_tokens / n_words * 1e3) |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_dotplot(method = \"histodot\", binwidth = .2, stackratio = 1) +\n  theme_dotplot(vertical = TRUE) +\n  scale_y_continuous(expand = c(.004,0)) +\n  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +\n  xlab(\"Normalized frequency of should\\n(per 1,000 words)\") +\n  annotate(\"text\", y = .5, x = 5, label = \"Each dot denotes a text\", size = 3, col = \"grey30\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nFigure 1: Should in the Brown Corpus: Dot diagram showing the distribution of normalized frequencies across the 500 texts in the corpus.\n\n\n\n\n\nOur analyses will include Genre as a predictor – specifically, the broad text categories Fiction, General prose, Learned, and Press. We therefore break down the text-level occurrence rates by this variable. Figure 2 shows that occurrence rates tend to be lower in Fiction, and that the outliers with exceptionally high rates of should are found in General prose.\n\n\nDraw Figure\nd_modals |&gt; filter(\n  corpus == \"Brown\",\n  modal == \"should\") |&gt; \n  mutate(genre_nice = factor(\n    genre, \n    levels = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    labels = c(\"Fiction\\n\", \"General prose\\n\", \"Learned\\n\", \"Press\\n\"))) |&gt; \n  mutate(rate_ptw = n_tokens / n_words * 1e3) |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_dotplot(method = \"histodot\", binwidth = .2, stackratio = 1) +\n  theme_dotplot(vertical = TRUE) +\n  facet_grid(. ~ genre_nice) +\n  scale_y_continuous(expand = c(.008,0)) +\n  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +\n  xlab(\"Normalized frequency of should\\n(per 1,000 words)\") +\n  ggh4x::force_panelsizes(cols = c(2,3.05,1.2,1.25)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nFigure 2: Should in the Brown Corpus: Distribution of text-level normalized frequencies by Genre.\n\n\n\n\n\n \n\n\nStatistical modeling\nWe can use a statistical model to learn about the uncertainty surrounding our sample-based estimates. This uncertainty is often reported in the form of confidence intervals (or standard errors), which indicate the precision of estimates, based on the model and data. In order to arrive at reliable uncertainty estimates, a suitable model must be used. For instance, it must adequately represent the structure of the data – in our case, the fact that Brown is primarily a sample of 500 texts (rather than 1 million words) from the language variety of interest.\nThe use of an inadequate model, which does not take into account the clustered nature of the data, will usually suggest a higher level of precision than is warranted – in other words, we will get overconfidence intervals. This will happen if we analyze the current data with a Poisson model. This model does not account for the structure of the data in the sense that it makes no provision for the possibility that the usage rate of should may vary from text to text. Thus, it assumes that the underlying frequency of should is the same for each text, with observable variation in rates being exclusively due to sampling variation. The “underlying frequency” can be thought of as the propensity of the author(s) to use should in the particular context of language use represented by the text. This means that the model does not allow for the possibility that there may be inter-speaker variation.\nWe will compare two modeling approaches, which are also discussed and contrasted in Sönning and Krug (2022), in the context of a similar research task (the frequency of actually in conversational British speech, as represented in the Spoken BNC2014).\nLet’s fit a Poisson model to these data using the base R function glm(), where the code chunk offset(log(n_words)) represents the offset, which adjusts for the fact that text files differ (slightly) in length (for some background on this, see this blog post).\n\nm_poi &lt;- glm(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Brown, \n    family = poisson())\n\nWe also fit a negative binomial model to the data, which makes allowances for variation in occurrence rates across the 500 texts in the corpus. It does so via something similar to a standard deviation parameter, which expresses text-to-text variation in the normalized frequency of should. This blog post provides some background on the negative binomial distribution.\nWe fit a negative binomial regression model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Brown)\n\n \n\n\nModel-based predictions (i.e. estimates)\nThe next step is to calculate model-based estimates of the frequency of should. To this end, we use the predictions() function in the very helpful {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). As explained in some more detail in this blog post, we use the datagrid() function to define the condition(s) for which we wish to obtain estimates. In our case, these are the four genres. We also specify n_words = 1000 in datagrid(), to obtain rates per thousand words.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    genre = c(\"press\", \"general_prose\", \"learned\", \"fiction\"),\n    n_words = 1000)) |&gt; \n  tidy()\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    genre = c(\"press\", \"general_prose\", \"learned\", \"fiction\"),\n    n_words = 1000)) |&gt; \n  tidy()\n\nHere is the (shortened) content of the output:\n\npreds_poi[,c(7,2,5,6)]\n\n# A tibble: 4 × 4\n  genre         estimate conf.low conf.high\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 press            0.864    0.745     1.00 \n2 general_prose    0.882    0.801     0.971\n3 learned          0.991    0.856     1.15 \n4 fiction          0.480    0.407     0.566\n\n\n\npreds_nb[,c(7,2,5,6)]\n\n# A tibble: 4 × 4\n  genre         estimate conf.low conf.high\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 press            0.872    0.672     1.13 \n2 general_prose    0.884    0.746     1.05 \n3 learned          0.996    0.761     1.30 \n4 fiction          0.480    0.376     0.613\n\n\nFigure 3 compares estimates based on the Poisson and the negative binomial model graphically. We observe that the point estimates are virtually identical, but the negative binomial model returns much wider error intervals.\n\n\nDraw Figure\npreds_poi$model &lt;- \"Poisson\"\npreds_nb$model &lt;- \"Negative binomial\"\n\npreds_conditions &lt;- rbind(\n  preds_nb[,c(10,7,2,5,6)],\n  preds_poi[,c(10, 7,2,5,6)]\n)\n\npreds_conditions$genre_nice &lt;- rep(c(\"Press\", \"General\\nprose\", \"Learned\", \"Fiction\"), 2)\n\npreds_conditions |&gt; \n  ggplot(aes(x=genre_nice, y=estimate, group=model, shape=model)) + \n  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_shape_manual(values=c(21, 19)) +\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,\n                position = position_dodge(.3)) +\n  geom_point(position = position_dodge(.3), fill=\"white\") +\n  theme_classic_ls() +\n  theme(plot.subtitle = element_text(face = \"italic\")) +\n  annotate(\"text\", x = 4.6, y = .1, label = \"Error bars: 95% confidence intervals\", \n           adj=1, color = \"grey40\", size = 3) + \n  annotate(\"text\", x = 3, y = c(.35, .25), label = c(\"Negative binomial\", \"Poisson\"), \n           adj=0, color = \"grey40\", size = 3) + \n  annotate(\"point\", x = 2.8, y = c(.35, .25), shape = c(21, 19), \n           color = \"grey40\", size = 1.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3: Estimated normalized frequency of should by Genre: Comparison of estimates based on a Poisson and a negative binomial model.\n\n\n\n\n\n\n\n\nRelevance for replicability\nIn general, a statistical result is considered as having been replicated if a close replication yields statistical conclusions that are consistent with those of the original study. Uncertainty intervals serve as a basis for judging whether estimates based on different sets of data are consistent or not. As discussed above, a 95% CI has a 83% probability of capturing a replication estimate.\nThe problem with overconfident uncertainty intervals, then, is that they produce unreasonable expectations about what should happen in a replication study. Put differently, a replication estimate may appear to be inconsistent with the original result even though it isn’t. Had the original analysis (or perhaps both analyses) used a more adequate model, the uncertainty bounds as well as the replication expectations would have been wider.\nWe can illustrate this issue using our data on the modals. Since our corpora were compiled using the same sampling design, let us imagine the Frown Corpus were a direct replication of the Brown Corpus. Of course, the purpose of Frown, which records written American English in the early 1990s, was the documentation of diachronic trends in this variety. The goal was therefore to create a corpus that is as close to Brown as possible, apart from the difference in time. What this means is that if the Frown estimate (our “replication” estimate) is consistent with the Brown estimate (our “original” estimate), this will be interpreted as indicating no change over time; statistical differences, on the other hand, will be interpreted as reflecting a diachronic change in this variety.\nTo illustrate, we concentrate on the genre Learned and run two regression models, a Poisson and a negative binomial model. We fit these models, which include the variable Corpus as a predictor, in the same way as above:\n\nm_poi_learned &lt;- glm(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_learned, \n    family = poisson())\n\nm_nb_learned &lt;- glm.nb(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_learned)\n\nThen we use the {marginaleffects} package to generate predictions, again specifying n_words = 1000 in datagrid(), to obtain rates per thousand words.\n\npreds_poi_learned &lt;- predictions(\n  m_poi_learned, \n  newdata = datagrid(\n    corpus = unique,\n    n_words = 1000)) |&gt; \n  tidy()\n\npreds_nb_learned &lt;- predictions(\n  m_nb_learned, \n  newdata = datagrid(\n    corpus = unique,\n    n_words = 1000)) |&gt; \n  tidy()\n\nFigure 4 compares these model-based estimates visually. The question of interest is whether the frequency estimate from Frown is consistent with the one from Brown. Due the wider uncertainty intervals, the estimates based on the negative binomial model appear more consistent with one another then those from the Poisson model. In other words, a Poisson analysis of these data might lead us to believe that there is a diachronic decrease in the frequency of should.\n\n\nDraw Figure\ncomparison_learned &lt;- rbind(\n  preds_poi_learned[,c(7,2,5,6)],\n  preds_nb_learned[,c(7,2,5,6)])\n\ncomparison_learned$model &lt;- rep(c(\"Poisson\", \"Negative\\nbinomial\"), each = 2)\n\ncomparison_learned |&gt; \n  ggplot(aes(x=model, y=estimate, group=corpus, shape=corpus)) + \n  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_shape_manual(values=c(21, 19)) +\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,\n                position = position_dodge(.3)) +\n  geom_point(position = position_dodge(.3), fill=\"white\") +\n  theme_classic_ls() +\n  theme(plot.subtitle = element_text(face = \"italic\")) +\n  annotate(\"text\", x = 2.6, y = .1, label = \"Error bars: 95% CIs\", \n           adj=1, color = \"grey40\", size = 3) + \n  annotate(\"text\", x = 2.1, y = c(.35, .25), label = c(\"Brown\", \"Frown\"), \n           adj=0, color = \"grey40\", size = 3) + \n  annotate(\"point\", x = 2, y = c(.35, .25), shape = c(21, 19), \n           color = \"grey40\", size = 1.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 4: Estimated frequency of should in the genre Learned: Comparison of estimates based on a Poisson and a negative binomial model.\n\n\n\n\n\nIf we look at the CI overlap of the 95% confidence intervals for the Poisson estimates, we note that it is more than moderate, i.e. amounts to more than half the average arm length. An approximate interpretation is therefore that the two estimates do not differ “significantly”.\nFor reassurance, we take a closer look at the model output. Both models also produce an estimate of the difference between the occurrence rates in the two corpora. This difference is represented by the coefficient for the predictor Corpus. This coefficient is very similar in the two models:\n\ncoef(m_poi_learned)[2]\n\ncorpusFrown \n -0.1978386 \n\ncoef(m_nb_learned)[2]\n\ncorpusFrown \n -0.2054762 \n\n\nSince count regression models operate on the link scale of natural logarithms, these coefficients express differences on that scale. If we back-transform them using exponentiation, we obtain ratios. This means that the difference between the two corpora is expressed in relative terms. Back-transformation tells us that the normalized frequency of should in Frown is only around 80% as large as that in Frown:\n\nround(exp(coef(m_poi_learned)[2]), 2)\n\ncorpusFrown \n       0.82 \n\nround(exp(coef(m_nb_learned)[2]), 2)\n\ncorpusFrown \n       0.81 \n\n\nWe can obtain 95% CIs for these rate ratios using the function confint(). It returns the limits of a 95% CI on the log scale, which means we need to back-transform these confidence limits to obtain uncertainty bounds for our rate ratios:\n\nround(exp(confint(m_poi_learned, \"corpusFrown\")), 2)\n\n 2.5 % 97.5 % \n  0.66   1.02 \n\nround(exp(confint(m_nb_learned, \"corpusFrown\")), 2)\n\n 2.5 % 97.5 % \n  0.55   1.20 \n\n\nWe note that the statistical uncertainty surrounding the estimate based on the negative binomial model is considerably wider; it suggests that the 1990s rate of should could be as small as 55% of the 1960s rate, or as large as 120% of the 1960s rate. The Poisson model, in contrast, provides stronger indication of a decline in occurrence rate, which could amount to 66% or 102% of the rate in Brown.\nThe Poisson model is therefore more consistent with the interpretation of change over time. This is also reflected in the p-value associated with the coefficient for Corpus: In the Poisson model, it is .07, in the negative binomial model it is .30.\n\n\nSummary\nCorpus data are often structured hierarchically, with multiple data points from the same text. If this feature is not taken into account in the analysis, statistical inferences based on confidence intervals or p-values will usually be overoptimistic. As a result, the bounds for a successful replication, if evaluated on inferential grounds, are too narrow, and replication attempts may be too readily dismissed as unsuccessful. This has the potential to trigger unwarranted discussions about replication failures in our field.\nWe should arguably always consider the possibility that a failed replication may be due to issues surrounding the statistical analysis of clustered data. To rule out this possibility, the replication team needs access to the data from the original study. This is another reason why researchers should routinely make their data available to the community.\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nCumming, Geoff. 2009. “Inference by Eye: Reading the Overlap of Independent Confidence Intervals.” Statistics in Medicine 28 (2): 205–20. https://doi.org/10.1002/sim.3471.\n\n\nCumming, Geoff, Jennifer Williams, and Fiona Fidler. 2004. “Replication and Researchers’ Understanding of Confidence Intervals and Standard Error Bars.” Understanding Statistics 3 (4): 299–311. https://doi.org/10.1207/s15328031us0304_5.\n\n\nEvert, Stephanie. 2023. Corpora: Statistics and Data Sets for Corpus Frequency Data. https://doi.org/10.32614/CRAN.package.corpora.\n\n\nLeech, Geoffrey N. 2003. “Modality on the Move: The English Modal Auxiliaries 1961-1992.” In Modality in Contemporary English, edited by Roberta Facchinetti, Frank Palmer Palmer, and Manfred Krug, 223–40. DE GRUYTER. https://doi.org/10.1515/9783110895339.223.\n\n\n———. 2011. “The Modals ARE Declining: Reply to Neil Millar’s ‘Modal Verbs in TIME: Frequency Changes 1923–2006,’ International Journal of Corpus Linguistics 14:2 (2009), 191–220.” International Journal of Corpus Linguistics 16 (4): 547–64. https://doi.org/10.1075/ijcl.16.4.05lee.\n\n\nMcEnery, Tony, and Vaclav Brezina. 2022. Fundamental Principles of Corpus Linguistics. Cambridge University Press. https://doi.org/10.1017/9781107110625.\n\n\nMillar, Neil. 2009. “Modal Verbs in TIME: Frequency Changes 1923–2006.” International Journal of Corpus Linguistics 14 (2): 191–220. https://doi.org/10.1075/ijcl.14.2.03mil.\n\n\nSönning, Lukas. 2024. “Background data for: Some obstacles to replication in corpus linguistics.” DataverseNO. https://doi.org/10.18710/7LNWJX.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Clustering in the Data Affects Statistical Uncertainty\n    Intervals},\n  date = {2025-05-02},\n  url = {https://lsoenning.github.io/posts/2025-05-01_clustering_uncertainty_intervals/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Clustering in the Data Affects Statistical\nUncertainty Intervals.” May 2, 2025. https://lsoenning.github.io/posts/2025-05-01_clustering_uncertainty_intervals/."
  },
  {
    "objectID": "posts/2025-05-03_imbalance_bias/index.html",
    "href": "posts/2025-05-03_imbalance_bias/index.html",
    "title": "Imbalance across predictor levels affects data summaries",
    "section": "",
    "text": "Corpus data are often characterized by a lack of balance. In contrast to experiments, where the researcher has (almost) full control over the distribution of data points across the conditions of interest, the spread of observations across the levels of relevant predictor variables is virtually always uneven in observational research. When the data break down into subgroups, many quantities of interest essentially represent some kind of average over these conditions. When dealing with unbalanced data, the researcher should actively decide whether they want subgroups to influence this average in proportion to their size, or whether a simple average is preferred. These are two different estimands, and in order for the estimates based on an original and a replication study to be comparable, they need to target the same estimand.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(corpora)         # to calculate a log-likelihood score\nlibrary(ggthemes)        # for colorblind color theme\nlibrary(kableExtra)      # for drawing html tables\nlibrary(ggthemes)        # for color-blind palette\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")\n\n\n\nTerminology\nBefore we get started, let me briefly clarify how a number of terms are used in this blog post:\n\nOriginal study: The study whose results are subject to a replication effort\nReplication study: A study that uses new data and repeats the original work in the closest possible way, i.e. using the same research design and methods; this type of replication is often referred to as a direct/close/exact/literal replication\nOriginal/replication estimate: For the quantity of interest (sometimes referred to as an effect size), the point estimate returned by the original/replication study\n\n\n\nEstimands, estimators, and estimates\nIn empirical work, the target of inquiry is sometimes referred to as the estimand. Estimands are formulated in substantive terms; in the language sciences, they represent the linguistic objectives of our research efforts.\nTo obtain linguistic insights from corpus data, we apply statistical procedures to calculate numerical summaries. A specific procedure (e.g. a formula) is referred to as an estimator. Well-known examples of estimators are the arithmetic mean and the median, which offer different ways of characterizing the typical unit under study (the estimand).\nFinally, the specific value we obtain by applying an estimator to a set of data is referred to as an estimate. Note that while the estimand hinges on our research objectives and a corresponding estimator is chosen by the researcher, the estimate we obtain will depend on the data at hand and therefore vary from study to study.\nAn excellent paper by Lundberg, Johnson, and Stewart (2021) discusses the importance of clearly defining the estimand of your study. While the authors go into much greater depth and consider the mapping between theoretical and empirical estimands, we will concentrate on what at first seems to be a rather superficial feature of estimands: The weight they give to different subgroups (or, more generally: conditions) in the data. It turns out, however, that this decision may not only affect the conclusions drawn from a study, but can also be challenging to motivate on linguistic grounds.\nReaders who are already familiar with this earlier blog post may skip to the section titled “Vague estimands”.\n \n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur linguistic focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work.\nFurther, questions about diachronic trends in the frequency of modals have generated discussions about replicability in corpus linguistics. Based on a comparison of the Brown and Frown corpus, Leech (2003) concluded that the frequency of English modal verbs declined in the latter half of the 20th century. This finding was challenged by Millar (2009), which in turn prompted a response by Leech (2011). McEnery and Brezina (2022) also used data on English modals as a case study for discussing and illustrating key ideas about replication in corpus linguistics.\nEnglish modal verbs therefore have a special place in the corpus-linguistic discourse on replication and replicability. I therefore decided to set up a dedicated TROLLing post (Sönning 2024), which includes frequency information on the English modals from the Brown Family of corpora. Perhaps this resource may be of value in future discussion on the topic. An excerpt from this dataset is used in the current series of blog posts, which concentrate on statistical issues that may get in the way of replication attempts in corpus work.\nWe will concentrate on a subset of these data: the modal verb should in Brown and Frown, i.e. written American English. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\n \n\n\nData\nWe start by downloading the data directly from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe dataset we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nNext, we extract the data for should in Brown and Frown and prepare them for analysis.\n\n\nLoad and prepare data\nd_modals &lt;- subset(dat, corpus %in% c(\"Brown\", \"Frown\"))\n\nd_modals$time_period &lt;- factor(d_modals$time_period)\nd_modals$genre &lt;- factor(d_modals$genre)\n\ncontrasts(d_modals$genre) &lt;- contr.sum(4)\ncontrasts(d_modals$time_period) &lt;- contr.sum(2)\n\nshould_data &lt;- subset(d_modals, modal==\"should\")\nshould_Brown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Brown\")\nshould_Frown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Frown\")\nshould_learned &lt;- subset(d_modals, modal==\"should\" & genre==\"learned\")\n\n\nBrown and Frown each consist of 500 texts, which are sampled from four different genres. The following table shows the word count and number of texts for each genre in Brown:\n\n\n\n\nTable 1: Distribution of words and texts across the four braod genres in the Brown Corpus.\n\n\n\n\n\n\nGenre\nWords\nTexts\n\n\n\n\nFiction\n295,779 (25.8%)\n126 (25.2%)\n\n\nGeneral prose\n470,726 (41.0%)\n206 (41.2%)\n\n\nLearned\n180,649 (15.7%)\n80 (16.0%)\n\n\nPress\n201,300 (17.5%)\n88 (17.6%)\n\n\n\n\n\n\n\n\n\n\n \n\n\nVague estimands\nWhile the research questions we formulated above outline the general direction of our analysis, they leave considerable room for specifying an estimand. Thus, we may ask what exactly is meant by “the frequency of should in written American English of the early 1960s”.\nThe focus in this blog post is on how the four genres are to be handled when calculating a frequency estimate. This question is relevant because of their differential size in the corpus. When measuring the frequency of should in Brown, the question would be whether the four genres should be weighted proportionally to their size, or whether they should be given equal weights. In other words, are we interested in the frequency of should in a population of written American English where each genre is equally important, or a population where the weights differ, perhaps reflecting their prevalence, or currency, in the linguistic community.\nThe representation of the four genres in Brown is linguistically motivated. The manual states: “The list of main categories and their subdivisions was drawn up at a conference held at Brown University in February 1963. The participants in the conference [John B. Carroll, W. Nelson Francis, Philip B. Gove, Henry Kucera, Patricia O’Connor, and Randolph Quirk.] also independently gave their opinions as to the number of samples there should be in each category. These figures were averaged to obtain the preliminary set of figures used.” While the manual does not specify the rationale underlying participants’ preferences for the number of text samples per text category, we may assume that the weighting is meant to reflect the currency of these subvarieties in written American English.\nDifferent estimands require different estimators: The currency-informed estimand of the normalized frequency weights genres in proportion to their representation in the corpus. This is equivalent to simply obtaining the corpus frequency of should: Divide the number of occurrences by the corpus size. The equal-importance estimand, on the other hand, requires a simple average over four occurrence rates, one for each genre.\n\n\nCorpus frequencies are weighted averages\nThe typical way of answering the research questions we formulated above would be the following:\n\nObtain the normalized corpus frequency of should in Brown and then in Frown\nCompare the two normalized frequencies, e.g. by dividing the Frown rate by the Brown rate.\n\nHere is how we could do this in R (with normalized frequencies expressed as ‘per thousand words’):\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nFor Brown, we get a rate of 0.79 per thousand words, which is also what CQPweb returns:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\n\n\n\n\n\n\nFor Frown, we get a rate of 0.68 per thousand words, in line with the CQPweb report:\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\n\n\n\n\n\nFinally, the comparison of the normalized frequencies indicates that the Frown rate is 86% as large as that in Brown:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\nCorpus frequencies and their differences are weighted averages, which assign differential importance to the four genres. In the case of the Brown Family of corpora, where considerable thought has been given to the representation of the text categories, this differential weighting may be desirable. When working with other corpora, crude corpus frequencies may need to be treated more cautiously.\nA case in point are spoken corpora, where the word count (and implicit weight) may vary considerably across speakers and speaker groups. In written corpora, however, texts may also vary in length. In both cases (imbalance across speakers or texts), there are few situations in which differences in size reflect differences in importance. We would then like to avoid the implicit weighting implemented by plain corpus frequencies. As discussed by Egbert and Burch (2023), a different type of estimator, the mean text frequency, may then be preferable. It first calculates normalized frequencies at the text (or speaker) level, and then averages over these.\nIn the Brown Family, word counts are roughly balanced across texts, and we therefore need not worry about this kind of imbalance when measuring frequency. The mean text frequencies almost coincide with the corpus frequencies:\n\nround(\n  mean(\n    (should_Brown$n_tokens / should_Brown$n_words) *1000),\n  2)\n\n[1] 0.8\n\nround(\n  mean(\n    (should_Frown$n_tokens / should_Frown$n_words) *1000),\n  2)\n\n[1] 0.69\n\n\nThis blog post discusses situations where corpus units (texts or speakers) differ in size, and how this can affect frequency estimates.\nAs noted above, the Brown Family shows a different form of imbalance: The size of the four broad genres (Fiction, General Prose, Learned, Press) differs. Table 1 showed that the genre General prose accounts for 41% of the corpus size, while Learned and Press are relatively underrepresented. We now look at how this disproportion can affect estimates of frequencies and their differences.\n\n\nFrequency\nWe first consider the estimation of normalized frequencies in Brown and Frown, which we will approach from two angles: We start with descriptive data summaries and then look at model-based estimates.\n\nDescriptive data summaries\nWhen using descriptive statistics to summarize the data, the equal-importance estimand is obtained using a simple average over the genre-specific normalized frequencies. In R, we can use the {dplyr} package to calculate this simple average in two steps:\n\ncalculate genre-specific rates\naverage over these\n\n\nBrown_simple &lt;- should_Brown |&gt;                  #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(mean(rate_ptw))                      # (2)\n\nFrown_simple &lt;- should_Frown |&gt;                  #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(mean(rate_ptw))                      # (2)\n\nThe currency-informed average, on the other hand, can be obtained using plain corpus frequencies (see above). An alternative procedure, which is more flexible, uses the same two-step approach but calculates a weighted average in step 2. For this, we need a set of weights that reflect the representation of these text categories in the Brown Family. We will calculate these on the basis of both corpora (Brown and Frown):\n\nbrown_family_weights &lt;- should_data |&gt; \n  group_by(genre) |&gt; \n  summarize(\n    n_words_genre = sum(n_words)\n  ) |&gt; \n  mutate(\n    genre_weight = n_words_genre / sum(n_words_genre)\n  )\n\nHere they are (in alphabetical order: Fiction, General prose, Learned, Press):\n\nround(\n  brown_family_weights$genre_weight, \n  2)\n\n[1] 0.26 0.41 0.16 0.17\n\n\nNow we can apply the two-step procedure:\n\nBrown_weighted &lt;- should_Brown |&gt;                #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(weighted.mean(                       # (2)\n    rate_ptw,                                    #\n    w = brown_family_weights$genre_weight))      #\n\nFrown_weighted &lt;- should_Frown |&gt;                #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(weighted.mean(                       # (2)\n    rate_ptw,                                    #\n    w = brown_family_weights$genre_weight))      #\n\nNote that we could have included the calculation of weights into the code above, which would have been more error-tight in the present case. The specification of “external” weights, however, gives us more flexibility when calculating weighted averages. For instance, we may decide to use custom weights, or weights based on the data distribution in a different (reference) corpus.\nFigure 1 compares these frequency estimates visually. Proportionally scaled circles are used to show the differential representation of the genres in the corpus. The weighted averages appear in grey, the simple ones in black.\nFor Brown, the simple average (0.81 ptw) and the weighted average (0.80 ptw) are very similar, indicating that the imbalance of word counts across genres does not affect the frequency estimate much. In Frown, the discrepancy is greater, with a simple average of 0.72 ptw, and a weighted average of 0.69 ptw. This is because the genres Learned and Press, which show relatively large occurrence rates of should, gain weight when calculating a simple (instead of a weighted) average: The mass for Learned increases from .16 to .25, that for Press from .17 to .25.\n\n\nDraw Figure\nBrown_simple &lt;- as.numeric(Brown_simple)\nBrown_weighted &lt;- as.numeric(Brown_weighted)\n\nFrown_simple &lt;- as.numeric(Frown_simple)\nFrown_weighted &lt;- as.numeric(Frown_weighted)\n\n\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ggplot(aes(x = corpus, y = ptw, size = n_words, group = genre, color = genre)) +\n  geom_point(shape = 1) +\n  geom_point(shape = 16, size = 1) +\n  scale_color_colorblind() +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +\n  scale_x_discrete(expand=c(.2,.2)) +\n  scale_size_area(max_size = 8) +\n  theme(legend.position = \"none\",\n        plot.margin = margin(.2, 2, .2, .2, \"cm\")) +\n  directlabels::geom_dl(aes(label = genre), method = list(\n    \"last.points\", cex = .75, x = 3.52, \n    label = c(\"Fiction\", \"General prose\", \"Learned\", \"Press\"))) +\n  \n  annotate(\"segment\", x = .9, xend = 1.1, y = Brown_weighted, yend = Brown_weighted, \n           linewidth = .8, color = \"grey50\") +\n  annotate(\"segment\", x = .9, xend = 1.1, y = Brown_simple, yend = Brown_simple,\n           linewidth = .8) +\n  annotate(\"segment\", x = 1.9, xend = 2.1, y = Frown_weighted, yend = Frown_weighted, \n           linewidth = .8, color = \"grey50\") +\n  annotate(\"segment\", x = 1.9, xend = 2.1, y = Frown_simple, yend = Frown_simple, \n           linewidth = .8) +\n  \n  annotate(\"segment\", x = 1.1, xend = 1.9, y = Brown_weighted, yend = Frown_weighted,\n           linewidth = .3, color = \"grey50\") +\n  annotate(\"segment\", x = 1.1, xend = 1.9, y = Brown_simple, yend = Frown_simple, \n           linewidth = .3) +\n  \n  annotate(\"text\", x = c(1.4, 1.6), y = c(.68, .85), label = c(\"weighted\", \"simple\"), \n           size = 3, color = c(\"grey50\", \"black\")) +\n  coord_cartesian(clip=\"off\")\n\nggsave(\"should_imbalance_brown.pdf\")\n\n\n\n\n\n\n\n\nFigure 1: Estimated frequency of should in Brown and Frown: Comparison of simple and weighted averages across genres.\n\n\n\n\n\n \n\n\nModel-based estimates\nWhen calculating model-based predictions, we can likewise decide whether we want to form simple or weighted averages. The default behavior in the {marginaleffects} package is to use the in-sample distribution of predictor variables to calculate average predictions. This is to say that, unless explicitly told to do otherwise, the functions in the package will usually calculate weighted averages.\nLet’s take a look at how to produce simple and weighted averages using a negative binomial model of should in Frown. The first step is to fit the model:\n\nm_nb_Frown &lt;- MASS::glm.nb(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Frown)\n\nThe function avg_predictions() calculates average predictions. Its default behavior for the data at hand returns a frequency estimate that is unlikely to be of interest to us. This is because it uses the in-sample mean text length (n_words) to adjust the predicted rate. The estimate of 1.58 is therefore the expected frequency ‘per 2,309 words’:\n\navg_predictions(\n  m_nb_Frown)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     1.58     0.0899 17.6   &lt;0.001 227.7   1.4   1.76\n\nType:  response \n\n\nWe must take control over the kind of normalized frequency we are getting. We prefer ‘per 1,000 words’ and therefore use the argument variables to specify n_words = 1000. Now we get a more interpretable estimate:\n\navg_predictions(\n  m_nb_Frown,\n  variables = list(\n    n_words = 1000))\n\n\n n_words Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    1000    0.686     0.0391 17.6   &lt;0.001 227.2  0.61  0.763\n\nType:  response \n\n\nWe note that this is close to the weighted mean we calculated above, which means that the genres are weighted in proportion to their size. This reflects the fact that the {marginaleffects} package by default averages over the estimation sample, and therefore propagates imbalances into the averages (which may be desirable or not). Specifically, the function avg_predictions() starts by calculating a model-based prediction for each text in the data, assuming it is 1,000 words long (as specified by variables = list(n_words = 1000))), and then averages over these 500 model-based estimates.\nAnother way of forming weighted predictions is to use the argument datagrid() to define the conditions over which we average, and then add another argument, wts, giving the weight of these conditions. This strategy is useful if we want to use an externally informed set of custom weights. The following returns (almost) the same results as the previous code:\n\navg_predictions(\n  m_nb_Frown,  \n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  wts = brown_family_weights$genre_weight)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.685     0.0389 17.6   &lt;0.001 227.7 0.608  0.761\n\nType:  response \n\n\nIf we instead prefer a simple average, we can use the argument newdata to explicitly define the conditions to average over. This way we tell the function not to take the estimation sample as a basis for calculating predictions (and weighting), but instead define the reference grid over which to average. The following code asks for a simple average over four conditions, which represent different genres but have the same length. The result is close to the simple average we calculated above.\n\navg_predictions(\n  m_nb_Frown,  \n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000))\n\n\n Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.715     0.0448 16   &lt;0.001 188.2 0.627  0.802\n\nType:  response \n\n\nThis shows that imbalances in the data can have an effect on simple data summaries such as estimates of average normalized frequencies. This is sometimes referred to as imbalance bias, and in the present case, we could refer to it as genre imbalance bias. This kind of distortion can occur when (i) there is imbalance across subgroups in the data and (ii) the quantity of interest varies from subgroup to subgroup. We have seen how to adjust for this form of bias using model-based predictions.  \n\n\n\nFrequency differences\nIn the same way, imbalance bias can affect frequency differences. Recall that a crude comparison of the corpora reveals that the frequency of should in Frown is only 86% as high as that in Brown. Since this comparison is based on two corpus frequencies, each of which is potentially affected by imbalance bias, the comparison may likewise be driven into the direction of more strongly represented genres.\n\nDescriptive data summaries\nFigure 2 shows the diachronic trends in the four genres. We note that while Press and Fiction show virtually no difference between Brown and Frown, a diachronic cline is apparent for Learned and General prose. Seeing that General prose is the most strongly represented genre, we would expect weighted differences to be pulled into its direction, meaning that a weighted frequency difference will be larger than a simple frequency difference, which would give the same weight to all genres.\n\n\nDraw Figure\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ggplot(aes(x = corpus, y = ptw, size = n_words, group = genre, color = genre)) +\n  geom_point(shape = 1) +\n  geom_point(shape = 16, size = 1) +\n  geom_line(size = .5) +\n  scale_color_colorblind() +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +\n  scale_size_area(max_size = 8) +\n  theme(legend.position = \"none\",\n        plot.margin = margin(.2, 2, .2, .2, \"cm\")) +\n  directlabels::geom_dl(aes(label = genre), method = list(\"last.points\", cex = .75, x = 3.52, label = c(\"Fiction\", \"General prose\", \"Learned\", \"Press\"))) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 2: Diachronic trends by text category: Frequency of should in Frown vs. Brown, broken down by genre.\n\n\n\n\n\nWe can calculate descriptive frequency comparisons according to the two schemes, i.e. by either weighting all genres equivalently, or in proportion to their representation in the data. This returns two slightly different estimates: A frequency decline by 11% (simple comparison) or by 13% (weighted comparison).\n\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ungroup() |&gt; \n  group_by(genre) |&gt; \n  dplyr::summarize(\n    freq_ratio_data = ptw[corpus == \"Frown\"]/ptw[corpus == \"Brown\"],\n    n_words_genre = sum(n_words)) |&gt; \n  mutate(\n    weight = n_words_genre/sum(n_words_genre)) |&gt; \n  dplyr::summarize(\n    simple_comparison = mean(freq_ratio_data),\n    weighted_comparison = weighted.mean(freq_ratio_data, w = weight)\n  ) |&gt; round(2)\n\n# A tibble: 1 × 2\n  simple_comparison weighted_comparison\n              &lt;dbl&gt;               &lt;dbl&gt;\n1              0.89                0.87\n\n\n \n\n\nModel-based estimates\nLet us again look at how to obtain these two types of comparison using a regression model. We start by fitting a negative binomial model that includes two predictors, Corpus and Genre, as well as their interaction.\n\nm_nb_corpus &lt;- MASS::glm.nb(\n    n_tokens ~ corpus * genre + offset(log(n_words)), \n    data = should_data)\n\nThis kind of model allows us to calculate frequency comparisons at the level of the individual genres (similar to what we saw in Figure 2 above). Alternatively, we may average over the four genres, to get a general estimate of how the frequency of should differs between the corpora.\nFor purposes of illustration, let’s use the {marginaleffects} package to get genre-level frequency comparisons. We use the function comparisons() to do so.\n\nThe argument variables specifies the focal variable(s), i.e. the one(s) whose levels are to be compared. In our case, this is the predictor Corpus.\nThe argument newdata allows us to specify the location in the predictor space at which to make comparisons. This means that it allows us to take control over the levels of the non-focal variables. Since we want a comparison for each genre, we specify all genres, and we also want to compare normalized frequencies ‘per 1,000 words’.\nFinally, by specifying transform = exp, we are asking comparisons() to exponentiate the log-scale differences, which yields rate ratios.\n\n\ncomparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  transform = exp)\n\n\n\n# A tibble: 4 × 5\n  genre         contrast      estimate conf.low conf.high\n  &lt;fct&gt;         &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 fiction       Frown - Brown     0.98     0.84      1.16\n2 general_prose Frown - Brown     0.82     0.68      0.99\n3 learned       Frown - Brown     0.83     0.59      1.17\n4 press         Frown - Brown     1.03     0.75      1.42\n\n\nWe can use the function avg_comparisons() to average over the four genres. The simple average is obtained as follows. Note that the code is almost identical to the one we used above, apart from replacing the function name:\n\navg_comparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  transform = exp)\n\n\n Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n    0.911    0.168 2.6 0.797   1.04\n\nTerm: corpus\nType:  response \nComparison: Frown - Brown\n\n\nUsing a simple average over the genres, the model-based estimate of the rate ratio is 91%: The normalized frequency in Frown is 91% as large as that in Brown. The regression model also provides a 95% confidence interval for this estimate, which ranges from 90% to 104%.\nTo obtain a model-based weighted average, we use the argument wts = brown_family_weights$genre_weight to specify the weights of the conditions in the reference grid. Our reference grid consists of four rows, so four weights are required, one for each genre.\n\navg_comparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  wts = brown_family_weights$genre_weight,\n  transform = exp)\n\n\n Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n    0.895   0.0648 3.9 0.796   1.01\n\nTerm: corpus\nType:  response \nComparison: Frown - Brown\n\n\nUsing a weighted average over the genres, the model-based estimate of the rate ratio is 89%, with the 95% CI ranging from 80% to 101%.\nWe observe that these model-based estimates differ from the descriptive ones reported above. Table 2 shows that the model-based estimates suggest slightly smaller differences between Brown and Frown. This is due to the fact that averaging was done on different scales: While the descriptive ratios were averaged on the data scale (i.e. ratios), the model-based estimates were averaged on the model scale (i.e. log ratios) and the average then back-transformed into a ratio. The discrepancy between these ways of forming averages will be discussed in a future blog post.\n\n\n\n\nTable 2: Comparison of simple and weighted summary statistics vs. model-based estimates.\n\n\n\n\n\n\nComparison\nDescriptive\nModel-based\n\n\n\n\nSimple\n89%\n91%\n\n\nWeighted\n87%\n89%\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelevance for replication\nIn order for the statistical results of an original and a replication study to be comparable, they need to be concerned with the same target quantity, or estimand. One issue that will always be relevant when working with observational data is the imbalance of observations across relevant subgroups, where “relevant” means that they show an association with the outcome variable. Corpus frequencies such as those reported in the CQPweb interface then represent weighted data summaries that reflect the disproportional representation of subgroups in the data.\nDue to the directly parallel design of Brown and Frown, we saw relatively minor differences between currency-based and equal-importance (i.e. weighted and simple) normalized frequencies. Nevertheless, the choice of estimand did have an effect on our estimates and statistical uncertainty intervals.\nThis issue will be more serious when we compare results across corpora that differ in composition. It is then arguably negligent to work with plain corpus frequencies. When comparing the BNC and COCA, for instance, comparisons should be adjusted for differences in genre coverage and representation (see Sönning and Schlüter 2022, 29–31 for a worked example).\nIn general, it is therefore necessary for any type of corpus analysis to think carefully about imbalances in the data and how these will propagate into the statistical results. If the disproportional representation of subgroups in the data is a meaningful feature of the population of interest, the researcher may wish to preserve it in their data summaries. A replication study, however, should be based on the same weighting scheme as the original study, which means that adjustments to the way in which weighted averages are formed will be necessary.\nThis issue should prompt us to generally think more carefully about imbalances in corpus data. Here, we have dealt with a single variable, genre, whose distribution in the data can usually be recovered from the corpus documentation. The issue, however, applies to any predictor variable that (i) is relevant, i.e. shows an association with the outcome; and (ii) whose distribution in the data is out of balance. Whether adjustments should be made for imbalances then depends on whether distributional asymmetries reflect a meaningful feature of the population of interest, or rather a nuisance.\nWe discuss the question of weighting model-based estimates in some more detail in Sönning and Grafmiller (2024, 163–69), where we suggest that a useful default approach may be to retain sample-based weights for internal (or linguistic) variables in the data, whose distribution cannot be controlled during corpus compilation. External variables, on the other hand, which primarily reflect corpus design (e.g. characteristics of the speaker or text) may be considered as candidates for adjustment. Note that this does not necessarily mean that they are assigned equivalent weights.\n\n\nSummmary\nDue to their observational nature, corpus data are often unbalanced. If imbalances affect variables that show an association with the outcome, a study may target different estimands, depending on how imbalances are handled when summarizing the data. While weighted averages propagate imbalances into our summaries, simple averages assign equal weight (and importance) to subgroups. In this blog post, we used the Brown Family of corpora, which is unbalanced by design. Genres differ in size, and when measuring and comparing normalized frequencies, we need to decide how to work with this asymmetry. Importantly, corpus frequencies, which are reported in corpus analysis software, are always weighted averages reflecting the composition of the corpus. The choice of estimand is a linguistic decision – it depends on the nature of the (hypothetical) population we are interested in. In replication work, however, it is also a methodological decision: A replication study must ensure that it targets the same estimand as the original study.\n\n\n\n\n\nReferences\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nLeech, Geoffrey N. 2003. “Modality on the Move: The English Modal Auxiliaries 1961-1992.” In Modality in Contemporary English, edited by Roberta Facchinetti, Frank Palmer Palmer, and Manfred Krug, 223–40. DE GRUYTER. https://doi.org/10.1515/9783110895339.223.\n\n\n———. 2011. “The Modals ARE Declining: Reply to Neil Millar’s ‘Modal Verbs in TIME: Frequency Changes 1923–2006,’ International Journal of Corpus Linguistics 14:2 (2009), 191–220.” International Journal of Corpus Linguistics 16 (4): 547–64. https://doi.org/10.1075/ijcl.16.4.05lee.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMcEnery, Tony, and Vaclav Brezina. 2022. Fundamental Principles of Corpus Linguistics. Cambridge University Press. https://doi.org/10.1017/9781107110625.\n\n\nMillar, Neil. 2009. “Modal Verbs in TIME: Frequency Changes 1923–2006.” International Journal of Corpus Linguistics 14 (2): 191–220. https://doi.org/10.1075/ijcl.14.2.03mil.\n\n\nSönning, Lukas. 2024. “Background data for: Some obstacles to replication in corpus linguistics.” DataverseNO. https://doi.org/10.18710/7LNWJX.\n\n\nSönning, Lukas, and Jason Grafmiller. 2024. “Seeing the Wood for the Trees: Predictive Margins for Random Forests.” Corpus Linguistics and Linguistic Theory 20 (1): 153–81. https://doi.org/10.1515/cllt-2022-0083.\n\n\nSönning, Lukas, and Julia Schlüter. 2022. “Comparing Standard Reference Corpora and Google Books Ngrams: Strengths, Limitations and Synergies in the Contrastive Study of Variable h- in British and American English.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 17–45. Cambridge University Press. https://doi.org/10.1017/9781108589314.002.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Imbalance Across Predictor Levels Affects Data Summaries},\n  date = {2025-05-04},\n  url = {https://lsoenning.github.io/posts/2025-05-03-imbalance_bias/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Imbalance Across Predictor Levels Affects\nData Summaries.” May 4, 2025. https://lsoenning.github.io/posts/2025-05-03-imbalance_bias/."
  },
  {
    "objectID": "posts/2023-12-13_negative_binomial_parameterization/index.html",
    "href": "posts/2023-12-13_negative_binomial_parameterization/index.html",
    "title": "Different parameterizations of the negative binomial distribution",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(gamlss)\nlibrary(COUNT)\nlibrary(brms)\nlibrary(rstanarm)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two different ways. These different parameterizations yield the same results for the mean of the distribution, i.e. the expected count or rate. Thus, if we are only interested in the occurrence rate (or normalized frequency) of an item, including a 95% statistical uncertainty interval, no problems arise. If we are interested in the negative binomial dispersion parameter, however, we must know which parameterization is implemented in the R package or R function we are using. The main purpose of this blog post is to divide R functions into two groups, depending on which version of the negative binomial distribution they use."
  },
  {
    "objectID": "posts/2023-12-13_negative_binomial_parameterization/index.html#footnotes",
    "href": "posts/2023-12-13_negative_binomial_parameterization/index.html#footnotes",
    "title": "Different parameterizations of the negative binomial distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor completeness, we provide the mathematical definition of the negative binomial distribution (more specifically, the NB2). The probability of a count y given x is \\(\\small{\\Pr(y\\;|\\;x) = \\frac{\\gamma(y+\\phi)}{y!\\;\\gamma(\\phi)} \\left(\\frac{\\phi}{\\phi + \\mu}\\right)^{\\phi} \\left(\\frac{\\mu}{\\phi + \\mu}\\right)^{y}}\\) where \\(\\small{\\gamma(\\dots)}\\) is the gamma function and \\(\\small{\\phi}\\) is the dispersion parameter. In R, we can use the function dNBI() in the package gamlss.↩︎\nDon’t be confused if the way the gamma distribution is introduced here does not correspond to descriptions you find in the literature. The distribution is actually characterized by two parameters \\(\\small{p_1}\\) and \\(\\small{p_2}\\) (see, e.g. Gelman et al. 2013, 578–79). For the negative binomial distribution, we need a gamma distribution that is centered at 1, which makes one of these parameters (i.e. \\(\\small{p_1}\\) or \\(\\small{p_2}\\)) redundant. The mean, or expected value, of a gamma distribution is given by \\(\\small{p_1/p_2}\\). In order for the mean to be 1, we have to set \\(\\small{p_1=p_2}\\), so the version of the gamma distribution we need for the NB distribution only has one parameter.↩︎"
  },
  {
    "objectID": "posts/2025-05-05_binomial_overdispersion/index.html",
    "href": "posts/2025-05-05_binomial_overdispersion/index.html",
    "title": "Modeling clustered binomial data",
    "section": "",
    "text": "A typical feature of corpus data is their hierarchical layout. Observations are usually clustered, which is the case if multiple data points are from the same text (or speaker). Observations from the same source are usually more similar to one another, reflecting idiosyncracies of the author/speaker or particularities of the context of language use. For binary outcome variables, there are different options for modeling such data. This blog post builds on a paper by Anderson (1988) and contrasts approaches that differ in the way they represent (or account for) the non-independence of data points.\n\n\nR setup\nlibrary(tidyverse)         # for data wrangling and visualization\nlibrary(marginaleffects)   # to compute model-based estimates\nlibrary(corpora)           # for data on passives\nlibrary(kableExtra)        # for drawing html tables\nlibrary(lattice)           # for data visualization\nlibrary(likelihoodExplore) # for drawing the binomial likelihood\nlibrary(gamlss)            # to fit a variant of the quasi-binomial model\nlibrary(aod)               # to fit a beta-binomial model\nlibrary(PropCIs)           # to calculate Wilson score CIs\nlibrary(doBy)              # to convert data from short to long format\nlibrary(lme4)              # to fit mixed-effects regression models\nlibrary(uls)               # pak::pak(\"lsoenning/uls\")\n\n\n\nData: Passives in academic writing\nWe use data on the frequency of the passive in the Brown Family of corpora, which is part of the {corpora} package (Evert 2023). We concentrate on the genre Learned and consider texts from Brown and Frown.\n\nd &lt;- PassiveBrownFam |&gt; \n  filter(\n    genre == \"learned\",\n    corpus %in% c(\"Brown\", \"Frown\")) |&gt; \n  select(id, corpus, act, pass, verbs)\n\nThis leaves us with 160 texts:\n\nstr(d)\n\n'data.frame':   160 obs. of  5 variables:\n $ id    : chr  \"brown_J01\" \"brown_J02\" \"brown_J03\" \"brown_J04\" ...\n $ corpus: Factor w/ 5 levels \"BLOB\",\"Brown\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ act   : int  88 66 134 117 119 65 95 108 40 192 ...\n $ pass  : int  43 73 61 46 53 65 46 67 84 26 ...\n $ verbs : int  131 139 195 163 172 130 141 175 124 218 ...\n\n\nThere is one row per text and the following variables are relevant for our analyses:\n\nid text identifier\ncorpus source corpus (“Brown” vs. “Frown”)\nact number of active verb phrases in the text\npass number of passive verb phrases in the text\nverbs total number of verb phrases in the text\n\nFor each text, the frequency of the passive can be expressed as a proportion: the proportion of verb phrases that are in the passive voice. We add this variable to the data frame:\n\nd$prop_passive &lt;- d$pass/d$verbs\n\nWe use a dot diagram to inspect the distribution of these proportions across the 160 texts. In Figure 1, each dot represents a text, and the scores reflect the share of passive verb phrases among all verb phrases in the text document. We will refer to this quantity as the text-specific proportion of passive verb phrases.\n\n\ndraw figure\nd |&gt; \n  ggplot(aes(x = prop_passive)) +\n  geom_dotplot(method = \"histodot\", binwidth = .015, dotsize=.8) +\n  theme_dotplot() +\n  scale_x_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .25, .5, .75, 1),\n    labels = c(\"0\", \".25\", \".50\", \".75\", \"1\")) +\n  xlab(\"Proportion of passive verb phrases\")\n\n\n\n\n\n\n\n\nFigure 1: Dot diagram showing the proportion of passive verb phrases in the 160 texts.\n\n\n\n\n\nThe 160 texts also differ in the number of verb phrases they contain, so let us also look at this distribution. Figure 2 shows that this count varies between roughly 100 and 250.\n\n\ndraw figure\nd |&gt; \n  ggplot(aes(x = verbs)) +\n  geom_dotplot(method = \"histodot\", binwidth = 3, dotsize = .7) +\n    theme_dotplot() +\n    xlab(\"Number of verb phrases\")\n\n\n\n\n\n\n\n\nFigure 2: Dot diagram showing the distirbution of the number of verb phrases per text file.\n\n\n\n\n\nThe 160 texts can be considered a sample of academic prose from the language variety of interest, written American English in the second half of the 20th century. In selecting (or sampling) these 160 academic texts, the corpus compilers essentially selected a set of authors, or speakers, of this language variety. In some sense, these individuals represent the primary sampling units: Our sample size for making inference about a larger population of speakers is 160.\nEach text in the Brown Family of corpora is around 2,000 words long. A text excerpt, and the verb phrases it contains, can be considered as a sample from a (hypothetical) population, the academic prose produced by a specific author. At this level, the language use (or writing style) of this individual is the population of interest. The 2,000 words, (or, e.g., 160 verb phrases) then represent the secondary sampling units.\nThis means that we can use the information in the text to make inferences about the underlying propensity of the author(s) to use the passive voice in their academic writing. Texts with fewer verb phrases provide less information, and – due to sampling variation – we would expect smaller samples to yield more variable proportions (see Sönning and Schlüter 2022 for an illustration).\nThis is indeed the case for the present data. The point cloud in the Figure 3 below shows a trumpet-like shape: the highest proportions are from the texts with the fewest verb phrases. We should note, however, that other factors may contribute to this pattern: Thus, texts with fewer verb phrases necessarily feature longer (and presumably more elaborate) sentences, an indicator of abstract writing style that is also associated with passive usage.\n\n\ndraw figure\nd |&gt; ggplot(aes(x = verbs, y = prop_passive)) +\n  geom_point() +\n  theme_classic_ls() +\n  scale_y_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .5, 1),\n    labels = c(\"0\", \".5\", \"1\")) +\n  ylab(\"Proportion of passives\") +\n  xlab(\"Number of verb phrases in the text\")\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot showing the relation between the proportion of passives and the sample size (number of verb phrases in the text).\n\n\n\n\n\nTo emphasize the two-stage sampling design involved in corpus compilation, let us make visual inferences about the language use of the individual authors. To this end, we can construct a 95% confidence interval for each text-specific estimate. We will use the package {PropCIs} (Scherer 2018) to calculate 95% Wilson score confidence intervals for each of the 160 texts.\n?@fig-dotplot-text-cis presents text-level estimates of the proportion of passives with a 95% confidence interval. The degree of overlap among the 160 intervals can be interpreted as giving an indication of the heterogeneity of the individual authors. If the 160 authors showed a similar inclination toward the passive, we would observe considerable overlap among the error bars. Judging from the figure below, however, there seems to be appreciably heterogeneity.\n\n\ndraw figure\nci_upper &lt;- NA\nci_lower &lt;- NA\n\nfor(i in 1:160){\n  ci_lower[i] &lt;- scoreci(x = d$pass[i], n = d$verbs[i], conf.level = .95)$conf.int[1]\n  ci_upper[i] &lt;- scoreci(x = d$pass[i], n = d$verbs[i], conf.level = .95)$conf.int[2]\n}\n\np1 &lt;- xyplot(1~1, type = \"n\", ylim=c(0,1), xlim = c(0,163),\n       par.settings = lattice_ls, axis = axis_left,\n       scales = list(\n         y = list(\n               at = c(0, .5, 1),\n               label = c(\"0\", \".5\", \"1\"))),\n       ylab = \"Proportion of passives\", xlab = NULL,\n       panel = function(x,y){\n         panel.points(x=c(1:80, 83:162), y = d$prop_passive, pch=19)\n         panel.segments(x0=c(1:80, 83:162), x1 = c(1:80, 83:162), y0 = ci_lower, y1 = ci_upper)\n         panel.segments(x0=1, x1=80, y0=0, y1=0)\n         panel.segments(x0=83, x1=162, y0=0, y1=0)\n         panel.text(x=c(40.5, 122.5), y = -.1, label = c(\"Brown\", \"Frown\"))\n         panel.text(x=162, y = -.25, label=\"Error bars: 95% CIs\", col = \"grey40\", cex = .8, adj=1)\n       })\n\ncairo_pdf(\"fig_passives_text_cis.pdf\", width = 7, height = 2)\np1\ndev.off()\n\n\npng \n  2 \n\n\n\n\nBinomial model\nWe start with a simple binomial model, which basically ignores the structure of the data. It uses a single parameter to express the mean proportion of the passive in the dataset. This means that it essentially treats all verb phrases in the data (n = 26772) as an unstructured sample from the population of interest, each one drawn independently of the other ones. The way the data are presented to the model, with one row per text, does not matter to the binomial model – it produces the same result if we supply just one row, with verbs representing the total number of verb phrases and pass the total number of passives in the data.\nSince the clustering variable Text is not taken into account, the model rests on the assumption that the 160 texts share the same underlying relative frequency of the passive. This means that the authors are assumed to be perfectly homogeneous with respect to their stylistic preferences. Under this model, the observed variability in proportions is merely a result of sampling variation, which in turn depends on (i) the number of verb phrases in the text, and (ii) the overall proportion of the passive.\nPoint (ii) deserves some more comment. For binomial data, sampling variation is smaller the closer we get to 0 and 1. This is due to the boundedness of the scale – near 0 or 1, there is less room for variation. Statistically speaking, the variance of the binomial distribution depends on its mean. As Figure 4 illustrates, it is greatest at .50 and decreases toward the endpoints of the proportion scale. This means that the variability of observed proportions depends on the mean of the binomial distribution.\n\n\ndraw figure\nxyplot(\n  1~1, type = \"n\", xlim=c(0,1), ylim = c(0,.25),\n  par.settings = lattice_ls, axis = axis_L,\n  scales = list(y = list(\n    at = c(0, .1, .2, .3, .4, .5),\n    label = c(\"0\", \".1\", \".2\", \".3\", \".4\", \".5\")),\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      labels = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  ylab = \"Variance\",\n  xlab = \"Mean\",\n  panel = function(x,y){\n    panel.points(x = seq(0, 1, .01),\n                 y = (seq(0, 1, .01)*(1-seq(0, 1, .01))),\n                 type=\"l\")\n       })\n\n\n\n\n\n\n\n\nFigure 4: Mean-variance relationship in the binomial model.\n\n\n\n\n\nWe can fit this model in R using the glm() function:\n\nm &lt;- glm(\n  cbind(pass, act) ~ 1, \n  data = d, \n  family = \"binomial\")\n\nThe model intercept is -1.37, which is the mean probability of the passive, expressed on the log odds scale. We can use the function plogis() to back-transform to the proportion scale:\n\nround(\n  plogis(coef(m)), 3)\n\n(Intercept) \n      0.203 \n\n\nA 95% CI can be constructed using the function confint():\n\nplogis(confint(m))\n\n    2.5 %    97.5 % \n0.1981468 0.2077818 \n\n\nModel-based estimates on the proportion scale are easy to obtain using the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). The function avg_predictions() returns a model-based prediction of the mean probability of a passive verb phrase in the population of interest, along with a 95% CI.\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.198     0.208\n\n\nThe estimated proportion of .203 comes with a crisp 95% CI, which ranges from .198 to .208. Apparently, the model is very confident in its predicted probability of the passive voice.\nTo check how well the binomial model fits the data, we can ask it to “retrodict” the data, i.e. to tell us what it thinks the distribution of the 160 text-level proportions looks like. For this task, the binomial model uses the estimated overall proportion (.203) and the sample size for each text, i.e. the number of verb phrases it contains. It can then produce a density curve for each text, which is centered on .203 and spread out in accordance with the sample size: for a text with more verb phrases, the density curve is more peaked, and for a text with fewer verb phrases it is spread more widely around .203. Importantly, however, all density curves are centered on .203, the estimate of the population proportion.\nFigure 5 draws the 160 density curves against the observed distribution of text-specific proportions. Apparently, the model fails to capture the heterogeneity among texts.\n\n\ndraw figure\nxyplot(\n  1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.16),\n  par.settings = lattice_ls, axis = axis_bottom,\n  scales = list(\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab.top = \"Binomial model\\n\",\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.dotdiagram(d$prop_passive, scale_y = .006, n_bins=50, set_col=\"grey\")\n    for(i in 1:160){\n      panel.points(\n        x = seq(0, 1, length = 100),\n        y = exp(likbinom(\n          x = round(d$verbs[i]*.203), \n          size = d$verbs[i], \n          prob = seq(0,1, length = 100), log = TRUE)) /\n          sum(exp(likbinom(\n            x = round(d$verbs[i]*.203), \n            size = d$verbs[i], \n            prob = seq(0,1, length = 100), log = TRUE))),\n        type = \"l\", alpha = .1)\n      }\n    panel.text(x = .4, y = .06, label = \"Observed distribution\", col = \"grey40\", cex = .9, adj = 0)\n    panel.text(x = .25, y = .13, label = \"Expected distribution\", col = 1, cex = .9, adj = 0)\n    })\n\n\n\n\n\n\n\n\nFigure 5: Fit between the binomial model and the data.\n\n\n\n\n\nIf the observed data show greater variation than anticipated by a statistical model, the data are said to be overdispersed relative to this model. Alternative modeling approaches take into account this overdispersion, or heterogeneity. As we will see, however, they do so in different ways.\n\n\nQuasi-binomial model including a heterogeneity parameter\nA quasi-binomial model includes a second parameter that explicitly captures the excess variation in the data (see Agresti 2013, 150–51). This dispersion parameter adjusts the variance of the binomial distribution and is often denoted as \\(\\phi\\). It is estimated on the basis of a global \\(\\chi^2\\) fit statistic for the model.\nIn this way, the quasi-binomial model allows the standard deviation of observed proportions to be greater than anticipated by the simple dependency on the mean of the distribution, which we saw in Figure 4 above. Essentially, the dispersion parameter is a multiplicative factor that adjusts the variance of the binomial distribution upwards. If the dispersion parameter is 1, the model reduces to the binomial model discussed above. The dispersion parameter also affects inferences from the model: standard errors are multiplied by \\(\\sqrt{\\phi}\\).\nWe can fit a quasi-binomial model using the glm() function in R:\n\nm &lt;- glm(\n  cbind(pass, act) ~ 1, \n  data = d, \n  family = \"quasibinomial\")\n\nThen we use the {marginaleffects} package to obtain the model-based predicted probability of a passive verb phrase (+ 95% CI):\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.186      0.22\n\n\nThis produces the same estimate as above, but with a wider uncertainty interval. The model intercept, once back-transformed to the probability scale, yields the same estimate:\n\nround(\n  plogis(coef(m)), 3)\n\n(Intercept) \n      0.203 \n\n\nThe heterogeneity factor adjusts the variance of the binomial distribution to align the model with the excess variability in the observed proportions. However, the model still assumes a constant underlying proportion for all authors. The heterogeneity parameter basically states that some perturbation, perhaps caused by omitted predictors or positively correlated (i.e. non-independent) observations in each row of the table, leads to greater variability of the observed proportions. The model does not point to a specific source of the overdispersion.\nSince the variance is increased proportionally to the overdisersion parameter, the bell-shaped curves are spread out more widely. This is illustrated in the figure below. We see that the fit between model and data is much better now.\n\n\ndraw figure\nxyplot(\n  1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.06),\n  par.settings = lattice_ls, axis = axis_bottom,\n  scales = list(\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab.top = \"Quasi-binomial model\\n\",\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.dotdiagram(d$prop_passive, scale_y = .004, n_bins=50, set_col=\"grey\")\n    for(i in 1:160){\n      interpolated_to_100_steps &lt;- approx(\n        x = (0:d$verbs[i])/d$verbs[i],\n        y = dDBI(0:d$verbs[i], mu=.203, sigma=11, bd=d$verbs[i]),\n        xout = seq(0, 1, length=100))\n      \n      panel.points(x = interpolated_to_100_steps$x,\n                   y = interpolated_to_100_steps$y/sum(interpolated_to_100_steps$y),\n                   type = \"l\", alpha = .1)\n    }\n    panel.text(x = .48, y = .03, label = \"Observed distribution\", \n               col = \"grey40\", cex = .9, adj = 0)\n    panel.text(x = .25, y = .055, label = \"Expected distribution\", \n               col = 1, cex = .9, adj = 0)\n  })\n\n\n\n\n\n\n\n\nFigure 6: Fit between the quasi-binomial model and the data.\n\n\n\n\n\nHowever, the quasi-binomial model does not represent the structure in our data. It does not attribute the excess variation in proportions to the fact that we are looking at 160 different texts, from speakers who may very well show different stylistic attitudes toward passive usage. Rather, it states that some noise variable increased the sampling variation when drawing verb phrases from each speaker, with speakers nevertheless being actually homogeneous with respect to their underlying usage rate of the passive.\nWhile the quasi-binomial model effectively adjusts inferences for the non-independence of observations, the way in which this is achieved may not be appropriate in all situations (see Finney 1971, 72). In particular, if the data are clustered, this information should be explicitly taken into account. The models we consider next embrace the data structure and introduce parameters that describe between-cluster variation, thereby linking overdispersion to a specific source. As noted by Agresti (2013, 151), this approach is preferable, because it actually models the observed heterogeneity.\n\n\nBeta-binomial model\nThe beta-binomial model also includes a second parameter, but this parameter has a different function (and interpretation). It explicitly allows for the possibility that the texts in the data differ in the underlying probability of passive usage. This parameter aims to represent the distribution of text-specific proportions, which means that it actively takes into account the clustering variable Text. If texts vary considerably, reflecting large overdispersion relative to the binomial model, the parameter describing the text-to-text variation will be large. If there is no evidence for surplus variation among texts, the beta-binomial model reduces to the binomial model. The relationship between the binomial mean and variance (see Figure 4) therefore remains unaltered.\nSince the text-specific proportions are bounded between 0 and 1, a distribution that respects these limits must be used. In the case of the beta-binomial model, this is the beta distribution. As discussed in more detail in this blog post, the beta distribution has two parameterizations. It can be defined using two so-called shape parameters, or it can be defined using a mean and a standard deviation parameter. The mean, in our case, is the model-based overall mean proportion of passive verb phrases.\nWe can fit a beta-binomial model using the function betabin() in the R package {aod} (Lesnoff et al. 2012):\n\nm &lt;- betabin(\n  cbind(pass, act) ~ 1, \n  ~ 1, \n  data = d)\n\nThe parameter controlling the spread of the text-specific proportions is termed \\(\\phi\\). It can be extracted from the model object as follows:\n\nm@random.param\n\nphi.(Intercept) \n      0.0649405 \n\n\nThe \\(\\phi\\) parameter returned by aod::betabin() is the reciprocal of the standard deviation of the beta distribution, so we convert it:\n\nsd_beta &lt;- 1/m@random.param\n\nThis gives us the mean and standard deviation of the beta distribution that describes the variability among texts. To graph this distribution, we need to translate these parameters into shape parameters (see this blog post):\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\nshape_parameters &lt;- muphi_to_shapes(\n  mu = plogis(coef(m)), \n  phi = 1/m@random.param)\n\nWe can now graph the beta distribution:\n\n\ndraw figure\nxyplot(\n  1~1, type = \"n\", xlim=c(0,1), ylim = c(0,5),\n  par.settings = lattice_ls, axis = axis_bottom,\n  scales = list(x = list(\n      at = c(0, .25, .5, .75, 1),\n      labels = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.points(x = seq(0, 1, .01),\n                 y = dbeta(seq(0, 1, .01), \n                           shape1 = shape_parameters$shape1,\n                           shape2 = shape_parameters$shape2),\n                 type=\"l\")\n    panel.text(x=.4, y=5, label=\"Beta distribution with parameters:\", \n               col = \"grey40\", cex=.8, adj=0)\n    panel.text(x=.45, y = 3.5, label = \"\\u2022 Mean = 0.21; SD = 15.4\",\n               col = \"grey40\", cex=.8, adj=0)\n    panel.text(x=.45, y = 2.5, label = \"\\u2022 Shape 1 = 3.3; Shape 2 = 12.1\",\n               col = \"grey40\", cex=.8, adj=0)\n       })\n\n\n\n\n\n\n\n\nFigure 7: Beta density describing the distirbution of text-specific proportions.\n\n\n\n\n\nThe intercept of the beta-binomial model translates into a slightly higher mean probability of the passive:\n\nplogis(coef(m))\n\n(Intercept) \n  0.2137407 \n\n\nThe function avg_predictions() returns the same estimate, along with an appropriately wide confidence interval:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.214    0.197     0.231\n\n\nFinally, we check the fit between model and data visually (see Figure 8). The beta distribution appears to capture the spread of the observed text-specific proportions quite well. We should note, however, that the density curve does not capture the additional variation among the observed proportions that is due to sampling variation. The dots are therefore expected to be spread out more widely, albeit only slightly so due to the large sample sizes (see Figure 2).\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = lattice_ls, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Beta-binomial model\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = -.125,, seq_max = 1)\n         panel.points(x = seq(0, 1, length=100),\n                      y = dbeta(x = seq(0, 1, length=100),\n                                shape1 = shape_parameters$shape1, \n                                shape2 = shape_parameters$shape2)/200, type = \"l\")\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 8: Fit between the beta-binomial model and the data.\n\n\n\n\n\nSince the variation in text-specific rates is represented using a probability distribution, we can use the beta-binomial model to describe this variation in informative ways. For instance, we may be interested in the interquartile range, i.e. the spread of the central 50% of the proportions.\n\nqbeta(\n  c(.25, .75), \n  shape1 = shape_parameters$shape1, \n  shape2 = shape_parameters$shape2) |&gt; \n  round(3)\n\n[1] 0.138 0.277\n\n\nOr, seeing that the the model estimate is the mean over the text-specific proportions, we may instead be interested in the median proportion:\n\nqbeta(\n  .5, \n  shape1 = shape_parameters$shape1, \n  shape2 = shape_parameters$shape2) |&gt; \n  round(3)\n\n[1] 0.201\n\n\nWe now move on to a class of models that may be more familiar to many researchers: Mixed-effects regression models.\n\n\nRandom-effects model with identity link\nWe start with an ordinary random-effects regression model, which models the data on the proportion scale. This kind of model fails to respect the scale limits and does not account for the relationship between the binomial mean and variance (see Figure 4).\nTo run this model, we first need to convert the data from frequency to case form, so that each row in the data represents a verb phrase:\n\nd_long &lt;- binomial_to_bernoulli_data(\n  response_name = \"passive\",\n  data = d, \n  y = pass,\n  size = verbs, \n  type = \"total\"  \n)\nd_long$passive &lt;- as.numeric(d_long$passive) - 1\n\nNow we fit the model using the function lmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- lmer(\n  passive ~ (1|id), \n  data = d_long)\n\nLet us first look at model-based predictions, which look fine:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.185     0.221\n\n\nHowever, these differ from the model intercept, which is higher:\n\nfixef(m)\n\n(Intercept) \n  0.2123103 \n\n\nWe will return to this discrepancy, which is due to the fact that these are two different means, at the end of this section.\nLet us look at the fit between model and data. The ordinary random-intercept model assumes that the text-specific proportions are distributed normally around the overall mean. It describes this distribution using a standard deviation parameter, which can be obtained as follows:\n\nsummary(m)$varcor$id[1] |&gt; \n  sqrt()\n\n[1] 0.11225\n\n\nIn Figure 9, the density curve shows what the model thinks the data look like, which does not match the observed distribution. In fact, the symmetric bell-shaped curve provides a rather poor fit to the text-specific proportions: It expects negative proportions (dotted part of the curve), and it fails to capture the scale-induced asymmetry of the distribution.\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = lattice_ls, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Ordinary regression with random intercepts\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = -.125,, seq_max = 1)\n         panel.points(x = seq(-.2, 0, length=20),\n                      y = dnorm(x = seq(-.2, 0, length=20),\n                                mean = .203, sd = .1122)/190, type = \"l\", lty = \"13\")\n         \n         panel.points(x = seq(0, 1, length=100),\n                      y = dnorm(x = seq(0, 1, length=100),\n                                mean = .203, sd = .1122)/190, type = \"l\")\n\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 9: Fit between the ordinary random-effects model and the data.\n\n\n\n\n\nFigure 9 helps us understand the discrepancy between the back-transformed model intercept (.212) and the mean prediction produced by the {marginaleffects} package (.203). The model intercept represents the center of the bell-shaped curve in Figure 9. If the text proportions were indeed distributed symmetrically around this center, the intercept would coincide with the mean prediction. This is because the default way in which the {marginaleffects} package calculates mean predictions from a model with random effects proceeds in two steps: First, a prediction is made for each cluster (here: text), based on the cluster-specific random intercept. In step 2, these cluster predictions are averaged.\nIf the cluster-specific means form a symmetric distribution, they will tend to cancel out, leading to no (or very minor) discrepancies between the two types of prediction. In the present case, the text-specific predictions do not form a symmetric pile. The center of gravity is below .212, and the mean over the estimated asymmetric distribution of cluster proportions therefore lower.\nWe can retrieve both types of means using the {marginaleffects} package. The model intercept is equivalent to ignoring (or “turning off”) the clustering variable when making predictions. This can be done as follows:\n\navg_predictions(\n  m, \n  newdata = datagrid(\n    id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.194      0.23\n\n\nThe following code, in contrast, works the estimated distribution of cluster-level proportions into the predictions:\n\navg_predictions(\n  m, \n  re.form = ~(1|id)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.185     0.221\n\n\n\n\nRandom-effects model with logit link\nFinally, we fit a logistic random-effects model, which uses a logit link function to respect scale constraints. This means that the proportions are not modeled directly, but instead on the unbounded logistic (or log-odds) scale.\nWe can fit the model with the glmer() function in the {lme4} package:\n\nm &lt;- glmer(\n  cbind(pass, act) ~ 1 + (1|id), \n  data = d, \n  family = binomial)\n\nHere is the mean prediction we get using the {marginaleffects} package:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.195     0.229\n\n\nAgain, we note that it differs from the back-transformed model intercept, which is now lower:\n\nplogis(fixef(m))\n\n(Intercept) \n  0.1933644 \n\n\nWe are again dealing with two different types of means. Before we consider how this discrepancy arises, we take a closer look at the structure of the model.\nIn this model, the random-intercept SD represents the variation between texts on the logit scale. We can extract the SD parameter as follows:\n\nsummary(m)$varcor$id[1] |&gt; \n  sqrt()\n\n[1] 0.6599597\n\n\nThe model therefore assumes that the distribution of the random intercepts is symmetric on the logit scale. We can graph the text-specific proportions on the logit scale and overlay the expected distribution. The distribution of logits seems to be quite well approximated by the normal density curve.\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(-3.5, 1.1), ylim = c(0,.02),\n       par.settings = lattice_ls, axis = axis_bottom,\n       xlab.top = \"Logistic regression with random intercepts\\n\",\n       xlab = \"Text-specific logit of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(qlogis(d$prop_passive), scale_y = .0012, n_bins=50,\n                          set_col=\"grey60\", seq_min = -3.5, seq_max = .9)\n         panel.points(x = seq(-3.5, .8, .1),\n                      y = dnorm(seq(-3-5, .8, .1), mean = fixef(m), sd = .66)/50,\n                      type = \"l\")\n         panel.text(x = -2, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 1)\n         panel.text(x = -.75, y = .011, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 10: Fit between text-specific logits and the random-intercept distribution assumed by the model.\n\n\n\n\n\nLet us also look at the fit between model and data on the proportion scale. Figure 11 shows that the match is also pretty good on this scale.\n\n\ndraw figure\ninterpolated_to_100_steps &lt;- approx(\n        x = plogis(seq(-4, 1, length = 300)),\n        y = dnorm(seq(-4, 1, length = 300), mean=-1.42830, sd=.66),\n        xout = seq(0, 1, length=100))\n\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = lattice_ls, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Logistic regression with random intercepts\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = 0, seq_max = 1)\n         panel.points(x = interpolated_to_100_steps$x,\n                      y = interpolated_to_100_steps$y/30, type = \"l\")\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 11: Fit between the logistic random-effetcs model and the data.\n\n\n\n\n\nWe can again use this probability distribution to summarize text-to-text variation. The interquartile range based on this model is\n\nplogis(\n  qnorm(\n    c(.25, .75), \n    mean = fixef(m), \n    sd = .66)) |&gt; \n  round(3)\n\n[1] 0.133 0.272\n\n\nThe median proportion coincides with the back-transformed model intercept:\n\nplogis(\n  qnorm(\n    .5, \n    mean = fixef(m), \n    sd = .66)) |&gt; \n  round(3)\n\n[1] 0.193\n\n\nLet us now consider the discrepancy between the back-transformed model intercept and the mean prediction returned by the {marginaleffects} package.\nThe model intercept represents the center of the bell-shaped curve in Figure 10. This means that by-text random intercepts are averaged on the logit scale, and this mean over text-specific logits is then back-transformed to the proportion scale. This estimate is often referred to as a conditional/cluster-specific estimate.\nIn contrast, the default average prediction returned by {marginaleffects} first back-transforms the text-specific logits to the proportion scale and then calculates a mean over text-specific proportions, i.e. the distribution in Figure 11. This estimate is often referred to as a marginal/population-averaged estimate. The difference between these means (or estimates) is whether the averaging over clusters (here: texts) was done on the logit or the proportion scale.\nWe can use the {marginaleffects} package to produce both kinds of averages. We get the mean over text-specific logits by ignoring (or “turning off”) the clustering variable when making predictions. This can be done as follows:\n\navg_predictions(\n  m, \n  newdata = datagrid(\n    id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.193    0.177      0.21\n\n\nThe following code, in contrast, works the estimated distribution of cluster-level proportions into the predictions:\n\navg_predictions(\n  m, \n  re.form = ~(1|id)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.195     0.229\n\n\n\n\nComparison\nFigure 12 brings together the estimates from the different models. The first thing we note is that, in terms of proposed statistical precision, they form two groups: The binomial model is the odd one out, with a very narrow confidence interval on the estimated proportion. All other models produce confidence intervals that are very similar in length.\nThe second thing to note is that estimates form three groups:\n\nThe lowest estimate is the conditional mean based on the logistic random-effects model. It is the mean over cluster-specific logits, back-transformed to the proportion scale.\nThe marginal mean from the same model, which is the mean over cluster-specific proportions (nearly) coincides with the mean proportion returned by the beta-binomial model, and (interestingly) the conditional estimate from the ordinary random-effects model.\nThe third group, which yields an intermediate predicted proportion, is formed by the binomial, the quasi-binomial, and the marginal estimate from the ordinary random-effects model.\n\n\n\ndraw figure\ncomp_models &lt;- tibble(\n  model = c(\"Binomial\", \"Quasi-binomial\", \"Beta-binomial\", \n                \"Ordinary random-effects (conditional)\",\n                \"Ordinary random-effects (marginal)\",\n                \"Logistic random-effects (conditional)\",\n                \"Logistic random-effects (marginal)\"),\n  estimate = c(pred_binomial$estimate,\n               pred_quasibin$estimate,\n               pred_betabin$estimate,\n               pred_ord_ranef_c$estimate,\n               pred_ord_ranef_m$estimate,\n               pred_log_ranef_c$estimate,\n               pred_log_ranef_m$estimate),\n  ci_lower = c(pred_binomial$conf.low,\n               pred_quasibin$conf.low,\n               pred_betabin$conf.low,\n               pred_ord_ranef_c$conf.low,\n               pred_ord_ranef_m$conf.low,\n               pred_log_ranef_c$conf.low,\n               pred_log_ranef_m$conf.low),\n  ci_upper = c(pred_binomial$conf.high,\n               pred_quasibin$conf.high,\n               pred_betabin$conf.high,\n               pred_ord_ranef_c$conf.high,\n               pred_ord_ranef_m$conf.high,\n               pred_log_ranef_c$conf.high,\n               pred_log_ranef_m$conf.high)\n)\n\ncomp_models$model &lt;- factor(\n  comp_models$model,\n  levels = c(\"Binomial\", \"Quasi-binomial\", \"Beta-binomial\", \n                \"Ordinary random-effects (conditional)\",\n                \"Ordinary random-effects (marginal)\",\n                \"Logistic random-effects (conditional)\",\n                \"Logistic random-effects (marginal)\"),\n  ordered = TRUE\n)\n\ncomp_models |&gt; \n  ggplot(aes(y = model, x = estimate)) +\n  geom_vline(xintercept = c(.1935, .203, .213), color = \"grey95\", lwd=3) +\n  geom_point() +\n  geom_linerange(aes(xmin = ci_lower, xmax = ci_upper)) +\n  scale_x_continuous(breaks = seq(.18, .23, .01), \n                     labels = c(\".18\", \".19\", \".20\", \".21\", \".22\", \".23\")) +\n  theme_classic_ls() +\n  xlab(\"Estimated proportion of passives\") +\n  ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 12: Comparison of model-based mean predictions.\n\n\n\n\n\n\n\nSummary\nWe discussed different approaches to modeling clustered binomial data. These differ in the way they address the resulting non-independence of observations in the data. If a clustering variable is present, it is generally preferable to use a model that links the observed non-independence to this source. These (proper) modeling approaches represent the observed variation across clusters in different ways, and they yield different types of estimates for the mean proportion in the population of interest. We saw how the {marginaleffects} package can be used to construct these mean predictions, which mainly differ in terms of the scale on which they average over cluster-specific quantities.\n\n\n\n\n\nReferences\n\nAgresti, Alan. 2013. Categorical Data Analysis. Hoboken, NJ: Wiley.\n\n\nAnderson, Dorothy A. 1988. “Some Models for Overdispersed Data.” Australian Journal of Statistics 30 (2): 125–48. https://doi.org/10.1111/j.1467-842x.1988.tb00844.x.\n\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nEvert, Stephanie. 2023. Corpora: Statistics and Data Sets for Corpus Frequency Data. https://doi.org/10.32614/CRAN.package.corpora.\n\n\nFinney, David J. 1971. Probit Analysis. New York: Cambridge University Press.\n\n\nLesnoff, M., Lancelot, and R. 2012. Aod: Analysis of Overdispersed Data. https://cran.r-project.org/package=aod.\n\n\nScherer, Ralph. 2018. PropCIs: Various Confidence Interval Methods for Proportions. https://doi.org/10.32614/CRAN.package.PropCIs.\n\n\nSönning, Lukas, and Julia Schlüter. 2022. “Comparing Standard Reference Corpora and Google Books Ngrams: Strengths, Limitations and Synergies in the Contrastive Study of Variable h- in British and American English.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 17–45. Cambridge University Press. https://doi.org/10.1017/9781108589314.002.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Binomial Data},\n  date = {2025-05-09},\n  url = {https://lsoenning.github.io/posts/2025-05-05_binomial_overdispersion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Binomial Data.”\nMay 9, 2025. https://lsoenning.github.io/posts/2025-05-05_binomial_overdispersion/."
  },
  {
    "objectID": "posts/2025-05-12_poisson_random_intercepts/index.html",
    "href": "posts/2025-05-12_poisson_random_intercepts/index.html",
    "title": "Frequency estimates based on random-intercept Poisson models",
    "section": "",
    "text": "When modeling frequency counts, the Poisson model is often inappropriate since the observed variation from text to text (or speaker to speaker) is greater than anticipated by this simple model. The observed overdispersion can be addressed using Poisson mixture models, which include an additional parameter that captures the variation among texts. A frequently used variant is the negative binomial model (also called a Poisson-gamma mixture model), which represents text-to-text variation using a gamma distribution. This blog post discusses another option, a random-effects Poisson regression model (also called a Poisson-lognormal mixture model). We look at the structure of this model and how it represents text-to-text variation, and draw comparisons with the negative binomial model. We will see that a Poisson random-intercept model yields two different types of average frequencies.\n\n\nR setup\nlibrary(tidyverse)          # for data wrangling and visualization\nlibrary(dataverse)          # for downloading data from TROLLing\nlibrary(marginaleffects)    # to compute model-based estimates\nlibrary(MASS)               # to fit a negative binomial regression model\nlibrary(kableExtra)         # for drawing html tables\nlibrary(lme4)               # to fit mixed-effects regression models\nlibrary(lattice)            # for data visualization\nlibrary(gamlss)             # for drawing gamma densities\n\n# pak::pak(\"lsoenning/uls\") # install package \"uls\"\nlibrary(uls)                # for ggplot2 dotplot theme\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative data records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). For more information on the dataset, please refer to Sönning and Krug (2021).\nWe start by downloading the data from TROLLing and rename a few variables for clarity.\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nd &lt;- dat |&gt; \n  dplyr::select(-c(Exact_age, age_bins)) |&gt; \n  rename(\n    n_actually = count,\n    n_words = total,\n    age_group = Age_range,\n    gender = Gender\n  )\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age and gender is missing.\n\nd &lt;- d |&gt; \n  filter(\n    n_words &gt; 100,\n    !(is.na(gender)))\n\nIn this blog post, we will concentrate on speakers between 19 and 29 years of age.\n\nd &lt;- d |&gt; \n  filter(age_group == \"19-29\") |&gt; \n  droplevels()\n\nWe add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per thousand words’:\n\nd$rate_ptw &lt;- (d$n_actually / d$n_words) * 1000\n\nThe data subset used in the present blog post includes 247 speakers and the following variables:\n\nan ID (speaker)\nthe number of times they used actually (n_actually)\nthe age group (age_group)\nself-reported gender (gender)\nthe total number of words contributed to the corpus by the speaker (n_words), and\nthe usage rate of actually, expressed as ‘per thousand words’ (rate_ptw)\n\n\nstr(d)\n\n'data.frame':   247 obs. of  6 variables:\n $ speaker   : chr  \"S0002\" \"S0003\" \"S0007\" \"S0009\" ...\n $ n_actually: int  21 8 6 0 0 1 21 39 1 0 ...\n $ age_group : chr  \"19-29\" \"19-29\" \"19-29\" \"19-29\" ...\n $ gender    : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n $ n_words   : int  8535 1893 11276 533 473 605 8209 13449 417 304 ...\n $ rate_ptw  : num  2.46 4.226 0.532 0 0 ...\n\n\n \n\n\nFocus of analysis\nThe key interest in the following is in the usage rate of actually (expressed as a normalized frequency) among young British adults in conversational speech. In the following, the terms normalized frequency, occurrence rate, and usage rate will be used interchangeably.\n\n\nData description\nWe start by inspecting some key characteristics of the data. First we examine the distribution of speakers by Gender. The ratio of female and male speakers is 3 to 2:\n\ntable(d$gender)\n\n\nFemale   Male \n   148     99 \n\n\nNext, we consider the distribution of word counts across speakers (i.e. the total number of word tokens each person contributed to the corpus). Figure 1 shows a very skewed profile, with a few speakers showing disproportionately high word counts.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = n_words)) + \n  geom_dotplot(binwidth = 3000, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 200000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers from the Spoken BNC2014 aged 19 to 29, excluding individuals who contribute fewer than 100 words to the corpus.\n\n\n\n\n\nTo see how the outcome variable is distributed at the speaker level, we draw a dot diagram of the speaker-specific usage rate of actually, expressed as “per thousand words”. Figure 2 shows a skewed arrangement, with a few individuals using the word at an exceptionally high rate.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .1, stackratio = 1, method = \"histodot\", dotsize = .8) +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0.002, 0.002)) +\n  scale_x_continuous(expand = c(0.002, 0.002)) +\n  xlab(\"Speaker-specific usage rate of actually (per thousand words)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words.\n\n\n\n\n\n\n\nNegative binomial regression\nFor a point of reference, we start by fitting a negative binomial regression model. This model takes into account the speakers, and represents the observed variability in the usage rate of actually using a probability distribution. The model therefore includes an additional parameter that represents the variability of usage rates. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates.\nThis is illustrated in Figure 3, which shows considerable variation among speakers. The vertical line marks a ratio of 1, which represents speakers whose usage rate coincides with the model-based average usage rate of actually (which , as we will see shortly, is 1.6 per thousand words). The density curve shows the distribution of speakers across multiplicative factors ranging from 0 to 3. A ratio of 0.5 represents a speaker whose usage rate of actually is only half as large as the overall average, and a ratio of 2 refers to speakers whose usage rate is twice as large as the overall average.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(0, 3.1), ylim=c(0,1),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,1,2,3,4))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1, col=1)\n    panel.points(x = seq(.01, 4.2, length=1000),\n                 y = dGA(seq(.01, 4.2, length=1000), mu=1, sigma=(1/sqrt(3.4363))),\n                 type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing between-speaker variability (in multiplicative terms) in the usage rate of actually.\n\n\n\n\n\nSince this is a probability distribution, we can summarize the estimated distribution of speakers. The following code finds the quartiles of the distribution:\n\nqGA(\n  p = c(.25, .5, .75), \n  mu = 1, \n  sigma = 1/sqrt(3.4363)) |&gt; \n  round(2)\n\n[1] 0.60 0.90 1.29\n\n\nThis tells us that ratios of 0.60 and 1.29 mark the interquartile range: The central 50% of the speakers are within this interval. Interestingly, and perhaps counterintuitively, the median of this gamma distribution is 0.90 (rather than 1), meaning that half of the speakers have a ratio below this mark. Let us also see how many speakers have ratio above and below 1:\n\npGA(\n  q = 1, \n  mu = 1, \n  sigma = 1/sqrt(3.4363)) |&gt; \n  round(2)\n\n[1] 0.57\n\n\n57% of the speakers have a ratio below 1, meaning that more than half of the speakers actually show a usage rate below the estimated mean. We will return to this rather puzzling feature of the negative binomial model further below.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n  n_actually ~ 1 + offset(log(n_words)),\n  data = d)\n\nThis produces the following regression table:\n\nsummary(m_nb)\n\n\nCall:\nMASS::glm.nb(formula = n_actually ~ 1 + offset(log(n_words)), \n    data = d, init.theta = 3.436299959, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.43610    0.04266  -150.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.4363) family taken to be 1)\n\n    Null deviance: 275.14  on 246  degrees of freedom\nResidual deviance: 275.14  on 246  degrees of freedom\nAIC: 1516.1\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.436 \n          Std. Err.:  0.462 \n\n 2 x log-likelihood:  -1512.118 \n\n\nThe intercept of negative binomial model represents its estimate of the average frequency of actually:\n\ncoef(m_nb)\n\n(Intercept) \n  -6.436104 \n\n\nWe can back-transform it to the scale of normalized frequencies (per thousand words):\n\nround(\n  exp(coef(m_nb)) *1000, 2)\n\n(Intercept) \n        1.6 \n\n\nWe can also retrieve frequency estimates using the function avg_predictions() in the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024), which also returns a 95% confidence interval:\n\navg_predictions(\n  m_nb, \n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.6\n1.47\n1.74\n\n\n\n\n\n \n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model is similar to the negative binomial since it also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms, using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution.\n\nModel fitting\nWe will illustrate this once we have fit our model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm_pr &lt;- glmer(\n    n_actually ~ 1 + offset(log(n_words)) + (1|speaker), \n    data = d,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nHere is a condensed regression table:\n\narm::display(m_pr)\n\nglmer(formula = n_actually ~ 1 + offset(log(n_words)) + (1 | \n    speaker), data = d, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\ncoef.est  coef.se \n   -6.58     0.04 \n\nError terms:\n Groups   Name        Std.Dev.\n speaker  (Intercept) 0.55    \n Residual             1.00    \n---\nnumber of obs: 247, groups: speaker, 247\nAIC = 1519.2, DIC = -1277.5\ndeviance = 118.9 \n\n\n \n\n\nRepresentation of between-speaker variation\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing between-speaker variation, is 0.55. Figure 4 shows the inferred distribution of speaker intercepts on the log scale. At the top of the figure, a second x-axis is drawn, which shows the normalized frequencies corresponding to these natural logs. The equidistant logs translate into a warped, non-linear spacing of the occurrence rates per thousand words.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(-8.2, -4.9), ylim=c(0,1.5),\n  par.settings=lattice_ls, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(\n    at=-9:-3,\n    label = c(\"\\u22129\", \"\\u22128\", \"\\u22127\",\"\\u22126\", \"\\u22125\", \"\\u22124\", \"\\u22123\"))),\n  ylab=NULL, xlab=\"Model scale: Natural logarithm\",\n  panel=function(x,y,...){\n    panel.segments(x0=0, x1=0, y0=0, y1=.9, col=1)\n    panel.points(x = seq(-8.2, -4.9, length=1000),\n                 y = dnorm(seq(-8.2, -4.9, length=1000), \n                           mean = fixef(m_pr), sd = .55),\n                 type=\"l\")\n    panel.segments(x0=-8.2, x1=-4.9, y0=1, y1=1)\n    panel.segments(x0 = log(seq(.001, .007, .001)),\n                   x1 = log(seq(.001, .007, .001)),\n                   y0 = 1, y1 = 1.05)\n    panel.segments(x0 = log(seq(.0005, .0075, .001)),\n                   x1 = log(seq(.0005, .0075, .001)),\n                   y0 = 1, y1 = 1.03)\n    panel.text(x = log(seq(.001, .007, .001)), \n               y = 1.17, \n               label = seq(.001, .007, .001)*1000)\n    panel.text(x = fixef(m_pr), y = 1.45, label = \"Occurrences per thousand words\")\n    })\n\n\n\n\n\n\n\n\nFigure 4: The normal distribution describing between-speaker variability in the usage rate of actually on the scale of natural logarithms.\n\n\n\n\n\nFigure 5 shows what this distribution looks like on the scale of normalized frequencies. Now the occurrence rates per thousand words are equidistant and the natural logs assume a non-linear spacing. The distribution we are looking at is a log-normal distribution, which consists of positive values only, and which is skewed toward large values.\n\n\ndraw figure\nx_seq &lt;- seq(0, .006, length = 100)\n\nxyplot(\n  1~1, type=\"n\", xlim=c(0, exp(-4.99)), ylim=c(0,1.5),\n  par.settings=lattice_ls, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=(0:7)/1000, label=0:7)),\n  ylab=\"Density           \", xlab=\"Occurrences per thousand words\",\n  panel=function(x,y,...){\n    panel.segments(x0 = 0, x1=0, y0=0, y1=.8)\n    panel.points(x = x_seq,\n                 y = dlnorm(x_seq, \n                            meanlog = fixef(m_pr), \n                            sdlog = 0.55)/800,\n                 type=\"l\")\n    \n    panel.segments(x0=exp(-8), x1=exp(-5), y0=1, y1=1)\n    panel.segments(x0 = exp(-8:-5),\n                   x1 = exp(-8:-5),\n                   y0 = 1, y1 = 1.07)\n    panel.segments(x0 = exp(-7.5:-5.5),\n                   x1 = exp(-7.5:-5.5),\n                   y0 = 1, y1 = 1.045)\n    \n    panel.segments(x0 = exp(seq(-8, 5, .1)),\n                   x1 = exp(seq(-8, 5, .1)),\n                   y0 = 1, y1 = 1.03)\n    panel.text(x = exp(-8:-5), \n               y = 1.2, \n               label = c(\"\\u22128\", \"\\u22127\",\"\\u22126\", \"\\u22125\"))\n    panel.text(x=.0035, y=1.5, label=\"Natural logarithm\")\n    })\n\n\n\n\n\n\n\n\nFigure 5: The log-normal distribution describing between-speaker variability in the usage rate of actually on the normlized frequency scale.\n\n\n\n\n\nTo clarify the relation between normalized frequencies (the data scale) and natural logarithms (the model scale), let us consider a group of 250 hypothetical speakers. These speakers, and their individual average usage rates of actually, are generated in perfect accordance with the Poisson random-intercept model. Since the model operates on the log scale, each speaker is characterized by their log usage rate of actually.\nFigure 6 shows that, according to the model, the speaker-specific log usage rates form a symmetric, bell-shaped pile, which resembles a normal distribution. The center of this pile of dots is the mean log usage rate in the sample of speakers. It is the mean over the 250 log rates.\n\n\ndraw figure\nsample_speakers &lt;- dnorm_to_dots(\n  n_dots = 250, \n  mean = fixef(m_pr), \n  sd = .55)\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(-8.3, -4.8), ylim=c(0,2.2),\n  par.settings=lattice_ls, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=-9:-3)),\n  ylab=NULL, xlab=\"Model scale: Log usage rate (base e)\",\n  panel=function(x,y,...){\n    panel.segments(x0 = fixef(m_pr), x1 = fixef(m_pr), y0 = 0, y1 = 2.1, col = \"grey\")\n    panel.dotdiagram(sample_speakers, scale_y = .07, n_bins = 34)\n    panel.text(x = fixef(m_pr)-.2, y = 1.9, label=\"Mean over\\nspeaker-specific\\nlog usage rates\", lineheight = .8, adj=1)\n    panel.text(x = fixef(m_pr)+.2, y = 1.9, label=\"Fixed intercept of\\nrandom-intercept\\nPoisson model\", lineheight = .8, adj=0)\n    panel.text(x = fixef(m_pr), y = 1.9, label = \"=\")\n    panel.text(x = fixef(m_pr)+.4, y = 1.2, label=\"\\u22126.58\", adj=0)\n    panel.text(x = fixef(m_pr)+1.2, y = 1.2, label=\"Back-transformation:\", adj=0, col = \"grey40\")\n    panel.text(x = fixef(m_pr)+1.4, y = .95, label=\"exp(\\u22126.58)\", adj=0, col = \"grey40\")\n    panel.text(x = fixef(m_pr)+1.4, y = .7, label=\"= 1.4 per thousand words\", adj=0, col = \"grey40\")\n    })\n\nprint(p1, position = c(0,0,.69,1))\n\n\n\n\n\n\n\n\nFigure 6: Dot diagram showing an idealized distribution of 250 speakers on the model scale (log normalized frequencies), based on the parameters of the Poisson random-intercept model.\n\n\n\n\n\nIn the Poisson random-intercept model, this mean log usage rate is represented by the fixed intercept of the model. We saw this fixed intercept in the regression table above. We can retrieve it from the model using the function fixef():\n\nfixef(m_pr)\n\n(Intercept) \n   -6.57786 \n\n\nTo make sense of this value, we back-transform this log normalized frequency to the scale of occurrence rates via exponentiation. To get occurrences ‘per thousand words’, we multiply this rate by 1,000, and round the result to one decimal place:\n\nround(\n  exp(fixef(m_pr)) * 1000,\n  1)\n\n(Intercept) \n        1.4 \n\n\nThe mean log normalized frequency, then, which is represented by the fixed intercept of the model, corresponds to a rate of 1.4 per thousand words.\n\n\nTwo types of average frequencies\nThe normalized frequency of 1.4 per thousand words is one kind of average usage rate we can report based on a random-intercept Poisson model. To recognize that there is a second type of average normalized frequency, let us consider the distribution of the 250 speaker-specific usage rates on the data scale of normalized frequencies. Figure 7 shows that the pile is no longer symmetric – it is skewed toward the right, with a longer upper tail. Note how this dot diagram matches the density curve shown in Figure 5 above.\n\n\ndraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, .0071), ylim=c(0,2.5),\n  par.settings=lattice_ls, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=(0:7)/1000, label=0:7)),\n  ylab=NULL, xlab=\"Data scale: Usage rate (per thousand words)\",\n  panel=function(x,y,...){\n    panel.segments(x0 = exp(fixef(m_pr) + (.55^2)/2), x1 = exp(fixef(m_pr) + (.55^2)/2), y0 = 0, y1 = 2.5)\n    panel.segments(x0 = exp(fixef(m_pr)), x1 = exp(fixef(m_pr)), y0 = 0, y1 = 2.5, col = \"grey\")\n    #panel.segments(x0 = 0, x1 = 0, y0 = 0, y1 = .9, col = 1)\n    panel.dotdiagram(exp(sample_speakers), scale_y = .06, n_bins = 45)\n    panel.text(x = exp(fixef(m_pr) + (.55^2)/2)+.0003, y = 2.3, label=\"Mean over\\nspeaker-specific\\nusage rates\", lineheight = .8, adj=0)\n    panel.text(x = exp(fixef(m_pr))-.0003, y = 2.3, label=\"Back-transformed\\nintercept of\\nPoisson model\", lineheight = .8, adj=1, col=\"grey40\")\n    panel.text(x = exp(fixef(m_pr) + (.55^2)/2)+.0003, y = 1.8, label=\"1.6 per thousand words\", lineheight = .8, adj=0)\n    panel.text(x = exp(fixef(m_pr))-.0003, y = 1.8, label=\"1.4 per thousand words\", adj=1, col=\"grey40\")\n    #panel.text(x = fixef(m_pr), y = 1.7, label = \"=\")\n    panel.points(x = c(exp(fixef(m_pr)), exp(fixef(m_pr) + (.55^2)/2)), y = 2.5, pch = 19, col = c(\"grey40\", \"black\"))\n    })\n\nprint(p1, position = c(.2,0,1,1))\n\n\n\n\n\n\n\n\nFigure 7: Dot diagram showing an idealized distribution of 250 speakers on the data scale (normalized frequencies), based on the parameters of the Poisson random-intercept model.\n\n\n\n\n\nCharacterizing the distribution of usage rates on this scale is more challenging due to the asymmetry. If we want to summarize the pile of dots by referring to the “typical”, or “average” occurrence rate in the group of speakers, we could use the mean or median usage rate. The mean is sensitive to outliers – the few speakers with unusually high usage rate will therefore pull it upwards slightly. The median, on the other hand, is the usage rate in the middle of the distribution and not affected by outliers – half the speakers are above, and half below the median rate.\nIf we calculate the mean usage rate based on the pile of dots in Figure 7, we obtain a normalized frequency of 1.6 per thousand words. This “average” is marked in Figure 7 using a black needle. For comparison, the back-transformed mean log usage rate of 1.4 per thousand words, which we calculated above, appears as a grey needle.\nIt is important to note that these two types of averages differ. Let us therefore repeat what they represent:\n\nThe grey average (1.4 ptw) is the mean over the speaker-specific log usage rates (see Figure 6), back-transformed into a normalized frequency.\nThe black average (1.6 ptw) is the mean over the speaker-specific usage rates (see Figure 7), i.e. over the back-transformed log usage rates.\n\nThe two averages represent two different measures of central tendency:\n\n1.6 ptw, in black, is the mean usage rate over the 250 speakers.\n1.4 ptw, in grey, is the median usage rate over the 250 speakers.\n\nIn other words, upon back-transforming the fixed intercept in a random-intercept Poisson regression model, we obtain the median normalized frequency.\nIf we look at Figure 7, we note that the median (grey) arguably does a better job at locating the typical occurrence rate in the group of 250 speakers – the mean (black) seems a bit too high, as most of the dots are below 1.6 ptw. This is consistent with the advice found in statistical textbooks: the median is often a better summary measure for skewed distributions.\n\n\nObtaining model-based estimates of the different average frequencies\nBoth kinds of average frequencies can be constructed based on a Poisson model with random-intercepts. We will look at two approaches: (i) an analytic approach based on the model parameters, and (ii) a predictive approach using the {marginaleffects} package.\nAs we saw above, the median normalized frequency is represented by the model intercept, and we can retrieve it as follows:\n\nround(\n  exp(fixef(m_pr)) * 1000,\n  2)\n\n(Intercept) \n       1.39 \n\n\nThe mean normalized frequency can be calculated based on the model intercept and the variance of the normal distribution describing between-speaker variation. This is the formula:\n\\[\n\\textrm{mean normalized frequency} = \\textrm{exp}(\\textrm{intercept} + \\frac{\\textrm{random-intercept variance}}{2})\n\\]\nTo apply this formula, we first extract the random-intercept variance from the model object:\n\nintercept_variance &lt;- as.numeric(\n  summary(m_pr)$varcor$speaker)\n\nNow we can apply the formula above to obtain the mean normalized frequency of actually:\n\nround(\n  exp(fixef(m_pr) + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.61 \n\n\nWe can also obtain these two types of average frequency using the {marginaleffects} package. To get the median normalized frequency of actually (i.e. the back-transformed mean log rate), we run the following code. The argument re.form = NA tells the function to ignore between-speaker variation:\n\navg_predictions(\n  m_pr, \n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.39\n1.27\n1.51\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m_pr, \n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.57\n1.43\n1.7\n\n\n\n\n\nThe result is not identical to the one we got above due to shrinkage: The speaker intercepts are partially pooled, and their variability is therefore smaller than implied by the random-intercept standard deviation.\n\n\n\nComparison: Poisson random-intercept vs. negative binomial model\nWe can now compare the two types of regression models in terms of (i) how they represent between-speaker variation and (ii) the kind of frequency estimates they return.\nThe two models describe the variation among speakers using different probability distributions. These are compared in Figure 8, which shows that they provide quite similar, though not identical, representations of the distribution of speaker-specific normalized frequencies.\n\nThe negative binomial model uses the gamma distribution to express between-speaker variation; it is therefore also referred to as a Poisson-gamma mixture model.\nPoisson regression with random intercepts uses the lognormal distribution to express between-speaker variation; it is therefore also referred to as a Poisson-lognormal mixture model.\n\n\n\ndraw figure\nx_seq &lt;- seq(0, .006, length = 100)\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, .006), ylim=c(0,1.1),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=(0:6)/1000, label=c(0,1,2,3,4,5,6))),\n  ylab=\"Density\", xlab=\"Ocurrences per thousand words\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1.5, col=1)\n    panel.points(x = seq(.01, 5, length=200)*exp(coef(m_nb)),\n                 y = dGA(seq(.01, 5, length=200), mu=1, sigma=1/sqrt(3.4363)),\n                 type=\"l\", col = \"grey40\", lty = \"32\", lineend = \"butt\")\n    \n    panel.points(x = x_seq,\n                 y = dlnorm(x_seq, \n                            meanlog = fixef(m_pr), \n                            sdlog = 0.55)/600,\n                 type=\"l\")\n    panel.segments(x0=0, x1=.006, y0=0, y1=0)\n    panel.text(x = 4, y = .2, label=\"Log-normal distribution\", adj=0, cex=.9)\n    panel.text(x = 1.5, y = .45, label=\"Gamma distribution\", adj=0, col = \"grey50\", cex=.9)\n\n    panel.abline(v = 1)\n    panel.text(x = .0008, y = 1.25, label = \"Poisson-lognormal model\\n(Poisson regression with random intercepts)\", adj=0, cex=.9, lineheight = .8)\n    panel.text(x = .0025, y = .6, label = \"Poisson-gamma model\\n(Negative binomial regression)\", adj=0, cex=.9, col = \"grey40\", lineheight = .8)\n    })\n\nprint(p1, position = c(0,0,1,.85))\n\n\n\ncairo_pdf(\"actually_gamma_lognormal_comparison.pdf\", width = 4, height = 2)\nprint(p1, position = c(0,0,1,.85))\ndev.off()\n\n\npng \n  2 \n\n\n\n\n\n\n\n\nFigure 8: The log-normal distribution (black) vs. the gamma distribution (grey) describing between-speaker variability in the usage rate of actually.\n\n\n\n\n\nAs for the model predictions, the intercepts in the two models represent different average frequencies:\n\nThe intercept in a negative binomial model represents the mean normalized frequency\nThe intercept in the Poisson random-intercept model represents the median normalized frequency\n\nThe following code therefore returns different frequency estimates:\n\nround(\n  exp(fixef(m_pr)) * 1e3, 2)\n\n(Intercept) \n       1.39 \n\n\n\nround(\n  exp(coef(m_nb)) * 1e3, 2)\n\n(Intercept) \n        1.6 \n\n\n\n\nSummary\nThe Poisson regression model with random intercepts is a strategy for modeling clustered frequency data. This model captures text-to-text (or speaker-to-speaker) variation in the occurrence rate (i.e. normalized frequency) of interest using a lognormal distribution. It is therefore also referred to as a Poisson-lognormal mixture model. This model is capable of producing two types of frequency estimates: the median normalized frequency across texts, and the mean normalized frequency across texts. It is important to recognize the difference between these measures, as they represent alternative ways of expressing the typical (or “average”) occurrence rate in the population of interest. Further, it allows us to make sense of model-based predictions and how they may differ from those produced by other count regression models (such as the negative binomial model).\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Frequency Estimates Based on Random-Intercept {Poisson}\n    Models},\n  date = {2025-05-13},\n  url = {https://lsoenning.github.io/posts/2025-05-12_poisson_random_intercept/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Frequency Estimates Based on\nRandom-Intercept Poisson Models.” May 13, 2025. https://lsoenning.github.io/posts/2025-05-12_poisson_random_intercept/."
  },
  {
    "objectID": "posts/2025-05-10_counts_overdispersion_unbalanced/index.html",
    "href": "posts/2025-05-10_counts_overdispersion_unbalanced/index.html",
    "title": "Modeling clustered frequency data II: Texts of disproportionate length",
    "section": "",
    "text": "When describing or modeling corpus-based frequency data, the fact that a corpus is divided into text files has consequences for statistical modeling. For count variables (which corpus linguists often summarize using normalized frequencies), there are several options. This blog post contrasts different regression approaches to clustered count data and clarifies how they deal with unequal text lengths.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(kableExtra)      # for drawing html tables\nlibrary(lme4)            # to fit mixed-effects regression models\nlibrary(lattice)         # for data visualization\nlibrary(gamlss)          # to draw the density of the gamma distribution\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative data records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). For more information on the dataset, please refer to Sönning and Krug (2021).\nWe start by downloading the data from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age and gender is missing.\n\nd &lt;- dat |&gt; \n  filter(\n    total &gt; 100,\n    Age_range != \"Unknown\",\n    !(is.na(Gender)))\n\nIn this blog post, we will concentrate on speakers aged 70 or older.\n\nd &lt;- d |&gt; \n  filter(Age_range %in% c(\"70-79\", \"80-89\", \"90-99\")) |&gt; \n  droplevels()\n\nWe add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per thousand words’:\n\nd$rate_ptw &lt;- (d$count / d$total) * 1000\n\nWe reduce the data frame to the variables we need for analysis and rename a few columns for consistency and clarity.\n\nd &lt;- d |&gt; dplyr::select(\n  speaker, Gender, Age_range, count, total, rate_ptw) |&gt; \n  dplyr::rename(\n    age_group = Age_range,\n    gender = Gender,\n    n_tokens = count,\n    n_words = total)\n\nThe data subset used in the present blog post includes 56 speakers and the following variables:\n\nan ID (speaker)\nthe number of times they used actually (n_actually)\nthe age group (age_group)\nself-reported gender (gender)\nthe total number of words contributed to the corpus by the speaker (n_words), and\nthe usage rate of actually, expressed as ‘per thousand words’ (rate_ptw)\n\n\nstr(d)\n\n'data.frame':   56 obs. of  6 variables:\n $ speaker  : chr  \"S0005\" \"S0006\" \"S0012\" \"S0017\" ...\n $ gender   : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ age_group: chr  \"80-89\" \"80-89\" \"70-79\" \"70-79\" ...\n $ n_tokens : int  3 1 69 4 1 1 3 6 2 2 ...\n $ n_words  : int  1449 4804 277953 7377 2084 7029 2497 3420 3460 6822 ...\n $ rate_ptw : num  2.07 0.208 0.248 0.542 0.48 ...\n\n\n \n\n\nFocus of analysis\nThe key interest in the following is in the usage rate of actually (expressed as a normalized frequency) in conversational speech. Two subgroups of British speakers are compared: Male speakers aged 70 or older (“Male 70+”), and female speakers aged 70 or older (“Female 70+”). The questions guiding our analyses are:\n\nWhat is the normalized frequency of actually in the two groups?\nDoes the usage rate of actually differ between the groups?\n\n\n\nData description\nWe start by inspecting some key characteristics of the data. First we examine the distribution of speakers across the groups, which turns out to be roughly balanced:\n\ntable(d$gender)\n\n\nFemale   Male \n    25     31 \n\n\nNext, we consider the distribution of word counts across speakers (i.e. the total number of word tokens each individual contributed to the corpus). Figure 1 shows a very skewed profile, with one speaker showing a disproportionately high word count.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = n_words)) + \n  geom_dotplot(binwidth = 3000, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers from the Spoken BNC2014 aged 70 or older, excluding individuals who contributed fewer than 100 words to the corpus.\n\n\n\n\n\nTo see how the outcome variable is distributed at the speaker level, we draw a dot diagram of the speaker-specific usage rate of actually, expressed as “per thousand words”. Figure 2 shows a skewed arrangement, with a few individuals using the word at an exceptionally high rate.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .1, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0, 0)) +\n  xlab(\"Speaker-specific usage rate of * (per thousand words)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words.\n\n\n\n\n\nDue to the skew in the distribution, we use a square-root transformation for visual group comparisons. Figure 3 reassures us that this effectively removes the skew.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .05, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0, 0)) +\n  scale_x_sqrt(breaks = c(0,2,4,6,8)) +\n  xlab(\"Speaker-specific usage rate of actually\\n(per thousand words, square-root-scaled)\")\n\n\n\n\n\n\n\n\nFigure 3: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words, square-root-scaled.\n\n\n\n\n\nNow we inspect the (square-root-scaled) distribution of speaker-specific rates of actually by Gender. Figure 4 shows that the median rate is very similar in the two groups. Between-speaker variation, as indicated by the height of the boxes, is slightly larger among male speakers.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = gender, y = rate_ptw)) +\n  geom_boxplot() +\n  scale_y_sqrt(breaks = 0:10) +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency of actually\\n(per thousand words, square-root-scaled)\\n\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nFigure 4: Boxplot showing the distribution of speaker-specific usage rates of actually (per thousand words, square-root-scaled) by Gender.\n\n\n\n\n\nLet us also draw a bubble chart, which simultaneously takes into account the speaker-specific (i) word count and (ii) usage rate of actually. This means that we look at the distribution of the data points behind the boxplot.\nIn Figure 5, each individual appears as a circle and the size of this circle is proportional to the speaker word count. Individuals contributing an overabundance of words to the corpus (and our data subset) appear as big circles. We observe that the person with the highest word count (the biggest circle) is male, with a relatively low rate of actually. Among female speakers, the two individuals with the largest word counts also show the highest usage rates.\n\n\ndraw figure\nset.seed(7)\n\nd |&gt; \n  ggplot(aes(x = gender, y = rate_ptw, size = n_words)) +\n  geom_jitter(shape = 1, width = .25, alpha=.7) +\n  scale_y_sqrt(breaks = 0:10) +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency of actually\\n(per thousand words, square-root-scaled)\\n\") +\n  scale_size_area(max_size = 15) +\n  theme(legend.position = \"none\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nFigure 5: Bubble chart showing the distribution of speaker-specific usage rates of actually (per thousand words, square-root-scaled) by Gender, with the size of circles reflecting the total word count for a speaker.\n\n\n\n\n\nA key insight that will emerge from our comparison of modeling approaches will be that they respond differently to this data feature, i.e. the combination of disproportionately high word counts and relatively high or low occurrences rates, for specific texts or speakers. Before we turn to regression analysis, however, let us jot down numerical summaries for the data.\n\n\nDescriptive measures: Subcorpus frequencies and mean speaker frequency\nThere are two straightforward ways of summarizing the frequencies in the two subgroups. Egbert and Burch (2023, 105) refer to these as corpus frequency and mean text frequency. In the present setting, we will talk about subcorpus frequencies (Male 70+ subcorpus vs. Female 70+ subcorpus) and mean speaker frequencies.\nTo obtain the subcorpus frequency of actually in each group, we divide the total number of actually-tokens by the subcorpus size. We multiply this rate by 1,000 to obtain a normalized frequency of ‘per thousand words’:\n\nd |&gt; \n  group_by(gender) |&gt; \n  dplyr::summarize(\n    n_actually = sum(n_tokens),\n    corpus_size = sum(n_words),\n    subcorpus_frequency = round(n_actually/corpus_size*1000, 2)\n  ) |&gt; kable()\n\n\n\n\ngender\nn_actually\ncorpus_size\nsubcorpus_frequency\n\n\n\n\nFemale\n731\n257786\n2.84\n\n\nMale\n403\n522671\n0.77\n\n\n\n\n\nThis gives us a subcorpus frequency of 2.84 ptw for female speakers and 0.77 ptw for male speakers. We get the same estimates when using CQPweb (Hardie 2012) to run a restricted corpus query:\n \nAnother way of estimating the average rate of actually in each subgroup is to proceed in two steps: We first determine the speaker-specific normalized frequencies (i.e. the variable rate_ptw) and then we average over these within each group. This yields much more similar frequency estimates, which is consistent with what we saw in Figure 4 above.\n\nd |&gt; \n  group_by(gender) |&gt; \n  dplyr::summarize(\n    mean_speaker_frequency = round(\n      mean(rate_ptw), 2)\n  ) |&gt; kable()\n\n\n\n\ngender\nmean_speaker_frequency\n\n\n\n\nFemale\n1.21\n\n\nMale\n1.23\n\n\n\n\n\nThe difference between these two ways of measuring frequency is that while the mean speaker frequency gives the same weight to each person, the corpus frequency weights speakers in proportion to the number of words they contribute to the corpus. In the present case, there is no reason why certain individuals should inform our frequency estimate more than others, so we clearly prefer the mean speaker frequency.\nWe keep these differences in mind as we consider alternative ways of modeling the data.\n \n\n\nPoisson regression\nWe start with a Poisson regression model, which does not take into account the grouping structure of the data. This means that it turns a blind eye on the speakers in our data and considers the actually tokens (and the corpus) as an unstructured bag of words.\nWe can fit a Poisson model with the glm() function:\n\nm &lt;- glm(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d,\n  family = \"poisson\")\n\nHere is the regression table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ gender + offset(log(n_words)), family = \"poisson\", \n    data = d)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.86547    0.03699 -158.59   &lt;2e-16 ***\ngenderMale  -1.30230    0.06204  -20.99   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1615.2  on 55  degrees of freedom\nResidual deviance: 1148.4  on 54  degrees of freedom\nAIC: 1321.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe use the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024) to calculate model-based predictions for male and female speakers. These coincide with the corpus frequencies reported above:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n2.84\n2.64\n3.05\n\n\nMale\n0.77\n0.70\n0.85\n\n\n\n\n\nThe function comparisons() in the {marginaleffects} package allows us to compare the two groups in relative terms: The usage rate of male speakers is estimated to be only 27% as large as that of female speakers:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n0.27\n0.24\n0.3\n\n\n\n\n\n \n\n\nQuasi-Poisson regression\nA Quasi-Poisson model includes a dispersion parameter, which adjust inferences to account for the lack of fit of the simple Poisson model. The dispersion parameter \\(\\phi\\) is estimated on the basis of a global \\(\\chi^2\\) statistic of model (mis)fit, and it is then used to adjust the standard errors returned by the model, which are multiplied by \\(\\sqrt{\\phi}\\). For some more background on this way of accounting for overdispersion, see this blog post.\nWe can run a Quasi-Poisson model as follows:\n\nm &lt;- glm(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d,\n  family = \"quasipoisson\")\n\nThe model is summarized in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ gender + offset(log(n_words)), family = \"quasipoisson\", \n    data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.8655     0.1861 -31.521  &lt; 2e-16 ***\ngenderMale   -1.3023     0.3121  -4.172  0.00011 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 25.31282)\n\n    Null deviance: 1615.2  on 55  degrees of freedom\nResidual deviance: 1148.4  on 54  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe regression table tells us that the dispersion parameter is estimated to be roughly 25, which means that the standard errors for the Quasi-Poisson model should be 5 times (\\(\\sqrt{25}\\)) larger than in the Poisson model.\nImportantly, however, the regression coefficients themselves do not change, and neither do the model-based predictions. We get the same point estimates, though with (appropriately) wider confidence intervals:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n2.84\n1.97\n4.08\n\n\nMale\n0.77\n0.47\n1.26\n\n\n\n\n\nThe Quasi-Poisson model also returns the same relative difference between the groups:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n0.27\n0.11\n0.44\n\n\n\n\n\n \n\n\nNegative binomial regression\nNegative binomial regression explicitly takes into account the speakers, and models the observed variability in the usage rate of actually using a probability distribution. The model therefore includes an additional parameter that represents the variability of usage rates. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates. For some more background, see this blog post.\nThis is illustrated in Figure 6, which shows high variability among speakers.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(0, 4.2), ylim=c(0,1.5),\n  par.settings=lattice_ls, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,1,2,3,4))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1.5, col=1)\n    panel.points(x = seq(.01, 4.2, length=1000),\n                 y = dGA(seq(.01, 4.2, length=1000), mu=1, sigma=(1/sqrt(0.9347))),\n                 type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 6: The gamma distribution describing between-speaker variability in the usage rate of actually.\n\n\n\n\n\nSince this is a probability distribution, we can summarize the estimated distribution of speakers around their subgroup means. The following code finds the quartiles of the distribution:\n\nqGA(\n  p = c(.25, .5, .75), \n  mu = 1, \n  sigma = 1/sqrt(0.9347)) |&gt; \n  round(2)\n\n[1] 0.27 0.67 1.39\n\n\nThis tells us that ratios of 0.27 and 1.39 mark the interquartile range: The central 50% of the speakers are within this interval. Interestingly, and perhaps counterintuitively, the median of this gamma distribution is 0.67, meaning that half of the speakers have a ratio below this mark. Let us also see how many speakers have ratio above and below 1:\n\npGA(\n  q = 1, \n  mu = 1, \n  sigma = 1/sqrt(0.9347)) |&gt; \n  round(2)\n\n[1] 0.64\n\n\n64% of the speakers have a ratio below 1, meaning that around two-thirds of the speakers actually show a usage rate below the estimated subgroup mean. We will return to this rather puzzling feature of the negative binomial model further below.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm &lt;- MASS::glm.nb(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d)\n\nThis produces the following regression table:\n\nsummary(m)\n\n\nCall:\nMASS::glm.nb(formula = n_tokens ~ gender + offset(log(n_words)), \n    data = d, init.theta = 0.9346725065, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.66312    0.22513 -29.596   &lt;2e-16 ***\ngenderMale   0.02303    0.30582   0.075     0.94    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9347) family taken to be 1)\n\n    Null deviance: 60.329  on 55  degrees of freedom\nResidual deviance: 60.323  on 54  degrees of freedom\nAIC: 339.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.935 \n          Std. Err.:  0.210 \n\n 2 x log-likelihood:  -333.121 \n\n\nFrequency estimates based on this model are much closer to the mean speaker frequencies we reported above:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n1.28\n0.82\n1.99\n\n\nMale\n1.31\n0.87\n1.96\n\n\n\n\n\nAccordingly, the estimated relative difference between the groups is negligible:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n1.02\n0.41\n1.64\n\n\n\n\n\n \n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model is similar to the negative binomial since it also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution. For a more detailed discussion of the structure of this model, see this blog post.\nWe will illustrate this once we have fit our model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- glmer(\n    n_tokens ~ gender + offset(log(n_words)) + (1|speaker), \n    data = d,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nHere is a condensed regression table:\n\narm::display(m)\n\nglmer(formula = n_tokens ~ gender + offset(log(n_words)) + (1 | \n    speaker), data = d, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\n            coef.est coef.se\n(Intercept) -7.27     0.25  \ngenderMale   0.15     0.33  \n\nError terms:\n Groups   Name        Std.Dev.\n speaker  (Intercept) 1.05    \n Residual             1.00    \n---\nnumber of obs: 56, groups: speaker, 56\nAIC = 338.4, DIC = -294.8\ndeviance = 18.8 \n\n\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing between-speaker variation, is 1.05. Figure 7 shows the inferred distribution of speaker intercepts on the log scale.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=fixef(m)[1] + c(-3.4, 3.5), ylim=c(0,.45),\n  par.settings=lattice_ls, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=-10:-4)),\n  ylab=\"Density\", xlab=\"Natural logarithm\",\n  panel=function(x,y,...){\n    panel.segments(x0=0, x1=0, y0=0, y1=.45, col=1)\n    panel.points(x = fixef(m)[1] +  seq(-3.5, 3.5, length=1000),\n                 y = dnorm(fixef(m)[1] + seq(-3.5, 3.5, length=1000), mean = fixef(m)[1], sd = 1.05),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\nFigure 7: The normal distribution describing between-speaker variability in the usage rate of actually on the scale of natural logarithms.\n\n\n\n\n\nAs discussed in detail in this blog post, there are two types of predictions we can calculate for the random-intercept Poisson model. Seeing that we are interested in the occurrence rate of actually rather than its natural logarithm, we will want to back-transform model-based predictions to the scale of normalized frequencies. Since there is between-speaker variation, our model-based estimate will have to somehow average over speakers. The question is whether we want to average over speakers on the scale of natural logarithms (the model scale) or on the scale of normalized frequencies (the data scale).\n\nBy averaging on the data scale of normalized frequencies, we obtain the mean usage rate across speakers.\nBy averaging on the model scale of log normalized frequencies, and then back-transforming this mean log rate, we obtain the median usage rate across speakers.\n\nThis blog post provides a detailed illustration of these two types of frequency estimates.\nThrough appropriate combination of the regression coefficients for the fixed effects, we obtain averages over speakers on the model scale. This is the estimated mean log rate of actually in the population of interest. We can back-transform this into a normalized frequency. This summary measure, however, does not represent the mean over normalized frequencies, since the averaging was done on another scale (the model scale).\nIn our model, the intercept represents the mean log rate for female speakers. If we add the coefficient for the predictor Gender, we get the mean log rate for male speakers. Back-transforming these values gives us:\n\n# female\nround(exp(fixef(m)[1]) * 1e3, 2)\n\n(Intercept) \n        0.7 \n\n# male\nround(exp(fixef(m)[1] + fixef(m)[2]) * 1e3, 2)\n\n(Intercept) \n       0.81 \n\n\nThese frequency estimates are lower than the ones we have obtained above. This is because they represent the median usage rate of actually, and in a distribution that skewed toward large values, the median is always smaller than the mean.\nAs illustrated in this blog post, we can also use the model to calculate mean normalized frequencies, using the model intercept and the random-effects variance:\n\\[\n\\textrm{mean normalized frequency} = \\textrm{exp}(\\textrm{intercept} + \\frac{\\textrm{random-intercept variance}}{2})\n\\] We first extract the random-intercept variance from the model object:\n\nintercept_variance &lt;- as.numeric(\n  summary(m)$varcor$speaker)\n\nAnd then calculate the mean normalized frequency of actually in the two groups:\n\n# female\nround(\n  exp(fixef(m)[1] + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.21 \n\n# female\nround(\n  exp(fixef(m)[1] + fixef(m)[2] + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.41 \n\n\nWe can also obtain these two types of estimates using the {marginaleffects} package. To get the mean log rate of actually, back-transformed to the normalized frequency scale, we run the following code. The argument re.form = NA tells the function to ignore the between-speaker variation:\n\navg_predictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n0.70\n0.36\n1.04\n\n\nMale\n0.81\n0.45\n1.17\n\n\n\n\n\nThe corresponding relative difference between the groups can be retrieved as follows:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA,\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n1.16\n0.41\n1.92\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n1.15\n0.58\n1.71\n\n\nMale\n1.33\n0.74\n1.93\n\n\n\n\n\nThe result is not identical to the one we got above due to shrinkage: The speaker intercepts are partially pooled, and their variability is therefore smaller than implied by the random-intercept standard deviation.\nThe relative difference between the groups remains the same:\n\navg_comparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Male) / mean(Female)\n1.16\n0.41\n1.92\n\n\n\n\n\n\n\nComparison\nFigure 8 compares the estimated average predictions we have collected in this blog post. For a point of reference, our descriptive summaries are shown in grey: The dotted lines are the two (sub)corpus frequencies, and the solid lines – which are almost identical in the groups – are the mean speaker frequencies.\nOur first observation is that estimates based on the Poisson and Quasi-Poisson model coincide with the plain subcorpus frequencies – as a result, they suffer from the imbalanced word counts across speakers. Just like the corpus frequency, both models give much greater weight to speakers who contributed a large number of words to the corpus. As we have noted above, this is undesirable in the present case. We therefore conclude that the Poisson and Quasi-Poisson model are inadequate for the data at hand, since they do not guard against imbalances.\nThe other models produce estimates that are close(r) to the mean speaker frequencies. The three models agree in the statement that the difference between the two groups, “Male 70+” and “Female 70+”, are minor. The estimates from the negative binomial model and the mean normalized frequency predicted by the Poisson random-intercept model are virtually indistinguishable from the mean speaker frequencies.\nFinally, the median normalized frequency predicted by the Poisson random-intercept model is considerably lower.\n\n\ndraw figure\npred_models &lt;- tibble(\n  model = rep(c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                \"Random-intercept\\nPoisson\\n(median usage rate)\"), each = 2),\n  gender = rep(c(\"Female\", \"Male\"), 5),\n  estimate = c(pred_poisson$estimate,\n               pred_quaspoi$estimate,\n               pred_negbin$estimate,\n               pred_ranef_m$estimate,\n               pred_ranef_c$estimate),\n  ci_lower = c(pred_poisson$conf.low,\n               pred_quaspoi$conf.low,\n               pred_negbin$conf.low,\n               pred_ranef_m$conf.low,\n               pred_ranef_c$conf.low),\n  ci_upper = c(pred_poisson$conf.high,\n               pred_quaspoi$conf.high,\n               pred_negbin$conf.high,\n               pred_ranef_m$conf.high,\n               pred_ranef_c$conf.high)\n)\n\npred_models$model &lt;- factor(\n  pred_models$model,\n  levels = c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n  ordered = TRUE\n)\n\nann_text &lt;- data.frame(\n  estimate = 3.3,\n  lab = \"Corpus frequency\",\n  gender = \"Female\",\n  model = factor(\"Random-intercept\\nPoisson\\n(median usage rate)\",\n                 levels = c(\"Poisson\", \"Quasi-Poisson\",\n                            \"Negative\\nbinomial\", \n                            \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                            \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n                 ordered = TRUE))\n\nann_text2 &lt;- data.frame(\n  estimate = 1.5,\n  lab = \"Mean speaker frequency\",\n  gender = \"Female\",\n  model = factor(\"Random-intercept\\nPoisson\\n(median usage rate)\",\n                 levels = c(\"Poisson\", \"Quasi-Poisson\",\n                            \"Negative\\nbinomial\", \n                            \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                            \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n                 ordered = TRUE))\n\n\npred_models |&gt; \n  ggplot(aes(x = gender, y = estimate, group = model)) +\n  geom_hline(yintercept = c(1.21, 1.23), col = \"grey\") +\n  geom_hline(yintercept = c(.77, 2.84), col = \"grey\", lty = \"22\", linetype=\"square\") +\n  geom_point() +\n  geom_line() +\n  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  facet_grid(. ~ model) +\n  theme_classic_ls() +\n  scale_y_sqrt(limits = c(0, 4.5), expand = c(0,0)) +\n  ylab(\"Usage rate of actually\\n(ptw, square-root-scaled)\\n\") +\n  xlab(NULL) +\n  geom_text(data = ann_text, label = \"                    Corpus frequency\", col=\"grey50\", size=3) +\n  geom_text(data = ann_text2, label = \"          Mean speaker frequency\", col=\"grey50\", size=3) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 8: Comparison of model-based predictions for the average usage rate of actually in the two subgroups.\n\n\n\n\n\n\n\nSummary\nThis blog post compared different approaches to modeling corpus-based frequency data. The regression models we considered address the non-independence of observations in the data in different ways and therefore return different estimates of average normalized frequencies. Differences between these estimates correspond to differences between two broad ways of measuring frequency: corpus frequency and mean (or median) text frequency. Models that account for the clustering in the data yield analogues of mean text frequencies, which are more suitable if texts differ in length, or speakers differ in the number of word tokens they contribute to a corpus. Models in this second group differ, however, in the way they average over speaker- (or text-)specific frequencies. Thus, we can summarize a distribution of frequencies on the log scale, and then transform this mean log rate into a normalized frequency. Or we can summarize the distribution on the scale of normalized frequencies. From the viewpoint of interpretation, it is essential to realize that these two estimates represent the median and the mean of the distribution of text-level normalized frequencies. We saw how the {marginaleffects} package can be used to construct both types of predictions.\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, edited by Ole Schützler and Julia Schlüter, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Frequency Data {II:} {Texts} of\n    Disproportionate Length},\n  date = {2025-05-15},\n  url = {https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion_unbalanced/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Frequency Data II: Texts\nof Disproportionate Length.” May 15, 2025. https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion_unbalanced/."
  },
  {
    "objectID": "posts/2025-05-09_counts_overdispersion/index.html",
    "href": "posts/2025-05-09_counts_overdispersion/index.html",
    "title": "Modeling clustered frequency data I: Texts of similar length",
    "section": "",
    "text": "When describing or modeling corpus-based frequency counts, the fact that a corpus is divided into texts has consequences for statistical modeling. For count variables, there are different options for modeling such data. This blog post contrasts approaches that differ in the way they represent (or account for) the non-independence of data points and looks at a setting where texts are very similar in length.\n\n\nR setup\nlibrary(tidyverse)          # for data wrangling and visualization\nlibrary(dataverse)          # for downloading data from TROLLing\nlibrary(marginaleffects)    # to compute model-based estimates\nlibrary(MASS)               # to fit a negative binomial regression model\nlibrary(corpora)            # to calculate a log-likelihood score\nlibrary(kableExtra)         # for drawing html tables\nlibrary(lme4)               # to fit mixed-effects regression models\nlibrary(lattice)            # for data visualization\nlibrary(gamlss)             # for drawing gamma densities\n\n# pak::pak(\"lsoenning/uls\") # install package \"uls\"\nlibrary(uls)                # for ggplot2 dotplot theme\n\n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\nWe will consider the imbalance across genres in the Brown Family of corpora a meaningful feature of the population of interest and therefore not adjust our estimates for the differential representation of these text categories. For an alternative approach, see this blog post.\nWe start by downloading the data from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe table we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nWe extract the data for should in Brown and Frown:\n\nshould_data &lt;- dat |&gt; \n  filter(\n    corpus %in% c(\"Brown\", \"Frown\"),\n    modal == \"should\"\n  )\n\nshould_Brown &lt;- should_data |&gt; \n  filter(\n    corpus == \"Brown\")\n\nshould_Frown &lt;- should_data |&gt; \n  filter(\n    corpus == \"Frown\")\n\n \n\n\nData description\nLet us start by summarizing key features of the data. There are 500 texts in each corpus:\n\ntable(should_data$corpus)\n\n\nBrown Frown \n  500   500 \n\n\nLet’s also take a look at the distribution of occurrence rates across texts. We first add a new variable that expresses the text-level frequency of should as a normalized frequency (per thousand words):\n\nshould_data$rate_ptw &lt;- should_data$n_tokens/should_data$n_words*1000\n\nThen we draw a histogram showing the distribution of these rates by Corpus.\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_histogram(binwidth = .15) +\n  facet_grid(corpus ~ .) +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\n\n\nFigure 1: Histogram showing the distribution of text-level occurrence rates by Corpus.\n\n\n\n\n\nSeeing that the normalized frequencies are skewed, we try a square-root-transformation, which somewhat mitigates the skew:\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_histogram(binwidth = .05) +\n  facet_grid(corpus ~ .) +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(\"Number of texts\") +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\nFigure 2: Histogram showing the distribution of text-level occurrence rates by Corpus, using a square-root-scale trnasformation to mitigate the skew.\n\n\n\n\n\nThen we compare the distributions with a boxplot:\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw, y = corpus)) +\n  geom_boxplot() +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(NULL) +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\nFigure 3: Boxplot comparing the distribution of text-level occurrence rates in Brown and Frown, using a square-root trnasformation.\n\n\n\n\n\nAs for numerical summaries, a quick measure of the frequency of should in Brown can be calculated by dividing its corpus frequency by the size of the corpus. We can do the same for Frown. We will multiply these rates by 1,000, to get normalized frequencies ‘per thousand words’.\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nAnd here they are, rounded to two decimal places:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\nFor Brown, we get a rate of 0.79 per thousand words, and for Frown the rate is 0.68 per thousand words.\nFor a quick answer to the second question, we divide the rate in Frown by that in Brown, which gives us a rate ratio of 0.86. This tells us that the frequency of should in the 1990s was only 86% as large as that in the 1960s:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\n\n\nPoisson regression\nWe start with a Poisson regression model, which does not take into account the fact that each corpus breaks down into 500 texts. Rather, the corpus is treated as an unstructured bag of words.\nWe can fit a Poisson model with the glm() function:\n\nm &lt;- glm(\n  n_tokens ~ corpus + offset(log(n_words)),\n  data = should_data,\n  family = \"poisson\")\n\nA model summary appears in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ corpus + offset(log(n_words)), family = \"poisson\", \n    data = should_data)\n\nCoefficients:\n            Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -7.14048    0.03315 -215.416  &lt; 2e-16 ***\ncorpusFrown -0.14774    0.04864   -3.037  0.00239 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2671.7  on 999  degrees of freedom\nResidual deviance: 2662.5  on 998  degrees of freedom\nAIC: 4352.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe use the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024) to calculate model-based predictions for the frequency of should in each corpus. These coincide with the plain corpus frequencies reported above. We specify the n_words = 1000 to get normalized frequencies ‘per thousand words’.\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.79\n0.74\n0.84\n\n\nFrown\n0.68\n0.64\n0.73\n\n\n\n\n\nThe function comparisons() in the {marginaleffects} package allows us to compare the two corpora in relative terms, in the form of a frequency ratio. The rate of should in Frown is only 86% of that in Brown, suggesting a decrease of 14 percentage points.\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\n\n\nQuasi-Poisson regression\nA Quasi-Poisson model introduces a dispersion parameter to adjust inferences for the non-independence of the data points. This parameter, \\(\\phi\\), is estimated on the basis of a global \\(\\chi^2\\) statistic of model (mis)fit, and it is then used to adjust the standard errors returned by the model, which are multiplied by \\(\\sqrt{\\phi}\\). For some more background on this way of accounting for overdispersion, see this blog post.\nWe can run a Quasi-Poisson model as follows:\n\nm &lt;- glm(\n  n_tokens ~ corpus + offset(log(n_words)),\n  data = should_data,\n  family = \"quasipoisson\")\n\nThe model is summarized in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ corpus + offset(log(n_words)), family = \"quasipoisson\", \n    data = should_data)\n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -7.14048    0.06479 -110.212   &lt;2e-16 ***\ncorpusFrown -0.14774    0.09507   -1.554    0.121    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 3.820299)\n\n    Null deviance: 2671.7  on 999  degrees of freedom\nResidual deviance: 2662.5  on 998  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe regression table tells us that the dispersion parameter is about 3.8, which means that the standard errors for the Quasi-Poisson model should be 1.95 times (\\(\\sqrt{3.8}\\)) larger than in the Poisson model.\nThe regression coefficients themselves do not change, and neither do the model-based predictions. We get the same point estimates, though with (appropriately) wider confidence intervals:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.79\n0.69\n0.89\n\n\nFrown\n0.68\n0.59\n0.78\n\n\n\n\n\nThe Quasi-Poisson model also returns the same relative difference between the corpora:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.7\n1.02\n\n\n\n\n\n\n\nNegative binomial regression\nNegative binomial regression explicitly takes into account the texts in the data, and represents the observed text-to-text variability in the frequency of should using a probability distribution. The model therefore has an additional parameter that represents the variability of text-level frequencies. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates. More background is provided in this blog post.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm &lt;- MASS::glm.nb(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_data)\n\nHere is the regression table for this model:\n\nsummary(m)\n\n\nCall:\nMASS::glm.nb(formula = n_tokens ~ corpus + offset(log(n_words)), \n    data = should_data, init.theta = 0.9278339214, link = log)\n\nCoefficients:\n            Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -7.13436    0.05699 -125.175   &lt;2e-16 ***\ncorpusFrown -0.15053    0.08166   -1.843   0.0653 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9278) family taken to be 1)\n\n    Null deviance: 1043.2  on 999  degrees of freedom\nResidual deviance: 1039.8  on 998  degrees of freedom\nAIC: 3569\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.9278 \n          Std. Err.:  0.0720 \n\n 2 x log-likelihood:  -3562.9510 \n\n\nModel-based predictions are again virtually identical to the ones from the Poisson and Quasi-Poisson model, with wider uncertainty intervals:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.80\n0.71\n0.89\n\n\nFrown\n0.69\n0.61\n0.76\n\n\n\n\n\nThe negative binomial model also returns the same relative difference between the corpora:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.72\n1\n\n\n\n\n\n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution. For a more detailed discussion of the structure of this model, see this blog post.\nWe can fit this model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- lme4::glmer(\n    n_tokens ~ corpus + offset(log(n_words)) + (1 | text_id), \n    data = should_data,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nWe print a condensed regression table:\n\narm::display(m)\n\nlme4::glmer(formula = n_tokens ~ corpus + offset(log(n_words)) + \n    (1 | text_id), data = should_data, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\n            coef.est coef.se\n(Intercept) -7.43     0.05  \ncorpusFrown -0.15     0.05  \n\nError terms:\n Groups   Name        Std.Dev.\n text_id  (Intercept) 0.76    \n Residual             1.00    \n---\nnumber of obs: 1000, groups: text_id, 500\nAIC = 3820.2, DIC = -1122.2\ndeviance = 1346.0 \n\n\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing text-to-text variation in the occurrence rate of should, is 0.76.\nAs discussed in detail in this blog post, there are two types of predictions we can calculate for the random-intercept Poisson model. Seeing that we are interested in the normalized frequency of should rather than its natural logarithm, we will want to back-transform model-based predictions to the scale of normalized frequencies. Since there is between-speaker variation, our model-based estimate will have to somehow average over speakers. The question is whether we want to average over speakers on the scale of natural logarithms (the model scale) or on the scale of normalized frequencies (the data scale).\n\nBy averaging on the data scale of normalized frequencies, we obtain the mean occurrence rate across texts.\nBy averaging on the model scale of log normalized frequencies, and then back-transforming this mean log rate, we obtain the median occurrence rate across texts.\n\nThis blog post provides a detailed illustration of these two types of frequency estimates.\nThrough appropriate combination of the regression coefficients for the fixed effects, we get averages over texts on the model scale. This is the estimated mean log rate of should in the population of interest. We can back-transform this into a normalized frequency. This summary measure, however, does not represent the mean over normalized frequencies, since the averaging was done on another scale (the model scale).\nIn our model, the intercept represents the mean log rate for Brown. If we add the coefficient for the predictor Corpus, we get the mean log rate for Frown. Back-transforming these values gives us:\n\n# Brown\nround(exp(fixef(m)[1]) * 1e3, 2)\n\n(Intercept) \n       0.59 \n\n# Frown\nround(exp(fixef(m)[1] + fixef(m)[2]) * 1e3, 2)\n\n(Intercept) \n       0.51 \n\n\nThese frequency estimates are lower than the ones we have obtained above. This is because they represent the median occurrence rate of should, and in a distribution that is skewed toward large values (see Figure 1), the median is always smaller than the mean.\nWe can also calculate these two types of estimates using the {marginaleffects} package. To get the mean log rate of should, back-transformed to the normalized frequency scale, we run the following code. The argument re.form = NA tells the function to ignore the between-speaker variation:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.59\n0.53\n0.65\n\n\nFrown\n0.51\n0.46\n0.56\n\n\n\n\n\nThe corresponding relative difference between the corpora can be retrieved as follows:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = NA),\n  re.form = NA,\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.77\n0.69\n0.85\n\n\nFrown\n0.66\n0.59\n0.73\n\n\n\n\n\nThe relative difference between the groups remains the same:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = unique),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\n\n\nComparison\nFigure 4 compares the estimated average predictions we have collected in this blog post. Two points are noteworthy:\n\nThe uncertainty intervals suggested by the Poisson model are narrower than the ones based on the other models. This is because the other model explicitly take into account the non-independence of observations.\nExcept for the median occurrence rate estimate based on the random-intercept Poisson model, all models return nearly identical estimates of the normalized frequency of should in Brown and Frown.\n\n\n\ndraw figure\npred_models &lt;- tibble(\n  model = rep(c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean rate)\",\n                \"Random-intercept\\nPoisson\\n(median rate)\"), each = 2),\n  corpus = rep(c(\"Brown\", \"Frown\"), 5),\n  estimate = c(pred_poisson$estimate,\n               pred_quaspoi$estimate,\n               pred_negbin$estimate,\n               pred_ranef_m$estimate,\n               pred_ranef_c$estimate),\n  ci_lower = c(pred_poisson$conf.low,\n               pred_quaspoi$conf.low,\n               pred_negbin$conf.low,\n               pred_ranef_m$conf.low,\n               pred_ranef_c$conf.low),\n  ci_upper = c(pred_poisson$conf.high,\n               pred_quaspoi$conf.high,\n               pred_negbin$conf.high,\n               pred_ranef_m$conf.high,\n               pred_ranef_c$conf.high)\n)\n\npred_models$model &lt;- factor(\n  pred_models$model,\n  levels = c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean rate)\",\n                \"Random-intercept\\nPoisson\\n(median rate)\"),\n  ordered = TRUE\n)\n\n\npred_models |&gt; \n  ggplot(aes(x = corpus, y = estimate, group = model)) +\n  geom_point() +\n  geom_line() +\n  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  facet_grid(. ~ model) +\n  theme_classic_ls() +\n  scale_y_continuous(limits = c(.4, .95), expand = c(0,0)) +\n  ylab(\"Occurrence rate of should\\n(per thousand words)\\n\") +\n  xlab(NULL) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 4: Comparison of model-based predictions for the average occurrence rate of should in the two corpora.\n\n\n\n\n\n\n\nSummary\nThis blog post contrasted different approaches to modeling clustered frequency counts. If count data are grouped by text (or speaker), a Poisson regression model is usually too restrictive. This is because it is insensitive to the very likely possibility that the normalized frequency of interest varies among texts (or speakers). We looked at different alternatives, and observed that all of these yielded very similar results. This will be the case in situations where texts (or speakers) are similar in length. We also noted that the random-intercept Poisson model is capable of producing two different average frequency estimates: The mean or the median occurrence rate (i.e. normalized frequency) of the item across texts (or speakers).\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Frequency Data {I:} {Texts} of Similar\n    Length},\n  date = {2025-05-14},\n  url = {https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Frequency Data I: Texts\nof Similar Length.” May 14, 2025. https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion/."
  },
  {
    "objectID": "posts/2025-05-19_data_vis_workflow/index.html",
    "href": "posts/2025-05-19_data_vis_workflow/index.html",
    "title": "Exporting R graphics: A basic workflow",
    "section": "",
    "text": "When it comes to exporting graphs from R, it took me some time to develop a workflow that I am happy with. In this blog post, I provide a brief run-down and illustrate the tools I use. The general steps are the following:\n\nDraw the figure in R, with the result almost looking the way I want it to\nSave as a PDF file\nIf necessary, do some polishing using Adobe Acrobat\nIf needed, create a PNG/JPG version of the PDF file\n\n\n\nR setup\n# These packages may need to be installed first:\n# pak::pak(\"lsoenning/uls\")\n# pak::pak(\"lsoenning/wls\")\n\nlibrary(wls)       # for illustrative data\nlibrary(uls)       # for customizing lattice plots\nlibrary(tidyverse) # for data wrangling and visualization\nlibrary(lattice)   # for data visualization\n\n\n\nStep 1: Draw the figure in R\nI almost exclusively use the R packages {lattice} (Sarkar 2008) and {ggplot2} (Wickham 2016) for data visualization. Both generate graphs that are very close to publication quality. Depending on how much extra formatting is needed, it probably makes sense to do parts of the fine-tuning with different software (see below). This is because certain modifications may require pretty involved code. For instance, if I want to italicize a single word in a longer axis title, I don’t do this in R.\nThe following code draws a histogram using {ggplot2}. The data show the usage rate of actually (per million words) for speakers in the Spoken BNC2014 (Love et al. 2017). (These data were analyzed in Sönning and Krug 2022; see Sönning and Krug 2021 for details).\n\ndata_actually |&gt; \n  ggplot(aes(x = rate_pmw)) +\n  geom_histogram() +\n  xlab(\"Usage rate of actually (per million words)\") +\n  ylab(\"Number of speakers\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNext, we draw a similar plot using {lattice}:\n\nhistogram(\n  ~ rate_pmw, \n  data = data_actually, \n  col = \"grey\",\n  par.settings = lattice_ls,\n  axis = axis_L,\n  nint = 35,\n  xlab = \"Usage rate of actually (per million words)\",\n  ylab = \"Number of speakers\")\n\n\n\n\n\n\n\n\n\n\nStep 2: Export as PDF\nI always save graphs as a PDF – this is for several reasons:\n\nIf the figure ends up in a publication, a vector image provides the best resolution. I always forward figures as PDF files to print production.\nPDF files require relatively little storage space.\nThe figure can be polished using graphics software, including Adobe Acrobat.\n\nThe function ggsave() makes it very convenient to write images created using {ggplot2} to file. Simply running ggsave() including the storage path saves the file using the current specifications for height and width. These settings can be modified using arguments of the same name. It is good practice to store images in the (manually created) “figures” sub-directory of the project folder.\n\ndata_actually |&gt; \n  ggplot(aes(x = rate_pmw)) +\n  geom_histogram() +\n  xlab(\"Usage rate of actually (per million words)\") +\n  ylab(\"Number of speakers\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggsave(\"figures/histogram_ggplot.pdf\")\n\nGraphs drawn with {lattice} can be saved using the function cairo_pdf(), which works in a similar way. We start by creating a graph object (p1 for plot 1) and then write this object to file. cairo_pdf() opens what is called a “device”, which we need to close afterwards using dev.off():\n\np1 &lt;- histogram(\n  ~ rate_pmw, \n  data = data_actually, \n  col = \"grey\",\n  par.settings = lattice_ls,\n  axis = axis_L,\n  nint = 35,\n  xlab = \"Usage rate of actually (per million words)\",\n  ylab = \"Number of speakers\")\n\ncairo_pdf(\"figures/histogram_lattice.pdf\", width = 4, height = 2)\np1\ndev.off()\n\npng \n  2 \n\n\nBoth PDF files can now be found in the folder “figures”.\n\n\nStep 3: Polishing in Adobe Acrobat\nIf I want to add final touches to the image, I use Adobe Acrobat. I am sure there is better software out there, but this alternative is pre-installed on my work laptop and good enough for my purposes. Regardless of the software you use, two things are important:\n\nBefore you invest time into fine-tuning, make sure that you are really looking at the final version of the graph. If you decide to alter the figure (or if the data change), you will have to redo all of the manual touches.\nStore the modified PDF under a different name – otherwise it may be overwritten when you (accidentally) rerun your code. I always add the suffix “_modified” to the file name, to protect the new version of the PDF file.\n\nIn Adobe Acrobat, click the tab “Edit” to start editing the PDF image:\n\n\n\n\n\nItalicizing the word “actually” in the x-axis title is then pretty straightforward:\n\n\n\n\n\nThere are a few things I sometimes change in PDF images:\n\nMove elements (e.g. text boxes, lines, etc.)\nAdd annotations in the form of text boxes (this only works by copying an existing text box and then modifying the text)\nChange font size and/or color\nCrop page to remove white figure margins\n\n\n\nStep 4: Create PNG/JPG version if necessary\nThere are two purposes for which I need raster-based versions of my graphs:\n\nWhen writing a paper, to include graphs into the Word document, and\nfor presentation slides (which I create using PowerPoint).\n\nThe quick way to get a raster-based version is via a screenshot:\n\nOpen the PDF\nZoom in for sufficient resolution\nTake a screenshot\nPaste the screenshot into the software IrfanView\nUse the mouse to draw a frame around the part of the screenshot I want to use\nHit Ctrl + C (to copy)\nPaste into Word or PowerPoint using Ctrl + V\n\n\n\n\n\n\nAlternatively, the PDF file can be properly converted into a PNG file using Adobe Acrobat (All tools &gt; Export a PDF &gt; Image format). The resolution (pixels/inch) can then be specified manually, which is attractive if the PNG file needs to be high(er)-resolution.\n\n\n\n\n\nReferences\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with r. New York: Springer.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Exporting {R} Graphics: {A} Basic Workflow},\n  date = {2025-05-20},\n  url = {https://lsoenning.github.io/posts/2025-05-19_data_vis_workflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Exporting R Graphics: A Basic\nWorkflow.” May 20, 2025. https://lsoenning.github.io/posts/2025-05-19_data_vis_workflow/."
  },
  {
    "objectID": "posts/2025-06-16_dendrogram_grouping_cues/index.html",
    "href": "posts/2025-06-16_dendrogram_grouping_cues/index.html",
    "title": "Color-coded dendrograms using the R function A2Rplot()",
    "section": "",
    "text": "Dendrograms are exploratory tools that help identify clusters, i.e. groups of relatively similar units, in multivariate data sets. Unfortunately, the resulting tree-like representations provide weak visual cues to the clusters in the data. The R function A2Rplot(), which was written by Romain Francois, uses color to distinguish a user-specified number of clusters in a dendrogram. Importantly, it not only colors the labels sitting at the final nodes, but also the branches that connect the members of a cluster. This provides much stronger visual cues to the groups in the data, and it allows us to recognize their degree of internal (dis)similarity more easily. In the following, I briefly describe the basic use of this function.\n\nPreparation: Clustering analysis\nFor illustration, we use the built-in dataset mtcars in R. We start by converting the data frame into a distance matrix:\n\nmtcars_dist &lt;- dist(mtcars)\n\nThen we carry out a clustering analysis:\n\nmtcars_hclust &lt;- hclust(mtcars_dist)\n\n\n\nStandard dendrogram\nThe plot() function can be used to produce a standard dendrogram:\n\nplot(mtcars_hclust, xlab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n\n\nDendrogram using A2Rplot()\nWe download the R function from Romain Francois’ (old) website:\n\nsource(\"http://addictedtor.free.fr/packages/A2R/lastVersion/R/code.R\")\n\nThe function A2Rplot() can now be used to draw a colored dendrogram. The argument k specifies the number of clusters that should be distinguished using different colors:\n\nA2Rplot(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE)\n\n\n\n\n\n\n\n\nNote how the the colored branches allow us to quickly recognize the degree of dissimilarity among the members of a cluster: The higher the horizontal line joining branches, the greater the dis(!)similarity between the conjoined units. This means that the green cluster is the most homogeneous one.\nYou can change the colors in the plot using the following arguments:\n\ncol.up The branches above the groups\ncol.down Colors for the groups\n\nThe following code backgrounds the branches above the clusters and uses a colorblind-friendly set of hues for the clusters:\n\nA2Rplot(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE,\n  col.up = \"grey\", \n  col.down = c(\"#E69F00\", \"#56B4E9\", \"#009E73\"))\n\n\n\n\n\n\n\n\nA slightly modified version of the function makes a few minor changes to the appearance of the plot (no title and annotation, thinner branches, more room for labels, avoidance of dashed line patterns). It can be downloaded from my website:\n\nsource(\"https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/A2Rplot_modified.R\")\n\nHere is the resulting alternative version of the dendrogram:\n\nA2Rplot_modified(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE,\n  col.up = \"grey\", \n  col.down = c(\"#E69F00\", \"#56B4E9\", \"#009E73\"))\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Color-Coded Dendrograms Using the {R} Function {`A2Rplot()`}},\n  date = {2025-06-16},\n  url = {https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Color-Coded Dendrograms Using the R\nFunction `A2Rplot()`.” June 16, 2025. https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/."
  },
  {
    "objectID": "posts/2025-10-23_dispersion_nelson_poissonness/index.html",
    "href": "posts/2025-10-23_dispersion_nelson_poissonness/index.html",
    "title": "Nelson’s (2025) Poisson-based dispersion measure",
    "section": "",
    "text": "R setup\nlibrary(tlda)\nlibrary(lattice)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")"
  },
  {
    "objectID": "posts/2025-10-23_dispersion_nelson_poissonness/index.html#footnotes",
    "href": "posts/2025-10-23_dispersion_nelson_poissonness/index.html#footnotes",
    "title": "Nelson’s (2025) Poisson-based dispersion measure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHowever, computational cost arises at the stage of data preparation: for each item, the corpus needs to be divided anew, into f parts.↩︎"
  },
  {
    "objectID": "posts/2025-10-25_dispersion_lyne/index.html",
    "href": "posts/2025-10-25_dispersion_lyne/index.html",
    "title": "Lyne’s (1985) graphical technique for the evaluation of dispersion measures",
    "section": "",
    "text": "R setup\nlibrary(tlda)\nlibrary(lattice)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gtools)\nlibrary(tidyverse)\nlibrary(uls)             # pak::pak(\"lsoenning/uls\")\n\n\n\nLyne (1985)’s graphical technique\nIn his dissertation, Lyne (1985) presents a corpus-based analysis of The vocabulary of French business correspondence. The book includes a chapter that examines the behavior of three dispersion measures: D, D2, and S. The question of main interest is whether Carroll’s D2 (Carroll 1970) and Rosengren’s S (Rosengren 1971) can really be considered an improvement over Juilland’s D, as claimed by these authors. To shed light on the way these measures respond to different subfrequency patterns, Lyne (1985) considers an item that occurs 10 times in a corpus that is divided into five parts. He then looks at all possible ways in which this item could be distributed over the five parts.\nIn R, we can use the function combinations() in the package {gtools} (Warnes et al. 2023) to find the number of ways in which 10 occurrences can be distributed across 5 corpus parts. It turns out that there are 30 possible ways, which are here listed from the most uneven distribution (dispersion pessimum) to the most even distribution (dispersion optimum).\n\n\nR code: Create all possible combinations of subfrequencies\ncomb &lt;- gtools::combinations(\n    n = 5, \n    r = 10, \n    v = 1:10, \n    repeats.allowed = TRUE)\n\ncomb_tbl &lt;- matrix(\n    NA, \n    nrow = nrow(comb), \n    ncol = ncol(comb))\n\nfor(i in 1:nrow(comb)){\n    comb_tbl[i,] &lt;- sort(table(factor(comb[i,], levels = 1:5)), decreasing = TRUE)\n}\n\ncombs &lt;- str_split(\n    unique(paste(\n        comb_tbl[,1],\n        comb_tbl[,2],\n        comb_tbl[,3],\n        comb_tbl[,4],\n        comb_tbl[,5],\n        sep = \"-\")),\n    \"-\", simplify = TRUE)\n\ncombs &lt;- data.frame(combs)\n\nfor(i in 1:5){\n    combs[,i] &lt;- as.numeric(combs[,i])\n}\n\ncombs$n_zeros &lt;- rowSums(combs[,1:5] == 0)\n\n# print all combinations\n\ndistr_matrix &lt;- (t(combs[,-6]))\n\nwrite.table(\n  format(\n    distr_matrix, \n    justify=\"right\"),\n  row.names=F, col.names=F, quote=F)\n\n\n10  9  8  8  7  7  7  6  6  6  6  6  5  5  5  5  5  5  4  4  4  4  4  4  4  3  3  3  3  2\n 0  1  2  1  3  2  1  4  3  2  2  1  5  4  3  3  2  2  4  4  3  3  3  2  2  3  3  3  2  2\n 0  0  0  1  0  1  1  0  1  2  1  1  0  1  2  1  2  1  2  1  3  2  1  2  2  3  2  2  2  2\n 0  0  0  0  0  0  1  0  0  0  1  1  0  0  0  1  1  1  0  1  0  1  1  2  1  1  2  1  2  2\n 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  1  1  2\n\n\nTo reproduce Lyne (1985)’s graphical analyses of the three indices, we calculate dispersion scores for these 30 distributions. We will make use of the function disp() in the R package {tlda} (Soenning 2025). Further below, we will extend Lyne (1985)’s approach to a total of six dispersion measures:\n\nD (Juilland and Chang-Rodriguez 1964)\nD2 (Carroll 1970)\nS (Rosengren 1971)\nDP (Gries 2008)\nDA (Burch, Egbert, and Biber 2017)\nDKL (Gries 2021)\n\n\n\nR code: Calculate dispersion scores for the 30 combinations\nresults &lt;- matrix(\n  NA, \n  nrow = nrow(combs),\n  ncol = 6\n)\n\nfor(i in 1:nrow(combs)){\n  results[i,] &lt;- disp(\n    subfreq = as.numeric(combs[i, 1:5]), \n    partsize = rep(1000, 5),\n    verbose = FALSE,\n    print_score = FALSE)[-1]\n}\ncolnames(results) &lt;- c(\"D\", \"D2\", \"S\", \"DP\", \"DA\", \"DKL\")\n\ncombs &lt;- cbind(combs, results)\n\ncombs$D_x_location &lt;- as.numeric(factor(rank(-combs$D, ties.method = \"min\")))\ncombs$D2_x_location &lt;- as.numeric(factor(rank(-combs$D2, ties.method = \"min\")))\ncombs$S_x_location &lt;- as.numeric(factor(rank(-combs$S, ties.method = \"min\")))\ncombs$DP_x_location &lt;- as.numeric(factor(rank(-combs$DP, ties.method = \"min\")))\ncombs$DA_x_location &lt;- as.numeric(factor(rank(-combs$DA, ties.method = \"min\")))\ncombs$DKL_x_location &lt;- as.numeric(factor(rank(-combs$DKL, ties.method = \"min\")))\n\ncombs$disp_avg &lt;- rowMeans(combs[,7:12])\ncombs$disp_avg_x_location &lt;- as.numeric(factor(rank(-combs$disp_avg, ties.method = \"min\")))\n\n\nLyne (1985) uses an elegant graphical technique to evaluate the behavior of D, D2 and S. In his study, Juilland’s D serves as a point of reference, so he starts by ordering the 30 possible combinations according to D. Then he draws this distribution of D scores into a scatterplot. In Figure 1 this sequence of D scores appears as a curved grey reference line, which runs from the top left corner (dispersion optimum: 2 2 2 2 2) to the bottom right corner (dispersion pessimum: 10 0 0 0 0). In a few cases, two combinations produce the same D score (e.g. 4 2 2 1 1 and 3 3 2 2 0), so there are only 22 D scores for the total set of 30 possible combinations.\nIn a second step, Lyne (1985) adds the D2 scores for all 30 distributions to the scatterplot. He groups them visually according to the number of zeros they contain. The D2 scores therefore form five traces, which are connected using lines. The top-most trace in Figure 1, for instance, connects D2 scores for those patterns that do not contain any zeros. This graphical arrangement allows Lyne (1985, 107) to make the following observations when comparing D2 against D:\n\n“D2 gives consistently higher values, except at the two end points, where they coincide”\n“D2 favours the absence of zeros”\n\n\n\nDraw Figure\ncombs &lt;- combs[order(combs$D),]\n\nzeros_labels &lt;- c(\"no zeros\",\n                  \"one zero\",\n                  \"two zeros\",\n                  \"three zeros\",\n                  \"four zeros\")\n\np1 &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$D))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, .25, .5, .75, 1), label = c(\"(uneven) 0\", \".25\", \".50\", \".75\", \"(even) 1\"))),\n       ylab = \"\\n\\nDispersion\", xlab = list(label = \"\\n\\n\\nDistribution\\n(ranked from most to least even (according to D)\", lineheight = .85, cex = .9),\n       panel = function(x,y){\n         panel.segments(x0 = 9, x1 = 9, y0 = 0, y1 = .75, col = \"grey\", lty = \"13\")\n        panel.points(x = combs$D_x_location, y = combs$D, type = \"l\", col = \"grey\")\n        panel.points(x = combs$D_x_location, y = combs$D, pch = 21, col = \"grey\", fill = \"white\")\n        panel.points(x = combs$D_x_location, y = combs$D2, cex = .8)\n        panel.text(x = 3, y = .6, label = \"D\", col = \"grey40\")\n        panel.text(x = 10, y = .95, label = expression(D[2]))\n        \n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$D_x_location, y = data_subset$D2, type = \"l\")\n            panel.text(x = max(data_subset$D_x_location + 1), \n                       y = data_subset$D2[which.max(data_subset$D_x_location)]+.02,\n                       label = zeros_labels[i+1],\n                       adj = 0, cex = .8)\n            }\n        })\n\nprint(p1, position = c(-.04,.08,.8,1))\n\n\n\n\n\n\n\n\nFigure 1: Figure 3 in Lyne (1985, 106), comparing D2 against D for all possible combinations of subfrequencies for 10 occurrences in 5 parts.\n\n\n\n\n\nThe fact that D2 favors zeros is evident from the layered structure of the traces. Thus, in cases where two distributions receive the same value of D, D2 assigns a higher score to the pattern with fewer zeros. The dotted vertical line in Figure 1 points out one such pair of distributions: A 5 3 1 1 0 and B 4 4 2 0 0. Both have a D score of .55, but A (one zero), has a D2 score of .66 compared with .73 for B (two zeros).\nThis leads Lyne (1985, 107) to consider the question of whether it is appropriate for a dispersion measure to be overly sensitive to the presence of zeros. He compares two sets of subfrequencies,\n\nA: 8  3  3  3  3\nB: 0  5  5  5  5\n\nfor which D yields the same score (.75) while D2 produces different scores (A .95, B .86). Since, in his eyes, the two distributions are “mirror images of each other”, he argues that D treats zeros impartially, while D2 penalizes them.\nLyne (1985, 110) makes the same graphical comparison for S (vs. D) and observes similar patterns. Figure 2 reproduces his Figure 4, which shows that “S penalizes zeros to an even greater extent than D2”.\n\n\nDraw Figure\np1 &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$D))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, .25, .5, .75, 1), label = c(\"(uneven) 0\", \".25\", \".50\", \".75\", \"(even) 1\"))),\n       ylab = \"\\n\\nDispersion\", xlab = list(label = \"\\n\\n\\nDistribution\\n(ranked from most to least even according to D)\", lineheight = .85, cex = .9),\n       panel = function(x,y){\n        panel.points(x = combs$D_x_location, y = combs$D, type = \"l\", col = \"grey\")\n        panel.points(x = combs$D_x_location, y = combs$D, pch = 21, col = \"grey\", fill = \"white\")\n        panel.points(x = combs$D_x_location, y = combs$S, cex = .8)\n        panel.text(x = 3, y = .6, label = \"D\", col = \"grey40\")\n        panel.text(x = 10, y = .95, label = \"S\")\n        \n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$D_x_location, y = data_subset$S, type = \"l\")\n            panel.text(x = max(data_subset$D_x_location + 1), \n                       y = data_subset$S[which.max(data_subset$D_x_location)]+.02,\n                       label = zeros_labels[i+1],\n                       adj = 0, cex = .8)\n            }\n        })\n\nprint(p1, position = c(-.04,.08,.8,1))\n\n\n\n\n\n\n\n\nFigure 2: Figure 4 in Lyne (1985, 110), comparing S against D for all possible combinations of subfrequencies for 10 occurrences in 5 parts.\n\n\n\n\n\nIn the course of this evaluation, Lyne (1985, 107) raises an important question:\n\n[I]s it desirable that a dispersion measure for word frequency counts should penalize zeros in this way? This question is of a different order […], since it is about the real world rather than the world of statistical models.\n\nHe later returns to this question, when discussing an example given by Rosengren (1971, 115), for which, she argues, it is inappropriate for a dispersion measure to give the same score:\n\nA: 3  2  1  1  1\nB: 2  2  2  2  0\n\nAs Lyne (1985, 115) rightly notes, it is essential to state in the first place why B should receive a lower score than A. To him, the reason why D2 and S give lower scores to B is the fact that they penalize zeros. And here, Lyne (1985, 115) clearly states his point of view:\n\nBut surely it is not the business of a dispersion measure to discriminate against particular distributions in this way. In the above example, […] it seems to us quite proper that B should be rated as highly as A, because the presence of a single low sub-frequency, 0, in B is balanced by the perfectly even distribution across the remaining four sections.\n\nEven though I will not pursue this point further here, I am not sure whether we are able to answer this question so confidently. Rather, the answer would seem to depend primarily on the linguistic purpose underlying a dispersion analysis. It is easy to imagine settings where the question of whether an item is used at all (i.e. the difference between 0 and 1 occurrence) is more important than the question of whether the item occurs repeatedly (i.e. the difference between 1 and 2 occurrences). In such settings, the feature of main interest may be pervasiveness rather than evenness of distribution, and penalization of zeros may be an attractive feature of a dispersion measure. Further, it seems that the answer would also depend on the kinds of linguistic units that are represented by the corpus parts (e.g. texts, genres, or arbitrary corpus chunks of the same length). The smaller the corpus parts, the more zeros there will be in the resulting set of subfrequencies, which could make penalization, if it is indeed considered an issue, even more of a concern.\n\n\nApplication of Lyne (1985)’s graphical technique to other parts-based dispersion measures\nSeeing that Lyne (1985)’s technique provides insights into the behavior of dispersion measures, I will now apply it to the other parts-based measures listed above. I will modify the strategy in one important regard: Instead of using D as a baseline, I will rely on a consensus opinion: The 30 combinations will be ordered according to the average score they receive across the six dispersion measures. For illustration, Figure 3 compares S against the consensus profile, which is again shown in grey. We also introduce color coding for signaling the number of zeros in a pattern, ranging from dark blue (no zeros) to dark red (four zeros).\n\n\nDraw Figure\ncombs &lt;- combs[order(combs$disp_avg),]\nmy_cols &lt;- rev(scales::pal_brewer(palette = \"RdBu\")(8)[c(1,2,3,6,7)])\n\np1 &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, .25, .5, .75, 1), label = c(\"(uneven) 0\", \".25\", \".50\", \".75\", \"(even) 1\"))),\n       ylab = \"\\n\\nDispersion\", xlab = list(label = \"\\n\\n\\n\\nDistribution\\n(ranked from most to least even based\\non the average across all measures)\", lineheight = .85, cex = .9),\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n        #panel.points(x = combs$disp_avg_x_location, y = combs$D2, cex = .8)\n        panel.text(x = 8, y = .5, label = \"All six\\ndispersion\\nmeasures\", col = \"grey40\", lineheight = .85, cex = .8)\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$S, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$S, col = my_cols[i+1], cex = .8, pch = 16)\n            panel.text(x = max(data_subset$disp_avg_x_location + 1), \n                       y = data_subset$S[which.max(data_subset$disp_avg_x_location)]+.02,\n                       label = zeros_labels[i+1],\n                       adj = 0, cex = .8, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(S))\n        })\n\nprint(p1, position = c(-.04,.15,.8,.925))\n\n\n\n\n\n\n\n\nFigure 3: Comparison of S vs. baseline for all possible combinations of subfrequencies for 10 occurrences in 5 parts.\n\n\n\n\n\nFigure 4 compares each of the six dispersion measures against the consensus benchmark. A number of instructive insights emerge:\n\nThe only measure that shows a clear “penalization” of zeros is S.\nFollowing the change in reference distribution (consensus opinion instead of D only), D2 no longer appears to penalize zeros.\nCompared against the average behavior over the six indices, D favors the presence of zeros: Compared to S, it produces the reversed layering.\nThere is also some indication that DP favors zeros.\n\n\n\nDraw Figure\ncombs &lt;- combs[order(combs$disp_avg),]\n\np_D &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$D, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$D, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(D))\n        })\n\np_D2 &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$D2, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$D2, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(D[2]))\n        })\n\np_S &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$S, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$S, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(S))\n        })\n\np_DP &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DP, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DP, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(D[P]))\n        })\n\np_DA &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DA, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DA, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(D[A]))\n        })\n\np_DKL &lt;- xyplot(1~1, xlim = c(0, length(unique(combs$disp_avg_x_location))), type = \"n\", ylim = c(0,1),\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(draw = FALSE),\n                     y = list(at = c(0, 1))),\n       ylab = \" \", xlab = \"\",\n       panel = function(x,y){\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, type = \"l\", col = \"grey\")\n        panel.points(x = combs$disp_avg_x_location, y = combs$disp_avg, pch = 21, col = \"grey\", fill = \"white\")\n\n        for ( i in 0:4){\n            data_subset &lt;- subset(combs, n_zeros == i)\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DKL, type = \"l\", col = my_cols[i+1])\n            panel.points(x = data_subset$disp_avg_x_location, y = data_subset$DKL, cex = .8, pch = 16, col = my_cols[i+1])\n            }\n        panel.text(x = 15.5, y = 1.1, label = expression(D[KL]))\n        })\n\ncowplot::plot_grid(\n  NULL, NULL, NULL,\n  p_D, p_D2, p_S, \n  NULL, NULL, NULL,\n  p_DP, p_DA, p_DKL, nrow = 4, \n  rel_heights = c(1,8,1,8))\n\n\n\n\n\n\n\n\nFigure 4: Comparison of all dispersion measures against baseline for all possible combinations of subfrequencies for 10 occurrences in 5 parts. See Figure 3 for a key.\n\n\n\n\n\nThe graphical technique developed by Lyne (1985) proves to be a useful tool for studying the behavior of indices. Considering the fact that Anthony Lyne drew all of his graphs by hand makes his methodological contribution even more impressive. In general, this examination is instructive for understanding how dispersion measures behave in settings where there are few corpus parts. Unfortunately, the technique does not scale well, meaning that it cannot be applied to study distributions across a much larger number of corpus parts. This is because the number of possible combinations grows exponentially as we increase the number of corpus parts and/or the number of occurrences of the item. Nevertheless, Lyne (1985)’s graphical technique has managed to shed more light on the performance of indices and, perhaps more importantly, it has confronted us with a deeper question, which is “beyond the world of statistical models” [p. 107] and needs to be answered from a linguistic perspective: Should dispersion measures deal with subfrequencies of 0 in a special way, i.e. should a drop from 1 to 0 depress a score more noticeably than a drop from 2 to 1?\n\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nCarroll, John B. 1970. “An Alternative to Juilland’s Usage Coefficient for Lexical Frequencies and a Proposal for a Standard Frequency Index.” Computer Studies in the Humanities and Verbal Behaviour 3 (2): 61–65. https://doi.org/10.1002/j.2333-8504.1970.tb00778.x.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2021. “A New Approach to (Key) Keywords Analysis: Using Frequency, and Now Also Dispersion.” Research in Corpus Linguistics 9 (2): 1–33. https://doi.org/10.32714/ricl.09.02.02.\n\n\nJuilland, Alphonse, and E. Chang-Rodriguez. 1964. Frequency Dictionary of Spanish Words. The Hague: De Gruyter. https://doi.org/10.1515/9783112415467.\n\n\nLyne, Anthony A. 1985. The Vocabulary of French Business Correspondence. Paris: Slatkine-Champion.\n\n\nRosengren, Inger. 1971. “The Quantitative Concept of Language and Its Relation to the Structure of Frequency Dictionaries.” Etudes de Linguistique Appliquee (Nouvelle Serie) 1: 103–27.\n\n\nSoenning, Lukas. 2025. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\nWarnes, Gregory R., Ben Bolker, Thomas Lumley, Arni Magnusson, Bill Venables, Genei Ryodan, and Steffen Moeller. 2023. Gtools: Various r Programming Tools. https://doi.org/10.32614/CRAN.package.gtools.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Lyne’s (1985) Graphical Technique for the Evaluation of\n    Dispersion Measures},\n  date = {2025-10-27},\n  url = {https://lsoenning.github.io/posts/2025-10-25_dispersion_lyne/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Lyne’s (1985) Graphical Technique for the\nEvaluation of Dispersion Measures .” October 27, 2025. https://lsoenning.github.io/posts/2025-10-25_dispersion_lyne/."
  },
  {
    "objectID": "posts/2025-11-11_dispersion_plot_corpus_design/index.html",
    "href": "posts/2025-11-11_dispersion_plot_corpus_design/index.html",
    "title": "Drawing structured dispersion plots in R",
    "section": "",
    "text": "R setup\n# may need to install latest development version directly from Github\n#pak::pak(\"lsoenning/tlda\")\n#pak::pak(\"lsoenning/wls\")\n\nlibrary(tlda)      # for access to datasets\nlibrary(wls)       # for custom ggplot theme\nlibrary(tidyverse) # for data wrangling\nlibrary(ggh4x)     # for drawing nested facets in ggplot\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nDispersion plots were introduced into corpus linguistics in the mid-1990s by Mike Scott and his popular software WordSmith Tools. They are useful for examining the distribution of an item in a text or corpus. The typical layout of a dispersion plot shows a layered arrangement, with each row showing a text file in the corpus. Thin vertical bars then show where in the document a word occurs. If you want to draw such a plot in R, the {quanteda} package (Benoit et al. 2018) is worth checking out (see Section Lexical dispersion plot on the package website).\nIf we are working with a corpus that consists of different text categories (such as registers, genres, etc.), it makes sense to enrich dispersion plots with information about the structure of the corpus. In this blog post, we look at one way of doing so, which I will refer to as a structured dispersion plot. It strings together all text files in the corpus and locates the occurrences of an item throughout the entire corpus. The structure of the text files is reflected through one or several annotation layers above the dispersion plot; this way, we can see in which sub-corpus a particular instance occurred.\n\nInformation about corpus position and source text of item\nTo draw a dispersion plot, we need information about the location (or position) of each occurrence of the item in the corpus. For illustration, we will consider the distribution of two word forms, methods and thirteen, in ICE-GB (Nelson, Wallis, and Aarts 2002). Biber et al. (2016) expect methods to be writing-skewed and thirteen to be speech-skewed. Both word forms are part of a list of items that was compiled by Biber et al. (2016) to study the behavior of dispersion measures in different distributional settings. The 150 items are intended to cover a broad range of frequency and dispersion levels (see Sönning 2025a and help(\"biber150_ice_gb\")).\nWe start by loading a data frame that lists every word and non-word token in the corpus and contains 1,072,393 rows in total. The table has three columns:\n\ntext_file the text files in which the item occurs\nitem the item, i.e. word form\ncorpus_position the corpus position of the item\n\n\nice_gb &lt;- readRDS(\"ice_gb.rds\")\nstr(ice_gb)\n\n'data.frame':   1072393 obs. of  3 variables:\n $ text_file      : chr  \"s1a-001\" \"s1a-001\" \"s1a-001\" \"s1a-001\" ...\n $ item           : chr  NA NA NA NA ...\n $ corpus_position: int  1 2 3 4 5 6 7 8 9 10 ...\n\n\nThe column item only identifies the word forms that are part of Biber et al.’s (2016) list; other tokens have been replaced by NA. The item methods occurs 63 times in ICE-GB, thirteen 27 times:\n\nsum(ice_gb$item == \"methods\", \n    na.rm = TRUE)\n\n[1] 63\n\n\n\nsum(ice_gb$item == \"thirteen\", \n    na.rm = TRUE)\n\n[1] 27\n\n\n\n\nInformation about corpus structure\nThe ICE family corpora rely on a standardized sampling frame (see here). Relevant metadata about the 500 text files in ICE-GB is provided in the data object metadata_ice_gb in the {tlda} package (Sönning 2025b). See help(\"metadata_ice_gb\") for more information about this data table.\n\nstr(metadata_ice_gb)\n\n'data.frame':   500 obs. of  7 variables:\n $ text_file    : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ mode         : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category: Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre  : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre        : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short  : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count   : int  2195 2159 2287 2290 2120 2060 2025 2177 2063 2146 ...\n\n\nLet us briefly explore the structure of ICE-GB. Starting at the broadest level of classification, it covers two modes of production. The number of spoken and written text files differs:\n\nftable(\n  metadata_ice_gb$mode, \n  row.vars = 1)\n\n            \nspoken   300\nwritten  200\n\n\nAt the next level, the corpus is divided into four text categories:\n\nftable(\n  metadata_ice_gb$text_category, \n  row.vars = 1)\n\n                \ndialogues    180\nmonologues   120\nnon_printed   50\nprinted      150\n\n\nAnd these break down further into 12 macro genres:\n\nftable(\n  metadata_ice_gb$macro_genre, \n  row.vars = 1)\n\n                          \nprivate_dialogues      100\npublic_dialogues        80\nunscripted_monologues   70\nscripted_monologues     50\nstudent_writing         20\nletters                 30\nacademic_writing        40\npopular_writing         40\nreportage               20\ninstructional_writing   20\npersuasive_writing      10\ncreative_writing        20\n\n\nWhen analyzing the dispersion of an item in ICE-GB, this structure needs to be taken into account.\n\n\nData preparation\nImportantly, the classification variables denoting text varieties (text_category, macro_genre, and genre) are already ordered based on the sampling frame that informs the design of the ICE family of corpora. In metadata_ice_gb, they are represented as ordered factors (see above). This is important for visualization later on, because we want to have a sensible visual arrangement of text varieties in our plot.\nWe now need to combine the information contained in the two tables. The linking column is text_file, which allows us to join ice_gb with metadata_ice_gb:\n\nice_gb &lt;- full_join(\n  ice_gb, \n  metadata_ice_gb)\n\nThis yields a data frame with more information about each token in the corpus:\n\nstr(ice_gb)\n\n'data.frame':   1072393 obs. of  9 variables:\n $ text_file      : chr  \"s1a-001\" \"s1a-001\" \"s1a-001\" \"s1a-001\" ...\n $ item           : chr  NA NA NA NA ...\n $ corpus_position: int  1 2 3 4 5 6 7 8 9 10 ...\n $ mode           : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category  : Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre    : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre          : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short    : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count     : int  2195 2195 2195 2195 2195 2195 2195 2195 2195 2195 ...\n\n\n\n\nPreparation for plotting\nThree more preparatory steps are necessary. First, we replace the NA code in the column item with a blank space (” “), as this will make it easier to plot the data:\n\nice_gb$item &lt;- ifelse(\n  is.na(ice_gb$item), \n  \" \",\n  ice_gb$item)\n\nThen we convert corpus_position into a factor (position_fct):\n\nice_gb &lt;- ice_gb |&gt; \n    mutate(position_fct = factor(\n        corpus_position, levels = 1:nrow(ice_gb)\n    ))\n\nAnd finally, to have nicer labels in the plot, I will replace the “_” symbols in the macro genre labels with line breaks. This is optional.\n\nice_gb$macro_genre_nice &lt;- factor(\n    ice_gb$macro_genre,\n    labels = str_replace(\n      levels(ice_gb$macro_genre), \n      pattern = \"_\", replacement = \"\\n\"))\n\nice_gb$text_category_nice &lt;- factor(\n    ice_gb$text_category,\n    labels = str_replace(\n      levels(ice_gb$text_category), \n      pattern = \"_\", replacement = \"-\"))\n\n\n\nDrawing the structured dispersion plot\nNow we are ready for plotting. The following annotated code draws a structured dispersion plot. It uses the function facet_nested() from the {ggh4x} package (van den Brand 2024) to draw nested facets. The function theme_dispersion_plot() from the {wls} package adjusts the ggplot2 theme for a nicer appearance. We start by drawing a structured dispersion plot for methods, with two structural annotation layers: mode (2 categories) and macro genre (12 categories):\n\nice_gb |&gt; \n  mutate(item_location = ifelse(          # create new column indicating the\n    item == \"methods\",                    #   occurrence of the item (for color \n    item, \"other\")) |&gt;                    #   coding further below)\n  ggplot(aes(x = position_fct)) +         # corpus position as the x-variable\n  geom_segment(aes(                       # draw vertical bars at location of\n    x = position_fct,                     #   occurrence (position_fct),\n    xend = position_fct,                  #   extending from 0 to 1 vertically  \n    y = 0, yend = 1,                      #\n    color = item_location)) +             # only draw bars where item occurs\n  facet_nested(                           # function from the {ggh4x} package:\n    . ~ mode + macro_genre_nice,          #   draw nested facets, width of facets\n    scales = \"free\",                      #   proportional to length\n    space = \"free_x\") +                   #   \n  scale_color_manual(                     # set color manually so bars appear \n    values = c(\"black\", \"transparent\")) + #   only where item occurs\n  scale_x_discrete(expand = c(0,0)) +     # avoid left/right padding in facets \n  scale_y_continuous(expand = c(.1,.1)) + # add some top/bottom padding\n  theme_bw() +                            # specify theme_bw() as basis\n  theme_dispersion_plot() +               # custom theme for dispersion plot\n  xlab(NULL) + ylab(NULL)                 # no axis labels\n\n\n\n\n\n\n\nFigure 1: Structured dispersion plot showing the distribution of methods in ICE-GB.\n\n\n\n\n\nIn line with Biber et al. (2016)’s expectation, methods is skewed toward written usage. The vertical bars gravitate toward the right, where written texts are located. Perhaps unsurprisingly, occurrences of methods are most densely clustered in the macro genre academic writing.\nUsing the filter() function in the {dplyr} package, we can also zoom in to the spoken part of the corpus:\n\n\nDraw Figure\nice_gb |&gt; \n    mutate(item_location = ifelse(\n            item == \"methods\",\n            item, \"other\")) |&gt;\n    filter(mode == \"written\") |&gt;             # filter: spoken texts only\n    ggplot(aes(x = position_fct)) +\n    geom_segment(aes(\n        x = position_fct,\n        xend = position_fct,\n        y = 0, yend = 1,\n        color = item_location)) +\n    facet_nested(\n      . ~ mode + macro_genre_nice,\n      scales = \"free\",\n      space = \"free_x\") +\n    scale_color_manual(\n      values = c(\"black\", \"transparent\")) +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_continuous(expand = c(.1,.1)) +\n    theme_bw() +\n    theme_dispersion_plot() +\n    xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 2: Structured dispersion plot showing the distribution of methods in the written part of ICE-GB.\n\n\n\n\n\nLet’s also look at a structured dispersion plot for thirteen, which shows the reverse pattern – it is more common in speech. Note that the annotation layers above the plot now include mode (2 categories) and text category (4 categories).\n\n\nDraw Figure\nice_gb |&gt; \n    mutate(item_location = ifelse(          # \n            item == \"thirteen\",                 # select new item \n            item, \"other\")) |&gt;                  #\n    ggplot(aes(x = position_fct)) +\n    geom_segment(aes(\n        x = position_fct,\n        xend = position_fct,\n        y = 0, yend = 1,\n        color = item_location)) +\n    facet_nested(\n      . ~ mode + text_category_nice,\n      scales = \"free\",\n      space = \"free_x\") +\n    scale_color_manual(\n      values = c(\"transparent\", \"black\")) +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_continuous(expand = c(.1,.1)) +\n    theme_bw() +\n    theme_dispersion_plot() +\n    xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 3: Structured dispersion plot showing the distribution of thirteen in ICE-GB.\n\n\n\n\n\nWhile I think structured dispersion plots can be useful in corpus analysis, I am not really happy with the way they are implemented here: Slow and lots of (unelegant) code. If you have a better idea of how to draw such plots in R, please let me know!\n\n\n\n\n\nReferences\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBiber, Douglas, Randi Reppen, Erin Schnur, and Romy Ghanem. 2016. “On the (Non)utility of Juilland’sDto Measure Lexical Dispersion in Large Corpora.” International Journal of Corpus Linguistics 21 (4): 439–64. https://doi.org/10.1075/ijcl.21.4.01bib.\n\n\nNelson, Gerald, Sean Wallis, and Bas Aarts. 2002. Exploring Natural Language: Working with the British Component of the International Corpus of English. John Benjamins. https://doi.org/10.1075/veaw.g29.\n\n\nSönning, Lukas. 2025a. “Biber et al.’s (2016) set of 150 BNC items for the analysis of dispersion measures: Dataset for ‘Evaluation of text-level measures of lexical dispersion’.” DataverseNO. https://doi.org/10.18710/ATCQZW.\n\n\n———. 2025b. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\nvan den Brand, Teun. 2024. Ggh4x: Hacks for ’Ggplot2’. https://doi.org/10.32614/CRAN.package.ggh4x.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Drawing Structured Dispersion Plots in {R}},\n  date = {2025-11-11},\n  url = {https://lsoenning.github.io/posts/2025-11-11_structured_dispersion_plot/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Drawing Structured Dispersion Plots in\nR.” November 11, 2025. https://lsoenning.github.io/posts/2025-11-11_structured_dispersion_plot/."
  },
  {
    "objectID": "posts/2025-11-12_dispersion_spike_graph/index.html",
    "href": "posts/2025-11-12_dispersion_spike_graph/index.html",
    "title": "Drawing spike graphs to examine dispersion across text files",
    "section": "",
    "text": "R setup\n# install development version directly from Github\n#pak::pak(\"lsoenning/tlda\")\n#pak::pak(\"lsoenning/wls\")\n\nlibrary(tlda)      # for access to datasets\nlibrary(wls)       # for custom ggplot theme\nlibrary(tidyverse) # for data wrangling\nlibrary(ggh4x)     # for drawing nested facets in ggplot\n\n\nI first came across spike graphs as a tool for visualizing the dispersion of an item in a corpus in a paper by Church and Gale (1995). Since this graph type is a great visual aid for examining and illustrating the distribution of an item across the text files in a corpus, I have since started to use spike graphs in my own work (e.g. Figure 8 in Sönning -Sönning (2025a) and Figure 2 in Sönning -Sönning (2025b)). The following figure, which appears in Sönning (2025a, 21), shows the distribution of which across the 500 text files in the Brown Corpus. Each spike denotes a text file, and gaps represent documents that contain no instances of this item. Text files are grouped by macro genre (four categories marked at the bottom), and genre (15 categories marked at the top). The ‘hairy’ appearance of the spike graph indicates that which is a common word – it appears in almost every document. In this blog post, I describe how to draw such annotated spike graphs in R using the {ggplot2} package.\n\n\n\nSpike graph showing the distribution of which in the Brown Corpus\n\n\n\nData format\nTo draw a spike graph, we need the following data for each text file in the corpus:\n\nThe frequency of the item in the text file\nThe length of the text file (number of word (and non-word) tokens)\nText metadata (e.g. mode, macro genre, genre, subgenre)\n\nOur illustrative item will be actually, and we will look at its distribution in ICE-GB (Nelson, Wallis, and Aarts 2002). Frequency information for actually in the 500 text files in ICE-GB is available in the dataset biber150_ice_gb (see help(\"biber150_ice_gb\")), which is part of the {tlda} package (Sönning 2025c). Let’s look at a small excerpt from this object, which is a term-document matrix:\n\nEach column represents a text file\nEach row represents an item (except for row 1, word_count, which gives the length of the text file)\n\n\nbiber150_ice_gb[1:10, 1:8]\n\n           s1a-001 s1a-002 s1a-003 s1a-004 s1a-005 s1a-006 s1a-007 s1a-008\nword_count    2195    2159    2287    2290    2120    2060    2025    2177\na               50      38      44      67      35      34      37      29\nable             2       4       4       0       0       0       0       0\nactually         3       6       2       2       6       3       0       8\nafter            0       0       0       0       4       1       1       0\nagainst          0       0       0       0       0       0       0       0\nah               1       0       0       0       1       6       1       2\naha              0       0       0       0       0       0       0       0\nall              2       5       6       9       7       5       8      13\namong            0       0       0       0       0       0       0       0\n\n\nWe extract the relevant data for actually. Importantly, this table includes every text file in the corpus, even if the number of occurrences of actually is 0.\n\nice_actually &lt;- data.frame(\n  text_file = colnames(biber150_ice_gb),\n    n_tokens = biber150_ice_gb[4,],\n    word_count = biber150_ice_gb[1,]\n)\n\nstr(ice_actually)\n\n'data.frame':   500 obs. of  3 variables:\n $ text_file : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ n_tokens  : num  3 6 2 2 6 3 0 8 2 6 ...\n $ word_count: num  2195 2159 2287 2290 2120 ...\n\n\nNow we need to add metadata for the 500 text files, which are provided in the dataset metadata_ice_gb in the {tlda} package (Sönning 2025c). See help(\"metadata_ice_gb\") for more information about this data table.\n\nstr(metadata_ice_gb)\n\n'data.frame':   500 obs. of  7 variables:\n $ text_file    : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ mode         : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category: Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre  : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre        : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short  : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count   : int  2195 2159 2287 2290 2120 2060 2025 2177 2063 2146 ...\n\n\nImportantly, the classification variables denoting text varieties (text_category, macro_genre, and genre) are already ordered based on the sampling frame that informs the design of the ICE family of corpora. In metadata_ice_gb, they are represented as ordered factors. This is important for visualization, because we want to order the text files (and higher-level text categories) in a sensible way.\nNext, we combine the two tables. The linking column is text_file, which allows us to join ice_gb with metadata_ice_gb:\n\nice_actually &lt;- full_join(\n  ice_actually, \n  metadata_ice_gb)\n\nThis yields a data frame with more information about each token in the corpus:\n\nstr(ice_actually)\n\n'data.frame':   500 obs. of  8 variables:\n $ text_file    : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ n_tokens     : num  3 6 2 2 6 3 0 8 2 6 ...\n $ word_count   : num  2195 2159 2287 2290 2120 ...\n $ mode         : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category: Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre  : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre        : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short  : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nAnd finally, to have nicer labels in the plot, I will replace the “_” symbols in the macro genre labels with an empty space. This is optional.\n\nice_actually$macro_genre_nice &lt;- factor(\n    ice_actually$macro_genre,\n    labels = str_replace(\n      levels(ice_actually$macro_genre), \n      pattern = \"_\",\n      replacement = \" \"))\n\nsaveRDS(ice_actually, \"ice_actually.rds\")\n\n\n\nDrawing the spike graph\nNow we are ready for plotting. The following annotated code draws a spike graph. It uses the function facet_nested() from the {ggh4x} package (van den Brand 2024) to draw nested facets. The function theme_spike_graph() from the {wls} package (Sönning 2025d) adjusts the ggplot2 theme for a clean appearance. In the following figure, we add two structural annotation layers as facets above the graph: mode (2 categories) and macro genre (12 categories):\n\nice_actually |&gt; \n  ggplot(aes(x = text_file,            # text_file as x-variable\n             y = n_tokens)) +          # frequency of item as y-variable\n  geom_segment(aes(xend = text_file),  # draw spikes for each text file\n               yend = 0,               #   spike starts at 0\n               linewidth = .2) +       #   draw thin lines\n  facet_nested(                        # nested facets with the {ggh4x} package:\n    . ~ mode + macro_genre_nice,       #   macro genre facets nested within mode \n    scales = \"free\",                   #   allow x-scale to vary across facets\n    space = \"free_x\",                  #   facet width proportional to # of texts\n    strip = strip_nested(              #   allow height of facet labels above\n      size = \"variable\")) +            #     graph to vary \n  theme_bw() +                         # specify theme_bw() as basis\n  theme_spike_graph() +                # custom theme for spike graph\n    scale_x_discrete(expand = c(0,0)) +  # avoid left/right padding in facets \n  ylab(\"Number of\\noccurrences\") +     # add y-axis title\n  xlab(\"\")  +                          # no title on x-axis\n  theme(                               # \n    strip.text.x.top = element_text(   # format facet labels\n      angle = 90, hjust = 0))          #   rotate by 90 degrees and left-align\n\n\n\n\n\n\n\nFigure 1: Spike graph showing the distribution of actually in ICE-GB: Number of occurrences (absolute frequency) in each text file.\n\n\n\n\n\n\n\nSpike graph showing normalized frequencies\nSince the text files in ICE-GB are all around 2,000 words long, it was OK for our spike graph to show the number of occurrences of actually in each text file (i.e. absolute frequencies). It is usually more appropriate, however, to show relative frequencies (i.e. normalized frequencies), because text files will differ in length. Doing so requires an intermediate step: We add a column to the data frame ice_actually, which gives the normalized frequency of actually in the text file. Here, we opt for frequency per 1,000 words as a basis:\n\nice_actually$rate_ptw &lt;- (ice_actually$n_tokens / ice_actually$word_count) * 1000\nstr(ice_actually)\n\n'data.frame':   500 obs. of  10 variables:\n $ text_file       : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ n_tokens        : num  3 6 2 2 6 3 0 8 2 6 ...\n $ word_count      : num  2195 2159 2287 2290 2120 ...\n $ mode            : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category   : Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre     : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre           : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short     : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre_nice: Ord.factor w/ 12 levels \"private dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ rate_ptw        : num  1.367 2.779 0.875 0.873 2.83 ...\n\n\nThen we can draw the plot. The annotations in the code below flag the changes we have made.\n\nice_actually |&gt; \n  mutate(                                    # add column with normalized frequency\n    rate_ptw = n_tokens / word_count * 1000  #   (per thousand words)\n  ) |&gt; \n  ggplot(aes(x = text_file,\n             y = rate_ptw)) +                # normalized frequency as y-variable\n  geom_segment(aes(xend = text_file),\n               yend = 0,\n               linewidth = .2) +\n  facet_nested(\n    . ~ mode + macro_genre_nice,\n    scales = \"free\",\n    space = \"free_x\",\n    strip = strip_nested(\n      size = \"variable\")) +\n  theme_bw() +\n  theme_spike_graph() +\n    scale_x_discrete(expand = c(0,0)) +\n  ylab(\"Frequency\\n per 1,000 words\") +      # change title of y-axis\n  scale_y_continuous(breaks = c(0, 5, 10)) + # nicer tick marks on y-axis\n  xlab(\"\")  +\n  theme(\n    strip.text.x.top = element_text(\n      angle = 90, hjust = 0))\n\n\n\n\n\n\n\nFigure 2: Spike graph showing the distribution of actually in ICE-GB: Rate of occurrence (normalized frequency) in each text file.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/s1351324900000139.\n\n\nNelson, Gerald, Sean Wallis, and Bas Aarts. 2002. Exploring Natural Language: Working with the British Component of the International Corpus of English. John Benjamins. https://doi.org/10.1075/veaw.g29.\n\n\nSönning, Lukas. 2025a. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” Corpora 20 (1): 3–35. https://doi.org/10.3366/cor.2025.0326.\n\n\n———. 2025b. “Dispersion Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/h3dyx_v2.\n\n\n———. 2025c. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\n———. 2025d. Wls: R Utilities for Workshops Taught by Lukas Soenning. https://github.com/lsoenning/wls.\n\n\nvan den Brand, Teun. 2024. Ggh4x: Hacks for ’Ggplot2’. https://doi.org/10.32614/CRAN.package.ggh4x.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Drawing Spike Graphs to Examine Dispersion Across Text Files},\n  date = {2025-11-12},\n  url = {https://lsoenning.github.io/posts/2025-11-12_dispersion_spike_graph/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Drawing Spike Graphs to Examine Dispersion\nAcross Text Files.” November 12, 2025. https://lsoenning.github.io/posts/2025-11-12_dispersion_spike_graph/."
  },
  {
    "objectID": "posts/2025-11-11_structured_dispersion_plot/index.html",
    "href": "posts/2025-11-11_structured_dispersion_plot/index.html",
    "title": "Drawing structured dispersion plots in R",
    "section": "",
    "text": "R setup\n# install development version directly from Github\n#pak::pak(\"lsoenning/tlda\")\n#pak::pak(\"lsoenning/wls\")\n\nlibrary(tlda)      # for access to datasets\nlibrary(wls)       # for custom ggplot theme\nlibrary(tidyverse) # for data wrangling\nlibrary(ggh4x)     # for drawing nested facets in ggplot\nlibrary(uls)       # pak::pak(\"lsoenning/uls\")\n\n\nDispersion plots were introduced into corpus linguistics in the mid-1990s by Mike Scott and his popular software WordSmith Tools. They are useful for examining the distribution of an item in a text or corpus. The typical layout of a dispersion plot shows a layered arrangement, with each row showing a text file in the corpus. Thin vertical bars then show where in the document a word occurs. If you want to draw such a plot in R, the {quanteda} package (Benoit et al. 2018) is worth checking out (see Section Lexical dispersion plot on the package website).\nIf we are working with a corpus that consists of different text categories (such as registers, genres, etc.), it makes sense to enrich dispersion plots with information about the structure of the corpus. In this blog post, we look at one way of doing so, which I will refer to as a structured dispersion plot. It strings together all text files in the corpus and locates the occurrences of an item throughout the entire corpus. The structure of the text files is reflected through one or several annotation layers above the dispersion plot; this way, we can see in which sub-corpus a particular instance occurred.\n\nInformation about corpus position and source text of item\nTo draw a dispersion plot, we need information about the location (or position) of each occurrence of the item in the corpus. For illustration, we will consider the distribution of two word forms, methods and thirteen, in ICE-GB (Nelson, Wallis, and Aarts 2002). Biber et al. (2016) expect methods to be writing-skewed and thirteen to be speech-skewed. Both word forms are part of a list of items that was compiled by Biber et al. (2016) to study the behavior of dispersion measures in different distributional settings. The 150 items are intended to cover a broad range of frequency and dispersion levels (see Sönning 2025a and help(\"biber150_ice_gb\")).\nWe start by loading a data frame that lists every word and non-word token in the corpus and contains 1,072,393 rows in total. The table has three columns:\n\ntext_file the text files in which the item occurs\nitem the item, i.e. word form\ncorpus_position the corpus position of the item\n\n\nice_gb &lt;- readRDS(\"ice_gb.rds\")\nstr(ice_gb)\n\n'data.frame':   1072393 obs. of  3 variables:\n $ text_file      : chr  \"s1a-001\" \"s1a-001\" \"s1a-001\" \"s1a-001\" ...\n $ item           : chr  NA NA NA NA ...\n $ corpus_position: int  1 2 3 4 5 6 7 8 9 10 ...\n\n\nThe column item only identifies the word forms that are part of Biber et al.’s (2016) list; other tokens have been replaced by NA. The item methods occurs 63 times in ICE-GB, thirteen 27 times:\n\nsum(ice_gb$item == \"methods\", \n    na.rm = TRUE)\n\n[1] 63\n\n\n\nsum(ice_gb$item == \"thirteen\", \n    na.rm = TRUE)\n\n[1] 27\n\n\n\n\nInformation about corpus structure\nThe ICE family corpora rely on a standardized sampling frame (see here). Relevant metadata about the 500 text files in ICE-GB is provided in the data object metadata_ice_gb in the {tlda} package (Sönning 2025b). See help(\"metadata_ice_gb\") for more information about this data table.\n\nstr(metadata_ice_gb)\n\n'data.frame':   500 obs. of  7 variables:\n $ text_file    : chr  \"s1a-001\" \"s1a-002\" \"s1a-003\" \"s1a-004\" ...\n $ mode         : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category: Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre  : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre        : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short  : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count   : int  2195 2159 2287 2290 2120 2060 2025 2177 2063 2146 ...\n\n\nLet us briefly explore the structure of ICE-GB. Starting at the broadest level of classification, it covers two modes of production. The number of spoken and written text files differs:\n\nftable(\n  metadata_ice_gb$mode, \n  row.vars = 1)\n\n            \nspoken   300\nwritten  200\n\n\nAt the next level, the corpus is divided into four text categories:\n\nftable(\n  metadata_ice_gb$text_category, \n  row.vars = 1)\n\n                \ndialogues    180\nmonologues   120\nnon_printed   50\nprinted      150\n\n\nAnd these break down further into 12 macro genres:\n\nftable(\n  metadata_ice_gb$macro_genre, \n  row.vars = 1)/500\n\n                           \nprivate_dialogues      0.20\npublic_dialogues       0.16\nunscripted_monologues  0.14\nscripted_monologues    0.10\nstudent_writing        0.04\nletters                0.06\nacademic_writing       0.08\npopular_writing        0.08\nreportage              0.04\ninstructional_writing  0.04\npersuasive_writing     0.02\ncreative_writing       0.04\n\n\nWhen analyzing the dispersion of an item in ICE-GB, this structure needs to be taken into account.\n\n\nData preparation\nImportantly, the classification variables denoting text varieties (text_category, macro_genre, and genre) are already ordered based on the sampling frame that informs the design of the ICE family of corpora. In metadata_ice_gb, they are represented as ordered factors (see above). This is important for visualization later on, because we want to order the text files (and higher-level text categories) in a sensible way.\nWe now need to combine the information contained in the two tables. The linking column is text_file, which allows us to join ice_gb with metadata_ice_gb:\n\nice_gb &lt;- full_join(\n  ice_gb, \n  metadata_ice_gb)\n\nThis yields a data frame with more information about each token in the corpus:\n\nstr(ice_gb)\n\n'data.frame':   1072393 obs. of  9 variables:\n $ text_file      : chr  \"s1a-001\" \"s1a-001\" \"s1a-001\" \"s1a-001\" ...\n $ item           : chr  NA NA NA NA ...\n $ corpus_position: int  1 2 3 4 5 6 7 8 9 10 ...\n $ mode           : chr  \"spoken\" \"spoken\" \"spoken\" \"spoken\" ...\n $ text_category  : Ord.factor w/ 4 levels \"dialogues\"&lt;\"monologues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ macro_genre    : Ord.factor w/ 12 levels \"private_dialogues\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre          : Ord.factor w/ 32 levels \"face_to_face_conversations\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre_short    : Ord.factor w/ 32 levels \"con\"&lt;\"ph\"&lt;\"les\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count     : int  2195 2195 2195 2195 2195 2195 2195 2195 2195 2195 ...\n\n\n\n\nPreparation for plotting\nThree more preparatory steps are necessary. First, we replace the NA code in the column item with a blank space (” “), as this will make it easier to plot the data:\n\nice_gb$item &lt;- ifelse(\n  is.na(ice_gb$item), \n  \" \",\n  ice_gb$item)\n\nThen we convert corpus_position into a factor (position_fct):\n\nice_gb &lt;- ice_gb |&gt; \n    mutate(position_fct = factor(\n        corpus_position, levels = 1:nrow(ice_gb)\n    ))\n\nAnd finally, to have nicer labels in the plot, I will replace the “_” symbols in the macro genre labels with line breaks. This is optional.\n\nice_gb$macro_genre_nice &lt;- factor(\n    ice_gb$macro_genre,\n    labels = str_replace(\n      levels(ice_gb$macro_genre), \n      pattern = \"_\", replacement = \"\\n\"))\n\nice_gb$text_category_nice &lt;- factor(\n    ice_gb$text_category,\n    labels = str_replace(\n      levels(ice_gb$text_category), \n      pattern = \"_\", replacement = \"-\"))\n\n\n\nDrawing the structured dispersion plot\nNow we are ready for plotting. The following annotated code draws a structured dispersion plot. It uses the function facet_nested() from the {ggh4x} package (van den Brand 2024) to draw nested facets. The function theme_dispersion_plot() from the {wls} package (Sönning 2025c) adjusts the ggplot2 theme for a nicer appearance. We start by drawing a structured dispersion plot for methods, with two structural annotation layers: text category (4 categories) nested within mode (2 categories):\n\nice_gb |&gt; \n  mutate(item_location = ifelse(          # create new column indicating the\n    item == \"methods\",                    #   occurrence of the item (for color \n    item, \"other\")) |&gt;                    #   coding further below)\n  ggplot(aes(x = position_fct)) +         # corpus position as the x-variable\n  geom_segment(aes(                       # draw vertical bars at location of\n    x = position_fct,                     #   occurrence (position_fct),\n    xend = position_fct,                  #   extending from 0 to 1 vertically  \n    y = 0, yend = 1,                      #\n    color = item_location)) +             # only draw bars where item occurs\n  facet_nested(                           # nested facets with the {ggh4x} package:\n    . ~ mode + text_category_nice,        #   text categ. facets nested within mode\n    scales = \"free\",                      #   allow x-scale to vary across facets\n    space = \"free_x\") +                   #   facet width proportional to length\n  scale_color_manual(                     # set color manually so bars appear \n    values = c(\"black\", \"transparent\")) + #   only where item occurs\n  scale_x_discrete(expand = c(0,0)) +     # avoid left/right padding in facets \n  scale_y_continuous(expand = c(.1,.1)) + # add some top/bottom padding\n  theme_bw() +                            # specify theme_bw() as basis\n  theme_dispersion_plot() +               # custom theme for dispersion plot\n  xlab(NULL) + ylab(NULL)                 # no axis labels\n\n\n\n\n\n\n\nFigure 1: Structured dispersion plot showing the distribution of methods in ICE-GB.\n\n\n\n\n\nIn line with Biber et al. (2016)’s expectation, methods is skewed toward written usage. The vertical bars gravitate toward the right, where written texts are located. Let us zoom in to the written part, to examine whether this item is associated with specific genres. Using the filter() function in the {dplyr} package, we can isolate the written part of the corpus. We also change the facetting scheme – annotation layers now show macro genre (8 categories) nested within text category (2 categories), nested within mode (here only 1 category):\n\n\nDraw Figure\nice_gb |&gt; \n    mutate(item_location = ifelse(\n            item == \"methods\",\n            item, \"other\")) |&gt;\n    filter(mode == \"written\") |&gt;             # filter: written texts only\n    ggplot(aes(x = position_fct)) +\n    geom_segment(aes(\n        x = position_fct,\n        xend = position_fct,\n        y = 0, yend = 1,\n        color = item_location)) +\n    facet_nested(\n      . ~ mode + text_category_nice + macro_genre_nice,  # new facetting scheme\n      scales = \"free\",\n      space = \"free_x\") +\n    scale_color_manual(\n      values = c(\"black\", \"transparent\")) +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_continuous(expand = c(.1,.1)) +\n    theme_bw() +\n    theme_dispersion_plot() +\n    xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 2: Structured dispersion plot showing the distribution of methods in the written part of ICE-GB.\n\n\n\n\n\nPerhaps unsurprisingly, occurrences of methods are most densely clustered in the macro genre academic writing.\nLet’s also look at a structured dispersion plot for thirteen, which shows the reverse pattern – it is more common in speech. Note that the annotation layers above the plot now include mode (2 categories) and text category (4 categories).\n\n\nDraw Figure\nice_gb |&gt; \n    mutate(item_location = ifelse(          # \n            item == \"thirteen\",                 # select new item \n            item, \"other\")) |&gt;                  #\n    ggplot(aes(x = position_fct)) +\n    geom_segment(aes(\n        x = position_fct,\n        xend = position_fct,\n        y = 0, yend = 1,\n        color = item_location)) +\n    facet_nested(\n      . ~ mode + text_category_nice,\n      scales = \"free\",\n      space = \"free_x\") +\n    scale_color_manual(\n      values = c(\"transparent\", \"black\")) +\n    scale_x_discrete(expand = c(0,0)) +\n    scale_y_continuous(expand = c(.1,.1)) +\n    theme_bw() +\n    theme_dispersion_plot() +\n    xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 3: Structured dispersion plot showing the distribution of thirteen in ICE-GB.\n\n\n\n\n\nWhile I think structured dispersion plots can be useful in corpus analysis, I am not really happy with the way they are implemented here: Slow and lots of (unelegant) code. If you have a better idea of how to draw such plots in R, please let me know!\n\n\n\n\n\nReferences\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “Quanteda: An r Package for the Quantitative Analysis of Textual Data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBiber, Douglas, Randi Reppen, Erin Schnur, and Romy Ghanem. 2016. “On the (Non)utility of Juilland’sDto Measure Lexical Dispersion in Large Corpora.” International Journal of Corpus Linguistics 21 (4): 439–64. https://doi.org/10.1075/ijcl.21.4.01bib.\n\n\nNelson, Gerald, Sean Wallis, and Bas Aarts. 2002. Exploring Natural Language: Working with the British Component of the International Corpus of English. John Benjamins. https://doi.org/10.1075/veaw.g29.\n\n\nSönning, Lukas. 2025a. “Biber et al.’s (2016) set of 150 BNC items for the analysis of dispersion measures: Dataset for ‘Evaluation of text-level measures of lexical dispersion’.” DataverseNO. https://doi.org/10.18710/ATCQZW.\n\n\n———. 2025b. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\n———. 2025c. Wls: R Utilities for Workshops Taught by Lukas Soenning. https://github.com/lsoenning/wls.\n\n\nvan den Brand, Teun. 2024. Ggh4x: Hacks for ’Ggplot2’. https://doi.org/10.32614/CRAN.package.ggh4x.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Drawing Structured Dispersion Plots in {R}},\n  date = {2025-11-11},\n  url = {https://lsoenning.github.io/posts/2025-11-11_structured_dispersion_plot/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Drawing Structured Dispersion Plots in\nR.” November 11, 2025. https://lsoenning.github.io/posts/2025-11-11_structured_dispersion_plot/."
  },
  {
    "objectID": "posts/2025-11-13_folded_power_transformation/index.html",
    "href": "posts/2025-11-13_folded_power_transformation/index.html",
    "title": "Tukey’s folded power transformation in R",
    "section": "",
    "text": "R setup\n# may need to install latest development version directly from Github\n#pak::pak(\"lsoenning/tlda\")\n\nlibrary(tlda)      # for access to datasets\nlibrary(lattice)   # for data visualization\nlibrary(tidyverse) # for data wrangling\nlibrary(uls)       # pak::pak(\"lsoenning/uls\")\n\n\nIn corpus linguistics, we often deal with quantities that range between 0 and 1. Examples are proportions (and percentages) and dispersion scores. Sometimes the distribution we observe gravitates toward the upper and/or lower end of the scale, which degrades resolution. In such cases, a transformation that stretches the scale near the endpoints can help. Three transformations are often applied to scores in the unit interval [0, 1] (see Fox 2016, 72–75). In the following, \\(x\\) refers to these bounded scores (e.g. proportions or dispersion scores):\n\nThe logit transformation, i.e. the (natural) log of the odds: \\(\\textrm{log}_e(\\frac{x}{1-x})\\); this transformation is unable to handle the endpoints, i.e. scores of 0 and 1\nThe probit transformation, i.e. the inverse standard normal distribution function; this transformation likewise shuns 0 and 1\nThe arcsine square-root (or angular) transformation: \\(\\textrm{sin}^{-1}\\sqrt{x}\\); this form of re-expression accepts the values 0 and 1\n\n\nFolded power transformation\nIn his landmark work Exploratory data analysis, Tukey (1977, 498–502) introduced folded roots and folded logs for the transformation of proportions. Both are special cases of what are known as folded power transformations (see also Mosteller and Tukey 1977, 92; Atkinson 1985, chap. 7). The formula for this class of re-expressions is the following, where \\(x\\) again refers to the bounded quantity that is to be transformed and \\(\\lambda\\) is the power to which the two terms, \\(x\\) and \\((1-x)\\), are raised:\n\\[\nx^\\lambda - (1-x)^\\lambda\n\\]\nThe choice of lambda affects how much the ends of the unit interval are stretched out relative to the center. Interestingly, the folded power transformation includes (approximations to) a number of familiar forms of re-expression:\n\nfolded roots: `\\(\\lambda = 0.5\\)\narcsine-square-root (or angular) transformation: a close approximation is given by \\(\\lambda = 0.41\\) (see Fox 2016, 74)\nfolded cube roots: \\(\\lambda = 1/3\\)\nprobit transformation: a close approximation is given by \\(\\lambda = 0.14\\) (see Fox 2016, 74) while accepting input scores of 0 and 1\nFor \\(\\lambda = 0\\) , the logit transformation is the limiting case; note that input scores of 0 and 1 are not allowed when \\(\\lambda\\) is set to 0\n\nThe following graph shows how the amount of local compression/expansion of the unit interval changes with the choice of \\(\\lambda\\), which ranges from 1 at the bottom (no transformation, i.e. identity) to 0.14 at the top. As \\(\\lambda\\) increases, the ends of the unit interval are stretched out more and more vigorously.\n\n\nDraw Figure\nx_seq &lt;- seq(0, 1, .05)\nlambda &lt;- seq(1, .14, -.0215)\n\nx_trans &lt;- matrix(\n    NA, nrow = 21, ncol = 41)\n\n\n\nfor(i in 1:41){\n    x_trans[,i] &lt;- fpower(x_seq, lambda[i], scaling = \"plus_minus_1\")\n}\n\n\np1 &lt;- xyplot(1~1, ylim=c(1,.14), xlim = c(-1, 1),\n       ylab = \"Lambda\", xlab = \"Unit interval\",\n       par.settings = lattice_ls, axis = axis_L,\n       scales = list(x = list(at = seq(-1, 1, .2), \n                           label = c(\"0\", \".10\", \".20\", \".30\", \".40\", \".50\", \".60\", \".70\", \".80\", \".90\", \"1\")),\n                  y = list(at = c(1, .5, .41, 1/3, .14)), label = c(\"1\", \".5\", \".41\", \"1/3\", \".14\")),\n       panel = function(x,y){\n        panel.segments(y0 = .5, y1 = .5, x0 = -1, x1 = 1, col = \"grey\")\n        panel.segments(y0 = .41, y1 = .41, x0 = -1, x1 = 1, col = \"grey\")\n        panel.segments(y0 = 1/3, y1 = 1/3, x0 = -1, x1 = 1, col = \"grey\")\n        panel.segments(y0 = .14, y1 = .14, x0 = -1, x1 = 1, col = \"grey\")\n        for(i in 1:21){\n            panel.points(y = lambda, x = x_trans[i,], type = \"l\", \n                         col = c(rep(c(\"black\", \"grey\"), 10), \"black\")[i])\n        }\n        panel.text(y = c(1, .5, .41, 1/3, .14), x = 1.1, label = c(\n            \"identity\", \"folded square root\", \"\\u2248 arcsine square-root\",\n            \"folded cube root\", \"\\u2248 probit\"\n        ), adj = 0, col = \"grey50\", cex = .9)\n       })\n\nprint(p1, position = c(0,0,.7,0.95))\n\n\n\n\n\n\n\n\nFigure 1: Diagram showing how the choice of lambda affects how much the center of the unit interval is compressed relative to the edges.\n\n\n\n\n\nThe power (\\(\\lambda\\)) typically ranges between 0 and 1 (no transformation). For \\(\\lambda = 0\\), the logit transformation is used. However, the logit of 0 and 1 is not defined, which is why \\(\\lambda = 0\\) is not included in the figure above.\n\n\nAnother version of the folded power transformation\nThe version of the folded power transformation we have encountered so far maps scores to the interval \\([-1, +1]\\), irrespective of the choice of power. There is another version that is frequently used, which does not impose these constraints:\n\\[\n\\frac{x^\\lambda - (1-x)^\\lambda}{\\lambda}\n\\]\nWe can also inspect this version graphically, by mimicking a plot that Tukey (1977, 502) drew. I really like the way he introduces this graph: “As always, it is nice to look at changes of expression graphically as well as in numbers. In this situation, where we think and write of ‘stretching the tails’, it is usually desirable to compare the different scales side by side and both watch and feel – to the extent that the feeling can be given – the tails stretch more and more […].” (Tukey 1977, 501–2)\nFigure 2 gives us a feel for the folded power transformation. It provides another way of appreciating the way in which different values of \\(\\lambda\\) stretch the ends of the interval relative to the middle.\n\n\nDraw Figure\nx_seq &lt;- seq(0, 1, .05)\nlambda &lt;- seq(1, .14, -.0215)\n\nx_trans &lt;- matrix(\n    NA, nrow = 21, ncol = 41)\n\nfor(i in 1:41){\n    x_trans[,i] &lt;- fpower(x_seq, lambda[i], scaling = \"free\")\n}\n\np1 &lt;- xyplot(1~1, ylim=c(1,.14), xlim = c(-7, 7),\n       ylab = NULL, xlab = \"Unit interval\",\n       par.settings = lattice_ls, \n       scales = list(x = list(at = c(-1, 1), \n                           label = c(\"0\", \"1\")),\n                  y = list(draw = FALSE)),\n       panel = function(x,y){\n        panel.segments(y0 = .50, y1 = .50, \n                       x0 = (0^.50 - (1-0)^.50) / .50,\n                       x1 = (1^.50 - (1-1)^.50) / .50)\n        panel.segments(y0 = .41, y1 = .41, \n                       x0 = (0^.41 - (1-0)^.41) / .41,\n                       x1 = (1^.41 - (1-1)^.41) / .41)\n        panel.segments(y0 = (1/3), y1 = (1/3), \n                       x0 = (0^(1/3) - (1-0)^(1/3)) / (1/3),\n                       x1 = (1^(1/3) - (1-1)^(1/3)) / (1/3))\n        panel.segments(y0 = .14, y1 = .14, \n                       x0 = (0^.14 - (1-0)^.14) / .14,\n                       x1 = (1^.14 - (1-1)^.14) / .14)\n        \n        panel.points(x = (c(0,1)^.50 - (1-c(0,1))^.50) / .50, y = .50, pch = 19, cex = 1)\n        panel.points(x = (c(0,1)^.41 - (1-c(0,1))^.41) / .41, y = .41, pch = 19, cex = 1)\n        panel.points(x = (c(0,1)^(1/3) - (1-c(0,1))^(1/3)) / (1/3), y = (1/3), pch = 19, cex = 1)\n        panel.points(x = (c(0,1)^.14 - (1-c(0,1))^.14) / .14, y = .14, pch = 19, cex = 1)\n        \n        for(i in 1:21){\n            panel.points(y = lambda, x = x_trans[i,], type = \"l\", \n                         col = c(rep(c(\"black\", \"grey\"), 10), \"black\")[i])\n        }\n        panel.text(x = x_trans[,41][c(1,3,11,19,21)], y = .05, \n                   label = c(\"0\",\".10\",\".50\",\".90\",\"1\"), cex = .8)\n        panel.text(y = c(.5, .41, 1/3, .14), x = c(\n          (1^.50 - (1-1)^.50) / .50,\n          (1^.41 - (1-1)^.41) / .41,\n          (1^(1/3) - (1-1)^(1/3)) / (1/3),\n          (1^.14 - (1-1)^.14) / .14)+.2, label = c(\n            \"folded square root\", \"\\u2248 arcsine square-root\", \" folded cube root\", \"\\u2248 probit\"\n        ), adj = 0, col = \"grey50\", cex = .9)\n        \n        panel.text(y = -.05, x = c(-1,1), label = c(0,1), cex = .8)\n        \n        panel.segments(x0 = -1, x1 = 1, y0 = 1, y1 = 1)\n        panel.segments(x0 = c(-1, 1), x1 = c(-1, 1), y0 = 1, y1 = 1.03)\n        panel.segments(x0 = x_trans[,41][c(1,3,11,19,21)], \n                       x1 = x_trans[,41][c(1,3,11,19,21)], y0 = .14, y1 = .11)\n       })\n\nprint(p1, position = c(.01,0,.92,.9))\n\n\n\n\n\n\n\n\nFigure 2: Diagram showing how the choice of lambda affects how much the edges of the unit interval are stretched out relative to the center.\n\n\n\n\n\nThere are two reasons why folded power transformations are attractive when working with scores in the unit interval. First, with the exception of the logit transformation, they can handle the endpoints (0 and 1). Further, they give the user flexibility in the choice of \\(\\lambda\\), with (approximations to) various special cases along the continuum from 0 to 1.\n\n\nImplementation in the {tlda} package\nFolded power transformations are implemented in the R package {tlda} (Sönning 2025). You may need to install the development version from Github:\n\npak::pak(\"lsoenning/tlda\")\n\nThe function fpower() can be used to re-express proportions using different powers (i.e. values for lambda). For illustration, consider the following sequence of proportions:\n\nx_seq &lt;- seq(0, 1, .1)\nx_seq\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nThe following code transforms this sequence to folded roots (\\(\\lambda = 0.5\\)), mapped to the interval [-1, +1]:\n\nfpower(\n  x_seq,\n  lambda = .5,\n  scaling = \"plus_minus_1\")\n\n [1] -1.0000000 -0.6324555 -0.4472136 -0.2889375 -0.1421411  0.0000000\n [7]  0.1421411  0.2889375  0.4472136  0.6324555  1.0000000\n\n\nTo switch to the version that returns scores that may exceed \\(-1\\) and \\(+1\\), we use scaling = \"free\":\n\nfpower(\n  x_seq,\n  lambda = .5,\n  scaling = \"free\")\n\n [1] -2.0000000 -1.2649111 -0.8944272 -0.5778749 -0.2842823  0.0000000\n [7]  0.2842823  0.5778749  0.8944272  1.2649111  2.0000000\n\n\nThe function invfpower() allows us to back-transform folded powers to the unit interval:\n\nx_seq_folded_roots_free &lt;- fpower(\n  x_seq,\n  lambda = .5,\n  scaling = \"free\")\n\ninvfpower(\n  x_seq_folded_roots_free,\n  lambda = 0.5,\n  scaling = \"free\")\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\n\n\nUsing folded-power-transformed scales with {ggplot2}\nThe {tlda} package also offers the functions scale_x_fpower() and scale_y_fpower(), which can be integrated into {ggplot2} code to plot data on a transformed scale (while keeping the original units as tick mark labels). For illustration, we consider a set of dispersion scores: Specifically, Rosengren’s S (Rosengren 1971) for the dispersion of 150 selected items (Biber et al. 2016) across speakers in the Spoken BNC2014. In this analysis, then, the speakers are the corpus parts. The data are available as the dataset biber150_spokenBNC2014 in the {tlda} package. We start by calculating Rosengren’s S for the 150 items (since two items from the list do not appear in the Spoken BNC2014, we are actually looking at only 148 items):\n\nS_spokenBNC2014 &lt;- disp_S_tdm(\n  tdm = biber150_spokenBNC2014, \n  row_partsize = \"first\",\n  print_score = FALSE,\n  verbose = FALSE)\n\nIf we look at the distribution of these dispersion scores, we note that they are pushing against the ceiling: as the following dot diagram shows, there are many items with a dispersion score near 1 (which indicates a very even, or balanced, distribution across speakers):\n\nS_spokenBNC2014 |&gt; \n    ggplot(aes(x = S)) +\n    geom_dotplot(\n      method = \"histodot\",\n      binwidth = .025) +\n  theme_dotplot() +\n  xlab(\"Rosengren's S\")\n\n\n\n\n\n\n\nFigure 3: Dot diagram showing the distribution of S scores for the 148 items.\n\n\n\n\n\nIt helps if we apply a folded power transformation with \\(\\lambda = 0.14\\), which approximates the shape of the probit transformation and stretches the tails considerably. We simply add the function scale_x_fpower() to the plotting call:\n\nS_spokenBNC2014 |&gt; \n  ggplot(aes(x = S)) +\n  geom_dotplot(\n    method = \"histodot\",\n    binwidth = .15) +\n  xlab(\"Transformed Rosengren's S\\n(folded powers, lambda = 0.14)\") +\n    theme_dotplot() +\n  scale_x_fpower(\n    lambda = .14, \n    breaks = c(.02, .1, .25, .5, .75, .9, .98))\n\n\n\n\n\n\n\nFigure 4: Dot diagram showing the distribution of the transformed S scores for the 148 items.\n\n\n\n\n\nAs a second example, let us compute the dispersion measure DA (Burch, Egbert, and Biber 2017) for the 150 items, this time in the Spoken BNC1994 (four items from the list do not appear in the Spoken BNC1994, so we are looking at only 146 items):\n\nDA_spokenBNC1994 &lt;- disp_DA_tdm(\n  tdm = biber150_spokenBNC1994, \n  row_partsize = \"first\",\n  unit_interval = TRUE,\n  print_score = FALSE,\n  verbose = FALSE)\n\nNow, many scores a clustered near the lower end of the scale:\n\nDA_spokenBNC1994 |&gt; \n    ggplot(aes(x = DA)) +\n    geom_dotplot(\n      method = \"histodot\",\n      binwidth = .025) +\n  xlim(0,1) +\n  theme_dotplot() +\n  xlab(expression(D[A]))\n\n\n\n\n\n\n\nFigure 5: Dot diagram showing the distribution of DA scores for the 146 items.\n\n\n\n\n\nWe can alleviate the skew with a folded power transformations, setting \\(\\lambda = 0.41\\). Recall that this gives us a close approximation to the arcsine-square-root (angular) transformation:\n\nDA_spokenBNC1994 |&gt; \n  ggplot(aes(x = DA)) +\n  geom_dotplot(\n    method = \"histodot\",\n    binwidth = .07) +\n  xlab(\"Transformed DA\\n(folded powers, lambda = 0.41)\") +\n    theme_dotplot() +\n  scale_x_fpower(\n    lambda = .41, \n    breaks = c(0, .02, .1, .25, .5, .75))\n\n\n\n\n\n\n\nFigure 6: Dot diagram showing the distribution of the transformed DA scores for the 146 items.\n\n\n\n\n\nAs a final example, we will draw a scatterplot with a transformed y-axis. The following graph looks at the association between dispersion (as measured by DKL, standardization using base e) and the corpus frequency of the 150 items in the Spoken BNC2014. The top panel shows untransfored dispersion scores on the y-axis, the bottom panel folded-power-transformed scores (\\(\\lambda = 1/3\\), i.e. folded cube roots).\n\n\nDraw figure\nDKL_spokenBNC2014 &lt;- disp_DKL_tdm(\n  tdm = biber150_spokenBNC2014, \n  row_partsize = \"first\",\n  standardization = \"base_e\",\n  print_score = FALSE,\n  verbose = FALSE)\n\np1 &lt;- DKL_spokenBNC2014 |&gt; \n  ggplot(aes(x = frequency, y = DKL)) +\n  geom_point() +\n  ylab(\"\\nDKL\") +\n  xlab(\"Frequency\") +\n    scale_x_log10(labels = scales::comma) +\n  theme_classic() +\n  labs(caption = \" \")\n\np2 &lt;- DKL_spokenBNC2014 |&gt; \n  ggplot(aes(x = frequency, y = DKL)) +\n  geom_point() +\n  ylab(\"Transformed DKL\\n(folded powers, lambda = 1/3)\") +\n  xlab(\"Frequency\") +\n    scale_x_log10(labels = scales::comma) +\n  theme_classic() +\n  scale_y_fpower(\n    lambda = 1/3, \n    breaks = c(0, .01, .1, .5, .9, .99)) +\n  labs(caption = \"Standardization to the unit interval using base e\")\n\n\ncowplot::plot_grid(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 7: Scatterplot graphing DKL scores against frequency for the 148 items.\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAtkinson, A. C. 1985. Plots, Transformations, and Regression: An Introduction to Graphical Methods of Diagnostic Regression Analysis. Oxford: Claredon Press.\n\n\nBiber, Douglas, Randi Reppen, Erin Schnur, and Romy Ghanem. 2016. “On the (Non)utility of Juilland’sDto Measure Lexical Dispersion in Large Corpora.” International Journal of Corpus Linguistics 21 (4): 439–64. https://doi.org/10.1075/ijcl.21.4.01bib.\n\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nFox, John. 2016. Applied Regression Analysis and Generalized Linear Models. London: Sage.\n\n\nMosteller, Frederick, and John W. Tukey. 1977. Data Analysis and Regression: A Second Course in Statistics. Reading, MA: Addison Wesley.\n\n\nRosengren, Inger. 1971. “The Quantitative Concept of Language and Its Relation to the Structure of Frequency Dictionaries.” Etudes de Linguistique Appliquee (Nouvelle Serie) 1: 103–27.\n\n\nSönning, Lukas. 2025. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Reading, MA: Addison Wesley.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Tukey’s Folded Power Transformation in {R}},\n  date = {2025-11-14},\n  url = {https://lsoenning.github.io/posts/2025-11-13_folded_power_transformation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Tukey’s Folded Power Transformation in\nR.” November 14, 2025. https://lsoenning.github.io/posts/2025-11-13_folded_power_transformation/."
  },
  {
    "objectID": "posts/2025-11-17_dispersion_levels_of_analysis/index.html",
    "href": "posts/2025-11-17_dispersion_levels_of_analysis/index.html",
    "title": "Dispersion: Levels of analysis",
    "section": "",
    "text": "R setup\nlibrary(knitr)\n\n\nWhen using parts-based dispersion measures, the choice of corpus parts is critical, as it determines the linguistic meaning of the scores we obtain (Egbert, Burch, and Biber 2020). In this blog post, I outline three basic levels at which dispersion analysis can isolate a particular type of distributional information and return scores with a relatively clear-cut meaning. In applied work, different levels are often combined or mixed, and the resulting scores then blend different types of distributional information. It would be a mistake to go as far as stating that contaminated dispersion scores are linguistically useless. However, their interpretation can be clouded considerably, and the goal of this blog post is to draw attention to this point.\n\nDispersion analysis: Three levels\nConceptually, we can distinguish between three levels at which dispersion can be analyzed. The first one is dispersion within a text. At this level of analysis, the dispersion plot is a very useful graphical tool: It shows us where in a text an item occurs, which provides insights into plotline, discourse structure, and (shifts in) topicality. Parts-based measures can also be applied at this level – the text is then divided into equal-sized chunks (or chapters).\nThe second level at which dispersion can be assessed is within a text category or language variety. This means that the corpus parts chosen for analysis all provide information about the same domain of language use and do not differ systematically in terms of the frequency of the item. It usually makes sense for the corpus parts to represent linguistically meaningful units such as texts, text files or individuals (speakers or authors). At this level, dispersion analysis tells us about the item’s generality in the text category, i.e. how widely and/or evenly the item is spread across texts representing this language variety.\nThe third level we will consider is dispersion across text categories. These may represent different registers or genres, different socio-stylistic categories, or other types of language varieties. Text categories are usually closely aligned with the design of a corpus, and the corpus parts therefore represent categories that are selected purposefully by the corpus compilers, and along which language use is known to vary. At this level, dispersion analysis reveals the register-specificity or social stratification of the item, i.e. the extent to which its usage rate depends on the dimension of variation represented by the text (or speaker) categories.\nHere is a short summary of the three basic levels for dispersion analysis:\n\nLevel I: Within a text (corpus parts: equal-sized chunks) to examine discourse structure\nLevel II: Within a text category (corpus parts: texts/speakers) to assess the generality of the item\nLevel III: Across text categories (corpus parts: text categories) to quantify register-specificity of the item\n\n\n\nBlended levels\nIn corpus-based work, these three levels are often blended. If a corpus is divided into equal-sized chunks, the resulting dispersion score conflates distributional information at all three levels: within-text discourse structure, the generality of the item in each of the text varities covered, and the register-specificity of the item.\nIt is particularly common in corpus analysis to measure dispersion across all text files in a corpus. This approach blends levels II and III, since we are comparing subfrequencies within and across different text categories. Dispersion score therefore conflate information about the generality of the item in each of the text categories as well as the register-specificity of the item, i.e. the degree to which its usage rate varies across the text categories. The amount of influence of each text category on the score depends on its proportional share in the corpus.\nThe combination of levels can yield counter-intuitive results. For illustration, let us consider the distribution of four items in ICE-GB (Nelson, Wallis, and Aarts 2002): another, actually, however, and wants. Frequency information for these items in the 500 text files in ICE-GB is available in the dataset biber150_ice_gb, which is part of the {tlda} package (Sönning 2025).\nLet us consider, as two broad text categories, the spoken part (60% of the text files) and the written part of the corpus (40% of the text files). The following table lists the dispersion scores we obtain for these items (Rosengren’s S (Rosengren 1971), frequency-adjusted using the {tlda} package):\n\nWithin speech (i.e. across the 300 spoken text files)\nWithin writing (i.e. across the 200 written text files)\nIn the whole corpus (i.e. across all 500 text files)\n\n\n\n\n\n\nAnalysis\nanother\nwants\nactually\nhowever\n\n\n\n\nSpoken (n = 300 text files)\n0.59\n0.68\n0.64\n0.45\n\n\nWritten (n = 200 text files)\n0.59\n0.47\n0.66\n0.67\n\n\nWhole corpus (n = 500 text files)\n0.59\n0.59\n0.48\n0.39\n\n\n\n\n\nLet us consider these triples of dispersion scores in turn:\n\nFor another, the results make sense: The dispersion is .59 in speech and .59 writing, and for the whole corpus it is also .59.\nFor wants, the results also make sense: The dispersion score for the whole corpus (.59) is intermediate between that in speech (.68) and that in writing (.47).\nFor actually, we observe very similar dispersion scores in speech (.64) and writing (.66). The score for the whole corpus, however, is .48 and therefore considerably lower.\nFor however, the score for the entire corpus (.39) likewise does not fall in between the one for the spoken (.45) and the written part (.67).\n\nTo understand how the dispersion scores for the whole corpus relate to those for the two text categories, we must take into consideration the frequency of the item in each sub-corpus. If it occurs at similar rates in the two text varieties, the dispersion score for the whole corpus represents a weighted average over the two category-specific scores. This is the case for another and wants, both of which have similar frequencies in speech and writing. The whole-corpus dispersion score for wants gravitates to that in speech since it contributes 60% of the texts to the corpus. The frequency of the other two items differs in the two sub-corpora. The way in which this affects the dispersion score is best considered visually.\n\n\nVisual illustration\nTo get a visual impression of the frequency and dispersion of the four items in ICE-GB, we draw a spike graph for each word form, starting with the item another. In the figure below, each spike shows the number of occurrences of another in one of the 500 text files in ICE-GB. Text files are grouped by mode (marked below the graph) and genre (marked above the figure). The plot looks like a lawn, with relatively few gaps (denoting text files in which the item does not occur) and very similar frequencies across the 500 files. The graph shows that the distributional profile of another, as expressed by its frequency and its dispersion, is very similar in the spoken and written part. The item occurs with an average rate of 1.2 words per text file in both modes.\n\n\n\nSpike graph showing the distribution of another in ICE-GB\n\n\nThe next spike graph shows the distribution of wants in ICE-GB. While the average frequency is very similar in the two modes (0.6 occurrences per text file), the distribution is less balanced in writing (.47) compared to speech (.68). When measuring dispersion across all 500 text files, we obtain a compromise score (.59).\n\n\n\nSpike graph showing the distribution of wants in ICE-GB\n\n\nNext, we consider actually. The spike graph below shows that it is considerably more common in speech (though not in all genres: scripted monologues in fact pattern with written genres). If we isolate speech, by covering the right half of the figure, we note that the usage rate of actually is fairly balanced (S = .64). If we cover the spoken side of the figure, the distribution across written texts likewise shows a good spread (S = .66). If we consider the entire corpus, however, the frequency gap between speech and writing yields a less balanced profile, and therefore a lower dispersion score (.48).\n\n\n\nSpike graph showing the distribution of actually in ICE-GB\n\n\nWe finally consider however, which yields the mirror image of the spike graph for actually. In writing, however is quite dispersed (S = .67), in speech less so (.45) – in private dialogues, it is particularly uncommon. Looking at the whole corpus, the higher usage rate in writing yields an even less balanced profile (S = .39).\n\n\n\nSpike graph showing the distribution of however in ICE-GB\n\n\n\n\nSummary\nWe have seen that dispersion analysis can isolate information on the generality of an item in a particular text category (level-2 analysis), or information on the register-specificity of an item (level-3 analysis). These are distinct pieces of distributional information – the purpose and linguistic goals of the analysis will determine which one is of primary interest. If we measure dispersion across the text files in a structured corpus, i.e. one that is composed of sub-corpora representing different language varieties, we combine these distributional features. This makes it more difficult to interpret (and compare) the scores we obtain.\n\n\n\n\n\nReferences\n\nEgbert, Jesse, Brent Burch, and Douglas Biber. 2020. “Lexical Dispersion and Corpus Design.” International Journal of Corpus Linguistics 25 (1): 89–115. https://doi.org/10.1075/ijcl.18010.egb.\n\n\nNelson, Gerald, Sean Wallis, and Bas Aarts. 2002. Exploring Natural Language: Working with the British Component of the International Corpus of English. John Benjamins. https://doi.org/10.1075/veaw.g29.\n\n\nRosengren, Inger. 1971. “The Quantitative Concept of Language and Its Relation to the Structure of Frequency Dictionaries.” Etudes de Linguistique Appliquee (Nouvelle Serie) 1: 103–27.\n\n\nSönning, Lukas. 2025. Tlda: Tools for Language Data Analysis. https://github.com/lsoenning/tlda.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Dispersion: {Levels} of Analysis},\n  date = {2025-11-17},\n  url = {https://lsoenning.github.io/posts/2025-11-17_dispersion_levels_of_analysis/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Dispersion: Levels of Analysis.”\nNovember 17, 2025. https://lsoenning.github.io/posts/2025-11-17_dispersion_levels_of_analysis/."
  },
  {
    "objectID": "posts/2025-11-15_bootstrapping_dispersion/index.html",
    "href": "posts/2025-11-15_bootstrapping_dispersion/index.html",
    "title": "Bootstrapping dispersion measures",
    "section": "",
    "text": "R setup\n# install development version directly from Github\n#pak::pak(\"lsoenning/tlda\")\n#pak::pak(\"lsoenning/uls\")\n\nlibrary(tlda)      # for access to datasets\nlibrary(tidyverse) # for data wrangling\nlibrary(uls)       # for plotting themes\n\n\n\nBootstrapping\nBootstrapping is a resampling method that has many different uses in data analysis. One of its key functions is to aid in the construction of confidence intervals. For helpful introductions to the use of bootstrapping in corpus linguistics, see Egbert and Plonsky (2020) and Gries (2022).\nIn dispersion analysis, bootstrapping can be used to assess the statistical uncertainty surrounding a dispersion returned by our analysis. This information is provided by a confidence interval (CI). As there is no statistical theory that would permit us to mathematically construct CIs for any of the commonly used parts-based dispersion indices, we need to rely on bootstrapping as a work-around strategy.\nThe technique works as follows. We draw a large number of bootstrap samples (say, 800) from the original data and then calculate, for each bootstrap sample, the quantity of interest (e.g. a dispersion score). This yields a distribution of bootstrap estimates. Each bootstrap sample has the same size as the original data and is obtained by randomly sampling with replacement from the original data. This means that some observations will occur multiple times in the bootstrap sample while others will not appear at all in it.\nThe distribution of the bootstrap estimates reflects the statistical variation associated with the sample statistic of interest. The average (mean or median) of this distribution is sometimes referred to as the bootstrap estimator. A percentile bootstrap confidence interval is then obtained by finding the appropriate quantiles of the distribution: For a 90% percentile bootstrap CI, we locate the .05 and the .95 quantile of the distribution, which enclose the middle 90% of the bootstrap estimates.\n\n\nData structure and bootstrap sampling\nIn general, the resampling scheme that is used to sample from the original data with replacement should align with the data-generating process. In corpus linguistics, this means that the resampling units should correspond to the sampling units in corpus compilation. These are the text files and speakers making up the corpus. Thus, corpus construction usually starts with a sampling frame, which delineates the range (and proportional share) of text categories, socio-demographic strata, or language varieties in general that the corpus is meant to represent. Then, within each category, texts, text excerpts, or speakers are selected or (randomly) sampled. Since the bootstrapping scheme should generally be coherent with the original sampling scheme, bootstrapping corpus data usually involves resampling texts, text files, or speakers (see Gries 2022).\nFor dispersion analysis, this means that it should always be these kinds of units (or corpus parts) that are selected with replacement. If it is across these units that dispersion is measured, bootstrapping is particularly straightforward. If corpus parts represent higher-level categories such as registers, genres, or socio-demographic groups, the bootstrapping procedure becomes a bit more complex. We will deal with both scenarios in turn.\n\n\nSimple bootstrapping: Dispersion across texts/speakers in a single text category\nWhen measuring dispersion across texts within a single text category, we learn about the generality of the item in this domain of language use (see this blog post). In this analysis setting, bootstrapping is simple. This is because, within a specific text category, corpus compilation can be considered as proceeding by simple random sampling: Each unit in the population has the same probability of being selected. Bootstrap samples are therefore drawn in the same way: By simple random sampling (with replacement) from the pool of text files in the data. This most basic type of bootstrap sampling is sometimes called case resampling.\nLet us consider, as an example, the item thirteen in the Spoken BNC2014. There are 668 speakers in this corpus, and speakers will be our corpus parts. We measure dispersion using Gries’s (2008) deviation of proportions (specifically, the modification proposed by Egbert, Burch, and Biber 2020). Scores will be scaled the conventional way, with 0 representing a maximally uneven, and 1 a maximally even distribution.\nWe will draw 500 bootstrap samples, with each being drawn randomly with replacement from the pool of 668 speakers. Recall the each bootstrap sample consists of 668 “speakers”, where a considerable number of individuals will appear (i) multiple times or (ii) not at all in the bootstrap sample. The figure below shows the distribution of the 500 bootstrap estimates for DP.\n\n\nBootstrap DP and draw graph\nset.seed(2025)\n\nDP_boot &lt;- disp_DP_boot(\n    subfreq = biber150_spokenBNC2014[130,],\n    partsize = biber150_spokenBNC2014[1,],\n    freq_adjust = TRUE,\n    n_boot = 500,\n    return_distribution = TRUE,\n    print_score = FALSE\n)\n\n\n\nBased on 500 bootstrap samples (random sampling with replacement)\n  Dispersion score is the median over the 500 bootstrap samples\n  Unweighted estimate (all corpus parts weighted equally)\n\nThe dispersion score is adjusted for frequency using the min-max\n  transformation (see Gries 2024: 196-208); please note that the\n  method implemented here does not work well if corpus parts differ\n  considerably in size; see vignette('frequency-adjustment')\n\nFor 0 bootstrap samples, the frequency-adjusted score exceeds the limits\n  of the unit interval [0,1]; these scores were replaced by 0 or 1\n\nScores follow conventional scaling:\n  0 = maximally uneven/bursty/concentrated distribution (pessimum)\n  1 = maximally even/dispersed/balanced distribution (optimum)\n\nComputed using the modification suggested by Egbert et al. (2020)\n\n\nBootstrap DP and draw graph\ndata.frame(\n    DP = DP_boot) |&gt; \n    ggplot(aes(\n        x = DP)) +\n    geom_dotplot(method = \"histodot\",\n                 binwidth = .007, \n                 dotsize = .6, \n                 stackratio = 1.2) +\n    scale_x_continuous(limits = c(0,1), expand = c(0,0),\n                       labels = c(\"0\", \".25\", \".50\", \".75\", \"1\")) +\n    scale_y_continuous(limits = c(0, 1.1), expand = c(.005, .005)) +\n    annotate(\"point\", x = median(DP_boot), y = 1.05) +\n    annotate(\"errorbar\", \n             xmin = quantile(DP_boot, .05),\n             xmax = quantile(DP_boot, .95), \n             y = 1.05, width = 0.03) +\n    theme_dotplot() +\n    xlab(expression(D[P]))\n\n\n\n\n\nDot diagramm showing the distribution of the 500 bootstrap estimates, as well as their median and a 90% percentile confidence interval.\n\n\n\n\nAbove the pile of dots, a filled circle marks the median of this distribution, and the error bars extend from the .05 to the .95 quantile of the distribution, marking a 90% percentile bootstrap CI.\n\n\nStratified bootstrapping: Dispersion across text categories\nWhen dispersion is measured across text categories, the goal is to quantify the register-specificity of an item (see this blog post). At this level of analysis, the corpus parts no longer coincide with the sampling units. Since it is preferable for bootstrapping to mimic the data-collection procedure as closely as possible, we need to resort to a different sampling scheme. The variant we will adopt is referred to as stratified bootstrapping. This resampling scheme takes into account the way in which the texts are organized, which means that it draws on those variables that informed data collection. A helpful blog post on stratified bootstrapping can be found here.\nBefore we go further, we should note that it does not make sense, in general, to resample text categories (i.e. sub-corpora). This is because text categories are (usually) not sampled during corpus compilation. Instead, they are delineated prior to data collection, sometimes in the form of a sampling frame.\nIn keeping with corpus creation, then, we must resample text files using what is referred to as stratified bootstrapping. The text categories then form the strata, and a bootstrap sample is composed of subsamples, one per stratum. From within each text category, text files are then drawn randomly with replacement.\nTo illustrate, let’s assume our corpus consists of four genres (A, B, C, and D), which form the corpus parts for a dispersion analysis:\n\nSub-corpus A: 20 text files\nSub-corpus B: 30 text files\nSub-corpus C: 50 text files\nSub-corpus D: 20 text files\n\nFor a stratified bootstrap sample, observations are randomly sampled (with replacement) within each stratum (here: sub-corpus). The number of observations sampled from each stratum is equal to its size. A bootstrap sample therefore looks as follows:\n\n20 text files drawn randomly and with replacement from sub-corpus A\n30 text files drawn randomly and with replacement from sub-corpus B\n50 text files drawn randomly and with replacement from sub-corpus C\n20 text files drawn randomly and with replacement from sub-corpus D\n\nTo measure dispersion across the four genres, the bootstrap sample must then be aggregated: For each corpus part (i.e. genre), the subfrequency and partsize are determined by summing the relevant counts across all text files that ended up in the respective bootstrap subsample. The dispersion score is calculated based on these aggregated tallies, to obtain the bootstrap estimate for this bootstrap sample.\nIn general, dispersion analysis needs to resort to stratified (instead of simple) bootstrapping if the corpus parts represent higher-level categories, above the level of the sampling units (texts and speakers). We can therefore safely talk about corpus parts (instead of strata), which simplifies the terminology.\n\n\nImplementation in R\n\nSimple bootstrapping\nThe {tlda} package includes a set of functions suffixed by _boot, which implement simple bootstrapping. To repeat the analysis of thirteen in the Spoken BNC2014, we use the function disp_DP_boot(). The item is sitting in row 130 of the term-document matrix biber150_spokenBNC2014 (available in the {tlda} package). We supply the following arguments:\n\nn_boot = 1000 for increasing the number of bootstrap samples (default: 500)\nboot_ci = TRUE for a percentile bootstrap confidence interval\nconf_level = .90 for a 90% CI (default: 95%)\n\n\ndisp_DP_boot(\n    subfreq = biber150_spokenBNC2014[130,],\n    partsize = biber150_spokenBNC2014[1,],\n    n_boot = 1000,\n    boot_ci = TRUE, \n    conf_level = .90\n)\n\n       DP  conf.low conf.high \n0.5860616 0.5393471 0.6275054 \n\nBased on 1000 bootstrap samples (random sampling with replacement)\n  Median and 90% percentile confidence interval limits\n  Unweighted estimate (all corpus parts weighted equally)\n\nFor 0 bootstrap samples, the frequency-adjusted score exceeds the limits\n  of the unit interval [0,1]; these scores were replaced by 0 or 1\n\nScores follow conventional scaling:\n  0 = maximally uneven/bursty/concentrated distribution (pessimum)\n  1 = maximally even/dispersed/balanced distribution (optimum)\n\nComputed using the modification suggested by Egbert et al. (2020)\n\n\nWe can also instruct the function to return the bootstrap estimates:\n\nreturn_distribution = TRUE to obtain the distribution of bootstrap estimates\nprint_score = FALSE to stop the function from printing all 1,000 estimates to the console\nverbose = FALSE to suppress the information print-out (which is identical to the one above)\n\n\nDP_distribution &lt;- disp_DP_boot(\n    subfreq = biber150_spokenBNC2014[130,],\n    partsize = biber150_spokenBNC2014[1,],\n    n_boot = 1000,\n    return_distribution = TRUE,\n    print_score = FALSE,\n    verbose = FALSE\n)\n\nWe can then graph this distribution:\n\ndata.frame(\n    DP = DP_distribution) |&gt; \n    ggplot(aes(\n        x = DP)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\n\n\nStratified bootstrapping\nThe function disp_DP_sboot() in the {tlda} package implements stratified bootstrapping. For illustration, let us consider the item methods in the Brown Corpus. We will look at its dispersion across the genres (n = 15) in the corpus. To implement this scheme, we need the following pieces of information about each text file in the corpus:\n\nits ID (optional, but good to have)\nits subfrequency (i.e. how often methods occurs in it)\nits length\nthe genre to which it belongs\n\nRelevant text-level tallies are available in row 87 of the data set biber150_brown:\n\nt(biber150_brown[c(1,87),1:10])\n\n    word_count methods\nA01       2242       0\nA02       2277       2\nA03       2275       0\nA04       2216       0\nA05       2244       0\nA06       2263       0\nA07       2270       1\nA08       2187       0\nA09       2234       0\nA10       2282       2\n\n\nWhat is missing, however, is information about genre, i.e. the corpus parts across which we want to measure dispersion. Metadata for the Brown Corpus is available in the dataset metadata_brown in the {tlda} package:\n\nstr(metadata_brown)\n\n'data.frame':   500 obs. of  4 variables:\n $ text_file  : chr  \"A01\" \"A02\" \"A03\" \"A04\" ...\n $ macro_genre: Ord.factor w/ 4 levels \"press\"&lt;\"general_prose\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre      : Ord.factor w/ 15 levels \"press_reportage\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ word_count : int  2258 2242 2277 2275 2216 2244 2263 2270 2187 2234 ...\n\n\nThe following code adds genre information to the text-level tallies above:\n\nd &lt;- data.frame(t(biber150_brown[c(1,87),]))\nd$text_file &lt;- rownames(d)\n\nd &lt;- full_join(\n    d, \n    metadata_brown[,-4],\n    by = \"text_file\")\n\nLet’s see whether this was successful:\n\nstr(d)\n\n'data.frame':   500 obs. of  5 variables:\n $ word_count : num  2242 2277 2275 2216 2244 ...\n $ methods    : num  0 2 0 0 0 0 1 0 0 2 ...\n $ text_file  : chr  \"A01\" \"A02\" \"A03\" \"A04\" ...\n $ macro_genre: Ord.factor w/ 4 levels \"press\"&lt;\"general_prose\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ genre      : Ord.factor w/ 15 levels \"press_reportage\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nNow we are good to go. Here is a brief run-down of the ensuing analysis:\n\nBrown Corpus\nItem: methods\nCorpus parts: Genres (n = 15)\nDispersion measure: DP\n\nUsing modified formula suggested by Egbert, Burch, and Biber (2020)\nWith frequency adjustment\nUsing conventional scaling\n\nBootstrapping\n\nStratified bootstrapping (sampling text files within genres)\n1,000 bootstrap samples\n95% percentile CI\n\n\nWe use the function disp_DP_sboot() bootstrap Gries’ deviation of proportions:\n\ntext_freq: the number of times the item occurs in each text\ntext_size: the length of the text files\ncorpus_parts: the corpus parts for the dispersion analysis (and strata for stratified resampling)\n\n\ndisp_DP_sboot(\n    text_freq = d$methods,\n    text_size = d$word_count,\n    corpus_parts = d$genre,\n    n_boot = 1000,\n    boot_ci = TRUE, \n    freq_adjust = TRUE,\n    formula = \"egbert_etal2020\",\n    directionality = \"conventional\"\n)\n\nDP_nofreq  conf.low conf.high \n0.6221049 0.5370112 0.6970551 \n\nBased on 1000 stratified bootstrap samples (sampled with replacement)\n  Median and 95% percentile confidence interval limits\n  Unweighted estimate (all corpus parts weighted equally)\n\nThe dispersion score is adjusted for frequency using the min-max\n  transformation (see Gries 2024: 196-208); please note that the\n  method implemented here does not work well if corpus parts differ\n  considerably in size; see vignette('frequency-adjustment')\n\nFor 0 bootstrap samples, the frequency-adjusted score exceeds the limits\n  of the unit interval [0,1]; these scores were replaced by 0 or 1\n\nScores follow conventional scaling:\n  0 = maximally uneven/bursty/concentrated distribution (pessimum)\n  1 = maximally even/dispersed/balanced distribution (optimum)\n\nComputed using the modification suggested by Egbert et al. (2020)\n\n\n\n\n\n\n\n\nReferences\n\nEgbert, Jesse, Brent Burch, and Douglas Biber. 2020. “Lexical Dispersion and Corpus Design.” International Journal of Corpus Linguistics 25 (1): 89–115. https://doi.org/10.1075/ijcl.18010.egb.\n\n\nEgbert, Jesse, and Luke Plonsky. 2020. “Bootstrapping Techniques.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 593–610. Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_24.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2022. “Toward More Careful Corpus Statistics: Uncertainty Estimates for Frequencies, Dispersions, Association Measures, and More.” Research Methods in Applied Linguistics 1 (1): 100002. https://doi.org/10.1016/j.rmal.2021.100002.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Bootstrapping Dispersion Measures},\n  date = {2025-11-18},\n  url = {https://lsoenning.github.io/posts/2025-11-15_bootstrapping_dispersion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Bootstrapping Dispersion Measures.”\nNovember 18, 2025. https://lsoenning.github.io/posts/2025-11-15_bootstrapping_dispersion/."
  },
  {
    "objectID": "posts/2025-11-28_drawing_panel_charts/index.html",
    "href": "posts/2025-11-28_drawing_panel_charts/index.html",
    "title": "Drawing panel charts in R",
    "section": "",
    "text": "I recently wrote a paper on the use of bar charts in corpus-linguistic research articles (see here). One of the questions of this systematic review was whether the bar chart is always the best choice. In Section 6.1 of the paper I discuss panel charts (see Camões (2016, 217–18), Schwabish (2021, 88–89)) as an alternative to stacked bar charts. In this blog post, I describe how to draw such a chart using the R package {ggplot2}.\n\n\nR setup\nlibrary(tidyverse)\n\n\n\nData\nFor illustration, we use data from Larsson et al. (2020), a nice corpus study that looks at adverb placement in learner writing. These data are also the basis of Figure 6 and Figure 17 of the bar chart paper referenced above. They include frequency counts for cross-classifications of two categorical variables:\n\nadverb: n = 13 levels\nposition: n = 4 levels\n\nThe counts reflect how often a particular adverb was observed in a particular position in the clause.\n\nd &lt;- data.frame(\n    adverb = rep(c(\n      \"maybe\", \"perhaps\", \"of course\", \"possibly\", \"apparently\", \n      \"obviously\", \"certainly\", \"actually\", \"clearly\", \"simply\", \n      \"probably\", \"definitely\", \"really\"), 4),\n    position = rep(c(\"M3\", \"M2\", \"M1\", \"I\"), each = 13),\n    count = c(\n      13, 83, 85, 10, 25, 50, 61, 124, 138, 152, 169, 36, 58,\n      12, 70, 57, 66, 12, 40, 47, 181, 54, 154, 163, 43, 100,\n      8, 24, 36, 21, 31, 48, 47, 305, 142, 171, 119, 16, 121,\n      61, 112, 105, 26, 16, 31, 17, 61, 28, 33, 28, 4, 5)\n)\n\nLet’s look at the contents of the data frame:\n\nstr(d)\n\n'data.frame':   52 obs. of  3 variables:\n $ adverb  : chr  \"maybe\" \"perhaps\" \"of course\" \"possibly\" ...\n $ position: chr  \"M3\" \"M3\" \"M3\" \"M3\" ...\n $ count   : num  13 83 85 10 25 50 61 124 138 152 ...\n\n\nThe next step is to add proportions to the data frame. These will reflect the relative frequency of the four positions within each adverb. For each adverb, the proportions therefore sum to 1. The {dplyr} package is useful for this task.\n\nd &lt;- d |&gt; \n  group_by(adverb) |&gt; \n    mutate(n_total = sum(count)) |&gt; \n    ungroup() |&gt; \n    mutate(prop = count / n_total)\n\nIn principle, we are now good to go. However, I would also like to be able to order the adverbs in the plot according to the proportional share of position I (initial), so we also add a new column giving this value.\n\nd &lt;- d |&gt; \n    group_by(adverb) |&gt; \n    mutate(prop_initial = prop[position == \"I\"]) |&gt; \n    mutate(position = fct_rev(position))\n\nHere is what the first few rows of the data frame look like:\n\nhead(d)\n\n# A tibble: 6 × 6\n# Groups:   adverb [6]\n  adverb     position count n_total   prop prop_initial\n  &lt;chr&gt;      &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n1 maybe      M3          13      94 0.138         0.649\n2 perhaps    M3          83     289 0.287         0.388\n3 of course  M3          85     283 0.300         0.371\n4 possibly   M3          10     123 0.0813        0.211\n5 apparently M3          25      84 0.298         0.190\n6 obviously  M3          50     169 0.296         0.183\n\n\n\n\nDraw a stacked bar chart\nWe start by drawing a stacked bar chart, the rudimentary version first.\n\nd |&gt; \n    ggplot(aes(\n      x = reorder(adverb,          # adverb on the x-axis\n                  -prop_initial),  # order adverbs by prop_initial\n      y = prop,                    # the proportional share as bar segments\n      fill = position)) +          # position as a fill variable\n    geom_col() +                   # geom for bar segments\n    scale_fill_grey()              # use grey fill colors\n\n\n\n\n\n\n\n\nAnd the following enhanced plotting call creates a graph that mimics Figure 3 in Larsson et al. (2020), and which appears as Figure 6 in the bar chart paper.\n\nd |&gt; \n    ggplot(aes(\n      x = reorder(adverb, \n                  -prop_initial), \n      y = prop, \n      fill = position)) +\n    geom_col(\n      col = 1,                      # add black contours around the bar segments\n      linewidth = .4,               # make these contours thin         \n      width = .6) +                 # reduce width of the columns of bars\n    scale_fill_grey(\n      start = .4, end = 1) +        # customize shades of grey\n    ylab(\"Percentage of tokens\") +  # new y-axis title\n  xlab(NULL) +                    # remove x-axis title\n    theme_minimal() +               # use different theme\n    scale_x_discrete(\n      expand = c(.05, .05)) +       # control padding at left/right margin of plot\n    scale_y_continuous(\n      labels = c(0, 25, 50, 75, 100), # define locations of tick marks on y-axis\n      expand = c(.01, .01),           # control padding at top/bottom margin of plot\n      minor_breaks = NULL) +          # omit minor grid lines\n  theme(\n    axis.text.x = element_text(            # modify tick mark labels on x-axis\n      angle = 45,                          #   rotate by 45 degrees\n      hjust=1.2,                           #   control horizontal adjustment\n      vjust=1.3,                           #   control vertical adjustment\n      face = \"italic\"),                    #   print adverbs in italics\n    panel.grid.major.x = element_blank(),  # no vertical grid lines\n    legend.key.size = unit(.3, 'cm'))      # reduce size of tiles in key\n\n\n\n\n\n\n\n\n\n\nPanel chart\nNext, we draw a panel chart, basic version first. Since we want to order the levels of position logically (initial at the far left), we must first reorder the factor levels of this variable:\n\nd &lt;- d |&gt; \n    mutate(position = fct_rev(position))\n\nTo switch to a panel chart, the key change is to make position into a faceting variable, which means that the different positions will appear in different panels. This requires adding four lines of code, which call facet_grid(). The arguments of this function do the following:\n\nthe first argument uses formula notation to state how the panels representing different levles of position should be arrange in the grid:\n\n. ~ position: side by side, in a single row\nposition ~ .: on top of one another, as a single column\n\nscales = \"free_x\" says that the panels need not have the same limits\nspace = \"free_x\" says that the panels need not have the same physical width\n\n\nd |&gt;  \n    ggplot(aes(\n      y = reorder(adverb, -prop_initial), \n      x = prop, \n      fill = position)) +\n    geom_col() +\n    scale_fill_grey() +\n    facet_grid(\n      . ~ position,      # draw separate facets for position, in rows\n      scales = \"free_x\", # allow upper limits of x-axis to vary across facets\n      space = \"free_x\")  # allow width of facets to vary\n\n\n\n\n\n\n\n\nAnd the following enhanced code draws the version that appears as Figure 17 in the bar chart paper.\n\nd |&gt;  \n  ggplot(aes(\n    y = reorder(adverb, -prop_initial), \n    x = prop, \n    fill = position)) +\n    geom_col(\n      col = 1,                      # add black contours around the bar segments\n      linewidth = .4,               # make these contours thin         \n      width = .6) +                 # reduce width of the columns of bars\n    scale_fill_grey(\n      start = .4, end = 1) +        # customize shades of grey\n    ylab(\"Percentage of tokens\") +  # new y-axis title\n  xlab(NULL) +                    # remove x-axis title\n    theme_minimal() +               # use different theme\n  facet_grid(\n    . ~ position, \n    scales = \"free_x\", \n    space = \"free_x\") +\n  geom_vline(xintercept = 0) +    # add a black reference line at 0\n  scale_x_continuous(\n    breaks = c(0, .2, .4, .6),    # define locations of tick marks on y-axis\n    labels = c(0, 20, 40, 60),    # define tick mark labels\n      expand = c(.01, .01),         # control padding at top/bottom margin of plot\n      minor_breaks = NULL) +        # omit minor grid lines\n  theme(\n    legend.position = \"none\",              # omit legend\n    panel.grid.major.y = element_blank(),  # no horizontal grid lines\n    axis.text.y = element_text(\n      face = \"italic\"),                    # print adverbs in italics\n    panel.spacing = unit(1.2, \"lines\"))    # adjust spacing between facets\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCamões, Jorge. 2016. Data at Work: Best Practices for Creating Effective Charts and Information Graphics in Microsoft Excel. New Riders.\n\n\nLarsson, Tove, Marcus Callies, Hilde Hasselgård, Natalia Judith Laso, Sanne van Vuuren, Isabel Verdaguer, and Magali Paquot. 2020. “Adverb Placement in EFL Academic Writing: Going Beyond Syntactic Transfer.” International Journal of Corpus Linguistics 25 (2): 156–85. https://doi.org/10.1075/ijcl.19131.lar.\n\n\nSchwabish, Johnathan. 2021. Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks. New York: Columbia University Press.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Drawing Panel Charts in {R}},\n  date = {2025-11-29},\n  url = {https://lsoenning.github.io/posts/2025-11-28_drawing_panel_charts/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Drawing Panel Charts in R.” November\n29, 2025. https://lsoenning.github.io/posts/2025-11-28_drawing_panel_charts/."
  },
  {
    "objectID": "posts/2025-11-29_drawing_grouped_dotplots/index.html",
    "href": "posts/2025-11-29_drawing_grouped_dotplots/index.html",
    "title": "Drawing grouped dot plots in R",
    "section": "",
    "text": "The dot plot was introduced by Cleveland (1984) to address some of the shortcomings of bar charts. In Sönning (2016), I discuss and illustrate the potential of this graph type for language data analysis and presentation. In the course of writing that paper, I also created templates for Microsoft Excel, which make it easy to draw such graphs. They can be found here.\nIn a recent study, I looked at the use of bar charts in corpus-linguistic research articles. In this systematic review, I came across settings in which a grouped bar chart could have been fruitfully replaced by a grouped dot plot. In this blog post, I describe how to draw such a chart using the R package {ggplot2}.\n\n\nR setup\nlibrary(tidyverse)\n\n\n\nData\nFor illustration, I will use data from Maguire, Wisniewski, and Storms (2010), a corpus study on semantic patterns in compounds consisting of two nominal constituents. These data are also the basis of Figure 5 and Figure 16 of the bar chart paper referenced above. The data include percentages for cross-classifications of two categorical variables:\n\nCategory: semantic category of the nominal constituent, n = 25 levels\nFunction: function of the nominal constituent in the compound, n = 2 levels (“Modifier” vs. “Head”)\n\nThe percentages, which are approximations taken from Figure 2 of Maguire, Wisniewski, and Storms (2010), express the rate at which nouns in these semantic categories appear in compounds, using as a baseline the total number of occurrences of nouns in the category in the corpus. These percentages are reported separately for the two functional roles, modifier and head.\n\nd &lt;- data.frame(\n  Category = rep(c(\n    \"Act\", \"Animal\", \"Artifact\", \"Attribute\", \"Body\", \"Cognition\",\n    \"Communication\", \"Event\", \"Feeling\", \"Food\", \"Group\", \"Location\",\n    \"Motive\", \"Object\", \"Person\", \"Phenomenon\", \"Plant\", \"Possession\",\n    \"Process\", \"Quantity\", \"Relation\", \"Shape\", \"State\", \"Substance\", \"Time\"),\n    each = 2),\n  Function = factor(rep(c(\"Modifier\", \"Head\"), 25)),\n  Percent = c(\n    5.9, 3.7, 7.6, 2.2, 6.9, 4.3, 2, 2.1, 6.2, 1.5, 2, 3.1, \n    3.3, 3.2, 5.3, 2.9, \n    1.3, 0.7, 6.4, 3.4, 4.8, 4.7, 9.7, 2.2, 4, 2.2, 8.1, 3.1,\n    3.2, 3.4, 7.1, 2.6, 10.5, 4.2, 8.9, 4.7, 4.2, 5.4, 6, 1.8,\n    5.9, 5.6, 4.1, 2.3, 5, 2.5, 13.2, 3.7, 5.3, 1.1)*2.5\n)\n\nTo mimic the original chart as closely as possible, we want the Modifier category to appear to the left of the Head category in each cluster of bars. We must therefore reorder the factor levels of the variable Function; the {dplyr} package is useful for this task.\n\nd &lt;- d |&gt; mutate(\n  Function = fct_rev(Function))\n\nLet’s print the first few rows of the table. Rows 1 and 2 represent nouns belonging to the semantic category Act. Out of all Act nouns in the corpus, 14.75% occur in the modifier slot of a N + N compound; 9.25% appear as the head noun of a N + N compound. This means that, overall, 24% of Act nouns (14.75% + 9.25%) occurred in N + N compounds.\n\nhead(d)\n\n  Category Function Percent\n1      Act Modifier   14.75\n2      Act     Head    9.25\n3   Animal Modifier   19.00\n4   Animal     Head    5.50\n5 Artifact Modifier   17.25\n6 Artifact     Head   10.75\n\n\n\n\nDraw a grouped bar chart\nWe start by drawing a grouped bar chart, the rudimentary version first.\n\nd |&gt; \n  ggplot(aes(\n    x = Category,         # Category on the x-axis\n    y = Percent,          # the percentage as bar segments\n    fill = Function)) +   # Function as a fill variable\n  geom_col(               # geom for bar segments\n    position = \"dodge\") + # dodging for *grouped* bar chart\n  scale_fill_grey()       # use grey fill colors\n\n\n\n\n\n\n\n\nAnd the following enhanced plotting call creates a graph that mimics Figure 2 in Maguire, Wisniewski, and Storms (2010), and which appears as Figure 5 in the bar chart paper. Note that the semantic categories on the x-axis appear in alphabetical order, which rarely makes sense in data visualization.\n\nd |&gt; \n  ggplot(aes(\n    x = Category, \n    y = Percent, \n    fill = Function)) +\n  geom_col(\n    position = \"dodge\", \n    col = 1,                      # add black contours around the bars\n    width = .66) +                # make these contours thin\n  scale_fill_grey(\n    start = .8, end = .6) +       # customize shades of grey\n  ylab(\"Percentage of tokens\") +  # add y-axis title\n  xlab(NULL) +                    # remove x-axis title\n  theme_minimal() +               # use different theme\n  scale_x_discrete(\n    expand = c(.02, .02)) +       # control padding at left/right margin\n  scale_y_continuous(\n    expand = c(.01, .01),         # control padding at top/bottom margin\n    minor_breaks = NULL) +        # omit minor grid lines\n  theme(\n    axis.text.x = element_text(   # modify tick mark labels on x-axis\n      angle = 90,                 #   rotate by 90 degrees\n      hjust = 1,                  #   control horizontal adjustment\n      vjust = .5,                 #   control vertical adjustment\n      size = 10),                 #   control font size\n    panel.grid.major.x = element_blank(),  # no vertical grid lines\n    legend.position = \"top\",               # remove key\n    legend.key.size = unit(.3, 'cm'))      # reduce size of tiles in key\n\n\n\n\n\n\n\n\n\n\nGrouped dot pots\nNext, we draw a grouped dot plot. To be able to order the semantic categories in the plot in a sensible way, we need to add new columns to the data frame. There are (at least) three ways in which we could rearrange them:\n\nbased on the proportional share of the Modifier category\nbased on the proportional share of the Head category, or\nbased on the difference between these percentages\n\nWe add three new columns that record this information:\n\nd &lt;- d |&gt; \n    group_by(Category) |&gt; \n    mutate(\n        percent_head = Percent[Function == \"Head\"],\n        percent_modifier = Percent[Function == \"Modifier\"],\n        difference = percent_head - percent_modifier)\n\nWe start by drawing a basic version of the grouped dot plot:\n\nd |&gt; \n    ggplot(aes(\n      y = reorder(\n        Category,           # Category on the y-axis\n        percent_modifier),  # order categories by percent_modifier\n      x = Percent,          # the percentage on the x-axis\n      shape = Function)) +  # different plotting symbols for Function\n    geom_point()            # geom for points\n\n\n\n\n\n\n\n\nNext, we reproduce the version that appears as Figure 16 in the bar chart paper. This version includes an appended panel that directly shows the difference between the two groups that appear in the dot plot; see Sönning (2016, 113); also Amit, Heiberger, and Lane (2008) and Heiberger and Holland (2015, 566). This means that we draw two plots, which we then combine into a single display with the function plot_grid() in the {cowplot} package (Wilke 2024).\nWe start by drawing the main panel, which we store as a new plot object p1:\n\np1 &lt;- d |&gt; \n  ggplot(aes(\n    y = reorder(\n      Category,                  # Category on the y-axis\n      percent_modifier),         # order categories by percent_modifier\n    x = Percent,                 # the percentage on the x-axis\n    shape = Function)) +         # different plotting symbols for Function\n  geom_point(                    # geom for points\n    size = rep(c(1.2, 1), 25)) + # control size of plotting symbols\n  scale_shape_manual(\n    values = c(1, 3)) +          # control choice of plotting symbols\n  xlab(\"Percentage of tokens\") + # new x-axis title\n  ylab(NULL) +                   # remove y-axis title\n  geom_vline(xintercept = 0) +   # add reference line at 0\n  theme_minimal() +              # use different theme\n  scale_y_discrete(\n    expand = c(.02, .02)) +      # control padding at top/bottom margin\n  scale_x_continuous(\n    expand = c(.02, .02),        # control padding at left/right margin\n    limits = c(0, NA),           # control limits of x-axis\n    minor_breaks = NULL) +       # omit minor grid lines\n  annotate(\"text\",               # add direct label \"Head\"\n           x = 25, y = 20,       #   position\n           label = \"Head\",       #   label\n           size = 3.1,           #   font size\n           color = \"grey30\") +   #   color\n  annotate(\"text\",               # add direct label \"Modifier\"\n           x = 15, y = 25,       #   position\n           label = \"Modifier\",   #   label\n           size = 3.1,           #   font size\n           color = \"grey30\") +   #   color\n  theme(\n    legend.position = \"none\")    # omit legend\n\nThe we draw the appended panel, which we assign to object p2. We have to be careful to use the same parameter settings as for p1 (relevant lines are marked by (!) in the code).\n\np2 &lt;- d |&gt; \n  ggplot(aes(\n    y = reorder(Category,          # (!) Category on the y-axis\n                percent_modifier), # (!) order categories by percent_modifier\n    x = difference)) +             # percentage point difference on the x-axis\n  geom_point(size = 1.1) +         # control size of plotting symbols\n  scale_shape_manual(\n    values = 19) +                 # control choice of plotting symbol\n  xlab(\"Difference\") +             # new x-axis title\n  ylab(NULL) +                     # remove y-axis title\n  geom_vline(xintercept = 0) +     # add reference line at 0\n  theme_minimal() +                # use different theme\n  scale_y_discrete(\n    expand = c(.02, .02)) +        # (!) control padding at top/bottom margin\n  scale_x_continuous(\n    expand = c(.05, .05),          # control padding at left/right margin\n    breaks = c(-20, -10, 0),       # control location of tick marks\n    labels = c(\"\\u221220\", \"\\u221210\", \"0\"), # tick mark labels: proper minus signs\n    minor_breaks = NULL) +                   # omit minor grid lines\n  theme(axis.text.y = element_blank())       # omit tick mark labels on y-axis\n\nThen we combine and print the two panels:\n\ncowplot::plot_grid(\n  p1, NULL, p2, \n  nrow = 1, \n  rel_widths = c(2,.05,1.2))\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAmit, Ohad, Richard M. Heiberger, and Peter W. Lane. 2008. “Graphical Approaches to the Analysis of Safety Data from Clinical Trials.” Pharmaceutical Statistics 7 (1): 20–35. https://doi.org/10.1002/pst.254.\n\n\nCleveland, William S. 1984. “Graphical Methods for Data Presentation: Full Scale Breaks, Dot Charts, and Multibased Logging.” The American Statistician 38 (4): 270–80. https://doi.org/10.1080/00031305.1984.10483224.\n\n\nHeiberger, Richard M., and Burt Holland. 2015. Statistical Analysis and Data Display: An Intermediate Course with Examples in r. New York: Springer.\n\n\nMaguire, Phil, Edward J. Wisniewski, and Gert Storms. 2010. “A Corpus Study of Semantic Patterns in Compounding.” Corpus Linguistics and Linguistic Theory 6 (1). https://doi.org/10.1515/cllt.2010.003.\n\n\nSönning, Lukas. 2016. “The Dot Plot: A Graphical Tool for Data Analysis and Presentation.” In A Blend of MaLT: Selected Contributions from the Methods and Linguistic Theories Symposium, edited by Hanna Christ, Daniel Klenovšak, Lukas Sönning, and Valentin Werner, 101–29. Bamberg: University of Bamberg Press. https://doi.org/10.20378/irbo-51101.\n\n\nWilke, Claus O. 2024. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://doi.org/10.32614/CRAN.package.cowplot.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Drawing Grouped Dot Plots in {R}},\n  date = {2025-11-30},\n  url = {https://lsoenning.github.io/posts/2025-11-28_drawing_grouped_dotplots/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Drawing Grouped Dot Plots in R.”\nNovember 30, 2025. https://lsoenning.github.io/posts/2025-11-28_drawing_grouped_dotplots/."
  },
  {
    "objectID": "posts/2024-03-01_sharoff_2018/index.html",
    "href": "posts/2024-03-01_sharoff_2018/index.html",
    "title": "Custom scoring systems in ordinal data analysis: A tribute to Sharoff (2018)",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(uls)         # pak::pak(\"lsoenning/uls\")\n\ndirectory_path &lt;- \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2024-03-01_sharoff_2018/\"\n\n\nI recently did a systematic review on how ordinal rating scale data are handled in linguistic research (see Sönning 2024). It included 4,441 publications from 16 linguistic journals (published between 2012 and 2022), covering a broad range of sub-disciplines. It turned out that a vast majority of researchers take a numeric-conversion approach: They translate the response categories into numeric scores and then analyze the data as though ratings were actually collected on a continuous scale. Further, almost all of these studies use a linear scoring system, i.e. equally-spaced integers, to analyze their data. The current blog post is devoted to Sharoff (2018), the only paper in my survey that used a custom set of scale values for the ordered response set.\n\nNumeric-conversion approach to ordinal data\nIn what follows, I will use the term scoring system (see Labovitz 1967) to refer to the set of values that are used to represent the ordinal responses. Analyses based on scoring systems involve the calculation of averages or the use of ordinary (mixed-effects) regression. This practice, which is widespread in linguistics (see Sönning et al. 2024; Sönning 2024), has sparked heated methodological debates. The widely accepted belief that an interval-level analysis of ordinal data is inappropriate goes back to an influential paper by Stevens (1946), who proposed a taxonomy of scale types (nominal, ordinal, interval, and ratio) along with “permissible statistics” for each. Among the caveats of the numeric-conversion approach is the fact that distances between consecutive categories are usually unknown. In particular, when all scale points are verbalized, the perceived distance between categories will depend on how informants interpret the labels.\n\n\nPsychometric research\nExperimental research has produced insights into the perception of quantificational expressions that are frequently used to build graded scales. Psycholinguistic studies on intensifiers, for instance, have shown that English native speakers recognize similar increments in intensity between hardly-slightly and considerably-highly (Rohrmann 2007). Such insights can inform the design and analysis stage of a study. Earlier methodological work has mainly focused on scale construction, i.e. the selection of approximately equal-interval sequences (e.g. Friedman and Amoo 1999; Rohrmann 2007; Beckstead 2014). As discussed in Sönning (2024), psychometric scale values can also suggest more appropriate scoring systems for data analysis. I find it surprising that only few methodological studies acknowledge this possibility (Labovitz 1967, 155; Worcester and Burns 1975, 191). In applied work, researchers almost universally assign equally-spaced integers to the categories. This is also true for linguistics – in my systematic review, custom scale values are virtually never found. The only exception is a paper by Sharoff (2018), which inspired the present blog post.\n\n\nSharoff (2018)\nThe paper by Sharoff (2018), which appeared in the journal Corpora, presents an approach to classifying texts into genres. To this end, a text-external framework relying on Functional Text Dimensions is used. Examples for such dimensions are ‘informative reporting’ or ‘argumentation’. Raters were asked to indicate the extent to which a text represents a certain functional category. To answer questions such as “To what extent does the text appear to be an informative report of events recent at the time of writing (for example, a news item)?”, informants were provided with four response options:\n\n“strongly or very much so”\n“somewhat or partly”\n“slightly”\n“none or hardly at all”\n\nAs Sharoff (2018, 72) explains, the response scale was purposefully constructed to exhibit a notable gap between “strongly or very much so” and “somewhat or partly”. The following custom scoring system was then used to analyze the data:\n\n2.0 “strongly or very much so”\n1.0 “somewhat or partly”\n0.5 “slightly”\n0 “none or hardly at all”\n\nThe step from “somewhat or partly” to “strongly or very much so” was made twice as large as the other steps, in line with the deliberate scale design. Seeing that the scale is essentially composed of intensifying adverbs (e.g. strongly, somewhat, slightly), let us compare this scoring system to experimental findings on the perception of these expressions.\n\n\nThe psychometrics of intensifiers\nA number of studies have looked into how speakers interpret intensifying adverbs. Here, I collect results from three studies that used similar methods to scale the meaning of relevant adverbs (Matthews et al. 1978; Krsacok 2001; Rohrmann 2007). To measure the relative level of intensity assigned to a specific expression, subjects are typically asked to locate it on an equally-apportioned 11-point scale. I map this scale to the [0,10] interval. Figure 1 gives an overview of the findings reported in the three studies:\n\nThe grey dot diagrams indicate the ratings for four speaker groups (Krsacok (2001) studied two groups, male vs. female subjects).\nThe black dots denote the average across the groups, which is recorded at the left end of the graph.\nThe expressions used in Sharoff (2018) are highlighted in grey.\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of Sharoff (2018)’s scoring system to the psychometric spacing of relevant intensity adverbs\n\n\n\n\n\n\n\nComparison of Sharoff’s (2018) scoring system with experimental findings\nWe can make an attempt to roughly pin down the psychometric scale values that may be considered good approximations for Sharoff (2018)’s response categories. We start by locating the appropriate averages in Figure 1:\n\n8.7 very much\n3.5 somewhat\n3.5 partly\n2.4 slightly\n1.5 hardly\n0.4 not\n\nThen we average across double designations (e.g. hardly/not; somewhat/partly). This allows us to establish an empirically grounded spacing between the response options. Figure 2 compares these relative distances to the ones used by Sharoff (2018). It lends empirical support to the custom scores used in that study: Three (roughly) equally-spaced categories at the lower end of the scale, with a disproportionate gap to the highest response option. In fact, the psychometric evidence would even have licensed a more pronounced numeric gap between very much and somewhat/partly, roughly:\n\n2.0 “strongly or very much so”\n0.7 “somewhat or partly”\n0.4 “slightly”\n0 “none or hardly at all”\n\nMore importantly, however, it is clear that it was appropriate for Sharoff (2018) to use a custom scoring system – the default linear set (e.g. 0, 1, 2, 3) would have misrepresented the way speakers interpret the response labels.\n\n\n\n\n\n\n\n\nFigure 2: Comparison of Sharoff (2018)’s scoring system to the psychometric spacing of relevant intensity adverbs\n\n\n\n\n\n\n\nConclusion\nWe have seen how experimental findings may inform the arrangement of custom scoring systems for the analysis of ordinal rating scale data. The fact that researchers almost exclusively rely on equal-spaced integers may be considered unsatisfactory. Following the good example of Sharoff (2018), more frequent use should be made of custom scoring systems. This methodological topic is discussed much more detail in Sönning (2024), where further psychometric insights are summarized and the inherent limitations of the numeric-conversion approach to ordinal data are given due consideration.\n\n\n\n\n\nReferences\n\nBeckstead, Jason W. 2014. “On Measurements and Their Quality. Paper 4: Verbal Anchors and the Number of Response Options in Rating Scales.” International Journal of Nursing Studies 51 (5): 807–14. https://doi.org/10.1016/j.ijnurstu.2013.09.004.\n\n\nFriedman, Hershey H., and Taiwo Amoo. 1999. “Rating the Rating Scales.” The Journal of Marketing Management 9 (3): 114–23.\n\n\nKrsacok, Stephen J. 2001. “Quantification of Adverb Intensifiers for Use in Ratings of Acceptability, Adequacy, and Relative Goodnessacceptability, Adequacy, and Relative Goodness.” PhD thesis, University of Dayton.\n\n\nLabovitz, Sanford. 1967. “Some Observations on Measurement and Statistics.” Social Forces 46 (2): 151–60. https://doi.org/10.2307/2574595.\n\n\nMatthews, Josephine L., Calvin E. Wright, Kenneth L. Yudowitch, James C. Geddie, and Palmer R. L. 1978. “The Perceived Favorableness of Selected Scale Anchors and Response Alternatives.” Technical report 319. U. S. Army Research Institute for the Behavioral; Social Sciences.\n\n\nRohrmann, Bernd. 2007. “Verbal Qualifiers for Rating Scales: Sociolinguistic Considerations and Psychometric Data.” Technical report. University of Melbourne.\n\n\nSharoff, Serge. 2018. “Functional Text Dimensions for the Annotation of Web Corpora.” Corpora 13 (1): 65–95. https://doi.org/10.3366/cor.2018.0136.\n\n\nSönning, Lukas. 2024. “Ordinal Rating Scales: Psychometric Grounding for Design and Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/jhv6b.\n\n\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht, and Paul Messer. 2024. “Latent-Variable Modeling of Ordinal Outcomes in Language Data Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/jhv6b.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.\n\n\nWorcester, Robert M., and Timothy R. Burns. 1975. “A Statistical Examination of the Relative Precision of Verbal Scales.” Journal of the Market Research Society 17 (3): 181–97.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Custom Scoring Systems in Ordinal Data Analysis: {A} Tribute\n    to {Sharoff} (2018)},\n  date = {2025-11-30},\n  url = {https://lsoenning.github.io/posts/2024-03-01_sharoff_2018/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Custom Scoring Systems in Ordinal Data\nAnalysis: A Tribute to Sharoff (2018).” November 30, 2025. https://lsoenning.github.io/posts/2024-03-01_sharoff_2018/."
  },
  {
    "objectID": "posts/2026-01-10_random_forests_clustering_variables/index.html",
    "href": "posts/2026-01-10_random_forests_clustering_variables/index.html",
    "title": "Issues in random-forest modeling: Treatment of clustering variables",
    "section": "",
    "text": "I recently did a systematic review on the use of random-forest models in corpus research (Sönning 2026). Perhaps unsurprisingly, 112 of the 125 RF models (90%) that entered this survey were fit to data with a grouping structure, where observations are clustered by text/speaker or item. In a considerable number of cases, the clustering variable was included into the RF model as an additional predictor variable:\n\nOf the 112 RF models with data clustered by speaker/text, 20 (18%) included the clustering variable into the analysis.\nOf the 80 RF models with data clustered by item (word form or lexeme), 5 (6%) included the clustering variable as a predictor.\n\nIn all cases, the clustering variable entered the analysis as an additional predictor, which means that the recursive partitioning scheme treated it on a par with the other variables in the model (i.e. as the equivalent of a fixed effect in a regression framework).\nIn this blog post, I want to comment on the situation where a clustering variable is included in an RF model alongside an associated cluster-level predictor. This is the case if a model includes, say, (i) the clustering variable speaker as well as speaker-level predictors such as age or gender; or (ii) the clustering variable word as well as word-level predictors such as frequency or word class.\nIn the survey I did, around half of the RF models that included a clustering variable also included a predictor that was measured at the cluster level:\n\nOf the 20 RFs including speaker as a predictor variable, 11 (55%) also had speaker/text-level predictors. Examples included: age, gender, education, generation, year of birth, ethnicity\nOf the 5 RFs that include item as a predictor variable, 3 (60%) also feature item-level predictors. Examples included: verb ending, verb modality, frequency, gradability, semantic category\n\nI can see why some may think that including a clustering variable into an RF model might be a way of addressing the structure of the data – however, ordinary random forests do not handle clustering variables (e.g. speaker and item) appropriately. By “ordinary”, I mean RFs that do not take a mixed-effects approach to modeling clustering variables. In this blog post, I show that if these variables are included into a random-forest model, the “effect” of cluster-level predictors will be attenuated. Despite prominent precedent (Tagliamonte and Baayen 2012), then, this practice cannot be recommended. In this blog post, I use a simulated set of data to demonstrate this issue.\n\n\nR setup\nlibrary(party)\nlibrary(permimp)\nlibrary(lattice)\nlibrary(vcdExtra)\nlibrary(tidyverse)\nlibrary(uls)\n\n\n\nSimulate data\nWe start by simulating data with a binary response variable that is sensitive to speaker age and gender. The fictitious data include 50 speakers, with each individual contributing 30 observations to the data.\n\n\nsimulate data\n# set seed for reproducibility\nset.seed(11)\n\n# number of speaker and number of observations per speaker\nn_speakers &lt;- 50\nn_tokens_per_speaker &lt;- 30\n\n# simulate speaker intercepts (on the logit scale)\nspeaker_intercepts_logit &lt;- rnorm(n_speakers, 0, 1)\n\n# simulate speaker age and gender\nspeaker_age &lt;- (runif(n_speakers) -.5) * 4\nspeaker_gender &lt;- rep(c(0,1), each = n_speakers/2)\n\n# translate speaker intercepts to the proportion scale\nspeaker_intercepts_prob &lt;- plogis(\n  -speaker_age*.8 + (speaker_gender - .5) + speaker_intercepts_logit)\n\n# use speaker-specific proportion to simulate binary outcome\nspeaker_were &lt;- rbinom(\n  n_speakers, n_tokens_per_speaker, speaker_intercepts_prob)\n\n# obtain empirical proportion for each speaker\nspeaker_were_prop &lt;- speaker_were/n_tokens_per_speaker\n\n\n# organize these data into case form (one row per observation):\nd &lt;- data.frame(\n  speaker = paste0(\"subj_\", 1:n_speakers),\n  age = speaker_age,\n  gender = speaker_gender,\n  were = speaker_were,\n  was = n_tokens_per_speaker - speaker_were\n) |&gt; \n  pivot_longer(\n    cols = c(\"was\", \"were\"), \n    names_to = \"variant\", \n    values_to = \"freq\")\n\nd &lt;- expand.dft(d, freq = \"freq\") \n\nd$variant &lt;- factor(d$variant)\nd$speaker &lt;- factor(d$speaker)\nd$gender &lt;- factor(d$gender, labels = c(\"male\", \"female\"))\n\n# add noise variables\nd$random_continuous &lt;- rnorm(nrow(d))\nd$random_binary &lt;- rbinom(nrow(d), 1, .5)\n\n\nFigure 1 shows the patterns of association we have built into the data:\n\nThe proportion of interest is linked to age: The younger the speaker, the lower the proportion.\nSpeaker gender also plays a role, with male speakers showing a higher proportion.\n\n\n\ndraw graph\np1 &lt;- xyplot(\n  speaker_were_prop ~ speaker_age, \n  groups = speaker_gender, pch = c(21,3), col = c(\"black\", \"black\"), cex = c(1, .8),\n  par.settings = lattice_ls, axis = axis_L, ylim = c(0,1),\n  xlab = \"Age\", ylab = \"Proportion of interest\",\n  scales = list(\n    x = list(at = -1.5:1.5, label = rev(c(20, 40, 60, 80)), cex = .9),\n    y = list(at = c(0, .5, 1), label = c(\"0\", \".5\", \"1\")), cex = .9),\n  panel = function(x,y){\n    #panel.abline(h = 0, col = \"black\")\n    panel.points(x[speaker_gender == 0], y[speaker_gender == 0], lwd = .75, pch = 3, cex = .9, col = \"black\")\n    panel.points(x[speaker_gender == 1], y[speaker_gender == 1], lwd = .75, pch = 21, cex = 1.1)\n    \n    panel.dotdiagram(speaker_were_prop[speaker_gender == 1], \n                     x_anchor = 2.6, seq_min = 0, seq_max = 1, \n                     n_bins = n_tokens_per_speaker, scale_y = .1, vertical = TRUE,\n                     set_pch = 21, set_lwd = .75, set_cex = 1.1)\n    \n    panel.dotdiagram(speaker_were_prop[speaker_gender == 0], \n                     x_anchor = 3.6, seq_min = 0, seq_max = 1, \n                     n_bins = n_tokens_per_speaker, scale_y = .1, vertical = TRUE,\n                     set_pch = 3, set_lwd = .75, set_cex = .9, set_col = \"black\")\n    panel.text(x = c(2.6, 3.6), y = -.075, label = c(\"Male\", \"Female\"), adj = 0, cex = 1, col = c(\"black\", \"black\"))\n  })\n\nprint(p1, position=c(0,0,.7,1))\n\n\n\n\n\n\n\n\nFigure 1: Scatterplot showing how the proportion of interest varies with age and gender.\n\n\n\n\n\nWe also add to the dataset two variables that represent random noise, a binary one (random_binary) and a continuous one (random_continuous). This is for two reasons:\n\nFirst, it will increase the number of predictors, making RF modeling (which includes predictor sampling) feasible\nSecond, and more importantly, the variable importance scores we obtain for these noise variables are a useful point of reference for the predictors of interest. Due to the associations we have built into the model, age and gender should exceed both noise variables in predictive utility.\n\nThis is the resulting dataset:\n\nstr(d)\n\ntibble [1,500 × 6] (S3: tbl_df/tbl/data.frame)\n $ speaker          : Factor w/ 50 levels \"subj_1\",\"subj_10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ age              : num [1:1500] 1.14 1.14 1.14 1.14 1.14 ...\n $ gender           : Factor w/ 2 levels \"male\",\"female\": 1 1 1 1 1 1 1 1 1 1 ...\n $ variant          : Factor w/ 2 levels \"was\",\"were\": 1 1 1 1 1 1 1 1 1 1 ...\n $ random_continuous: num [1:1500] -0.442 0.244 0.597 -0.12 -2.07 ...\n $ random_binary    : int [1:1500] 0 0 0 0 1 1 0 0 1 0 ...\n\n\n\n\nModeling\nIn what follows, we will examine the consequences of including the clustering variable speaker as an additional variable into the model. We will therefore fit and compare two models, M1 and M2:\n\nM1 does not include the clustering variable speaker\n\nformula: variant ~ age + gender + random_continuous + random_binary\n\nM2 includes the clustering variable speaker\n\nformula: variant ~ age + gender + random_continuous + random_binary + speaker\n\n\nFor didactic purposes, we start by fitting a conditional inference tree to the full dataset using the function ctree() in the {party} package (Hothorn, Hornik, and Zeileis 2006). The resulting splitting scheme is shown in Figure 2. It includes three splits: three according to speaker, and one according to gender. Surprisingly, age doesn’t appear at all as a splitting variable, which should make us skeptical.\n\nd_tree &lt;- party::ctree(\n  variant ~ age + gender +\n    random_continuous + random_binary +\n    speaker, \n  data = d)\n\nplot(d_tree)\n\n\n\n\n\n\n\nFigure 2: Conditional inference tree fit to the full dataset.\n\n\n\n\n\nThe next step is to fit two conditional random-forest models (M1 and M2) using the function cforest() in the {party} package (Hothorn, Hornik, and Zeileis 2006).\n\n# M1\nd_rf_1 &lt;- party::cforest(\n  variant ~ age + gender + \n    random_continuous + random_binary, \n  data = d,\n  control = party::cforest_unbiased(\n    mtry = 2, \n    ntree = 500))\n\n# M2\nd_rf_2 &lt;- party::cforest(\n  variant ~ \n    age + gender +\n    random_continuous + random_binary + \n    speaker,  \n  data = d,\n  control = party::cforest_unbiased(\n    mtry = 2, \n    ntree = 500))\n\nNext, we compute conditional variable importance scores for each model, using the permimp package (Debeer, Hothorn, and Strobl 2025):\n\n# M1\nvarimp_d_rf_1 &lt;- permimp::permimp(\n  d_rf_1,\n  conditional = TRUE,\n  progressBar = FALSE)$values\n\nvarimp_d_rf_1 &lt;- as.data.frame(varimp_d_rf_1)\nvarimp_d_rf_1$variable &lt;- rownames(varimp_d_rf_1)\n\n\n# M2\nvarimp_d_rf_2 &lt;- permimp::permimp(\n  d_rf_2,\n  conditional = TRUE,\n  progressBar = FALSE)$values\n\nvarimp_d_rf_2 &lt;- as.data.frame(varimp_d_rf_2)\nvarimp_d_rf_2$variable &lt;- rownames(varimp_d_rf_2)\n\nFinally, we obtain partial dependence scores for the predictors ageand gender for each model, using the partial() function in the pdp package (Greenwell 2017):\n\n# M1\npdp_gender_1 &lt;- pdp::partial(\n  d_rf_1, \n  pred.var = \"gender\",\n  type = \"classification\",\n  which.class = \"variant.were\",\n  prob = TRUE)\n\npdp_age_1 &lt;- pdp::partial(\n  d_rf_1, \n  pred.var = \"age\", \n  pred.grid = data.frame(\n    age = seq(-2, 2, .25)),\n  type = \"classification\",\n  which.class = \"variant.were\",\n  prob = TRUE)\n\n\n# M2\npdp_gender_2 &lt;- pdp::partial(\n  d_rf_2, \n  pred.var = \"gender\",\n  type = \"classification\",\n  which.class = \"variant.were\",\n  prob = TRUE)\n\npdp_age_2 &lt;- pdp::partial(\n  d_rf_2, \n  pred.var = \"age\", \n  pred.grid = data.frame(\n    age = seq(-2, 2, .25)),\n  grid.resolution = 10,  \n  type = \"classification\",\n  which.class = \"variant.were\",\n  prob = TRUE)\n\n\n\nVariable importance scores\nWe start by inspecting the variable importance scores, which are shown in the Figure 3. For comparison, I have scaled these in such a way that each plot extends to the maximum importance score in the respective model. This rescaling does not affect our interpretation here since the relevant benchmarks are the conditional importance scores for the predictors random_binary and random_continuous.\n\nFor M1, the importance measures suggest that both age and gender have predictive utility, in line with the structure we have built into the data.\nFor M2, it is age and speaker that emerge as predictively important; gender, however, ranks with the noise variables.\n\n\n\ndraw graph\nxyplot(\n  1~1, type = \"n\", xlim = c(-.2, 1.1), ylim = c(0, 12.5),\n  par.settings = lattice_ls,\n  xlab = \"Conditional importance (scaled)\",\n  ylab = NULL,\n  scales = list(\n    x = list(at = c(0, 1), label = c(\"0\", \"Max.\"), cex = .9),\n    y = list(\n      at = c(1:5, 8:11), label = c(\n        \"(Continuous)\", \"(Binary)\", \"Gender\", \"Speaker\", \"Age\",\n        \"(Continuous)\", \"(Binary)\", \"Gender\", \"Age\"\n      )), cex = .9),\n  panel = function(x,y){\n    panel.segments(x0 = -.2, x1 = 1.1, y0 = c(1:5, 8:11), y1 = c(1:5, 8:11), \n                   col = \"grey95\")\n    panel.segments(x0 = 0, x1 = 0, y0 = .5, y1 = 5.5)\n    panel.segments(x0 = 0, x1 = 0, y0 = 7.5, y1 = 11.5)\n    panel.segments(x0 = 0, x1 = sort(varimp_d_rf_2$varimp_scaled),\n                   y0 = 1:5, y1 = 1:5)\n    panel.segments(x0 = 0, x1 = sort(varimp_d_rf_1$varimp_scaled),\n                   y0 = 8:11, y1 = 8:11)\n    panel.points(x = sort(varimp_d_rf_2$varimp_scaled), y = 1:5,\n                 pch = 21, fill = \"white\", cex = 1)\n    \n    panel.points(x = sort(varimp_d_rf_1$varimp_scaled), y = 8:11,\n                 pch = 19, cex = 1)\n    panel.segments(x0 = 0, x1 = 1, y0 = 0, y1 = 0)\n    panel.segments(x0 = 0:1, x1 = 0:1, y0 = 0, y1 = -.3)\n    panel.text(x = .5, y = c(6, 12)+.2, label = c(\"M2\", \"M1\"), cex = 1)\n  })\n\n\n\n\n\n\n\n\nFigure 3: Conditional variable importance for the predictors in models M1 and M2.\n\n\n\n\n\nApparently, the inclusion of the variable speaker has affected the predictive utility of the variable gender.\n\n\nPartial dependence plots\nTo shed further light on this, we inspect partial dependence plots for the predictors age and gender based on both models. These are compared in Figure 4. It is clear from this figure that the inclusion of the clustering variable speaker has undermined the predictive power of both speaker-level variables: The partial dependence scores based on M1, which appear as open circles, are more extreme, indicating greater predictive discrimination for young vs. old and male vs. female individuals In contrast, partial dependence scores based on M2, which includes speaker, are flatter.\n\n\ndraw graph\np1 &lt;- xyplot(\n  1~1, type = \"n\", xlim = c(-2.2, 2.2), ylim = c(0,1),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = \"\", ylab = \"Proportion of interest\",\n  scales = list(\n    x = list(at = -1.5:1.5, label = rev(c(20, 40, 60, 80)), cex = .9),\n    y = list(at = c(0, .5, 1), label = c(\"0\", \".5\", \"1\")), cex = .9),\n  panel = function(x,y){\n    panel.points(x = pdp_age_2$age,\n                 y = pdp_age_2$yhat, type = \"l\")\n    \n    panel.points(x = pdp_age_1$age,\n                 y = pdp_age_1$yhat, type = \"l\")\n    \n    panel.points(x = pdp_age_2$age,\n                 y = pdp_age_2$yhat, pch = 19, cex = 1.1)\n    \n    panel.points(x = pdp_age_1$age,\n                 y = pdp_age_1$yhat, pch = 21, fill = \"white\", cex = 1.1)\n    panel.text(x = 1.5, y = 1-c(.55, .9), \n               label = c(\"M2\", \"M1\"), cex = .9)\n    panel.text(x = -2.1, y = 1.07, label = \" Age\", adj = 0)\n    panel.text(x = c(-2.1, -9.45), y = 1.15, label = c(\n      \" Partial dependence plots\\n\", \"Conditional variable\\nimportance scores (rescaled)\"), adj = 0, lineheight = .85)\n  })\n\np2 &lt;- xyplot(\n  1~1, type = \"n\", xlim = c(2.2, .8), ylim = c(0,1),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = \"\", ylab = \" \",\n  scales = list(\n    x = list(at = 1:2, label = c(\"Male\", \"Female\"), cex = .9),\n    y = list(at = c(0, .5, 1), label = c(\"0\", \".5\", \"1\")), cex = .9),\n  panel = function(x,y){\n    panel.points(x = pdp_gender_2$gender,\n                 y = pdp_gender_2$yhat, type = \"l\")\n    \n    panel.points(x = pdp_gender_1$gender,\n                 y = pdp_gender_1$yhat, type = \"l\")\n    \n    panel.points(x = pdp_gender_2$gender,\n                 y = pdp_gender_2$yhat, pch = 19, cex = 1.1)\n    \n    panel.points(x = pdp_gender_1$gender,\n                 y = pdp_gender_1$yhat, pch = 21, fill = \"white\", cex = 1.1)\n    panel.text(x = 1.9, y = 1-c(.33, .63), \n               label = c(\"M1\", \"M2\"), cex = .9)\n    panel.text(x = 2.2, y = 1.07, label = \" Gender\", adj = 0)\n  })\n\nprint(p1, position = c(0,-.1,.65,.86), more = TRUE)\nprint(p2, position = c(.6,-.1, 1,.86))\n\n\n\n\n\n\n\n\nFigure 4: Partial dependence plots for age and gender in models M1 and M2.\n\n\n\n\n\nWe can compare the “steepness” of the profiles numerically, by looking at the range of the partial dependence scores for the two models:\n\nage\n\nM1: .52 (Min: .20; Max: .72)\nM2: .25 (Min: .32; Max: .57)\n\ngender\n\nM1: .22 (Min: .35; Max: .56)\nM2: .10 (Min: .39; Max: .49)\n\n\n\n\nUnderstanding why\nTo understand why the inclusion of clustering variables undermines the predictive utility of the associated cluster-level predictors, it is instructive to inspect the first split in the conditional inference tree shown in Figure 2. The first split is made on speaker, and 18 individuals are assigned to the left branch, 32 to the right branch. The sample is therefore divided into two groups. The scatterplot in Figure 5 shows that this split is made at a proportion of around .55. The subset of speakers in the left branch is colored in gray, the ones in the right branch in blue.\nFor this split, the prediction scheme assigns predictive value to the variable speaker only. This is despite the fact that the two subgroups differ systematically with regard to the distribution of age and gender. This is clarified in the plots added at the top of Figure 5:\n\nThe bar chart shows that the right branch (blue) includes a greater proportion of male speakers (63% vs. 28% in the right branch). Recall that male speakers were simulated to have lower proportions (see Figure 1).\nThe boxplot shows that the right branch (blue) also includes speakers who are, on average, younger (median age 33 vs. 62 years). Recall that younger speakers were simulated to have lower proportions (see Figure 1).\n\n\n\ndraw graph\n# speaker summary\nd_summary &lt;- d |&gt; group_by(speaker) |&gt; \n  dplyr::summarize(\n    age = unique(age),\n    gender = unique(gender),\n    prop_were = mean(variant == \"were\")\n  )\n\nd_tree &lt;- ctree(\n  variant ~ age + gender + speaker +\n    random_continuous + random_binary, \n  data = d)\n\nspeaker_split &lt;- d_tree@tree$psplit$splitpoint\n\nspeakers_to_the_left &lt;- levels(speaker_split)[as.logical(speaker_split)]\nspeakers_to_the_right &lt;- levels(speaker_split)[as.logical(speaker_split*-1+1)]\n\nd_summary$branch &lt;- factor(\n  c(\"right\", \"left\")[as.numeric(d_summary$speaker %in% speakers_to_the_left)+1])\n\n\n# draw plot\np1 &lt;- xyplot(\n  prop_were ~ age, \n  col = col_ls[c(1,3)][as.numeric(d_summary$branch)], \n  pch = c(3,1)[as.numeric(d_summary$gender)],\n  cex = 1,\n  data = d_summary,\n  xlab = \"Age\", ylab = \"Proportion of interest\", ylim = c(0,1),\n  par.settings = lattice_ls, axis = axis_L,\nscales = list(\n    x = list(at = -1.5:1.5, label = rev(c(20, 40, 60, 80))),\n    y = list(at = c(0, .5, 1), label = c(\"0\", \".5\", \"1\"))),\npanel = function(x,y,...){\n  panel.xyplot(x,y,...)\n  panel.segments(x0 = 2.5, x1 = 2.5, col = col_ls[1],\n                 y0 = min(y[d_summary$branch == \"left\"]),\n                 y1 = max(y[d_summary$branch == \"left\"]))\n  panel.segments(x0 = 2.4, x1 = 2.5, col = col_ls[1],\n                 y0 = range(y[d_summary$branch == \"left\"]),\n                 y1 = range(y[d_summary$branch == \"left\"]))\n  \n  panel.segments(x0 = 2.5, x1 = 2.5,col = col_ls[3],\n                 y0 = min(y[d_summary$branch == \"right\"]),\n                 y1 = max(y[d_summary$branch == \"right\"]))\n  panel.segments(x0 = 2.4, x1 = 2.5,col = col_ls[3],\n                 y0 = range(y[d_summary$branch == \"right\"]),\n                 y1 = range(y[d_summary$branch == \"right\"]))\n  \n  panel.text(x = 2.7, y = c(.25, .75), label = c(\n    \"Assigned to\\nright branch\\n(n = 32)\", \"Assigned to\\nleft branch\\n(n = 18)\"\n  ), lineheight = .85, adj = 0, cex = .9, col = col_ls[c(3,1)])\n    })\n\np2 &lt;- bwplot(reorder(branch, -age) ~ age, data = d_summary,\n  par.settings = lattice_ls, box.ratio = 4,\n  scales = list(x = list(draw = FALSE),\n                y = list(cex = .9)),\n  fill = fill_ls[c(3,1)],\n  xlab = NULL,\n  panel = function(x,y,...){\n    panel.bwplot(x,y,...)   \n    panel.text(x = 2.7, y = 3, label = \"Median age\", cex = .9, adj = 0)\n    panel.text(x = 2.7, y = 2, label = \"  Left branch: 33 years\", cex = .9, adj = 0, col = col_ls[1])\n    panel.text(x = 2.7, y = 1, label = \"  Right branch: 62 years\", cex = .9, adj = 0, col = col_ls[3])\n  })\n\nd_barchart &lt;- data.frame(\n  prop.table(\n    table(d_summary$gender, d_summary$branch), margin = 2))\n\n\np3 &lt;- barchart(\n  reorder(Var2, -as.numeric(Var2)) ~ Freq, groups = Var1:Var2,\n               data = d_barchart, stack = TRUE,\n  par.settings = lattice_ls, box.ratio = 4,\n  xlab = NULL,\n  scales = list(x = list(draw = FALSE),\n                y = list(cex = .9)),\n  col = c(\n    fill_ls[1],\n    fill_ls[3], \n    \"white\",\n    \"white\"),\n  panel = function(x,y,...){\n    panel.barchart(x,y,...)\n    panel.text(x = c(.1, .88), y = 2.95, label = c(\"male\", \"female\"), cex = .9)\n    panel.text(x = 1.19, y = 3, label = \"Proportion male\", cex = .9, adj = 0)\n    panel.text(x = 1.19, y = 2, label = \"  Left branch: 28%\", cex = .9, adj = 0, col = col_ls[1])\n    panel.text(x = 1.19, y = 1, label = \"  Right branch: 63%\", cex = .9, adj = 0, col = col_ls[3])\n  })\n\nprint(p1, position = c(0,0,.62,.6), more = TRUE)\nprint(p2, position = c(-.02,.625,.625,.8), more = TRUE)\nprint(p3, position = c(-.02,.8,.625,.975))\n\n\n\n\n\n\n\n\nFigure 5: Explanation of the decrease in predictive utility of cluster-level variables in M2.\n\n\n\n\n\nSince we have built the data, we know that part of the difference between the two subgroups of speakers (grey and blue) is systematic, i.e. attributable to age and gender. For this particular split, however, the tree records no information about systematic differences between grey and blue individuals. Instead, predictive credit is given exclusively to speaker. Since the right branch (blue) mostly includes younger speakers, predictive power has been robbed from age and instead assigned to speaker. And the same is true for gender: Since the right branch (blue) mostly includes male speakers, predictive power has also been robbed from gender and instead assigned to speaker.\n\n\nConclusion\nCorpus data are often structured hierarchically, which poses a problem for random-forest modeling. We know from the regression context that ignoring clustering variables during data analysis is likely to yield overoptimistic models. Including clustering variables into an ordinary random-forest model has its own problems, however, one of which was discussed in the current blog post. If the data are to be analyzed with a random-forest model, the only way out of this dilemma is offered by mixed-effects random forests. For categorical response variables (i.e. classification tasks), such methods have emerged only recently (Pellagatti et al. 2021). If you come across an implementation of these models in R (or Python), spread the word!\n\n\n\n\n\nReferences\n\nDebeer, Dries, Torsten Hothorn, and Carolin Strobl. 2025. Permimp: Conditional Permutation Importance. https://doi.org/10.32614/CRAN.package.permimp.\n\n\nGreenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://doi.org/10.32614/RJ-2017-016.\n\n\nHothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” Journal of Computational and Graphical Statistics 15 (3): 651–74. https://doi.org/10.1198/106186006X133933.\n\n\nPellagatti, Massimo, Chiara Masci, Francesca Ieva, and Anna M. Paganoni. 2021. “Generalized Mixed‐effects Random Forest: A Flexible Approach to Predict University Student Dropout.” Statistical Analysis and Data Mining: The ASA Data Science Journal 14 (3): 241–57. https://doi.org/10.1002/sam.11505.\n\n\nSönning, Lukas. 2026. “Random Forests in Corpus Research: A Systematic Review.” January 13, 2026. https://osf.io/preprints/psyarxiv/byd5c_v1.\n\n\nTagliamonte, Sali A., and R. Harald Baayen. 2012. “Models, Forests, and Trees of York English:was/Werevariation as a Case Study for Statistical Practice.” Language Variation and Change 24 (2): 135–78. https://doi.org/10.1017/s0954394512000129.\n\nCitationBibTeX citation:@online{sönning2026,\n  author = {Sönning, Lukas},\n  title = {Issues in Random-Forest Modeling: {Treatment} of Clustering\n    Variables},\n  date = {2026-01-10},\n  url = {https://lsoenning.github.io/posts/2026-01-10_random_forest_clustering_variable/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2026. “Issues in Random-Forest Modeling: Treatment\nof Clustering Variables.” January 10, 2026. https://lsoenning.github.io/posts/2026-01-10_random_forest_clustering_variable/."
  },
  {
    "objectID": "posts/2026-01-12_random_forests_interaction_predictors/index.html",
    "href": "posts/2026-01-12_random_forests_interaction_predictors/index.html",
    "title": "Issues in random-forest modeling: Interaction predictors",
    "section": "",
    "text": "A practice that has emerged fairly recently in random-forest modeling is the inclusion of manually specified interaction predictors into RF models. Two motivations drive this approach: First, in the very unlikely situation where two predictors show no main effect but a perfect crossover interaction (referred to as the XOR problem), classification trees (and hence RF models) fail to recover the interaction. Further, variable importance scores derived from an RF model for a specific predictor conflate its main effect and its interaction effect(s). This means that variable importance scores, which are the most widespread method of interpretation for RF models, give no indication of the relative magnitude of these two types of effects.\nThese drawbacks have prompted the suggestion to manually specify interaction variables and include them into the RF model (Gries 2019, 638–40; see also Strobl, Malley, and Tutz 2009, 341). For two categorical predictors, for instance, an interaction predictor consists of the pairwise cross-combinations of all levels. For two binary variables – e.g. male/female and old/young – the interaction predictor would consist of four subgroups: male-young, male-old, female-young, female-old. Interaction predictors, it is argued, help shed light on the relative importance of interaction vs. main effects of predictors. I recently conducted a survey on the use of RF models in corpus research (Sönning 2026). In 8 out of 69 articles (12%) that applied an RF to corpus data, use was made of this modeling strategy, with the model including a manually specified interaction variable.\nThe recursive splitting algorithm operating under the hood of RF models handles interactions between predictor variables automatically. As such, RFs therefore have no problems detecting interactions (except for the case of the XOR problem mentioned above). The main motivation for including interaction variables, then, is to facilitate model interpretation.\nIn this blog post, I give two reasons why the practice of including interaction variables into a model is ill-advised. Contrary to the primary motivation behind including interaction predictors into a model, its interpretability actually suffers. This is for two reasons. The first concerns variable importance scores. As demonstrated by Strobl et al. (2024) using a simulation study, the importance attributed to an interaction predictor does not necessarily reflect the predictive importance of the interaction component. In fact, the interaction predictor will show spurious importance even if a statistical interaction is absent from the data. The second issue, which does not seem to have received much attention, concerns the construction and interpretation of partial dependence scores. I will show that, in the presence of both main effects and a statistical interaction between two variables, the partial dependence scores we construct from a model including an interaction predictor blend main effects an interaction effects and therefore fail to provide a clear picture of either form of relationship.\n\n\nR setup\nlibrary(party)\nlibrary(permimp)\nlibrary(lattice)\nlibrary(vcdExtra)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(uls)\n\n\n\nProblem 1: Interaction variables are predictively important even when there is no interaction present\nTo demonstrate this issue, which was highlighted by Strobl et al. (2024), I will create some data that show no statistical interaction, neither on the scale of outcome proportions nor on the logit scale. Then I will fit a number of RF models to these data with different settings for the mtry parameter. For each of these models, I will then obtain two types of variable importance scores: standard and conditional importance scores.\n\nData\nThe data consist of 1,200 speakers, which are divided into four groups:\n\n300 old, male\n300 old, female\n300 young, male\n300 young, female\n\nThis means that there are two binary predictors, gender (male vs. female) and age (young vs. old). For some binary outcome variable of interest, these are the percentages in the four cells:\n\n20% old, male\n40% old, female\n60% young, male\n80% young, female\n\nFigure 1 shows the structure we have built into the data.\n\n\nsimulate data\n# set seed for reproducibility\nset.seed(123)\n\n# number of observations per cell\nn_obs_per_cell &lt;- 300\n\n# outcome proportions for each cell\nprobs &lt;- c(.2, .4, .6, .8)\n\n# simulate binary outcome\ncondition_were &lt;- probs * n_obs_per_cell\n\n\nd &lt;- data.frame(\n  #speaker = paste0(\"subj_\", 1:n_speakers),\n  age = c(\"old\", \"old\", \"young\", \"young\"),\n  gender = c(\"male\", \"female\", \"male\", \"female\"),\n  were = condition_were,\n  was = n_obs_per_cell - condition_were\n) |&gt; \n  pivot_longer(\n    cols = c(\"was\", \"were\"), \n    names_to = \"variant\", \n    values_to = \"freq\")\n\nd &lt;- expand.dft(d, freq = \"freq\") \n\nd$age &lt;- factor(d$age)\nd$gender &lt;- factor(d$gender)\nd$variant &lt;- factor(d$variant)\nd$interaction &lt;- factor(d$age:d$gender)\nd$random_continuous &lt;- rnorm(nrow(d))\nd$random_binary &lt;- rbinom(nrow(d), 1, .5)\n\nd &lt;- d[order(d$variant, decreasing = TRUE),]\n\n\n\n\ndraw graph\nd |&gt; group_by(age, gender) |&gt; \n  dplyr::summarize(\n    mean_prop = mean(variant == \"were\")\n  ) |&gt; ggplot(aes(x = age, y = mean_prop, group = gender)) +\n  geom_point() +\n  scale_y_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .5, 1),\n    labels = c(\"0\", \".5\", \"1\")) +\n  geom_line() +\n  annotate(\"text\", x = 2.1, y = c(.6, .8) + .02, \n           label = c(\"male\", \"female\"),\n           size = 3.2, adj = 0) +\n  ylab(\"Proportion of interest\") +\n  xlab(NULL) +\n  theme_classic_ls()\n\n\n\n\n\n\n\n\nFigure 1: Line plot showing how the proportion of interest varies with age and gender.\n\n\n\n\n\nWe also add to the dataset two variables that represent random noise, a binary one (random_binary) and a continuous one (random_continuous). This is for two reasons:\n\nFirst, it will increase the number of predictors, making RF modeling (which includes predictor sampling) feasible\nSecond, and more importantly, the variable importance scores we obtain for these noise variables are a useful point of reference for the predictors of interest. Due to the associations we have built into the model, age and gender should exceed both noise variables in predictive utility.\n\nThis is the dataset:\n\nstr(d)\n\ntibble [1,200 × 6] (S3: tbl_df/tbl/data.frame)\n $ age              : Factor w/ 2 levels \"old\",\"young\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gender           : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ variant          : Factor w/ 2 levels \"was\",\"were\": 2 2 2 2 2 2 2 2 2 2 ...\n $ interaction      : Factor w/ 4 levels \"old:female\",\"old:male\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ random_continuous: num [1:1200] -0.789 -0.502 1.496 -1.137 -0.179 ...\n $ random_binary    : int [1:1200] 0 1 0 0 1 1 1 0 1 0 ...\n\n\nFor reassurance, we check that the four cells reflect our intended percentages:\n\nd |&gt; tabyl(variant, gender, age) |&gt; \n  adorn_percentages(\"col\") |&gt; \n  adorn_pct_formatting(digits = 0) |&gt;\n  adorn_ns()\n\n$old\n variant    female      male\n     was 60% (180) 80% (240)\n    were 40% (120) 20%  (60)\n\n$young\n variant    female      male\n     was 20%  (60) 40% (120)\n    were 80% (240) 60% (180)\n\n\n\n\nFit RF models and obtain variable importance scores\nNext, I will fit a conditional random-forest model to these data using different settings for the mtry parameter. This parameter determines the number of predictor variables that are sampled at each splitting occasion during model fitting. I will let it vary from 2 to the total number of predictor variables (here: 5). If mtry equals the number of predictor variables, no predictor sampling takes place; this special case is referred to as bagging (Breiman 1996). For each RF model, I will then obtain two types of variable importance scores: standard and conditional importance scores.\nThe following simulation therefore varies two parameters:\n\nmtry parameter: 2, 3, 4, 5 predictors\ntype of variable importance score: standard vs. conditional\n\n\n\nrun simulation\nset_mtry &lt;- 2:5\n\nsim_results &lt;- array(\n  NA, dim = c(5, 4, 2),\n  dimnames = list(\n    c(\"age\", \"gender\", \"interaction\", \"random_continuous\", \"random_binary\"),\n    set_mtry,\n    c(\"conditional\", \"standard\")))\n\n\nfor (i in 1:length(set_mtry)){\n  \n  # fit RF model\n  rf &lt;- party::cforest(\n  variant ~ age + gender + interaction +\n    random_continuous + random_binary, data = d,\n      control = party::cforest_unbiased(\n        mtry = set_mtry[i], \n        ntree = 1000))\n  \n  # obtain variable importance scores\n  # conditional\n  varimp_cond &lt;- permimp(\n    rf,\n    conditional = TRUE,\n    progressBar = FALSE)$values\n  \n  sim_results[,i,1] &lt;- varimp_cond\n  \n  # standard\n  varimp_std &lt;- permimp(\n    rf,\n    conditional = FALSE,\n    progressBar = FALSE)$values\n  \n  sim_results[,i,2] &lt;- varimp_std\n\n  }\n\nsim_results_df &lt;- as.data.frame(\n  as.table(sim_results))\n\ncolnames(sim_results_df) &lt;- c(\n  \"predictor\", \"mtry\", \"type\", \"score\"\n)\n\n\n\n\nComparison of importance scores\nFigure 2 compares conditional variable importance scores (left panel) and standard variable importance scores (right panel) for different settings of the mtry parameter. The first observation that strikes us is that the interaction predictor always ranks first in terms of importance, despite the fact that the data show no statistical interaction (see Figure 1). While smaller values of the mtry parameter bring about some reduction in the importance of the interaction predictor, it always comes out on top. The predictive utility of age depends dramatically on the mtry parameter. In the case of bagging (mtry = 5), the predictor receives zero importance. The predictor gender, on the other hand, always hovers near zero. Standard and conditional variable importance score yield very similar picture, which is unsurprising seeing that we did not build into the data correlations among predictor variables.\n\n\ndraw graph\nsim_results_df$predictor &lt;- factor(\n  sim_results_df$predictor,\n  levels = c(\"interaction\", \"age\", \"gender\", \"random_continuous\", \"random_binary\"),\n  ordered = TRUE\n)\n\nsim_results_df &lt;- sim_results_df[order(sim_results_df$predictor, decreasing = TRUE),]\n\nxyplot(\n  1~1, type = \"n\", xlim = c(-.12, .35), ylim = c(-1, 7),\n  par.settings = lattice_ls,\n  scales = list(draw = FALSE),\n  xlab = NULL, ylab = NULL,\n  panel = function(x,y){\n    panel.text(x = -.03, y = 1:5, label = c(\n      \"(Binary)\", \"(Continuous)\", \"Gender\", \"Age\", \"Interaction\"), adj = 1, cex = .9)\n    panel.segments(x0 = c(0, .2), x1 = c(0, .2), y0 = .3, y1 = 5.5, col = \"grey\")\n    \n    panel.points(x = subset(sim_results_df, mtry == \"2\" & type == \"conditional\")$score, \n                 y = 1:5, pch = \"2\", cex = 2.2)\n    panel.points(x = subset(sim_results_df, mtry == \"3\" & type == \"conditional\")$score, \n                 y = 1:5, pch = \"3\", cex = 2.2)\n    panel.points(x = subset(sim_results_df, mtry == \"4\" & type == \"conditional\")$score, \n                 y = 1:5, pch = \"4\", cex = 2.2)\n    panel.points(x = subset(sim_results_df, mtry == \"5\" & type == \"conditional\")$score, \n                 y = 1:5, pch = \"5\", cex = 2.2)\n    \n    panel.points(x = .2 + subset(sim_results_df, mtry == \"2\" & type == \"standard\")$score, \n                 y = 1:5, pch = \"2\", cex = 2.2)\n    panel.points(x = .2 + subset(sim_results_df, mtry == \"3\" & type == \"standard\")$score, \n                 y = 1:5, pch = \"3\", cex = 2.2)\n    panel.points(x = .2 + subset(sim_results_df, mtry == \"4\" & type == \"standard\")$score, \n                 y = 1:5, pch = \"4\", cex = 2.2)\n    panel.points(x = .2 + subset(sim_results_df, mtry == \"5\" & type == \"standard\")$score, \n                 y = 1:5, pch = \"5\", cex = 2.2)\n    \n    panel.segments(x0 = 0, x1 = .15, y0 = .3, y1 = .3)\n    panel.segments(x0 = c(0, .1), x1 = c(0, .1), y0 = .3, y1 = 0)\n    \n    panel.segments(x0 = .2, x1 = .35, y0 = .3, y1 = .3)\n    panel.segments(x0 = c(.2, .3), x1 = c(.2, .3), y0 = .3, y1 = 0)\n    \n    panel.text(x = c(0, .1, .2, .3), y = -.5, label = c(\"0\", \"0.1\"), cex = .8)\n    \n    panel.text(x = .175, y = -1.6, label = \"Variable importance\")\n    panel.text(x = .075, y = 6.75, label = \"Conditional\")\n    panel.text(x = .275, y = 6.75, label = \"Standard\")\n  })\n\n\n\n\n\n\n\n\nFigure 2: Dotplot showing standard and conditional variable importance scores for different settings of the mtry parameter.\n\n\n\n\n\n\n\nUnderstanding why\nTo understand why an interaction predictor “soaks up” predictive utility even though the data do not include an interaction, consider the situation where the interaction predictor is sampled alongside an unrelated variable (for instance, either the binary or the continuous noise variable). The splitting scheme then treats the interaction variable as a categorical predictor with four distinct levels. Due to the main effects that are present in the data, there are systematic differences among these subgroups. As a result, the algorithm will find a predictively useful split. This split will likely partition the data along the predictor age or gender, as these are related to the response variable. The interaction predictor will receive “Predictive credit” for this split, even though it is the variable age or gender that should be credited. As a result, the interaction predictor steals predictive power from each of these predictors. The variable importance scores we obtain are therefore not interpretable – they do not give accurate information and they do not tell us what we wanted to know. For amore in-depth discussion of this issue, please refer to Strobl et al. (2024).\n\n\n\nProblem 2: Partial dependence plots of interaction variables do not provide a clean picture\nTo demonstrate the second issue, I will create data that do show a statistical interaction, both on the scale of outcome proportions and the logit scale. Then I will fit two RF models to these data, one without and one with an interaction predictor. I will then draw conditional partial dependence plots for both models.\n\nData\nThe data consist of 1,200 speakers, which are gain divided into four groups:\n\n300 old, male\n300 old, female\n300 young, male\n300 young, female\n\nThis means that there are again two binary predictors, gender (male vs. female) and age (young vs. old). For some binary outcome variable of interest, these are the percentages in the four cells:\n\n20% old, male\n30% old, female\n45% young, male\n80% young, female\n\nFigure 3 shows the structure we have built into the data.\n\n\nsimulate data\nn_obs_per_cell &lt;- 300\n\nprobs &lt;- c(.2, .3, .45, .8)\n\ncondition_were &lt;-probs * n_obs_per_cell\n\n\nd &lt;- data.frame(\n  #speaker = paste0(\"subj_\", 1:n_speakers),\n  age = c(\"old\", \"old\", \"young\", \"young\"),\n  gender = c(\"male\", \"female\", \"male\", \"female\"),\n  were = condition_were,\n  was = n_obs_per_cell - condition_were\n) |&gt; \n  pivot_longer(\n    cols = c(\"was\", \"were\"), \n    names_to = \"variant\", \n    values_to = \"freq\")\n\nd &lt;- expand.dft(d, freq = \"freq\") \n\nd$age &lt;- factor(d$age)\nd$gender &lt;- factor(d$gender)\nd$variant &lt;- factor(d$variant)\nd$interaction &lt;- factor(d$age:d$gender)\nd$random_continuous &lt;- rnorm(nrow(d))\nd$random_binary &lt;- rbinom(nrow(d), 1, .5)\n\nd &lt;- d[order(d$variant, decreasing = TRUE),]\n\n\n\n\ndraw graph\nd |&gt; group_by(age, gender) |&gt; \n  dplyr::summarize(\n    mean_prop = round(mean(variant == \"were\"), 2)\n  ) |&gt; ggplot(aes(x = age, y = mean_prop, group = gender)) +\n  geom_point() +\n  scale_y_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .5, 1),\n    labels = c(\"0\", \".5\", \"1\")) +\n  geom_line() +\n  annotate(\"text\", x = 2.1, y = c(.45, .8) + .02, \n           label = c(\"male\", \"female\"),\n           size = 3.2, adj = 0) +\n  ylab(\"Proportion of interest\") +\n  xlab(NULL) +\n  theme_classic_ls()\n\n\n\n\n\n\n\n\nFigure 3: Line plot showing how the proportion of interest varies with age and gender.\n\n\n\n\n\nWe also add to the dataset two variables that represent random noise, a binary one (random_binary) and a continuous one (random_continuous).\n\n\nModeling\nTo examine the consequences of including the interaction predictor age:gender as an additional variable into the model, we will fit and compare two models, M1 and M2:\n\nM1 does not include an interaction predictor\n\nformula: variant ~ age + gender + random_continuous + random_binary\n\nM2 includes an interaction predictor\n\nformula: variant ~ age + gender + random_continuous + random_binary + interaction\n\n\nFor didactic purposes, we start by fitting a conditional inference tree to the full dataset using the function ctree() in the {party} package (Hothorn, Hornik, and Zeileis 2006). This conditional inference tree includes the manually specified predictor for the age-by-gender interaction. The resulting splitting scheme is shown in Figure 4. It includes three splits: two according to the interaction, and one according to gender. Surprisingly, age doesn’t appear at all as a splitting variable, which should make us skeptical.\n\nd_tree &lt;- ctree(\n  variant ~ age + gender + interaction +\n    random_continuous + random_binary, \n  data = d)\n\nplot(d_tree)\n\n\n\n\n\n\n\nFigure 4: Conditional inference tree fit to the full dataset including an interaction predictor.\n\n\n\n\n\nThe next step is to fit two conditional random forest models (M1 and M2) using the function cforest() in the {party} package (Hothorn, Hornik, and Zeileis 2006).\n\nrf_inter_1 &lt;- party::cforest(\n   variant ~ age + gender +\n    random_continuous + random_binary, data = d,\n      control = party::cforest_unbiased(\n        mtry = 2, \n        ntree = 500))\n\n\nrf_inter_2 &lt;- party::cforest(\n  variant ~ age + gender + interaction +\n    random_continuous + random_binary, data = d,\n      control = party::cforest_unbiased(\n        mtry = 2, \n        ntree = 500))\n\nNext, we compute conditional variable importance scores for each model, using the permimp package (Debeer, Hothorn, and Strobl 2025):\n\nvarimp_rf_inter_1 &lt;- permimp::permimp(\n  rf_inter_1,\n  conditional = TRUE,\n  progressBar = FALSE)$values\n\n\nvarimp_rf_inter_2 &lt;- permimp::permimp(\n  rf_inter_2,\n  conditional = TRUE,\n  progressBar = FALSE)$values\n\nWe start by inspecting the variable importance scores, which are shown in Figure 5. For comparison, I have scaled these in such a way that each plot extends to the maximum importance score in the respective model. This rescaling does not affect our interpretation here since the relevant benchmarks are the conditional importance scores for the predictors random_binary and random_continuous.\n\nFor M1, the importance measures suggest that both age and gender have predictive utility, in line with the structure we have built into the data.\nFor M2, it is age, gender and their interaction that emerge as predictively important, also in line with the structure we have built into the data.\n\n\n\ndraw graph\nxyplot(\n  1~1, type = \"n\", xlim = c(-.2, 1.1), ylim = c(0, 12.5),\n  par.settings = lattice_ls,\n  xlab = \"Conditional importance (scaled)\",\n  ylab = NULL,\n  scales = list(\n    x = list(at = c(0, 1), label = c(\"0\", \"Max.\")),\n    y = list(\n      at = c(1:5, 8:11), label = c(\n        \"(Binary)\", \"(Continuous)\", \"Interaction\", \"Gender\", \"Age\",\n        \"(Binary)\", \"(Continuous)\", \"Gender\", \"Age\"\n      )), cex = .9),\n  panel = function(x,y){\n    panel.segments(x0 = -.2, x1 = 1.1, y0 = c(1:5, 8:11), y1 = c(1:5, 8:11), \n                   col = \"grey95\")\n    panel.segments(x0 = 0, x1 = 0, y0 = .5, y1 = 5.5)\n    panel.segments(x0 = 0, x1 = 0, y0 = 7.5, y1 = 11.5)\n    panel.segments(x0 = 0, x1 = sort(varimp_rf_inter_2$varimp_scaled),\n                   y0 = 1:5, y1 = 1:5)\n    panel.segments(x0 = 0, x1 = sort(varimp_rf_inter_1$varimp_scaled),\n                   y0 = 8:11, y1 = 8:11)\n    panel.points(x = sort(varimp_rf_inter_2$varimp_scaled), y = 1:5,\n                 pch = 21, fill = \"white\", cex = 1.1)\n    \n    panel.points(x = sort(varimp_rf_inter_1$varimp_scaled), y = 8:11,\n                 pch = 19, cex = 1)\n    panel.segments(x0 = 0, x1 = 1, y0 = 0, y1 = 0)\n    panel.segments(x0 = 0:1, x1 = 0:1, y0 = 0, y1 = -.3)\n    panel.text(x = .5, y = c(6, 12)+.2, label = c(\"M2\", \"M1\"), cex = 1)\n  })\n\n\n\n\n\n\n\n\nFigure 5: Conditional variable importance for the predictors in model with and without an interaction predictor.\n\n\n\n\n\n\n\nComparison of partial dependence scores\nFinally, we obtain partial dependence scores based on each model. Since the main effects of age and gender as well as their interaction are the only patterns we have built into the data, a partial dependence plot for the predictor age conditional on gender should recover the pattern we saw in Figure 3. We use the partial() function in the pdp package (Greenwell 2017) to calculate the following partial dependence scores:\n\nM1: Partial dependence scores for age conditional on gender\nM2: Partial dependence scores for the interaction predictor\nM2: Partial dependence scores for age conditional on gender\n\n\npdp_1 &lt;- pdp::partial(\n  rf_inter_1, \n  pred.var = c(\"age\", \"gender\"),\n  type = \"classification\",\n  #which.class = \"were\",\n  prob = TRUE)\n\npdp_2 &lt;- pdp::partial(\n  rf_inter_2, \n  pred.var = \"interaction\",\n  type = \"classification\",\n  #which.class = \"were\",\n  prob = TRUE)\n\npdp_2_maineffects &lt;- pdp::partial(\n  rf_inter_2, \n  pred.var = c(\"age\", \"gender\"),\n  type = \"classification\",\n  #which.class = \"were\",\n  prob = TRUE)\n\nFigure 6 compares the results. Panel (a) at the far left shows, as a point of reference, descriptive statistics for the dataset. The plot is identical to Figure 3 above.\nPanel (b) shows a conditional partial dependence plot for M1, the model without an interaction predictor. The partial dependence scores for age conditional on gender align very closely with the patterns we built into the data.\nPanel (c) shows a partial dependence plot for the interaction predictor in M2. While the scores do give an idea of the interaction pattern, they also show traces of the two main effects: The scores for female speakers are higher, on average; and the scores for young speakers are also higher, on average.\nPanel (d) shows partial dependence scores for age conditional on gender. The plot also shows a trace of the interaction between age and gender, indicating that the interaction effect is not entirely captured by the interaction predictor.\n\n\ndraw graph\nd_descr &lt;- d |&gt; group_by(age, gender) |&gt; \n  dplyr::summarize(\n    prop_were = mean(variant == \"were\")\n  )\n\np0 &lt;- xyplot(\n  1 ~ 1, type = \"n\", ylim = c(0,1), xlim = c(.8, 2.2),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = NULL, ylab = \"Proportion\",\n  scales = list(x = list(at = 1:2, labels = c(\"old\", \"young\"), cex = .85),\n                y = list(at = c(0, .5, 1), label = c(\"0\", \".5\", \"1\"))),\n  xlab.top = list(label = \"\\n\\n\\nDescriptive\\nstatistics\\n\", lineheight = .85, cex = .9),\n  panel = function(x,y){\n    panel.points(x = 1:2, y = d_descr$prop_were[c(1,3)], type=\"l\")\n    panel.points(x = 1:2, y = d_descr$prop_were[c(2,4)], type=\"l\")\n    panel.points(x = 1:2, y = d_descr$prop_were[c(1,3)], pch = 19, cex = 1)\n    panel.points(x = 1:2, y = d_descr$prop_were[c(2,4)], pch = 21, fill = \"white\", cex = 1)\n    panel.text(x = 1.5, y = .95, label = \"(a)\", col = \"grey40\")\n  })\n\n\np1 &lt;- xyplot(\n  1 ~ 1, type = \"n\", ylim = c(0,1), xlim = c(.8, 2.2),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = NULL, ylab = NULL,\n  scales = list(x = list(at = 1:2, labels = c(\"old\", \"young\"), cex = .85),\n                y = list(at = c(0, .5, 1), label = NULL)),\n  xlab.top = list(label = \"\\n\\n\\nPDP\\nInteraction\\n\", lineheight = .85, cex = .9),\n  panel = function(x,y){\n    panel.points(x = 1:2, y = 1-pdp_2$yhat[c(1,3)], type=\"l\")\n    panel.points(x = 1:2, y = 1-pdp_2$yhat[c(2,4)], type=\"l\")\n    panel.points(x = 1:2, y = 1-pdp_2$yhat[c(1,3)], pch = 19, cex = 1)\n    panel.points(x = 1:2, y = 1-pdp_2$yhat[c(2,4)], pch = 21, fill = \"white\", cex = 1)\n    panel.text(x = 2.45, y = 1.4, label = \"\\nModel including interaction\", lineheight = .85, cex = .9)\n    panel.segments(x0 = .8, x1 = 4.1, y0 = 1.27, y1  = 1.27)\n    panel.text(x = 1.5, y = .95, label = \"(c)\", col = \"grey40\")\n  })\n\np2 &lt;- xyplot(\n  1 ~ 1, type = \"n\", ylim = c(0,1), xlim = c(.8, 2.2),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = NULL, ylab = NULL,\n  scales = list(x = list(at = 1:2, labels = c(\"old\", \"young\"), cex = .85),\n                y = list(at = c(0, .5, 1), label = NULL)),\n  xlab.top = list(label = \"\\n\\n\\nConditional PDP\\nAge, Gender\\n\", lineheight = .85, cex = .9),\n  panel = function(x,y){\n    panel.points(x = 1:2, y = 1-subset(pdp_2_maineffects, gender == \"female\",)$yhat, type=\"l\")\n    panel.points(x = 1:2, y = 1-subset(pdp_2_maineffects, gender == \"male\",)$yhat, type=\"l\")\n    panel.points(x = 1:2, y = 1-subset(pdp_2_maineffects, gender == \"female\",)$yhat, pch = 19, cex = 1)\n    panel.points(x = 1:2, y = 1-subset(pdp_2_maineffects, gender == \"male\",)$yhat, pch = 21, fill = \"white\", cex = 1)\n    panel.text(x = 1.5, y = .95, label = \"(d)\", col = \"grey40\")\n    panel.text(x = 1.5, y = c(.25, .7), label = c(\"male\", \"female\"), cex = .8)\n  })\n\np3 &lt;- xyplot(\n  1 ~ 1, type = \"n\", ylim = c(0,1), xlim = c(.8, 2.2),\n  par.settings = lattice_ls, axis = axis_L,\n  xlab = NULL, ylab = NULL,\n  scales = list(x = list(at = 1:2, labels = c(\"old\", \"young\"), cex = .85),\n                y = list(at = c(0, .5, 1), label = NULL)),\n  xlab.top = list(label = \"\\n\\n\\nConditional PDP\\nAge, Gender\\n\", lineheight = .85, cex = .9),\n  panel = function(x,y){\n    panel.points(x = 1:2, y = 1-subset(pdp_1, gender == \"female\",)$yhat, type=\"l\")\n    panel.points(x = 1:2, y = 1-subset(pdp_1, gender == \"male\",)$yhat, type=\"l\")\n    panel.points(x = 1:2, y = 1-subset(pdp_1, gender == \"female\",)$yhat, pch = 19, cex = 1)\n    panel.points(x = 1:2, y = 1-subset(pdp_1, gender == \"male\",)$yhat, pch = 21, fill = \"white\", cex = 1)\n    panel.text(x = 1.5, y = 1.4, label = \"Model without\\ninteraction\", lineheight = .85, cex = .9)\n    panel.text(x = 1.5, y = .95, label = \"(b)\", col = \"grey40\")\n  })\n\ncowplot::plot_grid(p0, NULL, p3, NULL, p1, NULL, p2, NULL, nrow = 1, rel_widths = c(1, -.1, 1, -.1, 1, -.1, 1, .1))\n\n\n\n\n\n\n\n\nFigure 6: Partial dependence plots based on M1 (no interaction predictor) and M2 (with interaction predictor).\n\n\n\n\n\nFigure 6 therefore shows that if the data include a pattern formed by both main and interaction effects, a model including a manually specified interaction predictor does not allow us to recover this pattern. At the same time, partial dependence scores do not allow us to isolate main-effect and interaction-effect components. Instead, the interaction predictor soaks up traces of the main effects of the constituent predictors and therefore yield a blended assessment.\nIn contrast, conditional partial dependence plots based on a model without manually specified interaction variables (panel b in Figure 6) provide a faithful representation of the patterns in the data.\n\n\nConclusion\nIt seems fair to say that the practice of including manually specified interaction variables should be discouraged in random-forest modeling. To detect relevant interactions in a random-forest model, researchers will need to stick to established methods such as Friedman’s H (Friedman and Popescu 2008), an index of interaction strength. To gauge the relative magnitude of interaction vs. main effects, (conditional) partial dependence plots need to be studied carefully.\n\n\n\n\n\n\nReferences\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. https://doi.org/10.1007/bf00058655.\n\n\nDebeer, Dries, Torsten Hothorn, and Carolin Strobl. 2025. Permimp: Conditional Permutation Importance. https://doi.org/10.32614/CRAN.package.permimp.\n\n\nFriedman, Jerome H., and Bogdan E. Popescu. 2008. “Predictive Learning via Rule Ensembles.” The Annals of Applied Statistics 2 (3). https://doi.org/10.1214/07-aoas148.\n\n\nGreenwell, Brandon M. 2017. “Pdp: An R Package for Constructing Partial Dependence Plots.” The R Journal 9 (1): 421–36. https://doi.org/10.32614/RJ-2017-016.\n\n\nGries, Stefan Th. 2019. “On Classification Trees and Random Forests in Corpus Linguistics: Some Words of Caution and Suggestions for Improvement.” Corpus Linguistics and Linguistic Theory 16 (3): 617–47. https://doi.org/10.1515/cllt-2018-0078.\n\n\nHothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased Recursive Partitioning: A Conditional Inference Framework.” Journal of Computational and Graphical Statistics 15 (3): 651–74. https://doi.org/10.1198/106186006X133933.\n\n\nSönning, Lukas. 2026. “Random Forests in Corpus Research: A Systematic Review.” January 13, 2026. https://osf.io/preprints/psyarxiv/byd5c_v1.\n\n\nStrobl, Carolin, James Malley, and Gerhard Tutz. 2009. “An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of Classification and Regression Trees, Bagging, and Random Forests.” Psychological Methods 14 (4): 323–48. https://doi.org/10.1037/a0016973.\n\n\nStrobl, Carolin, Yannick Rothacher, Sven Theiler, and Mirka Henninger. 2024. “Detecting Interactions with Random Forests: A Comment on Gries’ Words of Caution and Suggestions for Improvement.” Corpus Linguistics and Linguistic Theory, November. https://doi.org/10.1515/cllt-2024-0028.\n\nCitationBibTeX citation:@online{sönning2026,\n  author = {Sönning, Lukas},\n  title = {Issues in Random-Forest Modeling: {Interaction} Predictors},\n  date = {2026-01-12},\n  url = {https://lsoenning.github.io/posts/2026-01-12_random_forest_interaction_predictors/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2026. “Issues in Random-Forest Modeling:\nInteraction Predictors.” January 12, 2026. https://lsoenning.github.io/posts/2026-01-12_random_forest_interaction_predictors/."
  }
]