[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Sönning",
    "section": "",
    "text": "Post-doc in English linguistics at the University of Bamberg\n\nUniversity of Bamberg\nDepartment of English Linguistics\n\n\n\nContact\n\nAddress: An der Universität 9, D-96047 Bamberg\nOffice: U9/01.11\nPhone: +49 (0)951/863-2267\nEmail: lukas[dot]soenning[at]uni-bamberg[dot]de\n ORCID 0000-0002-2705-395X\n OSF\n\n\n\nResearch interests\n\nStatistical analysis of language data\nCorpus linguistics\nLanguage variation and change\nData visualization\nGerman Learner English\nL2 phonology"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Lukas Sönning\n\nShort academic CV\n\n2020 awarded PhD\n2012-2022 Research and teaching assistant at the University of Bamberg\n2006-2012 Studies in English, Geography and Pedagogy at the University of Bamberg\n\n\n\nAwards\n\n2014 Best poster award, Olinco conferenece, Olomouc (“Vowel reduction in German Learner English: Developmental patterns”)\n2018 Best paper by an early career researcher, ICAME39, Tampere (John Sinclair bursary) (“Visual inference for corpus linguistics”)\n\n\n\nTeaching\nUniversity courses\n\nForming (new) words: The morphological architecture of English\nApplied data analysis for linguists\nInvestigating Lerner English\nSecond Language speech: Theory and practice\nMeasuring (your) foreign accent: The acoustic analysis of non-native speech\nEnglish phonetics & phonology\nEnglish grammar analysis\nTranslation English-German (intermediate and advanced level)\nRevision course for state exam candidates: Synchronic linguistics\nRevision course for state exam candidates: Translation English-German\n\nWorkshops\n\n2023 (META-LING conference, Bamberg) Data publication using TROLLing  OSF\n2023 (University of Würzburg) Basics of data analysis using R and RStudio  OSF | slides part 1 | slides part 3\n2022 (University of Würzburg) Basic statistical methods for TEFL research  OSF\n2019 (FJUEL conference, Bayreuth): Using “statistics” to learn about language: What matters (and what doesn’t)  OSF\n2019 (BICLCE conference, Bamberg): The replication crisis in science: Challenges and chances for linguistics  OSF\n2018 (Uppsala University, Sweden) Statistical inference using estimation: Methods for corpus linguistics slides\n2014 (EmMeth conference, Bamberg): Data visualization with R\n2014 (FJUEL conference, Bamberg): Workshop on statistical methods"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Unpublished\n\nSönning, Lukas. (accepted). Down-sampling from hierarchically structured corpus data. International Journal of Corpus Linguistics  Submitted manuscript |  OSF\nSönning, Lukas. (accepted). Advancing our understanding of dispersion measures in corpus research. Corpora Accepted manuscript |  OSF\nSönning, Lukas, Manfred Krug, Fabian Vetter, Anne Leucht, Timo Schmid & Paul Messer. (in review). Latent-variable modeling of ordinal outcomes in language data analysis. Submitted manuscript |  OSF\nSönning, Lukas. (in review). Count regression models for keyness analysis.  Working paper |  OSF\nSönning, Lukas. (unpublished manuscript). Evaluation of text-level measures of lexical dispersion: Robustness and consistency.  Working paper |  Data |  OSF\n\n \n\n\nPublished\nMonograph\n\nSönning, Lukas. 2020. Phonological variation in German Learner English. University of Bamberg dissertation. DOI: 10.20378/irb-49135  Open access |  Datasets |  OSF\n\nJournal articles\n\nSönning, Lukas. 2023. Evaluation of keyness metrics: Performance and reliability. Corpus Linguistics and Linguistic Theory. https://doi.org/10.1515/cllt-2022-0116  Preprint |  Data |  OSF\nSönning, Lukas & Jason Grafmiller. 2023. Seeing the wood for the trees: Predictive margins for random forests. Corpus Linguistics and Linguistic Theory. https://doi.org/10.1515/cllt-2022-0083  Preprint |  Data |  OSF\nSönning, Lukas & Valentin Werner. 2021. The replication crisis, scientific revolutions, and linguistics. Linguistics 59(5). 1179–1206. https://doi.org/10.1515/ling-2019-0045  Open access\nSönning, Lukas. 2014. Unstressed vowels in German Learner English: An instrumental study. Research in Language 12(2). 163–173. https://doi.org/10.2478/rela-2014-0001  Open access |  OSF\n\nEdited volumes\n\nSönning, Lukas & Ole Schützler (eds.). 2023. Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22\nSönning, Lukas & Valentin Werner. 2021. The replication crisis: Implications for linguistics. Special issue in Linguistics.  Open access\nChrist, Hanna, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.). 2016. A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium. Bamberg: University of Bamberg Press.  Open access\n\nBook chapters\n\nSönning, Lukas. 2023. Drawing on principles of perception: The line plot. In Lukas Sönning & Ole Schützler (eds.), Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22-2  Preprint |  OSF\nSönning, Lukas. 2023. (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. In Robert Fuchs (ed.), Speech rhythm in learner and second language varieties of English, 123–157. Singapore: Springer. https://doi.org/10.1007/978-981-19-8940-7_6  Preprint |  Data |  OSF\nSönning, Lukas & Manfred Krug. 2022. Comparing study designs and down-sampling strategies in corpus analysis: The importance of speaker metadata in the BNCs of 1994 and 2014. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 127–159. Cambridge: Cambridge University Press. DOI: 10.1017/9781108589314.006  Link |  OSF |  Data\nSönning, Lukas & Julia Schlüter. 2022. Comparing standard reference corpora and Google Books Ngrams: Strengths, limitations and synergies in the contrastive study of variable h- in British and American English. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 17–45. Cambridge: Cambridge University Press. DOI: 10.1017/9781108589314.002  Link |  OSF\nKrug, Manfred & Lukas Sönning. 2018. Language change in Maltese English: The influence of age and parental languages. In: Patrizia Paggio & Albert Gatt (eds.), The languages of Malta, 247–270. Berlin: Language Science Press. DOI: 10.5281/zenodo.1181801  Open access\n\nProceedings\n\nSönning, Lukas. 2016. The dot plot: A graphical tool for data analysis and presentation. In Hanna Christ, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.), A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium, 101–129. Bamberg: University of Bamberg Press.  Open access\nSönning, Lukas. 2014. Developmental patterns in the reduction of unstressed vowels by German learners of English. In Ludmila Veselovská & Markéta Janebová (eds.), Complex visibles out there: Proceedings of the Olomouc Linguistics Colloquium 2014: Language use and linguistic structure, vol. 4 Olomouc modern language series, 765–778. Olomouc: Palacký University.  Open access |  OSF\nSönning, Lukas. 2013. Scrabble yourself to success: Methods in teaching transcription. In Joanna Przedlacka, John Maidment & Michael Ashby (eds.), Proceedings of the Phonetics Teaching and Learning Conference, UCL, London, 8-10 August 2013. London: Phonetics Teaching and Learning Conference, 87–90.  Open access |  OSF |  Data\n\n \n\n\nDatasets\n\nSönning, Lukas. 2023. Background data (adapted from Jenset & McGillivray 2017) for: Down-sampling from hierarchically structured corpus data, https://doi.org/10.18710/5KCE4U , DataverseNO, V1.\nSönning, Lukas. 2023. Key verbs in academic writing: Dataset for “Evaluation of keyness metrics: Performance and reliability”, https://doi.org/10.18710/EUXSMW , DataverseNO, V1.\nSönning, Lukas. 2022. Speech rhythm in German Learner English: Dataset for “(Re-)viewing the acquisition of rhythm in the light of L2 phonological theories”, https://doi.org/10.18710/GTI2BR , DataverseNO, V1.\nSönning, Lukas & Manfred Krug. 2021. Actually in contemporary British speech: Data from the Spoken BNC corpora, https://doi.org/10.18710/A3SATC , DataverseNO, V1.\nSönning, Lukas. 2022. Dataset for “Scrabble yourself to success: Methods in teaching transcription”, https://doi.org/10.18710/2UJHHU , DataverseNO, V1.\n\n\nDissertation\n\nSönning, Lukas. 2021. The TRAP-DRESS contrast in German Learner English: Dataset for chapter 4 in “Phonological variation in German Learner English”, https://doi.org/10.18710/ATIRRV , DataverseNO, V1.\nSönning, Lukas. 2021. Clear vs. dark /l/ in German Learner English: Dataset for chapter 5 in “Phonological variation in German Learner English”, https://doi.org/10.18710/G6PJ5F , DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. English /r/ in German Learner English: Dataset for chapter 6 in “Phonological variation in German Learner English”, https://doi.org/10.18710/YDKDFG , DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. The labio-velar glide /w/ in German Learner English: Dataset for chapter 7 in “Phonological variation in German Learner English”, https://doi.org/10.18710/F1A34O , DataverseNO, V1.\nSönning, Lukas & Isabel Rank. 2021. The labiodental fricative /v/ in German Learner English: Dataset for chapter 8 in “Phonological variation in German Learner English”, https://doi.org/10.18710/B276ZX , DataverseNO, V1.\nSönning, Lukas, Graham Pascoe & Christina Wunder. 2021. The voiced dental fricative in German Learner English: Dataset for chapter 9 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DYAGZG , DataverseNO, V1.\nSönning, Lukas & Graham Pascoe. 2021. Final voiced obstruents in German Learner English: Dataset for chapter 10 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DKIGE5 , DataverseNO, V1."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Plot templates for Microsoft Excel\nExcel templates for some useful graph types can be found in the following  OSF project .\n\nDot diagram\ntemplate | instructions\n\n\n\n\n\nSimple dot plot\ntemplate | instructions\n\n\n\n\n\nGrouped dot plot\ntemplate | instructions\n\n\n\n\n\nBox plot\ntemplate | instructions\n\n\n\n\n\nVertical dot plot\ntemplate\n\n\n\n\n\nScatter plot\ntemplate | instructions\n\n\n\n\n\n\nSpeaker slides for workshop\n\nWürzburg: speaker slides part 1 | speaker slides part 3"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Statistics for linguist(ic)s blog",
    "section": "",
    "text": "Different parameterizations of the negative binomial distribution\n\n\n\n\n\n\n\ncount data\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nnegative binomial distribution\n\n\n\n\nThis blog post discusses two different parameterizations of the negative binomial distribution and groups R packages (and functions) based on the version they implement.\n\n\n\n\n\n\nDec 13, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nThe negative binomial distribution: A visual explanation\n\n\n\n\n\n\n\ncount data\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nnegative binomial distribution\n\n\n\n\nThis blog post uses a visual approach to explain how the negative binomial distribution works.\n\n\n\n\n\n\nDec 12, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nA computational shortcut for the dispersion measure DA\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndispersion\n\n\n\n\nThis short blog post draws attention to the computational shortcut given in Wilcox (1973) for calculating the dispersion measure DA.\n\n\n\n\n\n\nDec 11, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nThe replication crisis: Implications for myself\n\n\n\n\n\n\n\nreplication crisis\n\n\nopen science\n\n\nreproducibility\n\n\n\n\nIn this blog post, I reflect on the ways in which learning about the replication crisis in science has affected my own work.\n\n\n\n\n\n\nNov 21, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nStructured down-sampling: Implementation in R\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\n\nThis blog post shows how to implement structured down-sampling in R.\n\n\n\n\n\n\nNov 18, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nTwo types of down-sampling in corpus-based work\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\n\nThis short blog post contrasts the different ways in which the term down-sampling is used in corpus-based work.\n\n\n\n\n\n\nNov 17, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\n‘Dispersion’ in corpus linguistics and statistics\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nterminology\n\n\n\n\nThis blog post clarifies the different ways in which the term dispersion is used in corpus linguistics and statistics.\n\n\n\n\n\n\nNov 16, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-18_dispersion_terminology/index.html",
    "href": "posts/2023-01-18_dispersion_terminology/index.html",
    "title": "‘Dispersion’ in corpus linguistics and statistics",
    "section": "",
    "text": "R setup\nlibrary(lattice)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nIn corpus linguistics, the term dispersion is used to describe the distribution of an item or structure in a corpus (see Gries 2008, 2020). For most dispersion measures, a corpus must first be divided into units (or parts). These units commonly reflect the design of the corpus – they can be text files, for instance, or text categories. A dispersion index then describes the distribution of an item across these units. There are two general classes of measures:\n\nthose measuring the pervasiveness of an item, which is reflected in the number of units that contain the item (Range and Text Dispersion, its proportional analog)\nthe much larger class of evenness measures, which express how evenly an item is distributed across the units (e.g. D, D2, S, DP, DA, DKL).\n\nMost dispersion measures range between 0 and 1, where 1 indicates a perfectly even distribution, or the maximal degree of pervasiveness (i.e. the item occurs in every unit).\nFrom a statistical viewpoint, the input for the calculation of evenness measures would be considered a count variable, since it records the number of events (occurrences of the item) that are observed during a certain period of observation. In corpus linguistics, the “period of observation” is “text time”, expressed as a word count.\nThere is an extensive literature on the use of regression models for count variables (e.g. Long 1997; Cameron and Trivedi 2013; Hilbe 2014), and such models have seen some successful applications to word frequency data (e.g. Mosteller and Wallace 1984; Church and Gale 1995); Winter and Bürkner (2021) provide an accessible introduction for linguists. In this literature, the term “dispersion” is also used, though with a different (apparently opposite) meaning.\nLet us first consider the corpus-linguistic (and lexicographic) sense, which can be best described visually, using a so-called “dispersion plot”. Figure 1 shows a dispersion plot for two corpora, A and B. The framed rectangles represent the sequence of words forming the corpus, and the spikes inside of these locate the occurrences of a specific item in the corpus. In corpus A, the item is spread out quite evenly. In corpus B, instances are more densely clustered, and there are large stretches where the item does not occur. In the corpus-linguistic sense, then, the dispersion of the item is greater in corpus A. The dispersion score for the item would be greater in Corpus A (i.e. closer to 1).\n\n\nR code: Figure 1\nset.seed(2000)\n\nn_tokens_A &lt;- c(3,5,4,4,3,4,4,5)\nn_tokens_B &lt;- c(5,0,1,9,0,1,0,3)\n\nn_texts &lt;- length(n_tokens_A)\n\nA_loc &lt;- rep(1:n_texts, n_tokens_A)+runif(sum(n_tokens_A))\nB_loc &lt;- rep((1:n_texts)[n_tokens_B!=0], n_tokens_B[n_tokens_B!=0])+runif(sum(n_tokens_B))\n\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,14), ylim=c(2.8,6),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=5.1, ybottom=4.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    panel.rect(xleft=1, xright=n_texts+1, ytop=5.1, ybottom=4.75, \n               border=\"grey50\", lwd=1)\n    \n\n        \n    panel.segments(x0=A_loc, x1=A_loc, y0=4.8, y1=5.05, lwd=.75)\n    panel.text(x=(1:n_texts)+.5, y=4.55, label=n_tokens_A, \n               col=\"grey50\", cex=.9)\n    \n    \n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=4.1, ybottom=3.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    \n    panel.rect(xleft=1, xright=n_texts+1, ytop=4.1, ybottom=3.75, \n               border=\"grey60\", lwd=1)\n    \n    panel.segments(x0=B_loc, x1=B_loc, y0=3.8, y1=4.05, lwd=.75)\n    \n    panel.text(x=(1:n_texts)+.5, y=3.55, label=n_tokens_B, \n               col=\"grey60\", cex=.9)\n    \n    panel.text(x=.4, y=c(4,5)-.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"Lower dispersion\\n\", \"Higher dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"\\n(more concentrated)\", \"\\n(more spread out)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.segments(x0=1, x1=2, y0=5.4, y1=5.4, lwd=.5, col=\"grey50\")\n    panel.segments(x0=1:2, x1=1:2, y0=5.4, y1=5.3, lwd=.5, col=\"grey50\")\n    \n    \n    panel.text(x=1.5, y=5.75, label=\"Text 1\", col=\"grey40\", cex=.8)\n    panel.text(x=7, y=2.75, label=\"Occurrences\\nof item in text\", col=\"grey40\", \n               cex=.9, lineheight=.85)\n    panel.segments(x0=5.9, x1=5.6, y0=3, y1=3.3, col=\"grey40\", lwd=.5)\n    })\n\n\n\n\n\nFigure 1: Dispersion in the corpus-linguistic sense: Distribution of word tokens in the corpus.\n\n\n\n\nNote how each corpus is divided into 8 texts, which are shown in Figure 1 using greyshading. The numbers below the dispersion plot for each corpus report the number of occurrences of the item in each text. For corpus A, they range between 3 and 5; for corpus B, between 0 and 9.\nFigure 2 shows a different representation of these data. Instead of looking at the corpus as a string of words, we consider the text-specific frequencies (sometimes called sub-frequencies) of the item. These indicate how often the item occurs in each document. Figure 2 shows these text-level token counts: Each text is represented by a dot, which marks how often the item appears in the text. In our hypothetical corpora, each text has the same length, which is why we can compare absolute counts. If texts differ in length, we would instead use normalized frequencies, i.e. occurrence rates such as “3.1 per thousand words”.\n\n\nR code: Figure 2\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,7.5), ylim=c(-.35,2.3),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.dotdiagram(1+(n_tokens_A/5), y_anchor=1, scale_y=.125, set_cex=1.3)\n    panel.dotdiagram(1+(n_tokens_B/5), y_anchor=0, scale_y=.125, set_cex=1.3)\n    panel.segments(x0=1, x1=3.2, y0=1, y1=1)\n    panel.segments(x0=1, x1=3.2, y0=0, y1=0)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=1, y1=.95)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=0, y1=-.05)\n    panel.text(x=1+c(0,5,10)/5, y=-.2, label=c(0,5,10), col=\"grey40\", cex=.8)\n    \n    panel.text(x=.6, y=c(0,1)+.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"Higher dispersion\\n\", \"Lower dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"\\n(more spread out)\", \"\\n(more concentrated)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.text(x=2, y=-.5, label=\"Occurrences of item\", cex=1, lineheight=.9)\n    panel.text(x=3, y=2.2, label=\"Each dot\\nrepresents a text\", cex=.9, \n               lineheight=.9, col=\"grey40\")\n    })\n\n\n\n\n\nFigure 2: Dispersion in the statistical sense: Distribution of text-level ocurrence rates.\n\n\n\n\nIf we compare the distribution of text-level occurrence rates in the two corpora, we note that while the texts in corpus A form a dense pile, the occurrence rates in corpus B are more widely spread out. At this level of description, then, it is the data from corpus B that show greater “dispersion”. In the statistical literature on count regression, the term dispersion is used in this sense, i.e. to refer to the variability of unit-specific (i.e. text-level) occurrence rates (e.g. Long 1997, 221; Gelman 2021, 264–68). An awareness of the different meanings of “dispersion” will prove helpful for corpus linguists (and lexicographers) when engaging with the statistical literature on count data modeling.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe term “dispersion” is used differently in corpus linguistics and statistics\nThe difference in meaning reflects a difference in perspective\nCorpus linguists picture the corpus as a sequence of words and understand the term as characterizing the spatial distribution of an item\nIn the statistical literature on count data modeling, the term describes the spread of a distribution of counts or occurrence rates\n\n\n\n\n\n\n\nReferences\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. Second edition. New York: Cambridge University Press.\n\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/S1351324900000139.\n\n\nGelman, Hill, Andrew. 2021. Regression and Other Stories. Cambridge: Cambridge University Press.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data. New York: Cambridge University Press.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage.\n\n\nMosteller, Frederick, and David L. Wallace. 1984. Applied Bayesian Inference: The Case of the Federalist Papers. New York: Springer.\n\n\nWinter, Bodo, and Paul‐Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11). https://doi.org/10.1111/lnc3.12439.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {“{Dispersion}” in Corpus Linguistics and Statistics},\n  date = {2023-11-16},\n  url = {https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “‘Dispersion’ in Corpus\nLinguistics and Statistics.” November 16, 2023. https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_two_types/index.html",
    "href": "posts/2023-11-17_downsampling_two_types/index.html",
    "title": "Two types of down-sampling in corpus-based work",
    "section": "",
    "text": "The data available from corpora are often too vast for certain types of linguistic analysis. Researchers are then forced to select a subset of the data, and this selection process can be referred to as “down-sampling”. Currently, the term is used to refer to two very different types of down-sizing.\nThe first deals with lists of occurrences extracted from a corpus and is used in studies that start out with a corpus query and a body of hits (often in the form of concordance lines). If the structure of interest is relatively frequent and/or the source corpus large, the researcher may need to reduce the number of data points studied. In particular, this will be necessary in variationist-type research, which often involves considerable manual work (e.g. disambiguation and annotation). In this form of down-sampling, the selection of elements usually proceeds (to some extent) at random, i.e. it involves a chance component. Simple techniques are implemented in corpus software, which allows users to extract from a list of hits a random sample. In CQPweb (Hardie 2012), for instance, this option is referred to as “thinning”. Depending on our research goals and the structure of our data, however, other strategies may be more efficient (e.g. structured down-sampling, see Sönning and Krug 2022).\nThe second type of down-sampling is concerned with the selection of texts for close reading. Here, the objective is to pick from a corpus those texts that are likely to be most informative for a thorough qualitative analysis. This method, which Gabrielatos et al. (2012) refer to as “targeted down-sampling”, uses surface-level features (such as the occurrence rate of certain forms) to detect relevant documents for a critical discourse analysis (see also Baker et al. 2008, 285). A procedure much in the same spirit is discussed in Anthony and Baker (2015), where prototypical exemplars, i.e. texts that are most representative of their corpus of origin, are selected based on keyword profiles.\nIt may therefore sometimes be helpful to distinguish the two types of down-sampling: We could call the first type “selection of concordance lines for annotation” and the second type “selection of texts for close reading”.\n\n\n\n\nReferences\n\nAnthony, Laurence, and Paul Baker. 2015. “ProtAnt: A Tool for Analysing the Prototypicality of Texts.” International Journal of Corpus Linguistics, August, 273–92. https://doi.org/10.1075/ijcl.20.3.01ant.\n\n\nBaker, Paul, Costas Gabrielatos, Majid KhosraviNik, Michał Krzyżanowski, Tony McEnery, and Ruth Wodak. 2008. “A Useful Methodological Synergy? Combining Critical Discourse Analysis and Corpus Linguistics to Examine Discourses of Refugees and Asylum Seekers in the UK Press.” Discourse &Amp; Society 19 (3): 273–306. https://doi.org/10.1177/0957926508088962.\n\n\nGabrielatos, Costas, Tony McEnery, Peter J. Diggle, and Paul Baker. 2012. “The Peaks and Troughs of Corpus-Based Contextual Analysis.” International Journal of Corpus Linguistics 17 (2): 151–75. https://doi.org/10.1075/ijcl.17.2.01gab.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Two Types of down-Sampling in Corpus-Based Work},\n  date = {2023-11-17},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Two Types of down-Sampling in Corpus-Based\nWork.” November 17, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_implementation/index.html",
    "href": "posts/2023-11-17_downsampling_implementation/index.html",
    "title": "Structured down-sampling: Implementation in R",
    "section": "",
    "text": "I recently consulted colleagues on how to down-sample their corpus data. Their study deals with modal auxiliaries in learner writing, and they are also interested in the semantics of modal verbs. This means that they have to manually annotate individual tokens of modals. In this blog post, I describe how we implemented structured down-sampling (Sönning and Krug 2022) in R. The data we use for illustration is a simplified subset of the originial list of corpus hits. We will concentrate on the modal verb can.\n\n\nR setup\nlibrary(tidyverse)\n\nd &lt;- read_tsv(\"./data/modals_data.tsv\")\n#d &lt;- read_tsv(\"./posts/2023-11-17_downsampling_implementation/data/modals_data.tsv\")\n\n\n\nThe data\nThe data include 300 tokens, which are grouped by Text (i.e. learner essay), and there are 162 texts where can occurs at least once. The distribution of tokens across texts is summarized in Figure 1: In most texts (n = 83), can occurs only once, 41 texts feature two occurrences, and so on.\n\n\nR code: Figure 1\nd |&gt; \n  group_by(text_id) |&gt; \n  tally() |&gt; \n  group_by(n) |&gt; \n  tally() |&gt; \n  ggplot(aes(x=n, y=nn)) + \n  geom_col(width = .7, fill=\"grey\") +\n  theme_classic() +\n  scale_x_continuous(breaks = 1:7) +\n  xlab(\"Number of occurrences\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\nFigure 1: Distribution of token counts across texts.\n\n\n\nA different arrangement of the data is shown in Figure 2, where texts are lined up from left to right. Each text is represented by a pile of dots, with each dot representing a can token. The text with the highest number of can tokens (n = 7) appears at the far left, and about half of the texts only have a single occurrence of can – these text are sitting in the right half of the graph.\n\n\nR code: Figure 2\nd |&gt;  \n  group_by(text_id) |&gt; \n  mutate(n_tokens = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x=reorder(text_id, -n_tokens))) + \n  geom_dotplot(dotsize = .13, stackratio=1.6) +\n  theme_void() +\n  labs(subtitle=\"Texts ranked by token count\",\n       caption = \"Each dot represents a token (can)\")\n\n\n\n\n\nFigure 2: Distribuition of tokens across texts.\n\n\n\n\n\n\nStructured down-sampling\nAs argued in Sönning and Krug (2022), structured down-sampling would be our preferred way of drawing a sub-sample from these data. In contrast to simple down-sampling (or thinning), where each token has the same probability of being selected, structured down-sampling aims for a balanced representation of texts in the sub-sample. Thus, we would aim for breadth of representation and only start selecting additional tokens from the same text if all texts are represented in our sub-sample. The statistical background for this strategy is discussed in Sönning and Krug (2022).\nLooking at Figure 2, this means that our selection of tokens would first consider the “bottom row” of dots in the graph, and then work upwards if necessary, i.e. sample one additional token (at random) from each text that contains two or more occurrences, and so on. It should be noted that, at some point, little more is learned by sampling yet further tokens from a specific text (see discussion in Sönning and Krug 2022, 147).\n\n\nImplementation in R\nOur first step is to add to the table a column that preserves the original order. This is important in case we want to return to the original arrangement at a later point. We will name the new column original_order.\n\nd$original_order &lt;- 1:nrow(d)\n\nThere may be settings where, due to resource constraints, we cannot pick a token from every single text. Or, similarly, where we cannot pick a second token from each text that contains at least two tokens. In such cases, a sensible default approach is to pick at random. Thus, if we were only able to analyze 100 tokens, but there are 162 texts in our data, we would like to pick texts at random. We therefore add another column where the sequence from 1 to N (the number of rows, i.e. tokens) is shuffled. This column will be called random_order. Further below, we will see how this helps us out.\n\nd$random_order &lt;- sample(\n  1:nrow(d), \n  nrow(d), \n  replace=F)\n\nThe next step is to add a column to the table which specifies the order in which tokens should be selected from a text. We will call the column ds_order (short for ‘down sampling order’). In texts with a single token, the token will receive the value 1, reflecting its priority in the down-sampling plan. For a text with two tokens, the numbers 1 and 2 are randomly assigned to the two tokens. For texts with three tokens, the numbers 1, 2 and 3 are shuffled, and so on. If we then sort the whole table according to the column ds_order, those tokens that are to be preferred, based on the rationale underlying structured down-sampling, appear at the top of the table.\nOur first step is to order the table by text_id, to make sure rows are grouped by Text.\n\nd &lt;- d[order(d$text_id),]\n\nWe then create a list of the texts in the data and sort it, so that it matches the way in which the table rows have just been ordered.\n\ntext_list &lt;- unique(d$text_id)\ntext_list &lt;- sort(text_list)\n\nWe now create the vector ds_order, which we will add to the table once it’s ready:\n\nds_order &lt;- NA\n\nThe following loop fills in the vector ds_order, text by text. It includes the following steps (marked in the script):\n\nProceed from text to text, from the first to the last in the text_list.\nFor text i, count the number of tokens in the text and store it as n_tokens.\nShuffle the sequence from 1 to n_tokens and store it as shuffled.\nAppend the shuffled sequence shuffled to the vector ds_order.\n\n\nfor(i in 1:length(text_list)){  # (1)\n  \n  n_tokens &lt;- sum(              # (2)\n    d$text_id == text_list[i])  # \n  \n  shuffled &lt;- sample(           # (3)\n    1:n_tokens,                 #\n    size = n_tokens,            #\n    replace = FALSE)            #\n  \n  ds_order &lt;- append(           # (4)\n    ds_order,                   #\n    shuffled)                   #\n}\n\nIf we look at the contents of ds_order, we note that it still has a leading NA:\n\nds_order\n\n  [1] NA  2  1  3  1  2  1  1  2  2  1  2  3  1  4  1  3  2  1  2  1  1  2  1  1\n [26]  1  1  1  3  4  1  2  1  1  2  3  1  1  2  3  1  1  2  1  1  1  1  2  1  4\n [51]  3  3  1  2  1  2  2  1  2  3  1  1  5  6  3  4  2  2  4  3  1  2  1  1  2\n [76]  4  3  1  2  1  2  1  4  3  1  1  1  1  2  1  1  1  1  3  2  1  1  2  2  4\n[101]  1  3  1  1  1  2  1  1  1  1  1  2  3  1  1  1  2  1  1  1  1  3  2  1  1\n[126]  1  2  2  3  1  1  1  2  1  3  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  3  2  1  2  1  1  1  2  1  1  2  3  1  1  1  2  1  1  2  1  1  1  2  1\n[176]  1  2  3  1  2  2  3  1  2  1  3  1  3  2  1  2  1  2  1  2  1  4  3  1  3\n[201]  2  1  2  1  3  1  1  2  2  1  1  3  2  1  1  1  1  2  2  1  1  1  1  2  2\n[226]  3  1  1  1  2  3  1  2  1  1  1  2  1  1  1  1  2  1  3  4  2  1  1  3  2\n[251]  4  3  2  1  1  2  1  2  1  1  3  5  2  4  1  1  2  1  1  2  1  2  1  1  2\n[276]  7  6  5  3  2  1  4  1  1  1  2  1  1  1  1  2  4  2  1  3  1  4  3  2  2\n[301]  1\n\n\nSo we get rid of it:\n\nds_order &lt;- ds_order[-1]\n\nWe can now add ds_order as a new column to our table:\n\nd$ds_order &lt;- ds_order\n\nThe final step is to order the rows of the table in a way that reflects our down-sampling priorities. We therefore primarily order the table based on ds_order. In addition, we order by the column random_order, which we created above. All tokens with the same priority level (e.g. all tokens with the value “1” in the column ds_order) will then be shuffled, ensuring that the order of tokens is random.\n\nd &lt;- d[order(d$ds_order, \n             d$random_order),]\n\nWe can now look at the result:\n\nhead(d)\n\n# A tibble: 6 x 7\n  text_id  left_context modal right_context original_order random_order ds_order\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;int&gt;\n1 text_19  music        can   just                     109            1        1\n2 text_158 you          can   only                     289            3        1\n3 text_123 and          cann~ distinguish              148            4        1\n4 text_69  How          can   this                      48            5        1\n5 text_88  music        can   also                      25            9        1\n6 text_156 One          can   not                      167           13        1\n\n\nNote that the strategy we have used, i.e. adding a column reflecting the priority of tokens for down-sampling, allows us to approach down-sampling in a flexible and adaptive way: Rather than actually selecting (or sampling) tokens (or rows) from the original data, we may now simply start analyzing from the top of the table. This way we remain flexibility when it comes to the choice of how many tokens to analyze.\n\n\n\n\n\nReferences\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Structured down-Sampling: {Implementation} in {R}},\n  date = {2023-11-18},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Structured down-Sampling: Implementation in\nR.” November 18, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "title": "The replication crisis: Implications for myself",
    "section": "",
    "text": "Since my research is almost exclusively quantitative, the methodological discourse surrounding the replication crisis has been directly relevant to my work. A recent invitation to take part in an online event by the International Society for the Linguistics of English (ISLE) on “Replication and Replicability” was an opportunity to reflect on the ways in which this “crisis” has affected how I do my job. In this blog post, I summarize these under three headings: (i) workflow and reproducibility, (2) open science, and (3) community discourse.\nI would like to start, however, with two preliminary remarks. For one, I consider the discussions, suggestions, and innovations that have arisen in the context of the credibility crisis in science as an opportunity – they should inspire us to improve the way(s) in which we do and communicate research. While there are some who point out that we actually don’t know whether there is a replication crisis in linguistics1, the suggested ways forward enable better science, so it is worth adopting them in any case.\nFurther, if we decide to change our research routines, we should be indulgent with ourselves: Many of the suggested improvements, especially concerning data analysis workflow, can be quite overwhelming at first. We should avoid setting our immediate aims too high – as I had to find out on numerous occasions, it is too easy to become frustrated. And this may also be something to keep in mind when making recommendations: The advice we give to others should be calibrated to the person across the table. Nothing is gained if a researcher with a genuine interest in adopting better practices ends up quitting in frustration."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "title": "The replication crisis: Implications for myself",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt seems that we are not particularly eager to find out (see, e.g., this preprint by Kristina Kobrock and Timo Roettger). It would be quite surprising, however, if linguist(ic)s were spared – after all, the same human factors are at work in language research as in neighboring disciplines such as psychology.↩︎"
  },
  {
    "objectID": "posts/2023-12-11_computation_DA/index.html",
    "href": "posts/2023-12-11_computation_DA/index.html",
    "title": "A computational shortcut for the dispersion measure DA",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by Burch, Egbert, and Biber (2017) as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to Wilcox (1973), a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While Wilcox (1973) focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as Burch, Egbert, and Biber (2017, 193) note, a measure equivalent to DP (Gries 2008) can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in Wilcox (1973) as the mean difference analog (MDA). Both Wilcox (1973) and Burch, Egbert, and Biber (2017) argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in Sönning (2023).\nGries (2020, 116) has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula Wilcox (1973) gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in Wilcox (1973).\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to Sönning (2023). We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1985)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.48 seconds to run. The computational formula is quicker – it completes the calculations in only 0 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 34.85 seconds to run; the shortcut procedure, on the other hand, is done after 0.01 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n1.48\n34.85\n0.6062\n0.6145\n\n\nComputational\n0.00\n0.01\n0.6064\n0.6145\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nSönning, Lukas. 2023. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” PsyArxiv Preprint. https://doi.org/10.31234/osf.io/ns4q9.\n\n\nWilcox, Allen R. 1973. “Indices of Qualitative Variation and Political Measurement.” The Western Political Quarterly 26 (2): 325–43. https://doi.org/10.2307/446831.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {A Computational Shortcut for the Dispersion Measure\n    {*D\\textasciitilde A\\textasciitilde*}},\n  date = {2023-12-11},\n  url = {https://lsoenning.github.io/posts/2023-12-11_computation_DA/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “A Computational Shortcut for the Dispersion\nMeasure *D~A~*.” December 11, 2023. https://lsoenning.github.io/posts/2023-12-11_computation_DA/."
  },
  {
    "objectID": "posts/2023-11-16_negative_binomial/index.html",
    "href": "posts/2023-11-16_negative_binomial/index.html",
    "title": "The negative binomial distribution: A visual explanation",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(gamlss)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe negative binomial distribution is a useful device for modeling word counts. A typical setting for its application in corpus linguistics is the modeling of word frequency data – for instance, if we wish to summarize (or compare) occurrence rates of an item in a corpus (or across sub-corpora). Each text then contributes information about the frequency of the item in the form of (i) a token count (the number of times the word occurs in the text) and (ii) a word count (the length of the text). Based on the token and word count we can calculate an occurrence rate (or normalized frequency) for each text, and these rates are then directly comparable across texts.\nFrom a statistical perspective, word frequency would be considered as a count variable, which is observed at the level of the text and can take on non-negative integer values (i.e. 0, 1, 2, 3, 4, …). The text length can be thought of as a period of observation (measured in text time, i.e. the number of running words), in which a tally is kept of the number of events (in this case the occurrence of the focal item). And this is the typical definition of a count variable.\nThis blog post takes a closer look at the negative binomial distribution – how it works and why it is a useful device for modeling word frequency data. It is helpful to start with a concrete example: the frequency of which in the Brown Corpus. To keep things simple, we will stick to this data setting, where texts have (nearly) the same length. Note, however, that the negative binomial distribution (like the Poisson) readily extends to situations where texts have different lengths.\n\nObserved and expected frequency distributions\nIf we count the number of occurrences of which in each text and then look at the distribution of token counts, we obtain what is referred to as a frequency distribution or a token distribution. The frequency distribution for which in the Brown Corpus, which consists of 500 texts, appears in Figure 1 a. It shows the distribution of token counts across texts: Each bar represents a specific token count, and the height of the bar is proportional to the number of texts that have this many instances of which. Token counts vary between 0 (n = 26 texts) and 40 (1 text), and the distribution is right-skewed, which is quite typical of count variables, since they have a lower bound at 0.\n\n\nLoad data\n# tdm &lt;- read_tsv(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/brown_tdm.tsv\")\n# \n# str(tdm)\n# \n# n_tokens &lt;- tdm[,which(colnames(tdm) == \"which\")]\n# \n# \n# saveRDS(n_tokens, \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\nn_tokens &lt;- readRDS(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\n\n\n\nDraw Figure 1\n# Poisson model\nm &lt;- glm(n_tokens$which ~ 1, family=\"poisson\")\npoisson_mean &lt;- exp(coef(m))\npoisson_density &lt;- dpois(0:40, lambda = poisson_mean)\n\n\nn_texts &lt;- as.integer(table(n_tokens))\ntoken_count &lt;- as.integer(names(table(n_tokens)))\n\np1 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 53), xlim=c(-1.5, NA),\n  xlab.top = \"(a)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"\",\n  ylab=\"Observed\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=30, y=40, label=\"Observed frequency distribution\", \n               col=\"grey30\", cex=.9)\n    })\n\np2 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 85), xlim=c(-1.5, NA),\n  xlab.top = \"(b)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"Number of instances of which\",\n  ylab=\"Expected\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, type=\"h\")\n    panel.text(x=30, y=60, label=\"Expected frequency distribution\\n(Poisson model)\",\n               col=\"grey30\", cex=.9)\n    })\n\n\n\n\n\n\n\nFigure 1: Which in the Brown Corpus: (a) observed frequency distribution and (b) expected frequency distribution based on the Poisson model.\n\n\n\n\nThe most basic probability distribution that is available for modeling count variables is the Poisson distribution. In general, we can check the fit of a distribution to the observed token counts by comparing the observed distribution (Figure 1 a) to the one expected under a Poisson model. The expected distribution appears in Figure 1 b. We note a mismatch with the observed data: Its tails are too thin – that is, the observed token counts are more widely spread out; counts of 0 are severely underpredicted (or underrepresented).\nIn fact, it is often the case that the Poisson distribution offers a poor fit to (language) data. This is because it rests on a simplistic assumption: It assumes that the expected frequency of which (or: the underlying probability of using which) is the same in every text. In our case, where we are dealing with texts of roughly 2,000 words in length, the expected number of instances of which, on average, is 7.1. Due to sampling variation, the actual number of instances per text will vary around this average. This sampling variation is accounted for in the Poisson distribution, giving it the (near-)bell-shaped appearance in Figure 1 b.\nIn linguistic terms, the model assumes that each text in the Brown Corpus, irrespective of genre or the idiosyncrasies of its author, has the same underlying probability of using which (i.e. about 7 in 2,000; or 3.5 per thousand words). Even for a function word such as which, this assumption seems difficult to defend. For instance, certain genres may use more postmodifying relative clauses, leading to a higher expected rate of which for texts in this category.\n\n\nPoisson mixture distributions\nTo offer a more adequate abstraction (or representation) of the observed token distribution, the assumption of equal rates across texts needs to be relaxed. We want the model to be able to represent variation among texts, and to record the amount of variation suggested by the data. On linguistic grounds, for instance, we would expect function words to vary less from text to text than lexical words, which are more sensitive to register and topic. The idea is to have an additional parameter in the model that acts like a standard deviation, essentially capturing (and measuring) the text-to-text variability in occurrence rates.\nIt is for this purpose that Poisson mixture distributions were invented. One such mixture distribution is the negative binomial distribution, which is also referred to as a Poisson-gamma mixture distribution. This is actually a more transparent label, as we will see shortly.\nThe idea behind Poisson mixtures is rather simple. Since the Poisson distribution on its own fails to adequately embrace high and low counts, its mean is allowed to vary. By allowing the Poisson mean to vary, i.e. shift up and down the count scale (or left and right in Figure 1), the probability distribution is more flexible, which allows it to accommodate the tails of the distribution.\nPoisson mixtures therefore include an additional dispersion parameter (similar to a standard deviation) and the Poisson mean is replaced by a distribution of Poisson means. Note that the way in which the term “dispersion” is used here differs from the sense it has acquired in lexicography and corpus linguistics (the difference is explained in this blog post).\n\n\nDraw Figure 2\nset.seed(1985)\n\ndelta_sample = rGA(20, mu=1, sigma=.1)\nlambda_plot = 7\nplot1 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    for(i in 1:length(delta_sample)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\ndelta_sample2 = rGA(20, mu=1, sigma=.25)\nlambda_plot = 7\nplot2 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    panel.text(x=7, y=.22, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_sample2)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\n\n\n\n\n\n\n\nFigure 2: Mixing Poisson distributions: Each panel shows a sample of 20 Poisson distributions whose means vary around the grand mean of 7. The variation among Poissons is greater in the top panel.\n\n\n\nPoisson mixtures can be thought of as consisting of multiple Poisson distributions with different individual means. This is illustrated in Figure 2. To be able to show multiple distributions in one graph, we now leave out the spikes and connect the dots – a single distribution therefore appears as a bell-shaped profile that looks like a pearl necklace. Each panel shows 20 Poisson distributions, and each of these 20 distributions has a different mean. The means vary around 7, the overall mean of the Poisson mixture.\nThe distributions in the upper panel are spread out more widely than in the lower panel, and it is the newly introduced dispersion parameter that expresses the amount of variation among Poisson means. This basic idea applies to all Poisson mixture distributions. They are called ‘mixture distributions’ because they mix two probability distributions: (i) the familiar Poisson distribution and (ii) an additional probability distribution which describes the variability in the Poisson means. Simplifying slightly, Poisson mixtures only differ in the probability distribution they employ to describe the distribution of the Poisson means.\n\n\nThe gamma distribution as a model of text-to-text variation\nThe negative binomial distribution, for instance, relies on the gamma distribution to describe the text-to-text variability in occurrence rates. It is therefore also called a Poisson-gamma mixture distribution. Figure 3 shows the two gamma distributions that were used to create Figure 2. The dashed curve, which shows greater spread, belongs to the upper panel.\n\n\nDraw Figure 3\nlambda_plot = 7\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20/7), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(a)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(b)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=expression(\"Number of instances of \"~italic(which)),\n  panel=function(x,y,...){\n    panel.segments(x0=7, x1=7, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing the variability of text-to-text occurrence rates.\n\n\n\nFigure 3 a shows the gamma distributions on their actual scale. These are spread out around a value of 1, because they indicate variability in Poisson means on a multiplicative scale. It makes sense to center the distribution around 1, since the overall occurrence rate (multiplied by 1) should be at the center. The x-axis therefore denotes factors, which means that variability between Poisson means is expressed as ratios. The dashed curve, for instance, ranges from roughly 0.5 to 1.5, which means that most Poisson means are found within ± 50% of the overall mean. Since this multiplicative factor cannot be smaller than 0, we need a probability distribution that is bounded at zero (like the gamma distribution).\nPanel (b) translates these distributions to the occurrence rate scale. To create this graph, the factors (i.e. the x-values) in panel (a) were simply multiplied by the overall mean of 7. Now we see that, for the dashed curve, most occurrence rates vary between 4 and 11 instances per text.\n\n\nNegative binomial distribution applied to which\nLet us now apply the negative binomial distribution to the data for which in the Brown Corpus. We first check the fit of this new model to the data. Figure 4 shows that it provides a much closer approximation to the observed token distribution. It accomodates low and high counts and there seems to be no systematic lack of fit.\n\n\nFit negative binomial model in R\nm &lt;- gamlss(n_tokens$which ~ 1, family=\"NBI\", trace = FALSE)\n\nnb_density &lt;- dNBI(\n  0:40, \n  mu = exp(coef(m, what = \"mu\")),\n  sigma = exp(coef(m, what = \"sigma\")))\n\n\n\n\nDraw Figure 4\nxyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 77), xlim=c(-1.5, NA),\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = expression(\"Number of instances of \"~italic(which)),\n  ylab=\"Number of texts\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=10, y=60, label=\"Poisson\", \n               col=\"grey30\", cex=.9, adj=0)\n    panel.text(x=20, y=12, label=\"Negative binomial\", \n               col=1, cex=.9, adj=0)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", type=\"l\")\n    \n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, type=\"l\")\n    })\n\n\n\n\n\nFigure 4: Which in the Brown Corpus: Observed token distribution compared against the Poisson and the negative binomial model.\n\n\n\n\nLet us consider the gamma distribution that describes the text-to-text variability in occurrence rates. Its density appears in Figure 5, which includes two x-axes: A multiplicative scale (bottom) and a scale showing the expected number of instances in a 2,000-word text (the average text length in Brown). The gamma distribution is centered at 1 (multiplicative scale) and 7.1 occurrences (number of instances of which).\n\n\nDraw Figure 5\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 21/7), ylim=c(0,1.4),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.polygon(x = c(seq(.01, 2.8, length=100), (seq(.01, 2.8, length=100))),\n                 y = c(dGA(seq(.01, 2.8, length=100), mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n                       rep(0,100)),\n                 col=\"lightgrey\", border=F)\n    panel.segments(x0=1, x1=1, y0=0, y1=1.4, col=1)\n    \n    panel.segments(\n      x0 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3,  lwd=2, col=1, lineend=\"butt\", alpha=.5)\n    \n    panel.segments(\n      x0 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3, lwd=.5, col=1, lineend=\"butt\", alpha=.5)\n                      \n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, \n                         sigma=exp(coef(m, what = \"sigma\"))),\n                 type=\"l\")\n    \n    panel.segments(x0=-.05, x1=21/7, y0=1.3, y1=1.3)\n    panel.segments(x0=seq(0,20,5)/exp(coef(m, what = \"mu\")), \n                   x1=seq(0,20,5)/exp(coef(m, what = \"mu\")), y0=1.3, y1=1.38)\n    \n    panel.text(x=seq(0,20,5)/exp(coef(m, what = \"mu\")), y=1.6,\n               label=seq(0,20,5), cex=.8)\n    \n    panel.text(x=1.5, y=2, label=expression(\"Number of instances of \"~italic(which)))\n    })\n\n\n\n\n\n\n\n\nFigure 5: The gamma distribution describing the variability of text-to-text occurrence rates of which in the Brown Corpus.\n\n\n\nThe gamma distribution represents a set of values, which specify the deviation of Poisson means from their overall mean in relative terms, as factors. For example, if the gamma distribution is restricted to the range [0.6; 1.7], the Poisson means will vary by a factor of 0.6 to 1.7 around their overall average. For a grand mean of 7, the Poisson means are then spread out between 7 \\(\\times\\) 0.6 = 4.2 and 7 \\(\\times\\) 1.7 = 11.9.\nThe grey vertical lines facilitate interpretation of the distribution: They show where the middle 50% of the texts (thick lines) and the middle 90% of the texts (thin lines) lie. Thus, half of the texts have an underlying expected number of occurrences between roughly 5 and 9; 90% of texts have expected counts between 2.5 and 14. This gives us a good idea of the underlying text-to-text variation in the Brown Corpus.\n\n\nGraphical derivation of the negative binomial distribution\nTo get a better understanding of the negative binomial distribution shown in Figure 4, let us now build one from scratch. Recall that the gamma distribution that is built into the negative binomial model provides us with a set of values with mean 1. We will refer to scores generated from this kind of gamma distribution as \\(\\delta\\) scores. To spread out the Poisson means, the overall mean is multiplied by the \\(\\delta\\) scores drawn from the gamma distribution. Since the \\(\\delta\\) scores are centered at 1, the overall mean is still 7. A gamma distribution that is spread out more widely produces more widely dispersed Poisson means.\nEssentially, then, a negative binomial distribution represents a batch of Poisson distributions whose individual means are spread out around the overall mean. This conceptual explanation of the negative binomial distribution illustrates the role of the gamma distribution and its auxiliary parameter \\(\\phi\\). We can translate this illustration into a simple simulation experiment. If we average over a large number of Poisson distributions produced by this procedure, we should arrive at the corresponding negative binomial distribution.\nThis is illustrated in Figure 6, which was constructed in the following way:\n\n\nDraw Figure 6\nset.seed(1985)\n\nset_nu = 2\ndelta_s = rGA(1000, mu=1, sigma=sqrt(1/set_nu))\n\nlambda_p = 7\npoisson_pool = matrix(NA, nrow=1000, ncol=21)\nfor (i in 1:1000){\n  poisson_pool[i,] = dpois(0:20, lambda=lambda_p*delta_s[i])\n}\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.45),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y){\n    panel.segments(x0=lambda_p, x1=lambda_p, y0=0, y1=.25, col=\"black\")\n    panel.text(x=7, y=.3, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_s)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", type=\"l\", alpha=.03)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", pch=16, cex=.4, alpha=.03)\n      }\n    # panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=\"white\", lwd=4)\n    # panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n    #              type=\"l\", col=\"white\", lwd=4)\n    panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n                 type=\"l\", col=\"white\", lwd=2)\n    panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=1, lty=\"22\", lineend=\"butt\")\n    })\n\nprint(p1, position=c(0,0,1,.95))\n\n\n\n\n\n\nFigure 6: Graphical derivation of the NB2 distribution: The blue curve shows the approximation based on averaging over 1,000 Poisson distributions whose means are random draws from a gamma distribution with \\(\\small{\\phi^{-1}}\\) = 0.25. The red curve shows the actual negative binomial distribution with \\(\\small{\\phi^{-1}}\\) = 0.25.\n\n\n\n\nSet the overall mean to 7, and the negative binomial dispersion parameter (here: the scale parameter) \\(\\phi^{-1}\\) to 0.5 (which is close to the value obtained for which in the Brown Corpus).\nTake 1,000 random draws from a gamma distribution defined by \\(\\mu\\) = 1 and \\(\\phi^{-1}\\) = 0.5. We refer to these draws as \\(\\delta\\) scores. The average of these scores is 1.\nMultiply 7 by these 1,000 \\(\\delta\\) scores. This produces 1,000 Poisson means, and hence 1,000 Poisson distributions.\nGraph these 1,000 Poisson distributions as pearl necklaces, adding transparency to avoid a cluttered display.\nDetermine the average probability for each count (0, 1, 2, etc.) by averaging over the 1,000 Poisson probabilities for each specific count. These averages should then resemble a negative binomial distribution with \\(\\mu\\) = 7 and \\(\\phi^{-1}\\) = 0.5.\n\nFigure Figure 6 shows the result of this simulation: The actual negative binomial distribution for these data is shown as a white trace, and the results of our simulation, i.e. average probability across the 1,000 simulated Poisson distributions, is shown as a dashed profile. The match is pretty good.\n\n\nDifferent parameterizations of the negative binomial distribution\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two ways. These different parameterizations have consequences for our interpretation of the negative binomial dispersion parameter returned by an analysis. This means that if we are interested in the dispersion parameter, we must know which parameterization our analysis is using. For an overview of which R packages/functions rely on which version of the negative binomial distribution, see this blog post\n\n\n\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {The Negative Binomial Distribution: {A} Visual Explanation},\n  date = {2023-12-12},\n  url = {https://lsoenning.github.io/posts/2023-11-16_negative_binomial/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “The Negative Binomial Distribution: A\nVisual Explanation.” December 12, 2023. https://lsoenning.github.io/posts/2023-11-16_negative_binomial/."
  }
]