[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Sönning",
    "section": "",
    "text": "Post-doc in English linguistics at the University of Bamberg\n\nUniversity of Bamberg\nDepartment of English Linguistics\n\n\n\nContact\n\nAddress: An der Universität 9, D-96047 Bamberg\nOffice: U9/01.11\nPhone: +49 (0)951/863-2267\nEmail: lukas[dot]soenning[at]uni-bamberg[dot]de\n ORCID 0000-0002-2705-395X\n OSF\n\n\n\nResearch interests\n\nStatistical analysis of corpus data\nLanguage variation and change\nGrammatical alternations\nGerman Learner English\nL2 phonology\nData visualization"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Lukas Sönning\n\nShort academic CV\n\n2020 awarded PhD\n2012-2022 Research and teaching assistant at the University of Bamberg\n2006-2012 Studies in English, Geography and Pedagogy at the University of Bamberg\n\n\n\nAwards\n\n2014 Best poster award, Olinco conferenece, Olomouc (“Vowel reduction in German Learner English: Developmental patterns”)\n2018 Best paper by an early career researcher, ICAME39, Tampere (John Sinclair bursary) (“Visual inference for corpus linguistics”)\n\n\n\nTeaching\nUniversity courses\n\nForming (new) words: The morphological architecture of English\nApplied data analysis for linguists\nInvestigating Lerner English\nSecond Language speech: Theory and practice\nMeasuring (your) foreign accent: The acoustic analysis of non-native speech\nEnglish phonetics & phonology\nEnglish grammar analysis\nTranslation English-German (intermediate and advanced level)\nRevision course for state exam candidates: Synchronic linguistics\nRevision course for state exam candidates: Translation English-German\n\nWorkshops\n\n2023 (META-LING conference, Bamberg) Data publication using TROLLing  OSF\n2023 (University of Würzburg) Basics of data analysis using R and RStudio  OSF | slides part 1 | slides part 3\n2022 (University of Würzburg) Basic statistical methods for TEFL research  OSF\n2019 (FJUEL conference, Bayreuth): Using “statistics” to learn about language: What matters (and what doesn’t)  OSF\n2019 (BICLCE conference, Bamberg): The replication crisis in science: Challenges and chances for linguistics  OSF\n2018 (Uppsala University, Sweden) Statistical inference using estimation: Methods for corpus linguistics slides\n2014 (EmMeth conference, Bamberg): Data visualization with R\n2014 (FJUEL conference, Bamberg): Workshop on statistical methods"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Unpublished\n\nSönning, Lukas. (to appear). Drawing on principles of perception: The line plot. In Lukas Sönning & Ole Schützler (eds.) Data visualization in corpus linguistics: Reflections and future directions, VARIENG.  Preprint |  OSF\nSönning, Lukas & Ole Schützler (eds.). (to appear). Data visualization in corpus linguistics: Reflections and future directions, VARIENG.\nSönning, Lukas. (unpublished manuscript). Evaluation of text-level measures of lexical dispersion: Robustness and consistency.  Preprint |  Data |  OSF\nSönning, Lukas. (in review). Count regression models for keyness analysis.  Manuscript |  OSF\nSönning, Lukas. (in review). Down-sampling from hierarchically structured corpus data.  Manuscript |  OSF\nSönning, Lukas. (in review). Advancing our understanding of dispersion measures in corpus research.  Manuscript |  OSF\nSönning, Lukas, Manfred Krug, Fabian Vetter, Anne Leucht, Timo Schmid & Paul Messer. (in review). Latent-variable modeling of ordinal outcomes in language data analysis.\n\n \n\n\nPublished\nMonograph\n\nSönning, Lukas. 2020. Phonological variation in German Learner English. University of Bamberg dissertation. DOI: 10.20378/irb-49135  Open access |  Datasets |  OSF\n\nJournal articles\n\nSönning, Lukas. 2023. Evaluation of keyness metrics: Performance and reliability. Corpus Linguistics and Linguistic Theory. https://doi.org/10.1515/cllt-2022-0116  Preprint |  Data |  OSF\nSönning, Lukas & Jason Grafmiller. 2023. Seeing the wood for the trees: Predictive margins for random forests. Corpus Linguistics and Linguistic Theory. https://doi.org/10.1515/cllt-2022-0083  Preprint |  Data |  OSF\nSönning, Lukas & Valentin Werner. 2021. The replication crisis, scientific revolutions, and linguistics. Linguistics 59(5). 1179–1206. https://doi.org/10.1515/ling-2019-0045  Open access\nSönning, Lukas. 2014. Unstressed vowels in German Learner English: An instrumental study. Research in Language 12(2). 163–173. https://doi.org/10.2478/rela-2014-0001  Open access |  OSF\n\nEdited volumes\n\nSönning, Lukas & Valentin Werner. 2021. The replication crisis: Implications for linguistics. Special issue in Linguistics.  Open access\nChrist, Hanna, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.). 2016. A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium. Bamberg: University of Bamberg Press.  Open access\n\nBook chapters\n\nSönning, Lukas. 2023. (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. In Robert Fuchs (ed.), Speech rhythm in learner and second language varieties of English, 123–157. Singapore: Springer. https://doi.org/10.1007/978-981-19-8940-7_6  Preprint |  Data |  OSF\nSönning, Lukas & Manfred Krug. 2022. Comparing study designs and down-sampling strategies in corpus analysis: The importance of speaker metadata in the BNCs of 1994 and 2014. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 127–159. Cambridge: Cambridge University Press. DOI: 10.1017/9781108589314.006  Link |  OSF |  Data\nSönning, Lukas & Julia Schlüter. 2022. Comparing standard reference corpora and Google Books Ngrams: Strengths, limitations and synergies in the contrastive study of variable h- in British and American English. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 17–45. Cambridge: Cambridge University Press. DOI: 10.1017/9781108589314.002  Link |  OSF\nKrug, Manfred & Lukas Sönning. 2018. Language change in Maltese English: The influence of age and parental languages. In: Patrizia Paggio & Albert Gatt (eds.), The languages of Malta, 247–270. Berlin: Language Science Press. DOI: 10.5281/zenodo.1181801  Open access\n\nProceedings\n\nSönning, Lukas. 2016. The dot plot: A graphical tool for data analysis and presentation. In Hanna Christ, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.), A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium, 101–129. Bamberg: University of Bamberg Press.  Open access\nSönning, Lukas. 2014. Developmental patterns in the reduction of unstressed vowels by German learners of English. In Ludmila Veselovská & Markéta Janebová (eds.), Complex visibles out there: Proceedings of the Olomouc Linguistics Colloquium 2014: Language use and linguistic structure, vol. 4 Olomouc modern language series, 765–778. Olomouc: Palacký University.  Open access |  OSF\nSönning, Lukas. 2013. Scrabble yourself to success: Methods in teaching transcription. In Joanna Przedlacka, John Maidment & Michael Ashby (eds.), Proceedings of the Phonetics Teaching and Learning Conference, UCL, London, 8-10 August 2013. London: Phonetics Teaching and Learning Conference, 87–90.  Open access |  OSF |  Data\n\n \n\n\nDatasets\n\nSönning, Lukas. 2023. Background data (adapted from Jenset & McGillivray 2017) for: Down-sampling from hierarchically structured corpus data, https://doi.org/10.18710/5KCE4U, DataverseNO, V1.\nSönning, Lukas. 2023. Key verbs in academic writing: Dataset for “Evaluation of keyness metrics: Performance and reliability”, https://doi.org/10.18710/EUXSMW, DataverseNO, V1.\nSönning, Lukas. 2022. Speech rhythm in German Learner English: Dataset for “(Re-)viewing the acquisition of rhythm in the light of L2 phonological theories”, https://doi.org/10.18710/GTI2BR, DataverseNO, V1.\nSönning, Lukas & Manfred Krug. 2021. Actually in contemporary British speech: Data from the Spoken BNC corpora, https://doi.org/10.18710/A3SATC, DataverseNO, V1.\nSönning, Lukas. 2022. Dataset for “Scrabble yourself to success: Methods in teaching transcription”, https://doi.org/10.18710/2UJHHU, DataverseNO, V1.\n\n\nDissertation\n\nSönning, Lukas. 2021. The TRAP-DRESS contrast in German Learner English: Dataset for chapter 4 in “Phonological variation in German Learner English”, https://doi.org/10.18710/ATIRRV, DataverseNO, V1.\nSönning, Lukas. 2021. Clear vs. dark /l/ in German Learner English: Dataset for chapter 5 in “Phonological variation in German Learner English”, https://doi.org/10.18710/G6PJ5F, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. English /r/ in German Learner English: Dataset for chapter 6 in “Phonological variation in German Learner English”, https://doi.org/10.18710/YDKDFG, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. The labio-velar glide /w/ in German Learner English: Dataset for chapter 7 in “Phonological variation in German Learner English”, https://doi.org/10.18710/F1A34O, DataverseNO, V1.\nSönning, Lukas & Isabel Rank. 2021. The labiodental fricative /v/ in German Learner English: Dataset for chapter 8 in “Phonological variation in German Learner English”, https://doi.org/10.18710/B276ZX, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe & Christina Wunder. 2021. The voiced dental fricative in German Learner English: Dataset for chapter 9 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DYAGZG, DataverseNO, V1.\nSönning, Lukas & Graham Pascoe. 2021. Final voiced obstruents in German Learner English: Dataset for chapter 10 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DKIGE5, DataverseNO, V1."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Plot templates for Microsoft Excel\nExcel templates for some useful graph types can be found in the following  OSF project.\n\nDot diagram\ntemplate | instructions\n\n\n\n\n\nSimple dot plot\ntemplate | instructions\n\n\n\n\n\nGrouped dot plot\ntemplate | instructions\n\n\n\n\n\nBox plot\ntemplate | instructions\n\n\n\n\n\nVertical dot plot\ntemplate\n\n\n\n\n\nScatter plot\ntemplate | instructions\n\n\n\n\n\n\nSpeaker slides for workshop\n\nWürzburg: speaker slides part 1 | speaker slides part 3"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Structured down-sampling: Implementation in R\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\n\nThis blog post shows how to implement structured down-sampling in R.\n\n\n\n\n\n\nNov 18, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\nTwo types of down-sampling in corpus-based work\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nmethodology\n\n\ndown-sampling\n\n\nterminology\n\n\n\n\nThis short blog post contrasts the different ways in which the term down-sampling is used in corpus-based work.\n\n\n\n\n\n\nNov 17, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n  \n\n\n\n\n‘Dispersion’ in corpus linguistics and statistics\n\n\n\n\n\n\n\ncorpus linguistics\n\n\nstatistics\n\n\ndispersion\n\n\nterminology\n\n\n\n\nThis blog post clarifies the different ways in which the term dispersion is used in corpus linguistics and statistics.\n\n\n\n\n\n\nNov 16, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-18_dispersion_terminology/index.html",
    "href": "posts/2023-01-18_dispersion_terminology/index.html",
    "title": "‘Dispersion’ in corpus linguistics and statistics",
    "section": "",
    "text": "R setup\nlibrary(lattice)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nIn corpus linguistics, the term dispersion is used to describe the distribution of an item or structure in a corpus (see Gries 2008, 2020). For most dispersion measures, a corpus must first be divided into units (or parts). These units commonly reflect the design of the corpus – they can be text files, for instance, or text categories. A dispersion index then describes the distribution of an item across these units. There are two general classes of measures:\n\nthose measuring the pervasiveness of an item, which is reflected in the number of units that contain the item (Range and Text Dispersion, its proportional analog)\nthe much larger class of evenness measures, which express how evenly an item is distributed across the units (e.g. D, D2, S, DP, DA, DKL).\n\nMost dispersion measures range between 0 and 1, where 1 indicates a perfectly even distribution, or the maximal degree of pervasiveness (i.e. the item occurs in every unit).\nFrom a statistical viewpoint, the input for the calculation of evenness measures would be considered a count variable, since it records the number of events (occurrences of the item) that are observed during a certain period of observation. In corpus linguistics, the “period of observation” is “text time”, expressed as a word count.\nThere is an extensive literature on the use of regression models for count variables (e.g. Long 1997; Cameron and Trivedi 2013; Hilbe 2014), and such models have seen some successful applications to word frequency data (e.g. Mosteller and Wallace 1984; Church and Gale 1995); Winter and Bürkner (2021) provide an accessible introduction for linguists. In this literature, the term “dispersion” is also used, though with a different (apparently opposite) meaning.\nLet us first consider the corpus-linguistic (and lexicographic) sense, which can be best described visually, using a so-called “dispersion plot”. Figure 1 shows a dispersion plot for two corpora, A and B. The framed rectangles represent the sequence of words forming the corpus, and the spikes inside of these locate the occurrences of a specific item in the corpus. In corpus A, the item is spread out quite evenly. In corpus B, instances are more densely clustered, and there are large stretches where the item does not occur. In the corpus-linguistic sense, then, the dispersion of the item is greater in corpus A. The dispersion score for the item would be greater in Corpus A (i.e. closer to 1).\n\n\nR code: Figure 1\nset.seed(2000)\n\nn_tokens_A &lt;- c(3,5,4,4,3,4,4,5)\nn_tokens_B &lt;- c(5,0,1,9,0,1,0,3)\n\nn_texts &lt;- length(n_tokens_A)\n\nA_loc &lt;- rep(1:n_texts, n_tokens_A)+runif(sum(n_tokens_A))\nB_loc &lt;- rep((1:n_texts)[n_tokens_B!=0], n_tokens_B[n_tokens_B!=0])+runif(sum(n_tokens_B))\n\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,14), ylim=c(2.8,6),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=5.1, ybottom=4.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    panel.rect(xleft=1, xright=n_texts+1, ytop=5.1, ybottom=4.75, \n               border=\"grey50\", lwd=1)\n    \n\n        \n    panel.segments(x0=A_loc, x1=A_loc, y0=4.8, y1=5.05, lwd=.75)\n    panel.text(x=(1:n_texts)+.5, y=4.55, label=n_tokens_A, \n               col=\"grey50\", cex=.9)\n    \n    \n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=4.1, ybottom=3.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    \n    panel.rect(xleft=1, xright=n_texts+1, ytop=4.1, ybottom=3.75, \n               border=\"grey60\", lwd=1)\n    \n    panel.segments(x0=B_loc, x1=B_loc, y0=3.8, y1=4.05, lwd=.75)\n    \n    panel.text(x=(1:n_texts)+.5, y=3.55, label=n_tokens_B, \n               col=\"grey60\", cex=.9)\n    \n    panel.text(x=.4, y=c(4,5)-.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"Lower dispersion\\n\", \"Higher dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"\\n(more concentrated)\", \"\\n(more spread out)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.segments(x0=1, x1=2, y0=5.4, y1=5.4, lwd=.5, col=\"grey50\")\n    panel.segments(x0=1:2, x1=1:2, y0=5.4, y1=5.3, lwd=.5, col=\"grey50\")\n    \n    \n    panel.text(x=1.5, y=5.75, label=\"Text 1\", col=\"grey40\", cex=.8)\n    panel.text(x=7, y=2.75, label=\"Occurrences\\nof item in text\", col=\"grey40\", \n               cex=.9, lineheight=.85)\n    panel.segments(x0=5.9, x1=5.6, y0=3, y1=3.3, col=\"grey40\", lwd=.5)\n    })\n\n\n\n\n\nFigure 1: Dispersion in the corpus-linguistic sense: Distribution of word tokens in the corpus.\n\n\n\n\nNote how each corpus is divided into 8 texts, which are shown in Figure 1 using greyshading. The numbers below the dispersion plot for each corpus report the number of occurrences of the item in each text. For corpus A, they range between 3 and 5; for corpus B, between 0 and 9.\nFigure 2 shows a different representation of these data. Instead of looking at the corpus as a string of words, we consider the text-specific frequencies (sometimes called sub-frequencies) of the item. These indicate how often the item occurs in each document. Figure 2 shows these text-level token counts: Each text is represented by a dot, which marks how often the item appears in the text. In our hypothetical corpora, each text has the same length, which is why we can compare absolute counts. If texts differ in length, we would instead use normalized frequencies, i.e. occurrence rates such as “3.1 per thousand words”.\n\n\nR code: Figure 2\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,7.5), ylim=c(-.35,2.3),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.dotdiagram(1+(n_tokens_A/5), y_anchor=1, scale_y=.125, set_cex=1.3)\n    panel.dotdiagram(1+(n_tokens_B/5), y_anchor=0, scale_y=.125, set_cex=1.3)\n    panel.segments(x0=1, x1=3.2, y0=1, y1=1)\n    panel.segments(x0=1, x1=3.2, y0=0, y1=0)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=1, y1=.95)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=0, y1=-.05)\n    panel.text(x=1+c(0,5,10)/5, y=-.2, label=c(0,5,10), col=\"grey40\", cex=.8)\n    \n    panel.text(x=.6, y=c(0,1)+.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"Higher dispersion\\n\", \"Lower dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"\\n(more spread out)\", \"\\n(more concentrated)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.text(x=2, y=-.5, label=\"Occurrences of item\", cex=1, lineheight=.9)\n    panel.text(x=3, y=2.2, label=\"Each dot\\nrepresents a text\", cex=.9, \n               lineheight=.9, col=\"grey40\")\n    })\n\n\n\n\n\nFigure 2: Dispersion in the statistical sense: Distribution of text-level ocurrence rates.\n\n\n\n\nIf we compare the distribution of text-level occurrence rates in the two corpora, we note that while the texts in corpus A form a dense pile, the occurrence rates in corpus B are more widely spread out. At this level of description, then, it is the data from corpus B that show greater “dispersion”. In the statistical literature on count regression, the term dispersion is used in this sense, i.e. to refer to the variability of unit-specific (i.e. text-level) occurrence rates (e.g. Long 1997, 221; Gelman 2021, 264–68). An awareness of the different meanings of “dispersion” will prove helpful for corpus linguists (and lexicographers) when engaging with the statistical literature on count data modeling.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe term “dispersion” is used differently in corpus linguistics and statistics\nThe difference in meaning reflects a difference in perspective\nCorpus linguists picture the corpus as a sequence of words and understand the term as characterizing the spatial distribution of an item\nIn the statistical literature on count data modeling, the term describes the spread of a distribution of counts or occurrence rates\n\n\n\n\n\n\n\nReferences\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. Second edition. New York: Cambridge University Press.\n\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/S1351324900000139.\n\n\nGelman, Hill, Andrew. 2021. Regression and Other Stories. Cambridge: Cambridge University Press.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data. New York: Cambridge University Press.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage.\n\n\nMosteller, Frederick, and David L. Wallace. 1984. Applied Bayesian Inference: The Case of the Federalist Papers. New York: Springer.\n\n\nWinter, Bodo, and Paul‐Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11). https://doi.org/10.1111/lnc3.12439.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {“{Dispersion}” in Corpus Linguistics and Statistics},\n  date = {2023-11-16},\n  url = {https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “‘Dispersion’ in Corpus\nLinguistics and Statistics.” November 16, 2023. https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_two_types/index.html",
    "href": "posts/2023-11-17_downsampling_two_types/index.html",
    "title": "Two types of down-sampling in corpus-based work",
    "section": "",
    "text": "The data available from corpora are often too vast for certain types of linguistic analysis. Researchers are then forced to select a subset of the data, and this selection process can be referred to as “down-sampling”. Currently, the term is used to refer to two very different types of down-sizing.\nThe first deals with lists of occurrences extracted from a corpus and is used in studies that start out with a corpus query and a body of hits (often in the form of concordance lines). If the structure of interest is relatively frequent and/or the source corpus large, the researcher may need to reduce the number of data points studied. In particular, this will be necessary in variationist-type research, which often involves considerable manual work (e.g. disambiguation and annotation). In this form of down-sampling, the selection of elements usually proceeds (to some extent) at random, i.e. it involves a chance component. Simple techniques are implemented in corpus software, which allows users to extract from a list of hits a random sample. In CQPweb (Hardie 2012), for instance, this option is referred to as “thinning”. Depending on our research goals and the structure of our data, however, other strategies may be more efficient (e.g. structured down-sampling, see Sönning and Krug 2022).\nThe second type of down-sampling is concerned with the selection of texts for close reading. Here, the objective is to pick from a corpus those texts that are likely to be most informative for a thorough qualitative analysis. This method, which Gabrielatos et al. (2012) refer to as “targeted down-sampling”, uses surface-level features (such as the occurrence rate of certain forms) to detect relevant documents for a critical discourse analysis (see also Baker et al. 2008, 285). A procedure much in the same spirit is discussed in Anthony and Baker (2015), where prototypical exemplars, i.e. texts that are most representative of their corpus of origin, are selected based on keyword profiles.\nIt may therefore sometimes be helpful to distinguish the two types of down-sampling: We could call the first type “selection of concordance lines for annotation” and the second type “selection of texts for close reading”.\n\n\n\n\nReferences\n\nAnthony, Laurence, and Paul Baker. 2015. “ProtAnt: A Tool for Analysing the Prototypicality of Texts.” International Journal of Corpus Linguistics, August, 273–92. https://doi.org/10.1075/ijcl.20.3.01ant.\n\n\nBaker, Paul, Costas Gabrielatos, Majid KhosraviNik, Michał Krzyżanowski, Tony McEnery, and Ruth Wodak. 2008. “A Useful Methodological Synergy? Combining Critical Discourse Analysis and Corpus Linguistics to Examine Discourses of Refugees and Asylum Seekers in the UK Press.” Discourse &Amp; Society 19 (3): 273–306. https://doi.org/10.1177/0957926508088962.\n\n\nGabrielatos, Costas, Tony McEnery, Peter J. Diggle, and Paul Baker. 2012. “The Peaks and Troughs of Corpus-Based Contextual Analysis.” International Journal of Corpus Linguistics 17 (2): 151–75. https://doi.org/10.1075/ijcl.17.2.01gab.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Two Types of down-Sampling in Corpus-Based Work},\n  date = {2023-11-17},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Two Types of down-Sampling in Corpus-Based\nWork.” November 17, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_implementation/index.html",
    "href": "posts/2023-11-17_downsampling_implementation/index.html",
    "title": "Structured down-sampling: Implementation in R",
    "section": "",
    "text": "I recently consulted colleagues on how to down-sample their corpus data. Their study deals with modal auxiliaries in learner writing, and they are also interested in the semantics of modal verbs. This means that they have to manually annotate individual tokens of modals. In this blog post, I describe how we implemented structured down-sampling (Sönning and Krug 2022) in R. The data we use for illustration is a simplified subset of the originial list of corpus hits. We will concentrate on the modal verb can.\n\n\nR setup\nlibrary(tidyverse)\n\nd &lt;- read_tsv(\"./data/modals_data.tsv\")\n#d &lt;- read_tsv(\"./posts/2023-11-17_downsampling_implementation/data/modals_data.tsv\")\n\n\n\nThe data\nThe data include 300 tokens, which are grouped by Text (i.e. learner essay), and there are 162 texts where can occurs at least once. The distribution of tokens across texts is summarized in Figure 1: In most texts (n = 83), can occurs only once, 41 texts feature two occurrences, and so on.\n\n\nR code: Figure 1\nd |&gt; \n  group_by(text_id) |&gt; \n  tally() |&gt; \n  group_by(n) |&gt; \n  tally() |&gt; \n  ggplot(aes(x=n, y=nn)) + \n  geom_col(width = .7, fill=\"grey\") +\n  theme_classic() +\n  scale_x_continuous(breaks = 1:7) +\n  xlab(\"Number of occurrences\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\nFigure 1: Distribution of token counts across texts.\n\n\n\nA different arrangement of the data is shown in Figure 2, where texts are lined up from left to right. Each text is represented by a pile of dots, with each dot representing a can token. The text with the highest number of can tokens (n = 7) appears at the far left, and about half of the texts only have a single occurrence of can – these text are sitting in the right half of the graph.\n\n\nR code: Figure 2\nd |&gt;  \n  group_by(text_id) |&gt; \n  mutate(n_tokens = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x=reorder(text_id, -n_tokens))) + \n  geom_dotplot(dotsize = .13, stackratio=1.6) +\n  theme_void() +\n  labs(subtitle=\"Texts ranked by token count\",\n       caption = \"Each dot represents a token (can)\")\n\n\n\n\n\nFigure 2: Distribuition of tokens across texts.\n\n\n\n\n\n\nStructured down-sampling\nAs argued in Sönning and Krug (2022), structured down-sampling would be our preferred way of drawing a sub-sample from these data. In contrast to simple down-sampling (or thinning), where each token has the same probability of being selected, structured down-sampling aims for a balanced representation of texts in the sub-sample. Thus, we would aim for breadth of representation and only start selecting additional tokens from the same text if all texts are represented in our sub-sample. The statistical background for this strategy is discussed in Sönning and Krug (2022).\nLooking at Figure 2, this means that our selection of tokens would first consider the “bottom row” of dots in the graph, and then work upwards if necessary, i.e. sample one additional token (at random) from each text that contains two or more occurrences, and so on. It should be noted that, at some point, little more is learned by sampling yet further tokens from a specific text (see discussion in Sönning and Krug 2022, 147).\n\n\nImplementation in R\nOur first step is to add to the table a column that preserves the original order. This is important in case we want to return to the original arrangement at a later point. We will name the new column original_order.\n\nd$original_order &lt;- 1:nrow(d)\n\nThere may be settings where, due to resource constraints, we cannot pick a token from every single text. Or, similarly, where we cannot pick a second token from each text that contains at least two tokens. In such cases, a sensible default approach is to pick at random. Thus, if we were only able to analyze 100 tokens, but there are 162 texts in our data, we would like to pick texts at random. We therefore add another column where the sequence from 1 to N (the number of rows, i.e. tokens) is shuffled. This column will be called random_order. Further below, we will see how this helps us out.\n\nd$random_order &lt;- sample(\n  1:nrow(d), \n  nrow(d), \n  replace=F)\n\nThe next step is to add a column to the table which specifies the order in which tokens should be selected from a text. We will call the column ds_order (short for ‘down sampling order’). In texts with a single token, the token will receive the value 1, reflecting its priority in the down-sampling plan. For a text with two tokens, the numbers 1 and 2 are randomly assigned to the two tokens. For texts with three tokens, the numbers 1, 2 and 3 are shuffled, and so on. If we then sort the whole table according to the column ds_order, those tokens that are to be preferred, based on the rationale underlying structured down-sampling, appear at the top of the table.\nOur first step is to order the table by text_id, to make sure rows are grouped by Text.\n\nd &lt;- d[order(d$text_id),]\n\nWe then create a list of the texts in the data and sort it, so that it matches the way in which the table rows have just been ordered.\n\ntext_list &lt;- unique(d$text_id)\ntext_list &lt;- sort(text_list)\n\nWe now create the vector ds_order, which we will add to the table once it’s ready:\n\nds_order &lt;- NA\n\nThe following loop fills in the vector ds_order, text by text. It includes the following steps (marked in the script):\n\nProceed from text to text, from the first to the last in the text_list.\nFor text i, count the number of tokens in the text and store it as n_tokens.\nShuffle the sequence from 1 to n_tokens and store it as shuffled.\nAppend the shuffled sequence shuffled to the vector ds_order.\n\n\nfor(i in 1:length(text_list)){  # (1)\n  \n  n_tokens &lt;- sum(              # (2)\n    d$text_id == text_list[i])  # \n  \n  shuffled &lt;- sample(           # (3)\n    1:n_tokens,                 #\n    size = n_tokens,            #\n    replace = FALSE)            #\n  \n  ds_order &lt;- append(           # (4)\n    ds_order,                   #\n    shuffled)                   #\n}\n\nIf we look at the contents of ds_order, we note that it still has a leading NA:\n\nds_order\n\n  [1] NA  2  1  3  1  2  1  1  2  2  1  2  3  1  4  1  3  2  1  2  1  1  2  1  1\n [26]  1  1  1  3  4  1  2  1  1  2  3  1  1  2  3  1  1  2  1  1  1  1  2  1  4\n [51]  3  3  1  2  1  2  2  1  2  3  1  1  5  6  3  4  2  2  4  3  1  2  1  1  2\n [76]  4  3  1  2  1  2  1  4  3  1  1  1  1  2  1  1  1  1  3  2  1  1  2  2  4\n[101]  1  3  1  1  1  2  1  1  1  1  1  2  3  1  1  1  2  1  1  1  1  3  2  1  1\n[126]  1  2  2  3  1  1  1  2  1  3  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  3  2  1  2  1  1  1  2  1  1  2  3  1  1  1  2  1  1  2  1  1  1  2  1\n[176]  1  2  3  1  2  2  3  1  2  1  3  1  3  2  1  2  1  2  1  2  1  4  3  1  3\n[201]  2  1  2  1  3  1  1  2  2  1  1  3  2  1  1  1  1  2  2  1  1  1  1  2  2\n[226]  3  1  1  1  2  3  1  2  1  1  1  2  1  1  1  1  2  1  3  4  2  1  1  3  2\n[251]  4  3  2  1  1  2  1  2  1  1  3  5  2  4  1  1  2  1  1  2  1  2  1  1  2\n[276]  7  6  5  3  2  1  4  1  1  1  2  1  1  1  1  2  4  2  1  3  1  4  3  2  2\n[301]  1\n\n\nSo we get rid of it:\n\nds_order &lt;- ds_order[-1]\n\nWe can now add ds_order as a new column to our table:\n\nd$ds_order &lt;- ds_order\n\nThe final step is to order the rows of the table in a way that reflects our down-sampling priorities. We therefore primarily order the table based on ds_order. In addition, we order by the column random_order, which we created above. All tokens with the same priority level (e.g. all tokens with the value “1” in the column ds_order) will then be shuffled, ensuring that the order of tokens is random.\n\nd &lt;- d[order(d$ds_order, \n             d$random_order),]\n\nWe can now look at the result:\n\nhead(d)\n\n# A tibble: 6 x 7\n  text_id  left_context modal right_context original_order random_order ds_order\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;int&gt;\n1 text_19  music        can   just                     109            1        1\n2 text_158 you          can   only                     289            3        1\n3 text_123 and          cann~ distinguish              148            4        1\n4 text_69  How          can   this                      48            5        1\n5 text_88  music        can   also                      25            9        1\n6 text_156 One          can   not                      167           13        1\n\n\nNote that the strategy we have used, i.e. adding a column reflecting the priority of tokens for down-sampling, allows us to approach down-sampling in a flexible and adaptive way: Rather than actually selecting (or sampling) tokens (or rows) from the original data, we may now simply start analyzing from the top of the table. This way we remain flexibility when it comes to the choice of how many tokens to analyze.\n\n\n\n\n\nReferences\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Structured down-Sampling: {Implementation} in {R}},\n  date = {2023-11-18},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Structured down-Sampling: Implementation in\nR.” November 18, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/."
  }
]