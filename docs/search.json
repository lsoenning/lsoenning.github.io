[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lukas Sönning",
    "section": "",
    "text": "Post-doc in English linguistics at the University of Bamberg\n\nUniversity of Bamberg\nDepartment of English Linguistics\n\n\n\nContact\n\nAddress: An der Universität 9, D-96047 Bamberg\nOffice: U9/00.10\nPhone: +49 (0)951/863-2267\nEmail: lukas[dot]soenning[at]uni-bamberg[dot]de\n ORCID 0000-0002-2705-395X\n Google scholar\n Github\n OSF\n\n\n\nResearch interests\n\nStatistical analysis of language data\nCorpus linguistics\nLanguage variation and change\nData visualization\nGerman Learner English\nL2 phonology"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. Lukas Sönning\nI am a post-doctoral researcher at the Chair of English Linguistics, University of Bamberg (Germany). Following my PhD project, which looked at phonological features in German Learner English, my interest shifted to statistical aspects of corpus-linguistic methodology. I have worked on topics such as keyness analysis, dispersion, and down-sampling, and my habilitation (post-doc) project concentrates on the linguistically grounded use of mixed-effects models in variationist corpus research. I have made an active effort to promote open-science practices and have a passion for data visualization. Currently, I am also involved in a DFG-funded project on the analysis of high-dimensional survey data drawn from the BSLVC (Bamberg Survey of Language Variation and Change).\n\nShort academic CV\n\n2012–present Research and teaching assistant at the University of Bamberg\n2020 awarded PhD\n2006–2012 Studies in English, Geography and Pedagogy at the University of Bamberg\n\n\n\nAwards\n\n2014 Best poster award, Olinco conference, Olomouc (“Vowel reduction in German Learner English: Developmental patterns”)\n2018 Best paper by an early career researcher, ICAME39, Tampere (John Sinclair bursary) (“Visual inference for corpus linguistics”)\n\n\n\nTeaching\nUniversity courses\n\nForming (new) words: The morphological architecture of English\nApplied data analysis for linguists\nInvestigating Learner English\nSecond language speech: Theory and practice\nMeasuring (your) foreign accent: The acoustic analysis of non-native speech\nEnglish phonetics & phonology\nEnglish grammar analysis\nTranslation English-German (intermediate and advanced level)\nRevision course for state exam candidates: Synchronic linguistics\nRevision course for state exam candidates: Translation English-German\n\nWorkshops\n\n2025 (TU Dresden) Deskriptive und inferentielle Statistik für Korpusdaten  OSF | slides Teil 1 | slides Teil 2 | slides Teil 3\n2025 (Ghent University, Belgium) Random Forests in language data analysis: Tutorial  OSF | slides\n2024 (Ghent University, Belgium) Tutorial: Latent-variable modeling of ordinal data  OSF | slides part I | slides part II | slides part III\n2024 (FJUEL conference, Bamberg) Dynamic documents in R: Introduction to Quarto\n\nslides session 1 | slides session 2 | slides session 3\npractice 1 | practice 2 | practice 3\n\n2024 (University of Bamberg) RStudio crash course for PhD students | slides session 1 | slides session 2\n2023 (META-LING conference, Bamberg) Data publication using TROLLing  OSF\n2023 (University of Würzburg) Basics of data analysis using R and RStudio  OSF | slides part 1 | slides part 3\n2022 (University of Würzburg) Basic statistical methods for TEFL research  OSF\n2019 (FJUEL conference, Bayreuth): Using “statistics” to learn about language: What matters (and what doesn’t)  OSF\n2019 (BICLCE conference, Bamberg): The replication crisis in science: Challenges and chances for linguistics  OSF\n2018 (Uppsala University, Sweden) Statistical inference using estimation: Methods for corpus linguistics slides\n2014 (EmMeth conference, Bamberg): Data visualization with R\n2014 (FJUEL conference, Bamberg): Workshop on statistical methods\n\n\n\nTalks\n\n2025 Case-control down-sampling in corpus research. CL2025, Birmingham, UK.\n2025 Raising the bar: An analysis of bar chart usage in corpus linguistics. CL2025, Birmingham, UK.\n2025 Per corpora et diagrammata ad astra: Data visualization in corpus linguistics. Plenary at ICAME 46, Vilnius, Lithuania.\n2024 The morpho-syntax of Scottish Standard English: Questionnaire-based insights. BICLCE 10, Alicante, Spain. (with Ole Schützler and Manfred Krug)\n2024 Down-sampling strategies in corpus phonology. BICLCE 10, Alicante, Spain.\n2024 Ordinal response scales: Psychometric grounding for design and analysis. BICLCE 10, Alicante, Spain.\n2024 Sensitivity of dispersion measures to distributional patterns and corpus design. ICAME45, Vigo, Spain. (with Jesse Egbert)\n2024 Regression and random forests: Synergies for variationist corpus research. ICAME45, Vigo, Spain. (with Jason Grafmiller and Raquel Romasanta)\n2023 Down-sampling from hierarchically structured corpus data. FJUEL 11, Erlangen, Germany.\n2023 Text-level measures of lexical dispersion: Robustness analysis. CL2023. Lancaster, UK.\n2023 Down-sampling from hierarchically structured corpus data: The case of third-person verb inflection in Early Modern English. CL2023. Lancaster, UK.\n2022 Seeing the wood for the trees: Predictive margins for random forests. ICAME 43, London, UK. (with Jason Grafmiller)\n2022 Keyword analysis: Progress through regression. ICAME 43, London, UK.\n2019 The English comparative alternation revisited: A fresh look at theory and data. ICAME 40, Neuchatel, Switzerland. (with Stefan Hartmann)\n2018 Frequency effects in the English comparative alternation: A reassessment. ISLE5, London, UK.\n2018 A normalization procedure for auditory vowel descriptions: Method and application. ISLE5, London, UK. (with Ole Schützler)\n2018 Drawing on principles of perception: The line plot. ICAME34, Tampere, Finland.\n2018 Visual inference for corpus data analysis: Dot plots of effect sizes with confidence intervals. ICAME34, Tampere, Finland.\n2018 A sociolinguistic study of actually in current spoken British English. ICAME34, Tampere, Finland.\n2017 (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. BICLCE 7, Vigo, Spain.\n2015 Developmental patterns in German Learner English: Vowel reduction and speech rhythm. Research seminar, University of Münster, Germany.\n2015 Methods for corpus data analysis: Dot plots of effect sizes with confidence intervals. Methods and Linguistic Theories (MaLT), Bamberg, Germany.\n2014 Vowel reduction in German Learner English: Developmental patterns. 47th Meeting of the Societas Linguistica Europaea (SLE 2014), Poznan, Poland.\n2014 Normalverteilungsannahme und Ausreißerwerte: Nachteile klassischer Statistik und robuste Alternativen. IV. Diskussionsforum Linguistik in Bayern. Munich, Germany.\n2014 [Poster] Vowel reduction in German Learner English: Developmental patterns. Olomouc Linguistics Colloquium (Olinco), Olomouc, Czeck Republic.\n2014 Vowel reduction in German Learner English. Research seminar, University of Münster, Germany.\n2014 The dot plot: A fine tool for data visualization. Advances in Visual Methods for Linguistics (AVML) 2014. Tübingen, Germany.\n2013 An acoustic analysis of unstressed vowels in German Learner English. Accents 2013, Łódź, Poland.\n2013 Vowel reduction in German Learner English. FJUEL 3, Regensburg, Germany.\n2013 Scrabble yourself to success: Methods in teaching transcription. Phonetics Teaching and Learning Conference (PTLC) 2013, London, UK."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Unpublished\n\nSönning, Lukas. (forthcoming). Count regression models for keyness analysis. Chapter for: Carolin Cholotta & Christine Renker (eds.), META-LING 2023 - Methodological Exploration and Technological Advances in Linguistics Accepted manuscript |  OSF\nSönning, Lukas & Jesse Egbert. (forthcoming). Sensitivity of dispersion measures to distributional patterns and corpus design. International Journal of Corpus Linguistics Submitted manuscript |  OSF\nSönning, Lukas. (under review). Dispersion analysis. In Hilary Nesi & Petar Milin (eds.), International Encyclopedia of Language and Linguistics, 3rd ed. Amsterdam: Elsevier. Submitted manuscript |  OSF\nSchützler, Ole, Lukas Sönning, Fabian Vetter & Manfred Krug. (under review). The morpho-syntax of Standard Scottish English: Questionnaire-based insights. Submitted manuscript |  OSF\nSönning, Lukas. (under review). Down-sampling strategies in corpus phonology. Chapter for: Philipp Meer & Ulrike Gut (eds.) English corpus phonetics and phonology: Current approaches and future directions Submitted manuscript  OSF\nSönning, Lukas. (under review). An analysis of bar chart usage in corpus data visualization. Submitted manuscript |  OSF\nSönning, Lukas. (under review). Case-control down-sampling in corpus-based research. Submitted manuscript |  OSF\nSönning, Lukas & Timo Roettger. (under review). Reproducibility, replication, and preregistration. In Stefan Hartmann & Laura J. Speed (eds.), Handbook of language and cognition. Bloomsbury. Submitted manuscript |  OSF\nSönning, Lukas, Jason Grafmiller & Raquel Romasanta. (under review). Regression and random forests: Synergies for variationist corpus research. Working paper |  OSF\nSönning, Lukas. (in preparation). Random forests in corpus research: A systematic review.\nSönning, Lukas. (unpublished manuscript). Evaluation of text-level measures of lexical dispersion: Robustness and consistency.  Working paper |  Data |  OSF\n\n \n\n\nPublished\nMonograph\n\nSönning, Lukas. 2020. Phonological variation in German Learner English. University of Bamberg dissertation. doi: 10.20378/irb-49135 |  Open access |  Datasets |  OSF\n\nJournal articles\n\nSönning, Lukas. 2025. Advancing our understanding of dispersion measures in corpus research. Corpora 20(1). 3–35. doi: 10.3366/cor.2025.0326 |  Open access |  Data |  OSF\nSönning, Lukas. 2024. Ordinal response scales: Psychometric grounding for design and analysis. Research Methods in Applied Linguistics 3(3). 100156. doi: 10.1016/j.rmal.2024.100156 |  Open access |  Data |  OSF\nSönning, Lukas, Manfred Krug, Fabian Vetter, Timo Schmid, Anne Leucht & Paul Messer. 2024. Latent-variable modelling of ordinal outcomes in language data analysis. Journal of Quantitative Linguistics 31(2). 77–106. doi: 10.1080/09296174.2024.2329448 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Down-sampling from hierarchically structured corpus data. International Journal of Corpus Linguistics 29(4). 507–533. doi: 10.1075/ijcl.23079.son |  Submitted manuscript |  Data |  OSF\nSönning, Lukas. 2024. Evaluation of keyness metrics: Performance and reliability. Corpus Linguistics and Linguistic Theory 20(2). 263–288. doi: 10.1515/cllt-2022-0116 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Jason Grafmiller. 2024. Seeing the wood for the trees: Predictive margins for random forests. Corpus Linguistics and Linguistic Theory 20(1). 153–181. doi: 10.1515/cllt-2022-0083 |  Submitted manuscript |  Data |  OSF\nSönning, Lukas & Valentin Werner. 2021. The replication crisis, scientific revolutions, and linguistics. Linguistics 59(5). 1179–1206. doi: 10.1515/ling-2019-0045 |  Open access\nSönning, Lukas. 2014. Unstressed vowels in German Learner English: An instrumental study. Research in Language 12(2). 163–173. doi: 10.2478/rela-2014-0001 |  Open access |  OSF\n\nEdited volumes\n\nSönning, Lukas & Ole Schützler (eds.). 2023. Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22\nSönning, Lukas & Valentin Werner. 2021. The replication crisis: Implications for linguistics. Special issue in Linguistics.  Open access\nChrist, Hanna, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.). 2016. A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium. Bamberg: University of Bamberg Press.  Open access\n\nBook chapters\n\nSönning, Lukas. 2023. Drawing on principles of perception: The line plot. In Lukas Sönning & Ole Schützler (eds.), Data visualization in corpus linguistics: Reflections and future directions (Studies in Variation, Contacts and Change in English; No. 22). University of Helsinki: VARIENG.  https://urn.fi/URN:NBN:fi:varieng:series-22-2 |  Preprint |  OSF\nSönning, Lukas. 2023. (Re-)viewing the acquisition of rhythm in the light of L2 phonological theories. In Robert Fuchs (ed.), Speech rhythm in learner and second language varieties of English, 123–157. Singapore: Springer. doi: 10.1007/978-981-19-8940-7_6 |  Preprint |  Data |  OSF\nSönning, Lukas & Manfred Krug. 2022. Comparing study designs and down-sampling strategies in corpus analysis: The importance of speaker metadata in the BNCs of 1994 and 2014. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 127–159. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.006 |  Published version |  Data |  OSF\nSönning, Lukas & Julia Schlüter. 2022. Comparing standard reference corpora and Google Books Ngrams: Strengths, limitations and synergies in the contrastive study of variable h- in British and American English. In Ole Schützler & Julia Schlüter (eds.), Data and methods in corpus linguistics: Comparative approaches, 17–45. Cambridge: Cambridge University Press. doi: 10.1017/9781108589314.002 |  Published version |  OSF\nKrug, Manfred & Lukas Sönning. 2018. Language change in Maltese English: The influence of age and parental languages. In: Patrizia Paggio & Albert Gatt (eds.), The languages of Malta, 247–270. Berlin: Language Science Press. doi: 10.5281/zenodo.1181801 |  Open access\n\nProceedings\n\nSönning, Lukas. 2016. The dot plot: A graphical tool for data analysis and presentation. In Hanna Christ, Daniel Klenovšak, Lukas Sönning & Valentin Werner (eds.), A blend of MaLT: Selected contributions from the Methods and Linguistic Theories Symposium, 101–129. Bamberg: University of Bamberg Press.  Open access\nSönning, Lukas. 2014. Developmental patterns in the reduction of unstressed vowels by German learners of English. In Ludmila Veselovská & Markéta Janebová (eds.), Complex visibles out there: Proceedings of the Olomouc Linguistics Colloquium 2014: Language use and linguistic structure, vol. 4 Olomouc modern language series, 765–778. Olomouc: Palacký University.  Open access |  OSF\nSönning, Lukas. 2013. Scrabble yourself to success: Methods in teaching transcription. In Joanna Przedlacka, John Maidment & Michael Ashby (eds.), Proceedings of the Phonetics Teaching and Learning Conference, UCL, London, 8-10 August 2013. London: Phonetics Teaching and Learning Conference, 87–90.  Open access |  OSF |  Data\n\nSoftware\n\nSönning, Lukas. 2025. tlda: Tools for language data analysis. R package version 0.1.0. doi: 10.32614/CRAN.package.tlda |  Github\n\nVignette on dispersion analysis using the tlda package\n\n\n \n\n\nDatasets\n\nBraun, Albert & Lukas Sönning. 2025. The comparative and superlative alternation in 1960s and 1970s written British English: Data from Braun (1982). https://doi.org/10.18710/4Z7OJG, DataverseNO, V1.\nSönning, Lukas. 2025. Biber et al.’s (2016) set of 150 BNC items for the analysis of dispersion measures: Dataset for “Evaluation of text-level measures of lexical dispersion. https://doi.org/10.18710/ATCQZW, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Advancing our understanding of dispersion measures in corpus research. https://doi.org/10.18710/FVHTFM, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Some obstacles to replication in corpus linguistics. https://doi.org/10.18710/7LNWJX, DataverseNO, V1.\nSönning, Lukas. 2024. Background data for: Ordinal response scales: Psychometric grounding for design and analysis, https://doi.org/10.18710/0VLSLW, DataverseNO, V1.\nKrug, Manfred, Fabian Vetter & Lukas Sönning. 2024. Background data for: Latent-variable modeling of ordinal outcomes in language data analysis. https://doi.org/10.18710/WI9TEH, DataverseNO, V1.\nSönning, Lukas. 2023. Background data (adapted from Jenset & McGillivray 2017) for: Down-sampling from hierarchically structured corpus data, https://doi.org/10.18710/5KCE4U, DataverseNO, V1.\nSönning, Lukas. 2023. Key verbs in academic writing: Dataset for “Evaluation of keyness metrics: Performance and reliability”, https://doi.org/10.18710/EUXSMW, DataverseNO, V1.\nSönning, Lukas. 2022. Speech rhythm in German Learner English: Dataset for “(Re-)viewing the acquisition of rhythm in the light of L2 phonological theories”, https://doi.org/10.18710/GTI2BR, DataverseNO, V1.\nSönning, Lukas & Manfred Krug. 2021. Actually in contemporary British speech: Data from the Spoken BNC corpora, https://doi.org/10.18710/A3SATC, DataverseNO, V1.\nSönning, Lukas. 2022. Dataset for “Scrabble yourself to success: Methods in teaching transcription”, https://doi.org/10.18710/2UJHHU, DataverseNO, V1.\n\n\nDissertation\n\nSönning, Lukas. 2021. The TRAP-DRESS contrast in German Learner English: Dataset for chapter 4 in “Phonological variation in German Learner English”, https://doi.org/10.18710/ATIRRV, DataverseNO, V1.\nSönning, Lukas. 2021. Clear vs. dark /l/ in German Learner English: Dataset for chapter 5 in “Phonological variation in German Learner English”, https://doi.org/10.18710/G6PJ5F, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. English /r/ in German Learner English: Dataset for chapter 6 in “Phonological variation in German Learner English”, https://doi.org/10.18710/YDKDFG, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe, Isabel Rank and Christina Wunder. 2021. The labio-velar glide /w/ in German Learner English: Dataset for chapter 7 in “Phonological variation in German Learner English”, https://doi.org/10.18710/F1A34O, DataverseNO, V1.\nSönning, Lukas & Isabel Rank. 2021. The labiodental fricative /v/ in German Learner English: Dataset for chapter 8 in “Phonological variation in German Learner English”, https://doi.org/10.18710/B276ZX, DataverseNO, V1.\nSönning, Lukas, Graham Pascoe & Christina Wunder. 2021. The voiced dental fricative in German Learner English: Dataset for chapter 9 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DYAGZG, DataverseNO, V1.\nSönning, Lukas & Graham Pascoe. 2021. Final voiced obstruents in German Learner English: Dataset for chapter 10 in “Phonological variation in German Learner English”, https://doi.org/10.18710/DKIGE5, DataverseNO, V1."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Plot templates for Microsoft Excel\nExcel templates and instructions for some useful graph types can be found in the following  OSF project .\n\nDot diagram\ntemplate | instructions\n\n\n\n\n\nSimple dot plot\ntemplate | instructions\n\n\n\n\n\nGrouped dot plot\ntemplate | instructions\n\n\n\n\n\nBox plot\ntemplate | instructions\n\n\n\n\n\nVertical dot plot\ntemplate\n\n\n\n\n\nScatter plot\ntemplate | instructions\n\n\n\n\n\n\nSpeaker slides for workshop\n\nFJUEL workshop, Bamberg: speaker slides session 1 | speaker slides session 2 | speaker slides session 3\n\n\n\nExtended notes: Ordinal regression models\n\nSection 1: Background\nSection 2: Descriptive statistics\nSection 3: Ordered regression models\nSection 4: A latent-variable model\nSection 5: Methods of interpretation\nSection 6: R workbench"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Statistics for linguist(ic)s blog",
    "section": "",
    "text": "Color-coded dendrograms using the R function A2Rplot()\n\n\n\n\n\n\ndata visualization\n\n\n\nIn this blog post, I illustrate how to use Romain Francois’ R function A2Rplot() to draw dendrograms with visually distinct clusters.\n\n\n\n\n\nJun 16, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nExporting R graphics: A basic workflow\n\n\n\n\n\n\ndata visualization\n\n\n\nIn this blog post, I describe my workflow for exporting and polishing graphs drawn in R.\n\n\n\n\n\nMay 20, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered frequency data II: Texts of disproportionate length\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nbias\n\n\nimbalance\n\n\nnegative binomial\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered count data. It describes how they handle the non-independence among observations and what kind of estimates they return. The focus is on a situation where texts have very different lengths.\n\n\n\n\n\nMay 15, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered frequency data I: Texts of similar length\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nnegative binomial\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered count data. It describes how they handle the non-independence among observations and what kind of estimates they return. The focus is on a situation where texts have roughly the same length.\n\n\n\n\n\nMay 14, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nFrequency estimates based on random-intercept Poisson models\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nfrequency data\n\n\nnegative binomial\n\n\n\nClustered count data can be modeled using a Poisson regression model including random intercepts. This blog post describes how this model represents the data and the different kinds of frequency estimates it produces.\n\n\n\n\n\nMay 13, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling clustered binomial data\n\n\n\n\n\n\ncorpus linguistics\n\n\nregression\n\n\nclustered data\n\n\nbinary data\n\n\n\nThis blog post illustrates a number of strategies for modeling clustered binomial data. It describes how they handle the non-independence among observations and what kind of estimates they return.\n\n\n\n\n\nMay 9, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nImbalance across predictor levels affects data summaries\n\n\nObstacles to replication in corpus linguistics\n\n\n\ncorpus linguistics\n\n\nreplication crisis\n\n\nregression\n\n\nbias\n\n\nimbalance\n\n\n\nThis blog post is part of a small series on obstacles to replication in corpus linguistics. It deals with problems that can arise if the observations drawn from a corpus are unbalanced across relevant subgroups in the data. I show how simple and comparative data summaries can vary depending on whether we (unintentionally) calculate weighted averages, or adjust estimates for imbalances by taking a simple average across subgroups. As these are two different estimands, the choice affects the comparability of studies – including an original study and its direct replication.\n\n\n\n\n\nMay 4, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nClustering in the data affects statistical uncertainty intervals\n\n\nObstacles to replication in corpus linguistics\n\n\n\ncorpus linguistics\n\n\nreplication crisis\n\n\nregression\n\n\nclustered data\n\n\n\nThis blog post is part of a small series on obstacles to replication in corpus linguistics. It deals with a prevalent issue in corpus data analysis: the non-independence of data points that results from clustered (or hierarchical) data layouts. I show how an inadequate analysis can produce unduly narrow expectations of a replication study.\n\n\n\n\n\nMay 2, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nUnbalanced distributions and their consequences: Speakers in the Spoken BNC2014\n\n\n\n\n\n\ncorpus linguistics\n\n\nclustered data\n\n\nnegative binomial\n\n\nclustered data\n\n\nimbalance\n\n\n\nThis blog post illustrates how the disproportionate representation of speakers in a corpus can lead to distorted results if the source of data points (i.e. the speaker ID) is not taken into account in the analysis.\n\n\n\n\n\nApr 29, 2025\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nModeling the interpretation of quantifiers using beta regression\n\n\n\n\n\n\nregression\n\n\ndistributional modeling\n\n\n\nThis blog post shows how to use beta regression to model the proportional interpretation of the quantifiers few, some, many, and most. We consider variable-dispersion and mixed-effects structures as well as diagnostics for frequentist and Bayesian models.\n\n\n\n\n\nFeb 29, 2024\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent parameterizations of the negative binomial distribution\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\nnegative binomial\n\n\n\nThis blog post discusses two different parameterizations of the negative binomial distribution and groups R packages (and functions) based on the version they implement.\n\n\n\n\n\nDec 13, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe negative binomial distribution: A visual explanation\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\nnegative binomial\n\n\n\nThis blog post uses a visual approach to explain how the negative binomial distribution works.\n\n\n\n\n\nDec 12, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nA computational shortcut for the dispersion measure DA\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis short blog post draws attention to the computational shortcut given in Wilcox (1973) for calculating the dispersion measure DA.\n\n\n\n\n\nDec 11, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nThe replication crisis: Implications for myself\n\n\n\n\n\n\nreplication crisis\n\n\nopen science\n\n\n\nIn this blog post, I reflect on the ways in which learning about the replication crisis in science has affected my own work.\n\n\n\n\n\nNov 21, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nStructured down-sampling: Implementation in R\n\n\n\n\n\n\ncorpus linguistics\n\n\ndown-sampling\n\n\n\nThis blog post shows how to implement structured down-sampling in R.\n\n\n\n\n\nNov 18, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\nTwo types of down-sampling in corpus-based work\n\n\n\n\n\n\ncorpus linguistics\n\n\ndown-sampling\n\n\n\nThis short blog post contrasts the different ways in which the term down-sampling is used in corpus-based work.\n\n\n\n\n\nNov 17, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\n\n\n\n\n\n\n‘Dispersion’ in corpus linguistics and statistics\n\n\n\n\n\n\ncorpus linguistics\n\n\ndispersion\n\n\n\nThis blog post clarifies the different ways in which the term dispersion is used in corpus linguistics and statistics.\n\n\n\n\n\nNov 16, 2023\n\n\nLukas Sönning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-18_dispersion_terminology/index.html",
    "href": "posts/2023-01-18_dispersion_terminology/index.html",
    "title": "‘Dispersion’ in corpus linguistics and statistics",
    "section": "",
    "text": "R setup\nlibrary(lattice)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nIn corpus linguistics, the term dispersion is used to describe the distribution of an item or structure in a corpus (see Gries 2008, 2020). For most dispersion measures, a corpus must first be divided into units (or parts). These units commonly reflect the design of the corpus – they can be text files, for instance, or text categories. A dispersion index then describes the distribution of an item across these units. There are two general classes of measures:\n\nthose measuring the pervasiveness of an item, which is reflected in the number of units that contain the item (Range and Text Dispersion, its proportional analog)\nthe much larger class of evenness measures, which express how evenly an item is distributed across the units (e.g. D, D2, S, DP, DA, DKL).\n\nMost dispersion measures range between 0 and 1, where 1 indicates a perfectly even distribution, or the maximal degree of pervasiveness (i.e. the item occurs in every unit).\nFrom a statistical viewpoint, the input for the calculation of evenness measures would be considered a count variable, since it records the number of events (occurrences of the item) that are observed during a certain period of observation. In corpus linguistics, the “period of observation” is “text time”, expressed as a word count.\nThere is an extensive literature on the use of regression models for count variables (e.g. Long 1997; Cameron and Trivedi 2013; Hilbe 2014), and such models have seen some successful applications to word frequency data (e.g. Mosteller and Wallace 1984; Church and Gale 1995); Winter and Bürkner (2021) provide an accessible introduction for linguists. In this literature, the term “dispersion” is also used, though with a different (apparently opposite) meaning.\nLet us first consider the corpus-linguistic (and lexicographic) sense, which can be best described visually, using a so-called “dispersion plot”. Figure 1 shows a dispersion plot for two corpora, A and B. The framed rectangles represent the sequence of words forming the corpus, and the spikes inside of these locate the occurrences of a specific item in the corpus. In corpus A, the item is spread out quite evenly. In corpus B, instances are more densely clustered, and there are large stretches where the item does not occur. In the corpus-linguistic sense, then, the dispersion of the item is greater in corpus A. The dispersion score for the item would be greater in Corpus A (i.e. closer to 1).\n\n\nR code: Figure 1\nset.seed(2000)\n\nn_tokens_A &lt;- c(3,5,4,4,3,4,4,5)\nn_tokens_B &lt;- c(5,0,1,9,0,1,0,3)\n\nn_texts &lt;- length(n_tokens_A)\n\nA_loc &lt;- rep(1:n_texts, n_tokens_A)+runif(sum(n_tokens_A))\nB_loc &lt;- rep((1:n_texts)[n_tokens_B!=0], n_tokens_B[n_tokens_B!=0])+runif(sum(n_tokens_B))\n\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,14), ylim=c(2.8,6),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=5.1, ybottom=4.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    panel.rect(xleft=1, xright=n_texts+1, ytop=5.1, ybottom=4.75, \n               border=\"grey50\", lwd=1)\n    \n\n        \n    panel.segments(x0=A_loc, x1=A_loc, y0=4.8, y1=5.05, lwd=.75)\n    panel.text(x=(1:n_texts)+.5, y=4.55, label=n_tokens_A, \n               col=\"grey50\", cex=.9)\n    \n    \n    panel.rect(xleft=c(1,3,5,7), xright=c(2,4,6,8), ytop=4.1, ybottom=3.75, \n               border=FALSE, col=\"grey90\", lwd=1)\n    \n    panel.rect(xleft=1, xright=n_texts+1, ytop=4.1, ybottom=3.75, \n               border=\"grey60\", lwd=1)\n    \n    panel.segments(x0=B_loc, x1=B_loc, y0=3.8, y1=4.05, lwd=.75)\n    \n    panel.text(x=(1:n_texts)+.5, y=3.55, label=n_tokens_B, \n               col=\"grey60\", cex=.9)\n    \n    panel.text(x=.4, y=c(4,5)-.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"Lower dispersion\\n\", \"Higher dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=9.7, y=c(3.8, 4.8), adj=0, \n               label=c(\"\\n(more concentrated)\", \"\\n(more spread out)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.segments(x0=1, x1=2, y0=5.4, y1=5.4, lwd=.5, col=\"grey50\")\n    panel.segments(x0=1:2, x1=1:2, y0=5.4, y1=5.3, lwd=.5, col=\"grey50\")\n    \n    \n    panel.text(x=1.5, y=5.75, label=\"Text 1\", col=\"grey40\", cex=.8)\n    panel.text(x=7, y=2.75, label=\"Occurrences\\nof item in text\", col=\"grey40\", \n               cex=.9, lineheight=.85)\n    panel.segments(x0=5.9, x1=5.6, y0=3, y1=3.3, col=\"grey40\", lwd=.5)\n    })\n\n\n\n\n\n\n\n\nFigure 1: Dispersion in the corpus-linguistic sense: Distribution of word tokens in the corpus.\n\n\n\n\n\nNote how each corpus is divided into 8 texts, which are shown in Figure 1 using greyshading. The numbers below the dispersion plot for each corpus report the number of occurrences of the item in each text. For corpus A, they range between 3 and 5; for corpus B, between 0 and 9.\nFigure 2 shows a different representation of these data. Instead of looking at the corpus as a string of words, we consider the text-specific frequencies (sometimes called sub-frequencies) of the item. These indicate how often the item occurs in each document. Figure 2 shows these text-level token counts: Each text is represented by a dot, which marks how often the item appears in the text. In our hypothetical corpora, each text has the same length, which is why we can compare absolute counts. If texts differ in length, we would instead use normalized frequencies, i.e. occurrence rates such as “3.1 per thousand words”.\n\n\nR code: Figure 2\nxyplot(\n  1~1, type=\"n\", xlim=c(-1,7.5), ylim=c(-.35,2.3),\n  par.settings=my_settings, scales=list(draw=F), xlab=\"\", ylab=\"\",\n  panel=function(...){\n    panel.dotdiagram(1+(n_tokens_A/5), y_anchor=1, scale_y=.125, set_cex=1.3)\n    panel.dotdiagram(1+(n_tokens_B/5), y_anchor=0, scale_y=.125, set_cex=1.3)\n    panel.segments(x0=1, x1=3.2, y0=1, y1=1)\n    panel.segments(x0=1, x1=3.2, y0=0, y1=0)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=1, y1=.95)\n    panel.segments(x0=1+c(0,5,10)/5, x1=1+c(0,5,10)/5, y0=0, y1=-.05)\n    panel.text(x=1+c(0,5,10)/5, y=-.2, label=c(0,5,10), col=\"grey40\", cex=.8)\n    \n    panel.text(x=.6, y=c(0,1)+.1, label=c(\"Corpus B\", \"Corpus A\"), \n               adj=1, cex=1)\n    \n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"Higher dispersion\\n\", \"Lower dispersion\\n\"), \n               col=1, lineheight=.85, cex=1)\n    panel.text(x=4, y=c(0.1 ,1.1), adj=0, \n               label=c(\"\\n(more spread out)\", \"\\n(more concentrated)\"), \n               col=\"grey40\", lineheight=.85, cex=.9)\n    \n    panel.text(x=2, y=-.5, label=\"Occurrences of item\", cex=1, lineheight=.9)\n    panel.text(x=3, y=2.2, label=\"Each dot\\nrepresents a text\", cex=.9, \n               lineheight=.9, col=\"grey40\")\n    })\n\n\n\n\n\n\n\n\nFigure 2: Dispersion in the statistical sense: Distribution of text-level ocurrence rates.\n\n\n\n\n\nIf we compare the distribution of text-level occurrence rates in the two corpora, we note that while the texts in corpus A form a dense pile, the occurrence rates in corpus B are more widely spread out. At this level of description, then, it is the data from corpus B that show greater “dispersion”. In the statistical literature on count regression, the term dispersion is used in this sense, i.e. to refer to the variability of unit-specific (i.e. text-level) occurrence rates (e.g. Long 1997, 221; Gelman 2021, 264–68). An awareness of the different meanings of “dispersion” will prove helpful for corpus linguists (and lexicographers) when engaging with the statistical literature on count data modeling.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe term “dispersion” is used differently in corpus linguistics and statistics\nThe difference in meaning reflects a difference in perspective\nCorpus linguists picture the corpus as a sequence of words and understand the term as characterizing the spatial distribution of an item\nIn the statistical literature on count data modeling, the term describes the spread of a distribution of counts or occurrence rates\n\n\n\n\n\n\n\nReferences\n\nCameron, A. Colin, and Pravin K. Trivedi. 2013. Regression Analysis of Count Data. Second edition. New York: Cambridge University Press.\n\n\nChurch, Kenneth W., and William A. Gale. 1995. “Poisson Mixtures.” Natural Language Engineering 1 (2): 163–90. https://doi.org/10.1017/S1351324900000139.\n\n\nGelman, Hill, Andrew. 2021. Regression and Other Stories. Cambridge: Cambridge University Press.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nHilbe, Joseph M. 2014. Modeling Count Data. New York: Cambridge University Press.\n\n\nLong, J. Scott. 1997. Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage.\n\n\nMosteller, Frederick, and David L. Wallace. 1984. Applied Bayesian Inference: The Case of the Federalist Papers. New York: Springer.\n\n\nWinter, Bodo, and Paul‐Christian Bürkner. 2021. “Poisson Regression for Linguists: A Tutorial Introduction to Modelling Count Data with Brms.” Language and Linguistics Compass 15 (11). https://doi.org/10.1111/lnc3.12439.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {“{Dispersion}” in Corpus Linguistics and Statistics},\n  date = {2023-11-16},\n  url = {https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “‘Dispersion’ in Corpus\nLinguistics and Statistics.” November 16, 2023. https://lsoenning.github.io/posts/2023-01-18_dispersion_terminology/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_two_types/index.html",
    "href": "posts/2023-11-17_downsampling_two_types/index.html",
    "title": "Two types of down-sampling in corpus-based work",
    "section": "",
    "text": "The data available from corpora are often too vast for certain types of linguistic analysis. Researchers are then forced to select a subset of the data, and this selection process can be referred to as “down-sampling”. Currently, the term is used to refer to two very different types of down-sizing.\nThe first deals with lists of occurrences extracted from a corpus and is used in studies that start out with a corpus query and a body of hits (often in the form of concordance lines). If the structure of interest is relatively frequent and/or the source corpus large, the researcher may need to reduce the number of data points studied. In particular, this will be necessary in variationist-type research, which often involves considerable manual work (e.g. disambiguation and annotation). In this form of down-sampling, the selection of elements usually proceeds (to some extent) at random, i.e. it involves a chance component. Simple techniques are implemented in corpus software, which allows users to extract from a list of hits a random sample. In CQPweb (Hardie 2012), for instance, this option is referred to as “thinning”. Depending on our research goals and the structure of our data, however, other strategies may be more efficient (e.g. structured down-sampling, see Sönning and Krug 2022).\nThe second type of down-sampling is concerned with the selection of texts for close reading. Here, the objective is to pick from a corpus those texts that are likely to be most informative for a thorough qualitative analysis. This method, which Gabrielatos et al. (2012) refer to as “targeted down-sampling”, uses surface-level features (such as the occurrence rate of certain forms) to detect relevant documents for a critical discourse analysis (see also Baker et al. 2008, 285). A procedure much in the same spirit is discussed in Anthony and Baker (2015), where prototypical exemplars, i.e. texts that are most representative of their corpus of origin, are selected based on keyword profiles.\nIt may therefore sometimes be helpful to distinguish the two types of down-sampling: We could call the first type “selection of concordance lines for annotation” and the second type “selection of texts for close reading”.\n\n\n\n\nReferences\n\nAnthony, Laurence, and Paul Baker. 2015. “ProtAnt: A Tool for Analysing the Prototypicality of Texts.” International Journal of Corpus Linguistics, August, 273–92. https://doi.org/10.1075/ijcl.20.3.01ant.\n\n\nBaker, Paul, Costas Gabrielatos, Majid KhosraviNik, Michał Krzyżanowski, Tony McEnery, and Ruth Wodak. 2008. “A Useful Methodological Synergy? Combining Critical Discourse Analysis and Corpus Linguistics to Examine Discourses of Refugees and Asylum Seekers in the UK Press.” Discourse &Amp; Society 19 (3): 273–306. https://doi.org/10.1177/0957926508088962.\n\n\nGabrielatos, Costas, Tony McEnery, Peter J. Diggle, and Paul Baker. 2012. “The Peaks and Troughs of Corpus-Based Contextual Analysis.” International Journal of Corpus Linguistics 17 (2): 151–75. https://doi.org/10.1075/ijcl.17.2.01gab.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Two Types of down-Sampling in Corpus-Based Work},\n  date = {2023-11-17},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Two Types of down-Sampling in Corpus-Based\nWork.” November 17, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_two_types/."
  },
  {
    "objectID": "posts/2023-11-17_downsampling_implementation/index.html",
    "href": "posts/2023-11-17_downsampling_implementation/index.html",
    "title": "Structured down-sampling: Implementation in R",
    "section": "",
    "text": "I recently consulted colleagues on how to down-sample their corpus data. Their study deals with modal auxiliaries in learner writing, and they are also interested in the semantics of modal verbs. This means that they have to manually annotate individual tokens of modals. In this blog post, I describe how we implemented structured down-sampling (Sönning and Krug 2022) in R. The data we use for illustration is a simplified subset of the originial list of corpus hits. We will concentrate on the modal verb can.\n\n\nR setup\nlibrary(tidyverse)\n\nd &lt;- read_tsv(\"./data/modals_data.tsv\")\n#d &lt;- read_tsv(\"./posts/2023-11-17_downsampling_implementation/data/modals_data.tsv\")\n\n\n\nThe data\nThe data include 300 tokens, which are grouped by Text (i.e. learner essay), and there are 162 texts where can occurs at least once. The distribution of tokens across texts is summarized in Figure 1: In most texts (n = 83), can occurs only once, 41 texts feature two occurrences, and so on.\n\n\nR code: Figure 1\nd |&gt; \n  group_by(text_id) |&gt; \n  tally() |&gt; \n  group_by(n) |&gt; \n  tally() |&gt; \n  ggplot(aes(x=n, y=nn)) + \n  geom_col(width = .7, fill=\"grey\") +\n  theme_classic() +\n  scale_x_continuous(breaks = 1:7) +\n  xlab(\"Number of occurrences\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of token counts across texts.\n\n\n\n\nA different arrangement of the data is shown in Figure 2, where texts are lined up from left to right. Each text is represented by a pile of dots, with each dot representing a can token. The text with the highest number of can tokens (n = 7) appears at the far left, and about half of the texts only have a single occurrence of can – these text are sitting in the right half of the graph.\n\n\nR code: Figure 2\nd |&gt;  \n  group_by(text_id) |&gt; \n  mutate(n_tokens = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x=reorder(text_id, -n_tokens))) + \n  geom_dotplot(dotsize = .13, stackratio=1.6) +\n  theme_void() +\n  labs(subtitle=\"Texts ranked by token count\",\n       caption = \"Each dot represents a token (can)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribuition of tokens across texts.\n\n\n\n\n\n\n\nStructured down-sampling\nAs argued in Sönning and Krug (2022), structured down-sampling would be our preferred way of drawing a sub-sample from these data. In contrast to simple down-sampling (or thinning), where each token has the same probability of being selected, structured down-sampling aims for a balanced representation of texts in the sub-sample. Thus, we would aim for breadth of representation and only start selecting additional tokens from the same text if all texts are represented in our sub-sample. The statistical background for this strategy is discussed in Sönning and Krug (2022).\nLooking at Figure 2, this means that our selection of tokens would first consider the “bottom row” of dots in the graph, and then work upwards if necessary, i.e. sample one additional token (at random) from each text that contains two or more occurrences, and so on. It should be noted that, at some point, little more is learned by sampling yet further tokens from a specific text (see discussion in Sönning and Krug 2022, 147).\n\n\nImplementation in R\nOur first step is to add to the table a column that preserves the original order. This is important in case we want to return to the original arrangement at a later point. We will name the new column original_order.\n\nd$original_order &lt;- 1:nrow(d)\n\nThere may be settings where, due to resource constraints, we cannot pick a token from every single text. Or, similarly, where we cannot pick a second token from each text that contains at least two tokens. In such cases, a sensible default approach is to pick at random. Thus, if we were only able to analyze 100 tokens, but there are 162 texts in our data, we would like to pick texts at random. We therefore add another column where the sequence from 1 to N (the number of rows, i.e. tokens) is shuffled. This column will be called random_order. Further below, we will see how this helps us out.\n\nd$random_order &lt;- sample(\n  1:nrow(d), \n  nrow(d), \n  replace=F)\n\nThe next step is to add a column to the table which specifies the order in which tokens should be selected from a text. We will call the column ds_order (short for ‘down sampling order’). In texts with a single token, the token will receive the value 1, reflecting its priority in the down-sampling plan. For a text with two tokens, the numbers 1 and 2 are randomly assigned to the two tokens. For texts with three tokens, the numbers 1, 2 and 3 are shuffled, and so on. If we then sort the whole table according to the column ds_order, those tokens that are to be preferred, based on the rationale underlying structured down-sampling, appear at the top of the table.\nOur first step is to order the table by text_id, to make sure rows are grouped by Text.\n\nd &lt;- d[order(d$text_id),]\n\nWe then create a list of the texts in the data and sort it, so that it matches the way in which the table rows have just been ordered.\n\ntext_list &lt;- unique(d$text_id)\ntext_list &lt;- sort(text_list)\n\nWe now create the vector ds_order, which we will add to the table once it’s ready:\n\nds_order &lt;- NA\n\nThe following loop fills in the vector ds_order, text by text. It includes the following steps (marked in the script):\n\nProceed from text to text, from the first to the last in the text_list.\nFor text i, count the number of tokens in the text and store it as n_tokens.\nShuffle the sequence from 1 to n_tokens and store it as shuffled.\nAppend the shuffled sequence shuffled to the vector ds_order.\n\n\nfor(i in 1:length(text_list)){  # (1)\n  \n  n_tokens &lt;- sum(              # (2)\n    d$text_id == text_list[i])  # \n  \n  shuffled &lt;- sample(           # (3)\n    1:n_tokens,                 #\n    size = n_tokens,            #\n    replace = FALSE)            #\n  \n  ds_order &lt;- append(           # (4)\n    ds_order,                   #\n    shuffled)                   #\n}\n\nIf we look at the contents of ds_order, we note that it still has a leading NA:\n\nds_order\n\n  [1] NA  1  2  3  1  2  1  2  1  2  1  1  3  2  4  3  1  2  1  2  1  2  1  1  1\n [26]  1  1  1  1  3  2  4  1  1  2  1  3  1  2  1  3  1  2  1  1  1  1  1  3  2\n [51]  4  2  1  3  2  1  1  2  3  2  1  1  6  3  4  5  2  1  2  3  4  2  1  1  3\n [76]  2  1  4  1  2  3  1  4  2  1  1  1  1  2  1  1  1  1  1  2  3  1  2  3  4\n[101]  1  2  1  1  1  2  1  1  1  1  1  2  1  3  1  1  2  1  1  1  1  1  2  3  1\n[126]  2  1  2  3  1  1  1  2  3  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n[151]  1  1  2  3  2  1  1  1  1  2  3  1  2  1  1  1  1  2  1  2  1  1  1  2  1\n[176]  1  2  3  1  2  1  2  3  1  3  2  1  2  1  3  1  2  2  1  4  3  1  2  1  2\n[201]  1  3  2  1  3  1  1  2  2  1  1  2  1  3  1  1  2  1  1  2  1  1  2  1  1\n[226]  2  3  1  2  3  1  1  1  2  1  2  1  1  1  1  1  1  2  3  4  1  2  1  2  3\n[251]  4  1  2  3  1  2  1  1  2  1  5  2  4  3  1  2  1  1  1  2  2  1  1  2  1\n[276]  1  4  2  5  7  6  3  1  1  2  1  1  1  1  1  2  2  4  1  3  1  4  3  2  2\n[301]  1\n\n\nSo we get rid of it:\n\nds_order &lt;- ds_order[-1]\n\nWe can now add ds_order as a new column to our table:\n\nd$ds_order &lt;- ds_order\n\nThe final step is to order the rows of the table in a way that reflects our down-sampling priorities. We therefore primarily order the table based on ds_order. In addition, we order by the column random_order, which we created above. All tokens with the same priority level (e.g. all tokens with the value “1” in the column ds_order) will then be shuffled, ensuring that the order of tokens is random.\n\nd &lt;- d[order(d$ds_order, \n             d$random_order),]\n\nWe can now look at the result:\n\nhead(d)\n\n# A tibble: 6 × 7\n  text_id left_context modal  right_context original_order random_order ds_order\n  &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;                  &lt;int&gt;        &lt;int&gt;    &lt;int&gt;\n1 text_1  she          can'   t                          8            2        1\n2 text_87 photograph   can    be                       278            4        1\n3 text_46 It           cannot be                        28            6        1\n4 text_45 you          can    put                      172            8        1\n5 text_41 still        can    see                      200           10        1\n6 text_8  you          can    have                     153           12        1\n\n\nNote that the strategy we have used, i.e. adding a column reflecting the priority of tokens for down-sampling, allows us to approach down-sampling in a flexible and adaptive way: Rather than actually selecting (or sampling) tokens (or rows) from the original data, we may now simply start analyzing from the top of the table. This way we remain flexibility when it comes to the choice of how many tokens to analyze.\n\n\n\n\n\nReferences\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {Structured down-Sampling: {Implementation} in {R}},\n  date = {2023-11-18},\n  url = {https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “Structured down-Sampling: Implementation in\nR.” November 18, 2023. https://lsoenning.github.io/posts/2023-11-17_downsampling_implementation/."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html",
    "title": "The replication crisis: Implications for myself",
    "section": "",
    "text": "Since my research is almost exclusively quantitative, the methodological discourse surrounding the replication crisis has been directly relevant to my work. A recent invitation to take part in an online event by the International Society for the Linguistics of English (ISLE) on “Replication and Replicability” was an opportunity to reflect on the ways in which this “crisis” has affected how I do my job. In this blog post, I summarize these under three headings: (i) workflow and reproducibility, (2) open science, and (3) community discourse.\nI would like to start, however, with two preliminary remarks. For one, I consider the discussions, suggestions, and innovations that have arisen in the context of the credibility crisis in science as an opportunity – they should inspire us to improve the way(s) in which we do and communicate research. While there are some who point out that we actually don’t know whether there is a replication crisis in linguistics1, the suggested ways forward enable better science, so it is worth adopting them in any case.\nFurther, if we decide to change our research routines, we should be indulgent with ourselves: Many of the suggested improvements, especially concerning data analysis workflow, can be quite overwhelming at first. We should avoid setting our immediate aims too high – as I had to find out on numerous occasions, it is too easy to become frustrated. And this may also be something to keep in mind when making recommendations: The advice we give to others should be calibrated to the person across the table. Nothing is gained if a researcher with a genuine interest in adopting better practices ends up quitting in frustration."
  },
  {
    "objectID": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "href": "posts/2023-11-19_replication_crisis_reflection/index.html#footnotes",
    "title": "The replication crisis: Implications for myself",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt seems that we are not particularly eager to find out (see, e.g., this preprint by Kristina Kobrock and Timo Roettger). It would be quite surprising, however, if linguist(ic)s were spared – after all, the same human factors are at work in language research as in neighboring disciplines such as psychology.↩︎"
  },
  {
    "objectID": "posts/2023-12-11_computation_DA/index.html",
    "href": "posts/2023-12-11_computation_DA/index.html",
    "title": "A computational shortcut for the dispersion measure DA",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by Burch, Egbert, and Biber (2017) as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to Wilcox (1973), a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While Wilcox (1973) focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as Burch, Egbert, and Biber (2017, 193) note, a measure equivalent to DP (Gries 2008) can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in Wilcox (1973) as the mean difference analog (MDA). Both Wilcox (1973) and Burch, Egbert, and Biber (2017) argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in Sönning (2023).\nGries (2020, 116) has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula Wilcox (1973) gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in Wilcox (1973).\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to Sönning (2023). We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 0.84 seconds to run. The computational formula is quicker – it completes the calculations in only 0.02 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 24.2 seconds to run; the shortcut procedure, on the other hand, is done after 0.03 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n0.84\n24.20\n0.6003\n0.6139\n\n\nComputational\n0.02\n0.03\n0.6005\n0.6140\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\n\nReferences\n\nBurch, Brent, Jesse Egbert, and Douglas Biber. 2017. “Measuring and Interpreting Lexical Dispersion in Corpus Linguistics.” Journal of Research Design and Statistics in Linguistics and Communication Science 3 (2): 189–216. https://doi.org/10.1558/jrds.33066.\n\n\nGries, Stefan Th. 2008. “Dispersions and Adjusted Frequencies in Corpora.” International Journal of Corpus Linguistics 13 (4): 403–37. https://doi.org/10.1075/ijcl.13.4.02gri.\n\n\n———. 2020. “Analyzing Dispersion.” In A Practical Handbook of Corpus Linguistics, 99–118. Springer. https://doi.org/10.1007/978-3-030-46216-1_5.\n\n\nSönning, Lukas. 2023. “Advancing Our Understanding of Dispersion Measures in Corpus Research.” PsyArxiv Preprint. https://doi.org/10.31234/osf.io/ns4q9.\n\n\nWilcox, Allen R. 1973. “Indices of Qualitative Variation and Political Measurement.” The Western Political Quarterly 26 (2): 325–43. https://doi.org/10.2307/446831.\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {A Computational Shortcut for the Dispersion Measure\n    {*D\\textasciitilde A\\textasciitilde*}},\n  date = {2023-12-11},\n  url = {https://lsoenning.github.io/posts/2023-12-11_computation_DA/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “A Computational Shortcut for the Dispersion\nMeasure *D~A~*.” December 11, 2023. https://lsoenning.github.io/posts/2023-12-11_computation_DA/."
  },
  {
    "objectID": "posts/2023-11-16_negative_binomial/index.html",
    "href": "posts/2023-11-16_negative_binomial/index.html",
    "title": "The negative binomial distribution: A visual explanation",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(gamlss)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe negative binomial distribution is a useful device for modeling word counts. A typical setting for its application in corpus linguistics is the modeling of word frequency data – for instance, if we wish to summarize (or compare) occurrence rates of an item in a corpus (or across sub-corpora). Each text then contributes information about the frequency of the item in the form of (i) a token count (the number of times the word occurs in the text) and (ii) a word count (the length of the text). Based on the token and word count we can calculate an occurrence rate (or normalized frequency) for each text, and these rates are then directly comparable across texts.\nFrom a statistical perspective, word frequency would be considered as a count variable, which is observed at the level of the text and can take on non-negative integer values (i.e. 0, 1, 2, 3, 4, …). The text length can be thought of as a period of observation (measured in text time, i.e. the number of running words), in which a tally is kept of the number of events (in this case the occurrence of the focal item). And this is the typical definition of a count variable.\nThis blog post takes a closer look at the negative binomial distribution – how it works and why it is a useful device for modeling word frequency data. It is helpful to start with a concrete example: the frequency of which in the Brown Corpus. To keep things simple, we will stick to this data setting, where texts have (nearly) the same length. Note, however, that the negative binomial distribution (like the Poisson) readily extends to situations where texts have different lengths.\n\nObserved and expected frequency distributions\nIf we count the number of occurrences of which in each text and then look at the distribution of token counts, we obtain what is referred to as a frequency distribution or a token distribution. The frequency distribution for which in the Brown Corpus, which consists of 500 texts, appears in Figure 1 a. It shows the distribution of token counts across texts: Each bar represents a specific token count, and the height of the bar is proportional to the number of texts that have this many instances of which. Token counts vary between 0 (n = 26 texts) and 40 (1 text), and the distribution is right-skewed, which is quite typical of count variables, since they have a lower bound at 0.\n\n\nLoad data\n# tdm &lt;- read_tsv(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/brown_tdm.tsv\")\n# \n# str(tdm)\n# \n# n_tokens &lt;- tdm[,which(colnames(tdm) == \"which\")]\n# \n# \n# saveRDS(n_tokens, \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\nn_tokens &lt;- readRDS(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2023-11-16_negative_binomial/data/frequency_distribution_which_Brown.rds\")\n\n\n\n\nDraw Figure 1\n# Poisson model\nm &lt;- glm(n_tokens$which ~ 1, family=\"poisson\")\npoisson_mean &lt;- exp(coef(m))\npoisson_density &lt;- dpois(0:40, lambda = poisson_mean)\n\n\nn_texts &lt;- as.integer(table(n_tokens))\ntoken_count &lt;- as.integer(names(table(n_tokens)))\n\np1 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 53), xlim=c(-1.5, NA),\n  xlab.top = \"(a)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"\",\n  ylab=\"Observed\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=30, y=40, label=\"Observed frequency distribution\", \n               col=\"grey30\", cex=.9)\n    })\n\np2 &lt;- xyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 85), xlim=c(-1.5, NA),\n  xlab.top = \"(b)\\n\",\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = \"Number of instances of which\",\n  ylab=\"Expected\\nnumber of texts\\n\",\n  panel=function(x,y,...){\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=1, type=\"h\")\n    panel.text(x=30, y=60, label=\"Expected frequency distribution\\n(Poisson model)\",\n               col=\"grey30\", cex=.9)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 1: Which in the Brown Corpus: (a) observed frequency distribution and (b) expected frequency distribution based on the Poisson model.\n\n\n\n\n\nThe most basic probability distribution that is available for modeling count variables is the Poisson distribution. In general, we can check the fit of a distribution to the observed token counts by comparing the observed distribution (Figure 1 a) to the one expected under a Poisson model. The expected distribution appears in Figure 1 b. We note a mismatch with the observed data: Its tails are too thin – that is, the observed token counts are more widely spread out; counts of 0 are severely underpredicted (or underrepresented).\nIn fact, it is often the case that the Poisson distribution offers a poor fit to (language) data. This is because it rests on a simplistic assumption: It assumes that the expected frequency of which (or: the underlying probability of using which) is the same in every text. In our case, where we are dealing with texts of roughly 2,000 words in length, the expected number of instances of which, on average, is 7.1. Due to sampling variation, the actual number of instances per text will vary around this average. This sampling variation is accounted for in the Poisson distribution, giving it the (near-)bell-shaped appearance in Figure 1 b.\nIn linguistic terms, the model assumes that each text in the Brown Corpus, irrespective of genre or the idiosyncrasies of its author, has the same underlying probability of using which (i.e. about 7 in 2,000; or 3.5 per thousand words). Even for a function word such as which, this assumption seems difficult to defend. For instance, certain genres may use more postmodifying relative clauses, leading to a higher expected rate of which for texts in this category.\n\n\nPoisson mixture distributions\nTo offer a more adequate abstraction (or representation) of the observed token distribution, the assumption of equal rates across texts needs to be relaxed. We want the model to be able to represent variation among texts, and to record the amount of variation suggested by the data. On linguistic grounds, for instance, we would expect function words to vary less from text to text than lexical words, which are more sensitive to register and topic. The idea is to have an additional parameter in the model that acts like a standard deviation, essentially capturing (and measuring) the text-to-text variability in occurrence rates.\nIt is for this purpose that Poisson mixture distributions were invented. One such mixture distribution is the negative binomial distribution, which is also referred to as a Poisson-gamma mixture distribution. This is actually a more transparent label, as we will see shortly.\nThe idea behind Poisson mixtures is rather simple. Since the Poisson distribution on its own fails to adequately embrace high and low counts, its mean is allowed to vary. By allowing the Poisson mean to vary, i.e. shift up and down the count scale (or left and right in Figure 1), the probability distribution is more flexible, which allows it to accommodate the tails of the distribution.\nPoisson mixtures therefore include an additional dispersion parameter (similar to a standard deviation) and the Poisson mean is replaced by a distribution of Poisson means. Note that the way in which the term “dispersion” is used here differs from the sense it has acquired in lexicography and corpus linguistics (the difference is explained in this blog post).\n\n\nDraw Figure 2\nset.seed(1985)\n\ndelta_sample = rGA(20, mu=1, sigma=.1)\nlambda_plot = 7\nplot1 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    for(i in 1:length(delta_sample)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\ndelta_sample2 = rGA(20, mu=1, sigma=.25)\nlambda_plot = 7\nplot2 = xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.2),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"\",\n  panel=function(x,y,...){\n    panel.segments(x0=lambda_plot, x1=lambda_plot, y0=0, y1=.19, col=\"black\")\n    panel.text(x=7, y=.22, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_sample2)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"l\", col=\"black\", alpha=.2)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_plot*delta_sample2[i]), \n                   type=\"p\", col=\"black\", pch=19, alpha=.2)\n      }\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Mixing Poisson distributions: Each panel shows a sample of 20 Poisson distributions whose means vary around the grand mean of 7. The variation among Poissons is greater in the top panel.\n\n\n\n\nPoisson mixtures can be thought of as consisting of multiple Poisson distributions with different individual means. This is illustrated in Figure 2. To be able to show multiple distributions in one graph, we now leave out the spikes and connect the dots – a single distribution therefore appears as a bell-shaped profile that looks like a pearl necklace. Each panel shows 20 Poisson distributions, and each of these 20 distributions has a different mean. The means vary around 7, the overall mean of the Poisson mixture.\nThe distributions in the upper panel are spread out more widely than in the lower panel, and it is the newly introduced dispersion parameter that expresses the amount of variation among Poisson means. This basic idea applies to all Poisson mixture distributions. They are called ‘mixture distributions’ because they mix two probability distributions: (i) the familiar Poisson distribution and (ii) an additional probability distribution which describes the variability in the Poisson means. Simplifying slightly, Poisson mixtures only differ in the probability distribution they employ to describe the distribution of the Poisson means.\n\n\nThe gamma distribution as a model of text-to-text variation\nThe negative binomial distribution, for instance, relies on the gamma distribution to describe the text-to-text variability in occurrence rates. It is therefore also called a Poisson-gamma mixture distribution. Figure 3 shows the two gamma distributions that were used to create Figure 2. The dashed curve, which shows greater spread, belongs to the upper panel.\n\n\nDraw Figure 3\nlambda_plot = 7\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20/7), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(a)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,4.5),\n  par.settings=my_settings, axis=axis_L,\n  xlab.top=\"(b)\\n\",\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=expression(\"Number of instances of \"~italic(which)),\n  panel=function(x,y,...){\n    panel.segments(x0=7, x1=7, y0=0, y1=4.5, col=1)\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.1),\n                 type=\"l\")\n    panel.points(x = seq(.01, 2.8, length=1000)*7,\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, sigma=.25),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing the variability of text-to-text occurrence rates.\n\n\n\n\nFigure 3 a shows the gamma distributions on their actual scale. These are spread out around a value of 1, because they indicate variability in Poisson means on a multiplicative scale. It makes sense to center the distribution around 1, since the overall occurrence rate (multiplied by 1) should be at the center. The x-axis therefore denotes factors, which means that variability between Poisson means is expressed as ratios. The dashed curve, for instance, ranges from roughly 0.5 to 1.5, which means that most Poisson means are found within ± 50% of the overall mean. Since this multiplicative factor cannot be smaller than 0, we need a probability distribution that is bounded at zero (like the gamma distribution).\nPanel (b) translates these distributions to the occurrence rate scale. To create this graph, the factors (i.e. the x-values) in panel (a) were simply multiplied by the overall mean of 7. Now we see that, for the dashed curve, most occurrence rates vary between 4 and 11 instances per text.\n\n\nNegative binomial distribution applied to which\nLet us now apply the negative binomial distribution to the data for which in the Brown Corpus. We first check the fit of this new model to the data. Figure 4 shows that it provides a much closer approximation to the observed token distribution. It accomodates low and high counts and there seems to be no systematic lack of fit.\n\n\nFit negative binomial model in R\nm &lt;- gamlss(n_tokens$which ~ 1, family=\"NBI\", trace = FALSE)\n\nnb_density &lt;- dNBI(\n  0:40, \n  mu = exp(coef(m, what = \"mu\")),\n  sigma = exp(coef(m, what = \"sigma\")))\n\n\n\n\nDraw Figure 4\nxyplot(\n  n_texts ~ token_count,\n  par.settings=my_settings, axis=axis_L, ylim=c(0, 77), xlim=c(-1.5, NA),\n  scales=list(y=list(at=c(0,20,40,60,80))),\n  type=\"h\", lwd=6, lineend=\"butt\", col=\"grey\",\n  xlab = expression(\"Number of instances of \"~italic(which)),\n  ylab=\"Number of texts\",\n  panel=function(x,y,...){\n    panel.xyplot(x,y,...)\n    panel.text(x=10, y=60, label=\"Poisson\", \n               col=\"grey30\", cex=.9, adj=0)\n    panel.text(x=20, y=12, label=\"Negative binomial\", \n               col=1, cex=.9, adj=0)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", cex=.8)\n    panel.points(x=0:40, y=poisson_density*500, pch=19, col=\"grey30\", type=\"l\")\n    \n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, cex=.8)\n    panel.points(x=0:40, y=nb_density*500, pch=19, col=1, type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 4: Which in the Brown Corpus: Observed token distribution compared against the Poisson and the negative binomial model.\n\n\n\n\n\nLet us consider the gamma distribution that describes the text-to-text variability in occurrence rates. Its density appears in Figure 5, which includes two x-axes: A multiplicative scale (bottom) and a scale showing the expected number of instances in a 2,000-word text (the average text length in Brown). The gamma distribution is centered at 1 (multiplicative scale) and 7.1 occurrences (number of instances of which).\n\n\nDraw Figure 5\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 21/7), ylim=c(0,1.4),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,.5,1,1.5, 2, 2.5))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.polygon(x = c(seq(.01, 2.8, length=100), (seq(.01, 2.8, length=100))),\n                 y = c(dGA(seq(.01, 2.8, length=100), mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n                       rep(0,100)),\n                 col=\"lightgrey\", border=F)\n    panel.segments(x0=1, x1=1, y0=0, y1=1.4, col=1)\n    \n    panel.segments(\n      x0 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.25, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.75, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3,  lwd=2, col=1, lineend=\"butt\", alpha=.5)\n    \n    panel.segments(\n      x0 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      x1 = c(qGA(.05, mu=1, sigma=exp(coef(m, what = \"sigma\"))),\n             qGA(.95, mu=1, sigma=exp(coef(m, what = \"sigma\")))),\n      y0 = 0, y1 = 1.3, lwd=.5, col=1, lineend=\"butt\", alpha=.5)\n                      \n    panel.points(x = seq(.01, 2.8, length=1000),\n                 y = dGA(seq(.01, 2.8, length=1000), mu=1, \n                         sigma=exp(coef(m, what = \"sigma\"))),\n                 type=\"l\")\n    \n    panel.segments(x0=-.05, x1=21/7, y0=1.3, y1=1.3)\n    panel.segments(x0=seq(0,20,5)/exp(coef(m, what = \"mu\")), \n                   x1=seq(0,20,5)/exp(coef(m, what = \"mu\")), y0=1.3, y1=1.38)\n    \n    panel.text(x=seq(0,20,5)/exp(coef(m, what = \"mu\")), y=1.6,\n               label=seq(0,20,5), cex=.8)\n    \n    panel.text(x=1.5, y=2, label=expression(\"Number of instances of \"~italic(which)))\n    })\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The gamma distribution describing the variability of text-to-text occurrence rates of which in the Brown Corpus.\n\n\n\n\nThe gamma distribution represents a set of values, which specify the deviation of Poisson means from their overall mean in relative terms, as factors. For example, if the gamma distribution is restricted to the range [0.6; 1.7], the Poisson means will vary by a factor of 0.6 to 1.7 around their overall average. For a grand mean of 7, the Poisson means are then spread out between 7 \\(\\times\\) 0.6 = 4.2 and 7 \\(\\times\\) 1.7 = 11.9.\nThe grey vertical lines facilitate interpretation of the distribution: They show where the middle 50% of the texts (thick lines) and the middle 90% of the texts (thin lines) lie. Thus, half of the texts have an underlying expected number of occurrences between roughly 5 and 9; 90% of texts have expected counts between 2.5 and 14. This gives us a good idea of the underlying text-to-text variation in the Brown Corpus.\n\n\nGraphical derivation of the negative binomial distribution\nTo get a better understanding of the negative binomial distribution shown in Figure 4, let us now build one from scratch. Recall that the gamma distribution that is built into the negative binomial model provides us with a set of values with mean 1. We will refer to scores generated from this kind of gamma distribution as \\(\\delta\\) scores. To spread out the Poisson means, the overall mean is multiplied by the \\(\\delta\\) scores drawn from the gamma distribution. Since the \\(\\delta\\) scores are centered at 1, the overall mean is still 7. A gamma distribution that is spread out more widely produces more widely dispersed Poisson means.\nEssentially, then, a negative binomial distribution represents a batch of Poisson distributions whose individual means are spread out around the overall mean. This conceptual explanation of the negative binomial distribution illustrates the role of the gamma distribution and its auxiliary parameter \\(\\phi\\). We can translate this illustration into a simple simulation experiment. If we average over a large number of Poisson distributions produced by this procedure, we should arrive at the corresponding negative binomial distribution.\nThis is illustrated in Figure 6, which was constructed in the following way:\n\n\nDraw Figure 6\nset.seed(1985)\n\nset_nu = 2\ndelta_s = rGA(1000, mu=1, sigma=sqrt(1/set_nu))\n\nlambda_p = 7\npoisson_pool = matrix(NA, nrow=1000, ncol=21)\nfor (i in 1:1000){\n  poisson_pool[i,] = dpois(0:20, lambda=lambda_p*delta_s[i])\n}\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, 20), ylim=c(0,.45),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,5,10,15, 20))),\n  ylab=\"Density\", xlab=\"Frequency\",\n  panel=function(x,y){\n    panel.segments(x0=lambda_p, x1=lambda_p, y0=0, y1=.25, col=\"black\")\n    panel.text(x=7, y=.3, label=\"\\u03BC = 7\", col=\"black\")\n    for(i in 1:length(delta_s)){\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", type=\"l\", alpha=.03)\n      panel.points(x=0:20, y=dpois(0:20, lambda=lambda_p*delta_s[i]), \n                   col=\"black\", pch=16, cex=.4, alpha=.03)\n      }\n    # panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=\"white\", lwd=4)\n    # panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n    #              type=\"l\", col=\"white\", lwd=4)\n    panel.points(x=0:20, y=dNBI(0:20, mu=lambda_p, sigma=1/set_nu), \n                 type=\"l\", col=\"white\", lwd=2)\n    panel.points(x=0:20, y=colMeans(poisson_pool), type=\"l\", col=1, lty=\"22\", lineend=\"butt\")\n    })\n\nprint(p1, position=c(0,0,1,.95))\n\n\n\n\n\n\n\n\n\nFigure 6: Graphical derivation of the NB2 distribution: The blue curve shows the approximation based on averaging over 1,000 Poisson distributions whose means are random draws from a gamma distribution with \\(\\small{\\phi^{-1}}\\) = 0.25. The red curve shows the actual negative binomial distribution with \\(\\small{\\phi^{-1}}\\) = 0.25.\n\n\n\n\n\nSet the overall mean to 7, and the negative binomial dispersion parameter (here: the scale parameter) \\(\\phi^{-1}\\) to 0.5 (which is close to the value obtained for which in the Brown Corpus).\nTake 1,000 random draws from a gamma distribution defined by \\(\\mu\\) = 1 and \\(\\phi^{-1}\\) = 0.5. We refer to these draws as \\(\\delta\\) scores. The average of these scores is 1.\nMultiply 7 by these 1,000 \\(\\delta\\) scores. This produces 1,000 Poisson means, and hence 1,000 Poisson distributions.\nGraph these 1,000 Poisson distributions as pearl necklaces, adding transparency to avoid a cluttered display.\nDetermine the average probability for each count (0, 1, 2, etc.) by averaging over the 1,000 Poisson probabilities for each specific count. These averages should then resemble a negative binomial distribution with \\(\\mu\\) = 7 and \\(\\phi^{-1}\\) = 0.5.\n\nFigure Figure 6 shows the result of this simulation: The actual negative binomial distribution for these data is shown as a white trace, and the results of our simulation, i.e. average probability across the 1,000 simulated Poisson distributions, is shown as a dashed profile. The match is pretty good.\n\n\nDifferent parameterizations of the negative binomial distribution\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two ways. These different parameterizations have consequences for our interpretation of the negative binomial dispersion parameter returned by an analysis. This means that if we are interested in the dispersion parameter, we must know which parameterization our analysis is using. For an overview of which R packages/functions rely on which version of the negative binomial distribution, see this blog post\n\n\n\n\nCitationBibTeX citation:@online{sönning2023,\n  author = {Sönning, Lukas},\n  title = {The Negative Binomial Distribution: {A} Visual Explanation},\n  date = {2023-12-12},\n  url = {https://lsoenning.github.io/posts/2023-11-16_negative_binomial/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2023. “The Negative Binomial Distribution: A\nVisual Explanation.” December 12, 2023. https://lsoenning.github.io/posts/2023-11-16_negative_binomial/."
  },
  {
    "objectID": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "href": "posts/2024-01-11_beta_regression_quantifiers/index.html",
    "title": "Modeling the interpretation of quantifiers using beta regression",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)\nlibrary(betareg)\nlibrary(lattice)\nlibrary(brms)\nlibrary(knitr)\nlibrary(tidybayes)\nlibrary(kableExtra)\nlibrary(sjPlot)\nlibrary(lmtest)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\ndirectory_path &lt;- \"C:/Users/ba4rh5/Work Folders/My Files/R projects/_lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/\"\n\n\nThe purpose of this blog post is to show how to model proportions using beta regression. This procedure is particularly suitable for outcome variables that assume values in the interval (0, 1) – but excluding 0 and 1 –, and which do not summarize underlying raw counts (e.g. .30 for 3 out of 10 “successes”), in which case they could be handled using logistic regression. Examples are proportions, rates, and other indices such as corpus-linguistic dispersion measures.\nOur illustrative data represent such a fractional variable: How speakers interpret the quantifiers few, some, many, and most on the percentage (or proportion) scale. Few, for instance, may be understood as referring to about 10% of the total. A critical feature of our data is that they do not include proportions of 0 and 1. And this is in fact a requirement of beta regression, which does not work if the outcome includes values of 0 and/or 1.\nWe will start by introducing our illustrative data, and then model them using different beta regression models.\n\nLinguistic research context\nThe need to model the perception of quantifiers arose in the context of the Bamberg Survey of Language Variation and Change (see Krug and Sell 2013), a large-scale survey on the use of various lexical and grammatical structures in varieties of English. The grammar part of the questionnaire is designed to permit estimates of the prevalence of a broad variety of (morpho-)syntactic features in two registers (speech vs. writing). To this end, respondents are asked to indicate, on a 6-point scale, how prevalent a specific feature is in their home country or region. Each sentence is presented in two modes (auditorily and in writing) and participants indicate how many speakers use this kind of structure by choosing from the following options: no-one, few, some, many, most, everyone.\nThe analysis and interpretation of these data can profit from the fact that these quantifiers can be understood as relative frequencies. When analyzing the data, we can assign sensible numeric scores to the ordered categories and then interpret the resulting quantities as approximate estimates of the prevalence of a specific feature. For more background on this approach and its measurement-theoretic and statistical drawbacks see Sönning (2024).\n\n\nExperimental work on the perception of quantifiers\nA few studies have looked at how speakers interpret the quantifiers few, some, many, and most (Newstead, Pollard, and Riezebos 1987; Borges and Sawyers 1974; Stateva et al. 2019; Tiel, Franke, and Sauerland 2021). Proportional estimates from the literature are collected in Figure 1. For more information on the corresponding studies, please refer to Sönning (2024).\n\n\n\n\n\n\n\n\n\nFigure 1: Literature summary: The perception of quantifiers, expressed as a percentage.\n\n\n\n\nWhile estimates for the individual items show some variation across studies (and experiments within studies), they allow us to roughly pin down the quantitative meaning of these expressions. If we take a weighted average across the studies, where the weight of the individual percentages is proportional to the number of subjects, we obtain the following means:\n\nfew (.11, or 11%)\nsome (.27, or 27%)\nmany (.67, or 67%)\nmost (.83, or 83%)\n\n\n\nIllustrative data: The perception of quantifiers\nI collected additional data on the perception of these expressions, using as participants university students and colleagues that took part in the English Linguistics research seminar at the University of Bamberg in the winter term of 2023. I obtained data from 20 individuals (around 2/3 being students) by handing out paper slips with the following instructions:\n\n\n\nSurvey task: Paper slips with instructions used for data collection\n\n\nParticipants were given two minutes to complete the task. I then collected the paper sheets and (later) entered the data into a spreadsheet (in wide format):\n\ndat &lt;- readxl::read_xlsx(paste0(directory_path, \"data/data_quantifiers.xlsx\"))\n\n\n\n\n\n\n\nsubject\nfew\nsome\nmany\nmost\n\n\n\n\nsubj_01\n5\n15.0\n30.0\n50.0\n\n\nsubj_02\n20\n35.0\n70.0\n80.0\n\n\nsubj_03\n10\n27.5\n65.0\n85.0\n\n\nsubj_04\n8\n20.0\n51.0\n80.0\n\n\nsubj_05\n12\n30.0\n50.0\n75.0\n\n\nsubj_06\n15\n35.0\n60.0\n90.0\n\n\nsubj_07\n25\n40.0\n67.5\n92.5\n\n\nsubj_08\n10\n30.0\n60.0\n80.0\n\n\nsubj_09\n15\n33.0\n67.0\n85.0\n\n\nsubj_10\n10\n33.0\n67.0\n90.0\n\n\nsubj_11\n10\n25.0\n50.0\n75.0\n\n\nsubj_12\n10\n40.0\n70.0\n90.0\n\n\nsubj_13\n7\n20.0\n60.0\n85.0\n\n\nsubj_14\n15\n40.0\n69.0\n90.0\n\n\nsubj_15\n15\n30.0\n75.0\n85.0\n\n\nsubj_16\n6\n14.0\n75.0\n91.0\n\n\nsubj_17\n5\n12.5\n65.0\n80.0\n\n\nsubj_18\n10\n25.0\n70.0\n90.0\n\n\nsubj_19\n15\n40.0\n75.0\n90.0\n\n\nsubj_20\n10\n20.0\n80.0\n90.0\n\n\n\n\n\n\n\n \nOur first step is to rearrange these data from wide to long form, and to translate percentages into proportions, as this is the scale on which beta regression operates. We also change the order of the quantifiers (from the default alphabetical arrangement) based on the relative frequency they express (see Figure 1).\n\nd &lt;- dat |&gt; \n  gather(\n    few:most, \n    key = quantifier, \n    value = percentage) |&gt; \n  mutate(\n    proportion = percentage/100\n  ) \n\nd$quantifier &lt;- factor(\n  d$quantifier, \n  levels = c(\"few\", \"some\", \"many\", \"most\"),\n  ordered = TRUE)\n\n\n\n\n\n\n\n\n\n\nFigure 2: Ratings collected from 20 informants.\n\n\n\n\nLet us start by looking at the distribution of the responses for each quantifier using a dot diagram. Figure 2 shows that there is some variation among subjects with regard to the perceived meaning of these expressions. For many and most, in particular, there is one unusually low estimate.\n\n\n\n\n\n\n\n\n\nFigure 3: Line plot linking the ratings provided by the same informant.\n\n\n\n\nTo check whether these outliers are responses from the same individual, we look at the data using a line plot that links observations from the same subject. In Figure 3, each profile represent a subject. The two unusually low responses for many and most are indeed due to the same individual, who also provided relatively (though not unusually) low ratings for few and some.\nApart from the fact that this respondent provides relatively low estimates for all quantifiers, there is no immediate reason why their data should be excluded from the analysis. We will therefore start by using the full data set and then rely on model diagnostics to see whether the data and model suggest that this informant be excluded from the analysis.\n\n\nBeta regression\nWe now look at how to use beta regression to model these data. First, however, let us be clear about the purpose of our analysis – i.e. the kind of information we wish to extract from the data. Our goals are descriptive and we will use beta regression to summarize the data. The following quantitative features of the data will be of interest:\n\nAveraging over the speakers in our sample, what is the typical proportional interpretation of each quantifier?\nSeeing that we are dealing with a rather small sample of speakers, what is the statistical uncertainty surrounding these typical values?\nHow much do speakers vary in their interpretation of the individual expressions, i.e. how dispersed are their perceptions around the typical interpretation?\n\nBeta regression can address these questions by providing (i) typical values (average proportions) for each quantifier (e.g. .12, or 12%, for few), (ii) a confidence (or posterior) interval for these estimates (e.g. 95% CI [.10; .14]), and (iii) a distributional summary of the dot diagrams we saw in Figure 2. To this end, the beta distribution serves as an abstraction of the observed distribution of responses and it allows us to make statements about, say, the share of speakers who interpret few as denoting a relative frequency of .05 or less. This kind of information is essential when we are interested in the stability (or consistency) of interpretations across speakers.\nOur main focus will be on how to run beta regression using R. We therefore only include a few essentials on the beta distribution and beta regression; for more background on both, please refer to this excellent blog post (Heiss 2021a).\n\n\nThe beta distribution\nThe beta distribution is a probability distribution bounded between (but excluding) 0 and 1. It is defined by two shape parameters, \\(a\\) and \\(b\\). Figure 4 shows two beta distributions with the same mean but different spreads. Note how, for each distribution, \\(a\\) is .30 (or 30%) of the sum of \\(a\\) and \\(b\\). In fact, \\(a/(a+b)\\) denotes the mean of the distribution. Further, we note that the spread of the distribution is related to the sum of \\(a\\) and \\(b\\): The greater the sum, the “tighter”, or more peaked, the beta distribution.\n\n\n\n\n\n\n\n\nFigure 4: The shape parameters \\(a\\) and \\(b\\) define the beta distribution.\n\n\n\n\n\nAs explained in much more detail by Heiss (2021a) in his blog post, the beta distribution can alternatively be written down using its mean \\(\\mu\\) and precision \\(\\phi\\). We have already encountered the mean, which is \\(a/(a+b)\\). The precision is simply \\((a+b)\\), and it is reflected in the spread of the distribution (see Figure 4): The greater the precision, the tighter the distribution of proportions about their mean. We can go back and forth between the two parameterization of the beta distribution as follows:\n\\[\\begin{align}\n\\mu &= a/(a+b) \\\\\n\\phi &= a+b \\\\\n\\\\\n\na &= \\mu\\phi \\\\\nb &= (1-\\mu)\\phi\n\\end{align}\\]\nWe need to be able to switch between these parameterizations. The reason is that regression models work with \\(\\mu\\) and \\(\\phi\\). The parameter \\(\\mu\\) answers questions (i) and (ii). To be able to draw a beta distribution, however, and use it to summarize variation in the perception of a specific quantifier in a population of speakers, we need the shape parameters \\(a\\) and \\(b\\). We therefore use code provided by Heiss (2021a) to write a function that allow us to switch from the \\(\\mu\\)-\\(\\phi\\) to the \\(a\\)-\\(b\\) parameterization:\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\n\n\nOverview of regression models\nWe will run a series of regression models, which differ in structure. We will start with frequentist models, using the {betareg} package (Cribari-Neto and Zeileis 2010) and then move on to Bayesian regression with the {brms} package (Bürkner 2017) to be able to incorporate by-subject random intercepts into our model.\nThe following table gives an overview of the series of models we are about to fit. Each model allows the outcome variable to vary by quantifier and therefore returns an estimate of \\(\\mu\\) for each expression. Model I assumes that quantifiers are interpreted with the same precision \\(\\phi\\), so it estimates only one \\(\\phi\\) parameter. This means that each quantifier has the same stability of interpretation in the populations of speakers represented by our sample. In contrast, Model II allows the precision to vary across quantifiers. This means that the model provides leeway for expressions to differ in the level of stability (or consistency) with which they are interpreted. A separate \\(\\phi\\) parameter is therefore estimated for each. This type of model has been referred to as a variable dispersion beta regression model (see Cribari-Neto and Zeileis 2010). Finally, for Model III we will switch to the {brms} package (and Bayesian inference), to be able to include random effects into our model.\n\n\n\n\n\nModel\nConstant precision\nRandom intercepts\nPackage\n\n\n\n\nI\nYes\nNo\nbetareg\n\n\nII\nNo\nNo\nbetareg\n\n\nIII\nYes\nYes\nbrms\n\n\n\n\n\n\n\nModel I\nWe start with a frequentist model (using {betareg}) that allows \\(\\mu\\) to vary across quantifiers but assumes a constant precision \\(\\phi\\). We model \\(\\mu\\) on the log-odds (i.e. logit) scale. We also drop the intercept from the model to directly obtain logits for each quantifier:\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d, \n  link = \"logit\")\n\nprint(m1)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier, data = d, link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9542         -0.9348          0.5649          1.6346  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n23.61  \n\n\nBefore we turn to the estimates returned by the model, let’s look at a number of diagnostic plots, which are shown in Figure 5:\n\nWhen graphing residuals against fitted values, we usually look for indications of heteroskedasticity, i.e. whether the spread of residuals increases with the fitted values. This is not the case for our data and model. However, the residuals surrounding the fitted values for many and most each include a large negative residual, i.e. a response that is unusually low for these quantifiers.\nThe quantile-quantile plot checks for normality of residuals. We have added a dot diagram at the right margin, which gives a more intuitive impression of the shape of the distribution and potential outliers. Both arrangements indicate that the residuals are negatively skewed (rather than normal), with two unusually large negative deviations.\nCook’s distance expresses the influence individual observations have on the regression coefficients. There are are two data points that appear to be particularly influential (number 41 and 61).\nSince the factor Subject is so far not included in the analysis, our model does not know about the fact that responses are clustered. This may induce non-independence and lead to correlated errors, because we would expect responses by the same subject to be similar. The graph shows residuals grouped by Subject. If responses were independent, the residuals should not correlate within the factor Subject. This means that residuals should not vary systematically across subjects. If correlated errors (and hence non-independence in the data) were indeed no concern, the 20 sets (of four residuals each) would look like they represent random draws from the dot diagram shown in panel (b). However, we observe that responses by the same subject in fact tend to be alike. Subjects 1, 4, and 11, for instance, have consistently negative residuals, indicating that they gave responses that were, overall, systematically lower compared to those of the other subjects.\n\n\n\nDraw figure\nm_diagnostics &lt;- d\nm_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3.6, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .1, .2, .3)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 5: Diagnostic plots for Model I.\n\n\n\n\n\nOur diagnostic plots reveal two things:\n\nThere are two unusual (and influential) data points\nThe residuals are not independent but correlated with the factor Subject.\n\nIt turns out that the two unusual and influential data points are from subject 1. These are the responses for many and most, which already stood out above in Figure 3. We will remove subject 1 from the data. As for the non-independence of errors, Model III will include by-subjects random intercepts, to account for the correlation of residuals with the factor Subject.\nLet us remove subject 1, refit Model I, and again draw diagnostic plots:\n\nd_19 &lt;- subset(d, subject != \"subj_01\")\n\nm1 &lt;- betareg(\n  proportion ~ -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\n\n\nDraw figure\nm_diagnostics &lt;- d_19\nm_diagnostics$subj_index &lt;- as.numeric(factor(d_19$subject))\nm_diagnostics$fitted &lt;- qlogis(fitted(m1))\nm_diagnostics$residual &lt;- residuals(m1, type = \"sweighted2\")\nm_diagnostics$leverage &lt;- hatvalues(m1)\nm_diagnostics$cooks_distance &lt;- cooks.distance(m1)\n\np1 &lt;- xyplot(residual ~ fitted, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m_diagnostics) - 1/2)/nrow(m_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4.2),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-3, ybottom=-4.5,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(cooks_distance ~ 1:nrow(m_diagnostics), data=m_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .05)),\n                   x=list(at=c(1, 20, 40, 60))),\n       xlab=\"Observation number\",\n       ylab=\"Cook's distance\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 77))\n\nsubj_resid &lt;- m_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_resid = min(residual),\n    max_resid = max(residual)\n  )\n\np4 &lt;- xyplot(residual ~ subj_index, data=m_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Std. weighted residuals 2\",\n       xlab.top=\"(d) Correlated errors\\n\",\n       scales=list(y=list(at=c(-2,0,2),\n                          labels=c(\"\\u22122\",\"0\",\"2\")),\n                   x=list(at=c(1, 5, 10, 15))),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:19, x1=1:19, y0=subj_resid$min_resid, y1=subj_resid$max_resid)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 6: Diagnostic plots for Model I, after exclusion of subject 1.\n\n\n\n\n\nFigure 6 looks much better:\n\nThere are no clear outliers when looking at residuals by quantifier.\nThe residuals are more nearly normally distributed.\nThere are no influential data points.\n\nAs panel (d) shows, however, errors are still correlated with Subject. We will address this issue further below, in Model III.\nLet us now use our model to address our descriptive objectives. To answer question (i), we need to consider the \\(\\mu\\) coefficients. These are on the logit scale, so we need to back-transform them to proportions using the function plogis():\n\nmu_m1 &lt;- plogis(coef(m1)[1:4])\nphi_m1 &lt;- coef(m1)[5]\n\nround(mu_m1, 2)\n\n quantifierfew quantifiersome quantifiermany quantifiermost \n          0.12           0.29           0.66           0.85 \n\n\nTo answer question (ii), we obtain 95% confidence intervals for these using the function confint(), keeping in mind that we again need to translate logits back into proportions. We collect estimates in Table 1.\n\n\nConstruct table of estimates\nci_mu_m1 &lt;- plogis(confint(m1)[1:4,])\n\n\nm1_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m1), \n  paste0(\"[\", numformat(ci_mu_m1[,1]), \"; \", numformat(ci_mu_m1[,2]), \"]\"))\n\nrownames(m1_estimates) &lt;- NULL\ncolnames(m1_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm1_estimates |&gt; kable()\n\n\n\n\nTable 1: Typical values and 95% confidence intervals based on Model I.\n\n\n\n\n\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .15]\n\n\nsome\n.29\n[.25; .32]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.82; .88]\n\n\n\n\n\n\n\nThe following code uses the {sjPlot} package to print a similar table (output not shown):\n\ntab_model(m1, transform = \"plogis\")\n\nTo be able to make statements about the stability of interpretations across speakers, we need the shape parameters of the beta distributions. Consider, as an example, the quantifier few. The estimates returned by our model are the average proportion \\(\\mu =\\) 0.12 and the precision \\(\\phi =\\) 35.3. We can use the function defined above to translate these into the shape parameters \\(a\\) and \\(b\\):\n\nshape1_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape1\nshape2_m1 &lt;- muphi_to_shapes(mu_m1, phi_m1)$shape2\n\nThese shape parameters define the beta distribution that describes the spread of responses, i.e. the variation among subjects in the relative-frequency interpretation of few. The percentages reported in Table 1 are averages over subjects. To obtain information on the distribution of responses around these typical values, we need to look at the associated beta density, which appears in Figure 7. This distribution is a model-based estimate of the variability in the responses across individuals in the population of speakers represented by the subjects in our sample. It shows us how stable (or consistent) the interpretation of few is across speakers.\n\n\n\n\n\n\n\n\n\nFigure 7: The beta density for few, based on model I.\n\n\n\n\nWe can summarize the information provided by such beta densities using informative quantiles; these can be located with the function qbeta(). A quantile is an x-value (here: a proportion/percentage) below which a certain portion of the probability mass lies. The .25 quantile, for instance, marks the x-value that cuts off the lower tail of the distribution that contains 25% of the mass.\nIn Figure 8, two intervals are marked using grey shading. The darker shade denotes the interval covering the central 50% of the subjects. It extends from the .25 quantile to the .75 quantile. The lighter shades denote the region where the middle 80% of the subjects are found. These quantiles tell us something about the speaker-to-speaker variability in the interpretation of few. While the population average is estimated at .12 (or 12%), speakers vary around this value: the central 50% of the speakers give estimates between .07 and .16, the central 80% of the population are found between .05 and .21. We will refer to these as coverage intervals.\n\n\n\n\n\n\n\n\nFigure 8: The beta distribution for few, including informative quantiles and coverage intervals; based on model I.\n\n\n\n\n\nTable 2 reports these quantiles, i.e. the central 50% and 80% coverage intervals for all quantifiers.\n\n\nConstruct table with quantiles\nbeta_quantiles_m1 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[1], shape2=shape2_m1[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[2], shape2=shape2_m1[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[3], shape2=shape2_m1[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m1[4], shape2=shape2_m1[4])))\n\nbeta_quantiles_m1 &lt;- cbind(c(\"few\", \"some\", \"many\", \"most\"),\n                        beta_quantiles_m1)\n\ncolnames(beta_quantiles_m1) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m1 |&gt; kable()\n\n\n\n\nTable 2: 50% and 80% coverage intervals, based on Model I.\n\n\n\n\n\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.16\n.20\n\n\nsome\n.19\n.23\n.34\n.39\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.77\n.82\n.90\n.92\n\n\n\n\n\n\n\nWe can now draw a graph based on Model I that gives a visual summary of the data: Figure 9 shows the typical values for each quantifier (average proportions), along with their 95% confidence interval and the associated beta density. Since Model I assumes constant precision across quantifiers, the visual spread of the beta distributions is constrained to be constant across these. The differences in spread that are evident from Figure 9 reflect boundary effects, as the variability of proportions is naturally constrained near the scale endpoints.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proporiton\", ylab=\"Density               \",\n  xlab.top=\"Model I\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[1], shape2=shape2_m1[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[2], shape2=shape2_m1[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[3], shape2=shape2_m1[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m1[4], shape2=shape2_m1[4]),\n      type=\"l\")\n    panel.points(x=mu_m1, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=13, y1=13)\n    panel.text(x=mu_m1, y=11, label=numformat(mu_m1))\n    panel.text(x=mu_m1, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 9: Visual summary of Model I: Typical values, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nModel II\nNext we fit a model that allows the precision \\(\\phi\\) to vary across quantifiers; \\(\\phi\\) is now modeled on the natural log scale. We again drop the intercept from the model to directly obtain logits (\\(\\mu\\)) and natural logs (\\(\\phi\\)) for each quantifier:\n\nm2 &lt;- betareg(\n  proportion ~ -1 + quantifier | -1 + quantifier, \n  data = d_19, \n  link = \"logit\")\n\nprint(m2)\n\n\nCall:\nbetareg(formula = proportion ~ -1 + quantifier | -1 + quantifier, data = d_19, \n    link = \"logit\")\n\nCoefficients (mean model with logit link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n       -1.9912         -0.9013          0.6445          1.7692  \n\nPhi coefficients (precision model with log link):\n quantifierfew  quantifiersome  quantifiermany  quantifiermost  \n         3.903           3.251           3.464           3.781  \n\n\nThe precision parameters (Phi coefficients in the printed output) are quite similar across quantifiers. Let us therefore formally compare models I and II to see whether the added complexity is needed to adequately describe the observed data. As illustrated by Cribari-Neto and Zeileis (2010), the function lrtest(), which compares nested models using a likelihood-ratio test, may be used to this end:\n\nlrtest(m1, m2)\n\nLikelihood ratio test\n\nModel 1: proportion ~ -1 + quantifier\nModel 2: proportion ~ -1 + quantifier | -1 + quantifier\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)\n1   5 100.77                     \n2   8 102.05  3 2.5591     0.4647\n\n\nWe note that there is little evidence from the data that the quantifiers are interpreted with different precision. Instead, the variability of responses around the typical value for each quantifier appears to be rather stable (on the logit scale) across expressions.\nFor illustrative purposes, we nevertheless stick to this model to extract relevant quantities. We start by back-transforming the logit-scaled coefficients and confidence intervals for \\(\\mu\\) using the function plogis() and collect the estimates and their confidence intervals in Table 3. To summarize the beta distributions, we present informative quantiles in Table 4.\n\n\nConstruct table of estimates\nmu_m2 &lt;- plogis(coef(m2)[1:4])\nci_mu_m2 &lt;- plogis(confint(m2)[1:4,])\nphi_m2 &lt;- exp(coef(m2)[5:8])\n\nshape1_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape1\nshape2_m2 &lt;- muphi_to_shapes(mu_m2, phi_m2)$shape2\n\nm2_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m2), \n  paste0(\"[\", numformat(ci_mu_m2[,1]), \"; \", numformat(ci_mu_m2[,2]), \"]\"))\n\nrownames(m2_estimates) &lt;- NULL\ncolnames(m2_estimates)[2:3] &lt;- c(\"Mean\", \"95% CI\")\n\nm2_estimates |&gt; kable()\n\n\n\n\nTable 3: Typical values and 95% confidence intervals based on Model II.\n\n\n\n\n\n\n\nQuantifier\nMean\n95% CI\n\n\n\n\nfew\n.12\n[.10; .14]\n\n\nsome\n.29\n[.25; .33]\n\n\nmany\n.66\n[.62; .69]\n\n\nmost\n.85\n[.83; .88]\n\n\n\n\n\n\n\n\n\nConstruct table of quantiles\nbeta_quantiles_m2 &lt;- rbind(\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[1], shape2=shape2_m2[1])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[2], shape2=shape2_m2[2])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[3], shape2=shape2_m2[3])),\n  numformat(qbeta(c(.1,.25,.75,.9), shape1=shape1_m2[4], shape2=shape2_m2[4])))\n\nbeta_quantiles_m2 &lt;- cbind(\n  c(\"few\", \"some\", \"many\", \"most\"),\n  beta_quantiles_m2)\n\ncolnames(beta_quantiles_m2) &lt;- c(\"Quantifier\", \"[80%\", \"[50%\", \"50%]\", \"80%]\")\nbeta_quantiles_m2 |&gt; kable()\n\n\n\n\nTable 4: 50% and 80% coverage intervals, based on Model II.\n\n\n\n\n\n\n\nQuantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.07\n.09\n.15\n.18\n\n\nsome\n.18\n.23\n.35\n.41\n\n\nmany\n.55\n.60\n.71\n.76\n\n\nmost\n.78\n.82\n.89\n.92\n\n\n\n\n\n\n\nA visual summary of Model II appears in Figure 10. We observe that the beta density for few is more peaked than in Figure 9. This information can also be read from the printed regression table, which shows that few has the highest precision. This is also reflected in the coverage intervals. Whereas in Model I, the 80% coverage intervals extended from .06 to .20, Model II suggests a slightly narrower 80% coverage interval, ranging from .07 to .18.\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model II\\n\",\n  panel=function(x,y){\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.polygon(\n      x = c(seq(0,1,length=100), rev(seq(0,1,length=100))),\n      y = c(dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n            rep(0, 100)), col=1, alpha=.2)\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[1], shape2=shape2_m2[1]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[2], shape2=shape2_m2[2]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[3], shape2=shape2_m2[3]),\n      type=\"l\")\n    panel.points(\n      x = seq(0,1,length=100),\n      y = dbeta(seq(0,1,length=100), shape1=shape1_m2[4], shape2=shape2_m2[4]),\n      type=\"l\")\n    panel.points(x=mu_m2, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=13, y1=13)\n    panel.text(x=mu_m2, y=11, label=numformat(mu_m2))\n    panel.text(x=mu_m2, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 10: Visual summary of Model II: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nModel III\nTo address the issue of correlated errors (i.e. the non-independence of observations), we now add by-speaker random intercepts to our model. Since this feature is not implemented in the {betareg} package, we switch to Bayesian regression using the {brms} package. As we will now be explicitly modeling Speaker as a source of variation, let us give subject 1 a second chance. After all, the primary reason for excluding them from our analysis were the unusually low values given for many and most. We will then again look at diagnostic plots to see whether the data points from this speaker also appear problematic in the context of Model III.\nAs can be seen from the following R code, using {brms} requires a change in model syntax. We will rely on the default priors.\n\nm3 &lt;- brm(\n  proportion ~ -1 + quantifier + (1|subject),\n  data = d,\n  family = Beta(),\n  chains = 4, iter = 3000, warmup = 1000, cores = 4,\n  backend = \"cmdstanr\",\n  file = paste0(directory_path, \"m3\")\n)\n\nm3 &lt;- readRDS(paste0(directory_path, \"m3.rds\"))\n\nLet us first draw some diagnostic plots. Since we are dealing with a Bayesian regression model, some of the quantities we use for plotting are different. Their meaning and interpretation, however, is very similar:\n\nInstead of weighted residuals, we look at Pearson residuals.\nWe will use Pareto k values as influence indicators (for more information on this, see this paper and this video tutorial). For these, thresholds have been proposed to signal “good”, “ok”, “bad”, and “very bad” Pareto k values.\nSince correlated errors (within subject) should no longer be a concern, we will instead group influence measures (i.e. Pareto k values) by subject, to see if there are any influential individuals.\n\nThe diagnostic plots appear in Figure 11. We note the following:\n\nThere appear to be no outliers when looking at residuals by quantifier. This is because the random intercept for subject 1 now accomodates their unusually low responses.\nThe distribution of the Pearson residuals looks OK.\nThere is only one “bad” data point.\nThis “bad” observation stems from subject 16, whose responses sit less well with the model, on average. It turns out that the responses of subject 16 were drawn to the extremes: They gave relatively low responses to few (6%) and some (14%), but quite high responses to many (75%) and most (91%). This tendency towards the extremes is not captures by the random intercept for subject 16, since this parameter only represents their average response (46.5%), which is not particularly unusual.\n\nOverall, then, we are happy with Model III and welcome subject 1 back into the dataset.\n\n\nDraw figure\nloo_m3 &lt;- loo(m3)\nloo_m3_pointwise &lt;- data.frame(loo_m3$pointwise)\n\nm3_diagnostics &lt;- d\nm3_diagnostics$subj_index &lt;- as.numeric(factor(d$subject))\nm3_diagnostics$fitted_with_random_intercepts &lt;- qlogis(fitted(m3)[,1])\nm3_diagnostics$residual &lt;- residuals(m3, type = \"pearson\")[,1]\nm3_diagnostics$influence_pareto_k &lt;- loo_m3_pointwise$influence_pareto_k\nm3_diagnostics &lt;- m3_diagnostics |&gt; \n  group_by(quantifier) |&gt; \n  mutate(\n    fitted_quantifier = mean(fitted_with_random_intercepts))\n\n\np1 &lt;- xyplot(residual ~ fitted_quantifier, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Fitted values (logit scale)\",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(a) Constant variance\\n\",\n       scales=list(x=list(at=c(-2,-1,0,1),\n                          labels=c(\"\\u22122\",\"\\u22121\",\"0\",\"1\")),\n                   y=list(at=c(-4,-2,0,2),\n                          labels=c(\"\\u22124\",\"\\u22122\",\"0\",\"2\"))),\n       ylim=c(-5, 2.5),\n       panel=function(x,y,...){\n         panel.abline(h=0, col=\"grey\")\n         panel.points(x,y,...)\n         panel.text(x=m1$coefficients$mean, y=-4.3,\n                    label = c(\"few\", \"some\", \"many\", \"most\"),\n                    col = \"grey30\", font = \"italic\")\n       })\n\nnormal_quantiles &lt;- (1:nrow(m3_diagnostics) - 1/2)/nrow(m3_diagnostics)\n\np2 &lt;- qqmath(~residual, data = m3_diagnostics,\n              axis=axis_L, par.settings=my_settings,\n       xlab=\"Normal quantiles    \",\n       ylab=\"Pearson residuals\",\n       xlab.top=\"(b) Normality of residuals\\n\",\n       xlim=c(NA, 4),\n              panel = function(x, ...) {\n          panel.qqmathline(x, ...)\n          panel.qqmath(x, ...)\n          panel.dotdiagram(m3_diagnostics$residual, x_anchor = 3.1,\n                           set_cex = .8, set_col=\"grey30\", \n                           n_bins = 40, scale_y = .1, vertical = TRUE)\n          panel.rect(xleft=2.8, xright=4.2, ytop=-2, ybottom=-3,\n                     border=FALSE, fill=\"white\")\n       })\n\np3 &lt;- xyplot(influence_pareto_k ~ 1:nrow(m3_diagnostics), data=m3_diagnostics, \n       type=\"h\", col=1,\n       axis=axis_L, par.settings=my_settings,\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 20, 40, 60, 80))),\n       xlab=\"Observation number\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(c) Influential observations\\n\",\n       ylim=c(0, NA), xlim=c(-1, 81),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.xyplot(x,y,...)\n         panel.text(x=-5, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n       })\n\nsubj_pareto_k &lt;- m3_diagnostics |&gt; \n  group_by(subject) |&gt; \n  dplyr::summarize(\n    min_pareto_k = min(influence_pareto_k),\n    max_pareto_k = max(influence_pareto_k)\n  )\n\np4 &lt;- xyplot(influence_pareto_k ~ subj_index, data=m3_diagnostics,\n       axis=axis_L, par.settings=my_settings,\n       xlab=\"Subject ID\",\n       ylab=\"Influence (Pareto k)\",\n       xlab.top=\"(d) Influence by subject\\n\",\n       scales=list(y=list(at=c(0, .5, .7, 1)),\n                   x=list(at=c(1, 5, 10, 15, 20))),\n       panel=function(x,y,...){\n         panel.abline(h=.5, lty=3, lineend=\"butt\", col=\"grey\")\n         panel.abline(h=.7, lty=2, lineend=\"butt\", col=\"grey\")\n         panel.text(x=-1.35, y=c(.25, .6, .8), label=c(\"good\", \"ok\", \"bad\"), col=\"grey40\", srt=90)\n         panel.xyplot(x,y,...)\n         panel.segments(x0=1:20, x1=1:20, \n                        y0=subj_pareto_k$min_pareto_k, \n                        y1=subj_pareto_k$max_pareto_k)\n       })\n\n\n\n\n\n\n\n\n\n\nFigure 11: Diagnostic plots for Model III.\n\n\n\n\n\nWe now use Model III to address questions (i) and (ii). As described in more detail in the Marginal Effects Zoo book and in another fantastic blog post by Heiss (2021b), we use the {tidybayes} package to process the posterior distribution.\nWe use the model to generate posterior predictions and then summarize these. The kind of prediction we are interested in at the moment (for questions i and ii) are typical values, i.e. averages. These are sometimes referred to as expected values. We therefore use the function epred_draws(), which uses the full posterior distribution to generate expected predictions (epred) for conditions, i.e. (constellations of) predictor values. We need to specify the conditions of interest using the argument newdata. The additional argument re_formula = NA tells epred_draws() to disregard the by-subject random intercepts in this predictive task. After all, we are presently only concerned with the average across subjects.\n\nm3_epred &lt;- m3 |&gt;  \n  epred_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\nWe then summarize the posterior distribution of these expected values using the median and the .025 and .975 quantiles, which provide uncertainty estimtes similar to the 95% CIs reported for the other models.\n\nmu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    median = median(.epred)\n  )\n\nci_mu_m3 &lt;- m3_epred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    ci_lower = quantile(.epred, .025),\n    ci_upper = quantile(.epred, .975)\n  )\n\nWe collect these point and interval estimates for the expected value (i.e. proportions) of each quantifier in Table 5.\n\n\nConstruct table of estimates\nm3_estimates &lt;- cbind(\n  Quantifier = c(\"few\", \"some\", \"many\", \"most\"),\n  numformat(mu_m3$median), \n  paste0(\"[\", numformat(ci_mu_m3$ci_lower), \"; \", numformat(ci_mu_m3$ci_upper), \"]\"))\n\nm3_estimates |&gt; kable()\n\n\n\n\nTable 5: Typical values and 95% posterior intervals based on Model III.\n\n\n\n\n\n\n\nQuantifier\n\n\n\n\n\n\nfew\n.11\n[.09; .14]\n\n\nsome\n.27\n[.23; .32]\n\n\nmany\n.64\n[.59; .69]\n\n\nmost\n.84\n[.81; .87]\n\n\n\n\n\n\n\nOur next task is to obtain estimates for the variability of subjects’ perceptions around these typical values. To this end, we also use posterior predictions from the model. We are no longer interested in the expected (or typical) value, however, but in the specific responses given by the speakers. We therefore use the function predicted_draws() which retains this variability in the distribution of responses.\nWe must make sure, however, that the variability among subjects, which is captured by the random intercepts in our model, is worked into these predictions. Thus, in a model with random intercepts, the extraction of information about the beta distribution describing the variability of interpretations across subjects requires some care. This is because between-subjects variation is now absorbed by this model component. If we fail to actively incorporate this source of variation into our model-based predictions, these do not answer question iii, which asks about the variation across subjects in the interpretation of quantifiers.\nFor purposes of illustration, let us first (inappropriately) disregard the by-speaker random effects. This means that we use the expected predictions generated above to construct beta distributions. This yields Figure 12, which does not show beta densities with the intended meaning. Recall that our question (iii) concerned variability across speakers. In Model III, between-subjects variation is captured by the random intercept SD (or variance) in our model, and this parameter is not built into the graph. As a result, the beta distribution are not spread out enough.\n\nm3_pred_wrong &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier)),\n    re_formula = NA)\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\".20\",\".40\",\".60\",\".80\",\"1\"))),\n  xlab=\"Proportion\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred_wrong, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=.5, y=7, label=\"Warning: Beta densities do not\\nhave the intended meaning\", \n               col=\"darkred\", alpha=.7, cex=.9, lineheight=.9)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 12: Wrong visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Distribution of by-speaker averages on the proportion scale.\n\n\n\n\nBefore we go further, it is perhaps worth considering what kind of information the densities in Figure 12 do show. They show the “residual” variation in perceived interpretation for each quantifier, after removing the model component that describes the between-speaker variability in the overall average response. Thus, the by-speaker random intercepts capture the extent to which subjects vary in their average response. To get a better understanding of the kind of variability represented by the random intercepts, Figure 13 shows the distribution of speaker averages. Most speakers vary between roughly .40 and .55. It is this kind of between-speaker variation that is captured by the random intercepts in the model.\n\n\nConstruct wrong table of quantiles\nbeta_quantiles_m3_wrong &lt;- m3_pred_wrong |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3_wrong |&gt; kable()\n\n\n\n\nTable 6: 50% and 80% coverage intervals, based on Model III (random intercept variation not included).\n\n\n\n\n\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.06\n.08\n.14\n.18\n\n\nsome\n.19\n.23\n.32\n.36\n\n\nmany\n.55\n.59\n.69\n.73\n\n\nmost\n.77\n.81\n.88\n.91\n\n\n\n\n\n\n\nAfter removing this source of variation, responses will still vary, due to other sources of variation. One such source is measurement error – speakers’ response to our short survey may fluctuate (perhaps erratically) over time. If we handed the same paper slip to our 20 respondents one year later, they would probably give (slightly) different responses. Further, speakers may not only differ in their average response (as captured by the random intercepts), but also in their perception of the individual quantifiers. For instance, a speaker may have a systematically lower perception of the quantifier few compared to other speakers. In statistical terms, this would be referred to as a subject-by-quantifier interaction.\nTable 6 shows 50% and 80% coverage intervals for the beta distributions that do not contain the between-speaker variation.\nTo get beta densities with the intended meaning, we need to factor in the random effect. This means that we form model-based predictions that also incorporate (or represent) between-subjects variability. This is sometimes referred to as “integrating out” the random effects. As explained in more detail in the Marginal Effects Zoo book and in this blog post by Heiss (2021b), this is done by sampling from a normal distribution representing the spread of random intercepts and factoring these deviations into the posterior predicted distribution.\nTable 7 shows 50% and 80% coverage intervals for the beta distributions that now contain the between-speaker variation. A visual summary of Model III with the appropriate densities appears in Figure 14.\n\n\nConstruct table of quantiles\nm3_pred &lt;- m3 |&gt; \n  predicted_draws(\n    newdata = expand.grid(\n      quantifier = unique(d$quantifier),\n      subject = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n\nbeta_quantiles_m3 &lt;- m3_pred |&gt; group_by(quantifier) |&gt; \n  dplyr::summarize(\n    `[80%` = numformat(quantile(.prediction, .1)),\n    `[50%` = numformat(quantile(.prediction, .25)),\n    `50%]` = numformat(quantile(.prediction, .75)),\n    `80%]` = numformat(quantile(.prediction, .9))\n  )\n\nbeta_quantiles_m3 |&gt; kable()\n\n\n\n\nTable 7: 50% and 80% coverage intervals, based on Model III (random intercept variation included).\n\n\n\n\n\n\n\nquantifier\n[80%\n[50%\n50%]\n80%]\n\n\n\n\nfew\n.05\n.07\n.16\n.21\n\n\nsome\n.15\n.20\n.35\n.43\n\n\nmany\n.48\n.56\n.72\n.79\n\n\nmost\n.72\n.79\n.89\n.93\n\n\n\n\n\n\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(0, 16),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"Density               \",\n  xlab.top=\"Model III\\n\",\n  panel=function(x,y){\n    panel.densityplot(x = subset(m3_pred, quantifier == \"few\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"some\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"many\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n    panel.densityplot(x = subset(m3_pred, quantifier == \"most\")$.prediction, ref = FALSE, plot.points = FALSE, col=1)\n\n    panel.points(x=mu_m3$median, y=13, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=13, y1=13)\n    panel.text(x=mu_m3$median, y=11, label=numformat(mu_m3$median))\n    panel.text(x=mu_m3$median, y=15.5, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 14: Visual summary of Model III: Estimates, confidence intervals and the underlying beta distributions.\n\n\n\n\n\n\n\nComparison of estimates\nLet us finally compare the quantities of interest across our models. Figure 15 (a) shows estimates for the typical relative-frequency interpretation of the expressions. Difference between models are minor, but Model III returns slightly wider uncertainty intervals. This is mainly because the model has been informed that the sample size is 20 (rather than 80), but also because the model has (re)incorporated subject 1.\nIn panel (b), the 50% and 80% coverage intervals are shown. Two comparisons are particularly informative:\n\nComparing models I and II, we note the effect of allowing precision to vary across quantifiers. For few, Model II returns tighter coverage intervals, since this expression is estimated to show a higher level of precision, i.e. a more stable interpretation across subjects. For many, the situation is the other way around. The differences between models I and II are minor, however, which is consistent with the results of the likelihood-ratio test reported above.\nA comparison of the grey and red coverage intervals for Model III is also revealing: The red intervals are the ones that do not incorporate between-subject variability (as captured by the random intercepts). They therefore fail to answer question iii. The grey bands, on the other hand, do factor this source of variation into the predictions, and they therefore show us what we were looking for.\n\n\n\nDraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2, 4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(a) Typical values + uncertainty\\n\",\n  panel=function(x,y){\n    panel.points(x=mu_m1, y=3, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m1[,1], x1=ci_mu_m1[,2], y0=3, y1=3)\n\n    panel.points(x=mu_m2, y=2, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m2[,1], x1=ci_mu_m2[,2], y0=2, y1=2)\n\n    panel.points(x=mu_m3$median, y=1, pch=19, cex=1.25)\n    panel.segments(x0=ci_mu_m3$ci_lower, x1=ci_mu_m3$ci_upper, y0=1, y1=1)\n\n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n    panel.text(x=-.3, y=3:1, label=c(\"Model I\", \"Model II\", \"Model III\"), adj=0)\n    })\n\n\nbeta_quantiles_m3 &lt;- data.frame(beta_quantiles_m3)\nbeta_quantiles_m3_wrong &lt;- data.frame(beta_quantiles_m3_wrong)\n\np2 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0,1), ylim=c(-.2,4.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(x=list(at=c(0,.2,.4,.6,.8,1),\n                     labels=c(\"0\",\"20\",\"40\",\"60\",\"80\",\"100  \"))),\n  xlab=\"Estimated percentage\", ylab=\"\",\n  xlab.top=\"(b) 50% and 80% coverage intervals\\n\",\n  panel=function(x,y){\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,2]), x1=as.numeric(beta_quantiles_m1[,5]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m1[,3]), x1=as.numeric(beta_quantiles_m1[,4]), \n                 y0=3+c(-.15, .15, -.15, .15), y1=3+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m2[,2]), x1=as.numeric(beta_quantiles_m2[,5]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m2[,3]), x1=as.numeric(beta_quantiles_m2[,4]), \n                 y0=2+c(-.15, .15, -.15, .15), y1=2+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3[,2]), x1=as.numeric(beta_quantiles_m3[,5]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey80\", lineend=\"butt\")\n    panel.segments(x0=as.numeric(beta_quantiles_m3[,3]), x1=as.numeric(beta_quantiles_m3[,4]), \n                 y0=1+c(-.15, .15, -.15, .15), y1=1+c(-.15, .15, -.15, .15), lwd=4, col=\"grey50\", lineend=\"butt\")\n    \n    \n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,2]), x1=as.numeric(beta_quantiles_m3_wrong[,5]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    panel.segments(x0=as.numeric(beta_quantiles_m3_wrong[,3]), x1=as.numeric(beta_quantiles_m3_wrong[,4]), \n                 y0=.25+c(-.15, .15, -.15, .15), y1=.25+c(-.15, .15, -.15, .15), lwd=4, col=\"darkred\", lineend=\"butt\", alpha=.25)\n    \n    panel.text(x=mu_m1, y=4.25, label=c(\"few\", \"many\", \"most\", \"some\"), font=3)\n\n    })\n\n\n\n\n\n\n\n\n\n\nFigure 15: Comparison of quantities across models: (a) typical values and their uncertainty, and (b) coverage intervals.\n\n\n\n\n\n\n\nConclusion\nIn this blog post, we have looked at how to model proportional data using beta regression. Our focus was on the typical (average) interpretation speakers assign to quantifiers and its statistical uncertainty. Apart from this, we considered how to use and summarize beta densities to obtain information about the variability of interpretations (or, more specifically, responses) across subjects to quantify the stability of interpretations in the population represented by our sample of informants. We also looked at how to use model diagnostics for a frequentist and a mixed-effects Bayesian regression model to detect problematic data points and/or speakers.\n\n\n\n\n\nReferences\n\nBorges, Marilyn A., and Barbara K. Sawyers. 1974. “Common Verbal Quantifiers: Usage and Interpretation.” Journal of Experimental Psychology 102 (2): 335–38. https://doi.org/10.1037/h0036023.\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCribari-Neto, Francisco, and Achim Zeileis. 2010. “Beta Regression in R.” Journal of Statistical Software 34 (2): 1–24. https://doi.org/10.18637/jss.v034.i02.\n\n\nHeiss, Andrew. 2021a. “A Guide to Modeling Proportions with Bayesian Beta and Zero-Inflated Beta Regression Models.” November 8, 2021. https://doi.org/10.59350/7p1a4-0tw75.\n\n\n———. 2021b. “A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel Bayesian Models.” November 10, 2021. https://doi.org/10.59350/wbn93-edb02.\n\n\nKrug, Manfred, and Katrin Sell. 2013. “Designing and Conducting Interviews and Questionnaires.” In Research Methods in Language Variation and Change, edited by Manfred Krug and Julia Schlüter, 69–98. Cambridge University Press.\n\n\nNewstead, Stephen E., Paul Pollard, and D. Riezebos. 1987. “The Effect of Set Size on the Interpretation of Quantifiers Used in Rating Scales.” Applied Ergonomics 18 (3): 178–82. https://doi.org/10.1016/0003-6870(87)90001-9.\n\n\nSönning, Lukas. 2024. “Ordinal Rating Scales: Psychometric Grounding for Design and Analysis.” OSF Preprints. https://doi.org/10.31219/osf.io/jhv6b.\n\n\nStateva, Penka, Arthur Stepanov, Viviane Déprez, Ludivine Emma Dupuy, and Anne Colette Reboul. 2019. “Cross-Linguistic Variation in the Meaning of Quantifiers: Implications for Pragmatic Enrichment.” Frontiers in Psychology 10: 957. https://doi.org/10.3389/fpsyg.2019.00957.\n\n\nTiel, Bob van, Michael Franke, and Uli Sauerland. 2021. “Probabilistic Pragmatics Explains Gradience and Focality in Natural Language Quantification.” Proceedings of the National Academy of Sciences 118 (9). https://doi.org/10.1073/pnas.2005453118.\n\nCitationBibTeX citation:@online{sönning2024,\n  author = {Sönning, Lukas},\n  title = {Modeling the Interpretation of Quantifiers Using Beta\n    Regression},\n  date = {2024-02-29},\n  url = {https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2024. “Modeling the Interpretation of Quantifiers\nUsing Beta Regression.” February 29, 2024. https://lsoenning.github.io/posts/2024-01-11_beta_regression_quantifiers/."
  },
  {
    "objectID": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "href": "posts_draft/2024-01-07_modified_lobanov/index.html",
    "title": "Vowel normalization using modified Lobanov",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(tictoc)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\nThe dispersion measure DA was proposed by @Burch_etal2017 as a way of quantifying how evenly an item is distributed across the texts (or, more generally, the units) in a corpus. The authors attribute this measure to @Wilcox1973, a nice and very readable paper that compares different indices of qualitative variation, i.e. measures of variability for nominal-scale variables. While @Wilcox1973 focuses on categorical variables (with 10 or fewer levels), the measures discussed in that paper are also relevant for quantifying what lexicographers and corpus linguists refer to as “dispersion”. Interestingly, as @Burch_etal2017 [p. 193] note, a measure equivalent to DP [@Gries2008] can be found in the 1973 paper (the average deviation analog ADA). The index on which DA is based appears in @Wilcox1973 as the mean difference analog (MDA). Both @Wilcox1973 and @Burch_etal2017 argue that DA (or MDA) has a number of advantages over DP (or ADA). An intuitive explanation of the rationale underlying DA can be found in @Soenning2023.\n@Gries2020 [p. 116] has pointed out, however, that DA is computationally expensive. This is because the measure relies on pairwise differences between texts. To calculate DA, we first obtain the occurrence rate (or normalized frequency) of a given item in each text. These occurrences rates can then be compared, to see how evenly the item is distributed across texts. The basic formula for DA requires pairwise comparisons between all texts. If we have 10 texts, the number of pairwise comparisons is 45; for 20 texts, this number climbs to 190. In general, if there are n texts (or units), the number of pairwise comparisons is \\((n(n-1))/2\\). This number (and hence the computational task) grows exponentially: For 500 texts (e.g. ICE or Brown Corpus), 124,750 comparisons are involved. For the BNC2014, with 88,171 texts, there are almost 4 billion comparisons to compute.\nThe purpose of this blog post is to draw attention to a shortcut formula @Wilcox1973 gives in the Appendix of his paper. There, he distinguishes between “basic formulas” and “computational formulas”, which run faster. The formula we will use here is the one listed in the rightmost column (Computational Formulas: Proportions). We will give R code for both the basic and the computational procedure and then compare them in terms of speed.\nWe start by writing two R functions:\n\nDA_basic(), which uses the basic, slow formula; and\nDA_quick(), which implements the shortcut given in @Wilcox1973.\n\nThese functions also work if texts differ in length. They take two arguments:\n\nn_tokens: A vector of length n, giving the number occurrences of the item in each of the n texts\nword_count: A vector of length n, giving the length of each text (number of running words)\n\nFor the rationale underlying the intermediate quantities R_i and r_i, please refer to @Soenning2023. We first define the basic formula:\n\nDA_basic &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    dist_r &lt;- as.matrix(dist(r_i))\n    DA &lt;- 1 - ( mean(dist_r[lower.tri(dist_r)]) / (2/k) )\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nAnd now the computational formula:\n\nDA_quick &lt;- function(n_tokens, word_count){\n  \n    R_i &lt;- n_tokens / word_count\n    r_i &lt;- R_i / sum(R_i)\n    k   &lt;- length(n_tokens)\n\n    DA &lt;- (2*sum((sort(r_i, decreasing=TRUE) * 1:k)) -1) / (k-1)\n\n    names(DA) &lt;- \"DA\"\n    return(DA)\n}\n\nLet’s now compare them in two settings: 4,000 texts (about 8 million pairwise comparisons) and 20,000 texts (about 200 million comparisons). We will go directly to the results; to see the background code, click on the triangle below (“R code for comparison of computation time”), which unfolds the commented script.\n\n\nR code for comparison of computation time\n# We start by creating synthetic data. We use the Poisson distribution to \n# generate tokens counts for the smaller corpus (n_tokens_4000) and the \n# larger corpus (n_tokens_20000)\n\nset.seed(1)\n\nn_tokens_4000 &lt;- rpois(n = 4000, lambda = 2)\nn_tokens_20000 &lt;- rpois(n = 20000, lambda = 2)\n\n# Then we create corresponding vectors giving the length of the texts (each is \n# 2,000 words long):\n\nword_count_4000 &lt;- rep(2000, length(n_tokens_4000))\nword_count_20000  &lt;- rep(2000, length(n_tokens_20000))\n\n# Next, we use the R package {tictoc} to compare the two functions (i.e. \n# computational procedures) in terms of speed, starting with the 4,000-text \n# setting. We start with the basic formula:\n\ntic()\nDA_basic_4000 &lt;- DA_basic(n_tokens_4000, word_count_4000)\ntime_basic_4000 &lt;- toc()\n\n# And now we use the computational formula:\n\ntic()\nDA_quick_4000 &lt;- DA_quick(n_tokens_4000, word_count_4000)\ntime_quick_4000 &lt;- toc()\n\n# Next, we compare the 20,000-text setting:\n\ntic()\nDA_basic_20000 &lt;- DA_basic(n_tokens_20000, word_count_20000)\ntime_basic_20000 &lt;- toc()\n\ntic()\nDA_quick_20000 &lt;- DA_quick(n_tokens_20000, word_count_20000)\ntime_quick_20000 &lt;- toc()\n\n\nTable 1 shows the results: let us first consider computation time. For 4,000 texts, the basic procedure takes 1.58 seconds to run. The computational formula is quicker – it completes the calculations in only 0.01 seconds. For the 20,000-word corpus, the difference is much more dramatic: The basic formula takes 35.81 seconds to run; the shortcut procedure, on the other hand, is done after 0.02 seconds. This is an impressive improvement in efficiency.\n\n\nR code for Table 1\ntibble(\n  Formula = c(\"Basic\", \"Computational\"),\n  `4,000 texts` = c((time_basic_4000$toc - time_basic_4000$tic), \n                    (time_quick_4000$toc - time_quick_4000$tic)) ,\n  `20,000 texts` = c((time_basic_20000$toc - time_basic_20000$tic), \n                     (time_quick_20000$toc - time_quick_20000$tic)),\n  `4,000 texts ` = round(c(DA_basic_4000, DA_quick_4000), 4) ,\n  `20,000 texts ` = round(c(DA_basic_20000, DA_quick_20000), 4)) |&gt; \n  kbl() |&gt; \n  add_header_above(c(\" \" = 1, \"Time (seconds)\" = 2, \"Dispersion score\" = 2))\n\n\n\n\nTable 1: Computation time (in seconds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime (seconds)\n\n\nDispersion score\n\n\n\nFormula\n4,000 texts\n20,000 texts\n4,000 texts\n20,000 texts\n\n\n\n\nBasic\n1.58\n35.81\n0.6003\n0.6139\n\n\nComputational\n0.01\n0.02\n0.6005\n0.6140\n\n\n\n\n\n\n\n\n\n\nTable 1 also shows the dispersion scores that the functions return. We note that the two procedures do not yield identical results. However, the approximation offered by the computational shortcut is pretty good, especially considering the fact that dispersion measures are usually (and quite sensibly) reported to two decimal places only.\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {Vowel Normalization Using Modified {Lobanov}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Vowel Normalization Using Modified Lobanov .” 2024.\nJanuary 7, 2024. https://lsoenning.github.io/posts/2024-01-07_modified_lobanov/."
  },
  {
    "objectID": "posts_draft/2025-04-28_disproportionate_representation_speaker/index.html",
    "href": "posts_draft/2025-04-28_disproportionate_representation_speaker/index.html",
    "title": "Distributional disproportions and their consequences: Speakers in the Spoken BNC2014",
    "section": "",
    "text": "R setup\nlibrary(here)\nlibrary(tidyverse)\nlibrary(lattice)\nlibrary(knitr)\nlibrary(kableExtra)\n\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative dataset records the distribution of actually in the Spoken BNC2014 [@Love_etal2017], which was analyzed in [@Soenning_Krug2022]. The question ist whether and how the usage rate of actually in conversational speech varies by Age and Sex.\n\nData preparation\nFor more information on the dataset, please refer to [@Krug_Soenning2021]. We start by downloading the data directly from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nThis dataset includes 668 speakers in total. For each individual, the data frame includes:\n\nan ID (speaker)\nthe number of times they used actually (count)\ntheir age in year, if provided in the metadata (Exact_age)\nthe age range (Age_range)\nself-reported gender (Gender)\nthe total number of words contributed to the corpus by the speaker (total), and\na lightly aggregated version of Age range (age_bins)\n\n\nstr(dat)\n\n'data.frame':   668 obs. of  7 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ Exact_age: int  32 NA NA NA NA NA NA 66 NA NA ...\n $ Age_range: chr  \"30-39\" \"19-29\" \"19-29\" \"30-39\" ...\n $ Gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ age_bins : chr  \"30-39\" \"20-29\" \"20-29\" \"30-39\" ...\n\n\n\nsubset(dat, is.na(age_bins))\n\n    speaker count Exact_age Age_range Gender total age_bins\n362   S0371     3        NA   Unknown   Male  6071     &lt;NA&gt;\n376   S0386     8        NA   Unknown   Male  3034     &lt;NA&gt;\n588   S0608     2        NA   Unknown   Male  1277     &lt;NA&gt;\n589   S0609     5        NA   Unknown Female  1738     &lt;NA&gt;\n592   S0612    17        NA   Unknown Female 10599     &lt;NA&gt;\n623   S0643     2        NA   Unknown   Male  2461     &lt;NA&gt;\n\n\nIn line with [@Soenning_Krug2022], we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age is missing. This leaves us with n = 656 individuals.\n\nd &lt;- dat |&gt; \n  filter(total &gt; 100,\n         Age_range != \"Unknown\")\n\nThen we add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as per million words:\n\nd$rate_pmw &lt;- (d$count / d$total) * 1000000\n\nFinally, we reverse the order of the levels of the categorical variable Age_range, so that they are listed in decreasing order. This is because we will interpret differences between age groups (i.e. differences in apparent time) as indicating differences in real time. The new variable is called age_group:\n\nd &lt;- d |&gt; \n  mutate(\n    age_group = factor(\n      Age_range, \n      levels = rev(sort(unique(Age_range))),\n      ordered = TRUE)) \n\nWe reduce the data frame to the variables we need for analysis:\n\nd &lt;- d |&gt; select(speaker, Gender, age_group, count, total, rate_pmw) |&gt; \n  rename(gender = Gender)\n\nInspect the data frame:\n\nstr(d)\n\n'data.frame':   656 obs. of  6 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ age_group: Ord.factor w/ 10 levels \"90-99\"&lt;\"80-89\"&lt;..: 7 8 8 7 2 2 8 4 8 7 ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ rate_pmw : num  2000 2460 4226 275 2070 ...\n\n\n\n\nData structure\nLet us start by looking at how speakers are distributed across the sociolinguistic categories:\n\nftable(gender ~ age_group, d)\n\n          gender Female Male\nage_group                   \n90-99                 1    3\n80-89                 8   11\n70-79                16   17\n60-69                29   36\n50-59                43   34\n40-49                43   32\n30-39                49   40\n19-29               148   99\n11-18                17   23\n0-10                  4    3\n\n\nNext, we consider the distribution of speaker word counts, i.e. the number of words they contributed to the corpus. In the Spoken BNC2014, the total number of word tokens contributed by each speaker varies markedly across individuals. The following dot diagram shows the skew in this distribution: The word count ranges from 19 to 362,107 and 81% of the speakers contribute fewer than 20,000 words to the corpus.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + \n  geom_dotplot(binwidth = 1500, stackratio = .75, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\nWhat is quite interesting is that this distribution is nicely symmetric on the log scale.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + geom_dotplot(binwidth = .04, method = \"histodot\") +\n  scale_x_log10(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n    theme_dotplot() +\n    xlab(\"Number of words per speaker (log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 2: Log-scaled distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\n\n\n\nFrequency of actually: Data summary\nTo obtain the corpus frequency of actually, we divide the total number of actually-tokens in the corpus by the corpus size. We multiply this rate by 1,000,000 to obtain a normalized frequency of per million words:\n\nn_tokens &lt;- sum(d$count)\ncorpus_size &lt;- sum(d$total)\n\nround((n_tokens / corpus_size) * 1000000)\n\n[1] 1537\n\n\nThis is very similar to the (normalized) corpus frequency reported in CQPweb [@Hardie2012]:\n\n\n\nCorpus frequency results in CQPweb\n\n\nDue to the skewed word count distribution across speaker, however, this corpus frequency is problematic. It turns out that the top 20 speakers (in terms of overall word count) account for 31% of the corpus size – together, they contribute around 350,000 words to the corpus. The corpus frequency is therefore potentially biased into the direction of the language use of these individuals.\n\nsum(d$total &gt; 110000)\n\n[1] 20\n\nsum(d$total[d$total &gt; 110000]) / corpus_size\n\n[1] 0.3081076\n\nsum(d$total[d$total &gt; 200000]) / corpus_size\n\n[1] 0.117284\n\n\nWe can consider each socio-demographic subgroup (i.e. age-by-gender combination) as a subcorpus and likewise calculate the (sub-)corpus frequency of actually. This means that we divide the total number of actually-tokens in the subcorpus by its size. We can visualize the resulting set of normalized subcorpus frequencies:\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = round((sum(count)/sum(total))*1e6)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_point() +\n  geom_line(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Subcorpus frequency of\\nactually (pmw, log-scaled)\")\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\nWarning in geom_line(bg = \"white\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 3: Subcorpus frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nAnother way of estimating the average rate of actually in each subgroup is to consider the speaker-specific normalized frequencies (i.e. he variable rate_pmw) and average over these. @Egbert_Burch2023 [p. 105] refer to these two ways of measuring frequency as corpus frequency and mean text frequency. In the present context, we slightly adapt these labels to subcorpus frequency and mean speaker frequency.\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = mean(rate_pmw)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_point() +\n  geom_line(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Mean speaker frequency of\\nactually (pmw, log-scaled)\")\n\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\nWarning in geom_line(bg = \"white\"): Ignoring unknown parameters: `fill`\n\n\n\n\n\n\n\n\nFigure 4: Mean speaker frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nThe frequency estimates differ considerably. Especially for 70-to-79-year-olds, the two methods yield very different usage rates. To understand why this is the case, we need to zoom in on the individual speakers. It helps to draw what is sometimes referred to as a bubble chart, where each speaker is drawn as a circle, and the size of the circles is proportional to the speaker-specific word count. The figure below arranges speakers by Gender (left panel: female speakers, right panel: male speakers) and by Age group within each panel. The y-axis shows the speaker-specific usage rate of actually, and the size of the circles reflects how many word the speaker contributed to the corpus. Note that the y-axis is log-scaled. To be able to include normalized frequencies of 0, we opted for a full scale break that is signaled by the grey horizontal lines.\n\n\nDraw Figure\nd |&gt; \n  mutate(\n    rate_pmw_0_start = ifelse(rate_pmw == 0, 60, rate_pmw)) |&gt; \n  ggplot(\n  aes(x = age_group, \n      y = rate_pmw_0_start, \n      size = count)) + \n  geom_jitter(shape = 1, alpha = .5, width = .2) +\n  facet_grid(. ~ gender) +\n  scale_y_log10(breaks = c(60, 100, 1000, 10000),\n                label = c(0, 100, 1000, 10000)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_size_area() +\n  geom_hline(yintercept = 80, col = \"grey\") +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Normalized frequency of actually\\n(per million words, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 5: Log-scaled normalized speaker frequency of actually by Age group and Gender, with symbol size reflecting the total number of words the speaker contributed to the corpus.\n\n\n\n\n\nWe note that the subgroup of female speakers aged 70 to 79 includes a speaker with both a disproportionately high rate of actually and a disproportionately high word count. This speaker exert considerable influence on the subcorpus frequency, which is upwardly biased as a result. In the subgroup of male speakers aged 70 to 79, the speaker with the greatest word count (i.e. the largest circle) uses actually at a relatively low rate, which likewise distorts the subcorpus frequency of this group.\n\n\nFrequency of actually: Statistical modeling\nLet us also consider how usage rate estimates can be formed using regression modeling. Using a statistical model allows us to obtain uncertainty intervals (e.g. 95% CIs) for the subgroup estimates.\nThe variable we are dealing with is a count variable. This is because it consists of non-negative integers that express the number of events (here: occurrences of actually) in a certain period of observation (here: text time, i.e. the number of running words). We therefore turn to the family of count regression models.\nThe most basic version of this family is the Poisson model. It turns out that it returns the same frequency estimates as the subcorpus frequencies. This is because it ignores the fact that each subgroups consists of different speakers, who in turn (may) show different usage rates of actually. More specifically, is assumes that all speakers in a specific subgroup have the same underlying usage rate of actually. The assumed absence of individual variation (or inter-speaker differences) appears implausible on linguistic grounds and for the data at hand, Figure 5 shows that this assumption is indeed not tenable for the data at hand.\nFor a point of reference, we nevertheless start by fitting a Poisson model to the data. Since the total word count speakers differs across speakers (see Figure 1), a count regression model must include what is referred to as an offset. The idea is the same as when we calculate normalized frequencies. To be able to compare usage rates across speakers in the first place, these must be expressed in relative terms, i.e. divided by thetotal number of words produced by the speaker.\nWe can fit a Poisson model using the base R function glm(), where the code chunk offset(log(total)) represents the offset.\n\nm_poi &lt;- glm(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d, \n    family = poisson())\n\nWe skip regression tables and directly proceed to the calculation of model-based estimates. To this end, we use the predictions() function in the very helpful {marginaleffects} package [@ArelBundock_etal2024]. We request estimates (i.e. predictions) for all combinations of Age (age_group) and Gender (gender), which means that we are not averaging over any predictor variables in the model. We specify these conditions (i.e. combinations of Age and Gender) using the datagrid() function. It creates a data frame of all predictor combinations of interest. Note that we must also supply a word count (total) to datagrid(); for model-based predictions, this variable controls the type of rate (or normalized frequency) returned by the predictions() function. We will stick to ‘per million words’ and therefore add total = 1e6 to the datagrid() function. If you fail to specify a value for total, the predictions() function will press ahead and use the in-sample mean of this variable (17282.57) – clearly, this is not what we want.\nTwo further argument require some explanation. Count regression models are generalized linear models that do not model the data on the original data scale (as is the case in ordinary linear regression). Instead, a link scale is used, similar to logistic regression models. For count regression models, the counts (or in our case: rates) are modeled on the log scale. For interpretation, however, we prefer the orginial data scale – in our case, normalized frequencies on the per-million-word scale. The predictions() function therefore includes an argumetn type, which allows us to specify the scale on which predictions should be returned. Specifying type = 'response' asks for predictions on the data scale (i.e. normalized frequencies). Unfortunately, this will also return uncertainty intervals that are computed on the data scale, i.e. after back-transformation. As you can check for yourself, this does not work well in the represent case, as some of the lower CI limits are negative (which is impossible). We therefore ask for predictions on the model scale (i.e. natural logarithms) using type = 'link' and then backtransform these to the data scale using transform = exp.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nHere is the (shortened) content of the table:\n\n\n# A tibble: 20 × 6\n   estimate conf.low conf.high age_group gender   total\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1    1604.   1529.      1683. 30-39     Female 1000000\n 2    1316.   1229.      1409. 30-39     Male   1000000\n 3    1641.   1596.      1689. 19-29     Female 1000000\n 4    1679.   1609.      1751. 19-29     Male   1000000\n 5     326.    189.       561. 80-89     Female 1000000\n 6    1364.   1131.      1646. 80-89     Male   1000000\n 7    1438.   1330.      1556. 60-69     Female 1000000\n 8     985.    910.      1065. 60-69     Male   1000000\n 9    3478.   3232.      3742. 70-79     Female 1000000\n10     589.    516.       673. 70-79     Male   1000000\n11    1876.   1803.      1952. 40-49     Female 1000000\n12    1397.   1276.      1529. 40-49     Male   1000000\n13    1605.   1514.      1701. 50-59     Female 1000000\n14     965.    879.      1059. 50-59     Male   1000000\n15     167.     41.7      667. 90-99     Female 1000000\n16    1042.    832.      1305. 90-99     Male   1000000\n17    1810.   1666.      1968. 11-18     Female 1000000\n18    1007.    912.      1112. 11-18     Male   1000000\n19    1320.   1090.      1598. 0-10      Female 1000000\n20    1668.   1382.      2015. 0-10      Male   1000000\n\n\nWe go ahead and graph these estimates. The result is virtually identical to the descriptive Figure 3. The error bars represent 95% CIs, and we note that the Poisson model return very confident estimates for most subgroups.\n\npreds_poi |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.5, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nPoisson model can be extended to account for the structure of the data, and for the fact that the usage rate of actually may very well vary from speaker to speaker, even within the same socio-demographic subgroup. The result is sometimes referred to as a Poisson mixture model, and it includes an extra parameter that captures the amount of observed between-speaker variation. A frequently used Poisson mixture distribution is the negative binomial distribution. For a visual explanation of this distribution, see this blog post. What matters for the present case study is the fact that this type of model knows about and adequately represents the structure of the data.\nNegative binomial model\n\nm_nb &lt;- MASS::glm.nb(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d)\n\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\n\npreds_nb |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.75, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {Distributional Disproportions and Their Consequences:\n    {Speakers} in the {Spoken} {BNC2014}},\n  date = {2024-01-07},\n  url = {https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Distributional Disproportions and Their Consequences: Speakers in\nthe Spoken BNC2014.” 2024. January 7, 2024. https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/."
  },
  {
    "objectID": "posts/2025-04-28_disproportionate_representation_speaker/index.html",
    "href": "posts/2025-04-28_disproportionate_representation_speaker/index.html",
    "title": "Unbalanced distributions and their consequences: Speakers in the Spoken BNC2014",
    "section": "",
    "text": "R setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative dataset records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). The question of main interest is whether and how the usage rate of actually in conversational speech varies by Age and Gender.\n\nData preparation\nFor more information on the dataset, please refer to Sönning and Krug (2021). We start by downloading the data from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nThis dataset includes 668 speakers and the following speaker variables:\n\nan ID (speaker)\nthe number of times they used actually (count)\ntheir age in years, if provided in the metadata (Exact_age)\nthe age range (Age_range)\nself-reported gender (Gender)\nthe total number of words contributed to the corpus by the speaker (total), and\na slightly aggregated version of age range (age_bins)\n\n\nstr(dat)\n\n'data.frame':   668 obs. of  7 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ Exact_age: int  32 NA NA NA NA NA NA 66 NA NA ...\n $ Age_range: chr  \"30-39\" \"19-29\" \"19-29\" \"30-39\" ...\n $ Gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ age_bins : chr  \"30-39\" \"20-29\" \"20-29\" \"30-39\" ...\n\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age is missing. This leaves us with n = 656 individuals.\n\nd &lt;- dat |&gt; \n  filter(total &gt; 100,\n         Age_range != \"Unknown\")\n\nThen we add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per million words’:\n\nd$rate_pmw &lt;- (d$count / d$total) * 1000000\n\nFinally, we reverse the order of the levels of the categorical variable Age_range, so that they are listed in decreasing order. This is because we will interpret differences between age groups (i.e. differences in apparent time) as indicating differences in real time. The new variable is called age_group:\n\nd &lt;- d |&gt; \n  mutate(\n    age_group = factor(\n      Age_range, \n      levels = rev(sort(unique(Age_range))),\n      ordered = TRUE)) \n\nWe reduce the data frame to the variables we need for analysis:\n\nd &lt;- d |&gt; dplyr::select(speaker, Gender, age_group, count, total, rate_pmw) |&gt; \n  dplyr::rename(gender = Gender)\n\nInspect the data frame:\n\nstr(d)\n\n'data.frame':   656 obs. of  6 variables:\n $ speaker  : chr  \"S0001\" \"S0002\" \"S0003\" \"S0004\" ...\n $ gender   : chr  \"Female\" \"Female\" \"Female\" \"Male\" ...\n $ age_group: Ord.factor w/ 10 levels \"90-99\"&lt;\"80-89\"&lt;..: 7 8 8 7 2 2 8 4 8 7 ...\n $ count    : int  6 21 8 1 3 1 6 101 0 7 ...\n $ total    : int  3000 8535 1893 3634 1449 4804 11276 139888 533 3105 ...\n $ rate_pmw : num  2000 2460 4226 275 2070 ...\n\n\n\n\nData structure\nLet us start by looking at how speakers are distributed across the sociolinguistic categories. The table below shows that the youngest and oldest cohorts are rather sparsely populated.\n\nftable(gender ~ age_group, d)\n\n          gender Female Male\nage_group                   \n90-99                 1    3\n80-89                 8   11\n70-79                16   17\n60-69                29   36\n50-59                43   34\n40-49                43   32\n30-39                49   40\n19-29               148   99\n11-18                17   23\n0-10                  4    3\n\n\nNext, we consider the distribution of speaker word counts, i.e. the number of words they contributed to the corpus. In the Spoken BNC2014, this count varies markedly across individuals. The following dot diagram shows the skewed distribution: The word count ranges from 117 to 362,107 (after removing speakers with fewer than 100 words) and 81% of the speakers contribute fewer than 20,000 words to the corpus.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + \n  geom_dotplot(binwidth = 1600, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\nWhat is quite interesting is that this distribution is perfectly symmetric on the log scale.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = total)) + geom_dotplot(binwidth = .041, method = \"histodot\") +\n  scale_x_log10(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n    theme_dotplot() +\n    xlab(\"Number of words per speaker (log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 2: Log-scaled distribution of word counts across speakers in the Spoken BNC2014 (excluding speakers who contributed fewer than 100 words to the corpus).\n\n\n\n\n\n\n\n\nFrequency of actually: Data summary\nTo obtain the corpus frequency of actually, we divide the total number of actually-tokens in the corpus by the corpus size. We multiply this rate by 1,000,000 to obtain a normalized frequency of ‘per million words’:\n\nn_tokens &lt;- sum(d$count)\ncorpus_size &lt;- sum(d$total)\n\nround((n_tokens / corpus_size) * 1000000)\n\n[1] 1537\n\n\nThis is very similar to the (normalized) corpus frequency reported in CQPweb (Hardie 2012):\n\n\n\n\n\nDue to the skewed word count distribution across speakers, however, this corpus frequency is potentially problematic. It turns out that the top 20 speakers (in terms of overall word count) together make up 31% of the corpus – their word count adds to around 350,000. The corpus frequency is therefore potentially biased toward the language use of these individuals.\nWe can consider each socio-demographic subgroup (i.e. age-by-gender combination) as a subcorpus and likewise calculate the (sub-)corpus frequency of actually. This means that we divide the total number of actually-tokens in the subcorpus by its size. We can visualize the resulting set of normalized subcorpus frequencies:\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = round((sum(count)/sum(total))*1e6)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Subcorpus frequency of\\nactually (pmw, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 3: Subcorpus frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nAnother way of estimating the average rate of actually in each subgroup is to consider the speaker-specific normalized frequencies (i.e. the variable rate_pmw) and average over these. Egbert and Burch (2023, 105) refer to these two types of frequency estimates as corpus frequency and mean text frequency. In the present context, we slightly adapt these labels to subcorpus frequency and mean speaker frequency.\n\n\nDraw Figure\nd |&gt; group_by(age_group, gender) |&gt; \n  dplyr::summarize(\n    rate_pmw = mean(rate_pmw)\n  ) |&gt; \n  ggplot(\n  aes(x = age_group,\n      y = rate_pmw, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(100, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.3, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Mean speaker frequency of\\nactually (pmw, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 4: Mean speaker frequency of actually in the socio-demographic subgroups.\n\n\n\n\n\nThe frequency estimates in Figure 3 and Figure 4 differ considerably. Especially for 70-to-79-year-olds, the two methods yield very different usage rates. To understand why this is the case, we need to zoom in on the individual speakers. It helps to draw what is sometimes referred to as a bubble chart, where each data point (here: speaker) appears as a circle, and the size of the circles is proportional to some quantity (here: the speaker-specific word count). The figure below arranges speakers by Gender (female speakers on the left) and by Age, within each panel. The y-axis shows the speaker-specific usage rate of actually, and the size of the circles reflects how many words a person contributed to the corpus. Note that the y-axis is log-scaled. To be able to include normalized frequencies of 0 (for which the logarithm is not defined), we opt for a full scale break that is signaled by the grey horizontal line.\n\n\nDraw Figure\nd |&gt; \n  mutate(\n    rate_pmw_0_start = ifelse(rate_pmw == 0, 60, rate_pmw)) |&gt; \n  ggplot(\n  aes(x = age_group, \n      y = rate_pmw_0_start, \n      size = total)) + \n  geom_jitter(shape = 1, alpha = .5, width = .2) +\n  facet_grid(. ~ gender) +\n  scale_y_log10(breaks = c(60, 100, 1000, 10000),\n                label = c(0, 100, 1000, 10000)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  scale_size_area() +\n  geom_hline(yintercept = 80, col = \"grey\") +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Normalized frequency of actually\\n(per million words, log-scaled)\")\n\n\n\n\n\n\n\n\nFigure 5: Log-scaled normalized speaker frequency of actually by Age group and Gender, with symbol size reflecting the total number of words the speaker contributed to the corpus.\n\n\n\n\n\nWe note that the subgroup of male speakers aged 70 to 79 includes an exceptionally large bubble, which represents a speaker with both (i) a disproportionately low rate of actually and (ii) a disproportionately high word count. This speaker exerts considerable influence on the subcorpus frequency, which is downwardly biased as a result. In the subgroup of female speakers aged 70 to 79, the two largest bubbles also show the highest rate of actually in this subgroup, which likewise distorts the subcorpus frequency of this group.\nTo summarize, corpus frequencies can be quite misleading if the size of the units in the corpus (i.e. texts or speakers) varies appreciably. A simple reassurance check compares the corpus frequency (or subcorpus frequency) with the mean text (or speaker) frequency. If these methods yield different results, we must decide which kind of frequency estimate is more appropriate for the research task at hand. In the current setting, there is no reason why a particular individual should receive greater weight when estimating a population quantity such as the usage rate of actually among 70-to-79-year-old female speakers of British English. In other words, the imbalance of word counts across speakers is a nuisance that must be adjusted for in the summary and analysis of these data.\n\n\nFrequency of actually: Statistical modeling\nLet us also consider how frequency estimates can be obtained using regression modeling. Using a statistical model allows us to construct uncertainty intervals (e.g. 95% CIs) around the subgroup estimates.\nThe variable we are dealing with is a count variable. This is because it consists of non-negative integers that express the number of events (here: occurrences of actually) in a certain period of observation (here: text time, i.e. the number of running words). We therefore turn to the family of count regression models.\nThe most basic version of this family is the Poisson model. It turns out that it produces the same frequency estimates as the subcorpus frequencies we reported above. This is because it ignores the fact that each subgroup consists of different speakers, who in turn (may) show different usage rates of actually. More specifically, is assumes that all speakers in a specific subgroup have the same underlying usage rate of actually. The assumed absence of individual variation (or inter-speaker differences) appears implausible on linguistic grounds and for the data at hand, Figure 5 shows that this assumption is indeed not tenable.\nFor a point of reference, we nevertheless start by fitting a Poisson model to the data. Since the total word count differs across speakers (see Figure 1), a count regression model must include what is referred to as an offset. The idea is the same as when we calculate normalized frequencies. To be able to compare usage rates across speakers in the first place, these must be expressed in relative terms, i.e. divided by the total number of words produced by the speaker.\nWe can fit a Poisson model using the base R function glm(), where the code chunk offset(log(total)) represents the offset.\n\nm_poi &lt;- glm(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d, \n    family = poisson())\n\nWe skip regression tables and directly proceed to the calculation of model-based estimates. To this end, we use the predictions() function in the very helpful {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). We request estimates (i.e. predictions) for all combinations of Age (age_group) and Gender (gender), which means that we are not averaging over any predictor variables in the model. We specify these conditions (i.e. combinations of Age and Gender) using the datagrid() function. It creates a data frame of all predictor combinations of interest. Note that we must also supply a word count (total) to datagrid(); for model-based predictions, this variable controls the type of rate (or normalized frequency) returned by the predictions() function. We will stick to ‘per million words’ and therefore add total = 1e6 to the datagrid() function. If you fail to specify a value for total, the predictions() function will press ahead and use the in-sample mean of this variable (i.e. 17,283) – clearly, this is not what we want.\nTwo further arguments in the code box below require some explanation. Count regression models are generalized linear models that do not model the data on the original data scale (as is the case in ordinary linear regression). Instead, a link scale is used, similar to logistic regression models. For count regression models, the counts (or in our case: rates) are modeled on the log scale. For interpretation, however, we prefer the original data scale – in our case, normalized frequencies on the per-million-word scale.\nThe predictions() function therefore includes an argument type, which allows us to specify the scale on which predictions should be returned. Specifying type = 'response' asks for predictions on the data scale (i.e. normalized frequencies). Unfortunately, this will also return uncertainty intervals that are computed using the data-scale standard error. As you can check for yourself, this does not work well in the represent case, as some of the lower CI limits are then negative (which is impossible). We therefore ask for predictions on the model scale (i.e. natural logarithms) using type = 'link' and then back-transform these to the data scale using transform = exp.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nHere is the (shortened) content of the output:\n\n\n# A tibble: 20 × 6\n   estimate conf.low conf.high age_group gender   total\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1    1604.   1529.      1683. 30-39     Female 1000000\n 2    1316.   1229.      1409. 30-39     Male   1000000\n 3    1641.   1596.      1689. 19-29     Female 1000000\n 4    1679.   1609.      1751. 19-29     Male   1000000\n 5     326.    189.       561. 80-89     Female 1000000\n 6    1364.   1131.      1646. 80-89     Male   1000000\n 7    1438.   1330.      1556. 60-69     Female 1000000\n 8     985.    910.      1065. 60-69     Male   1000000\n 9    3478.   3232.      3742. 70-79     Female 1000000\n10     589.    516.       673. 70-79     Male   1000000\n11    1876.   1803.      1952. 40-49     Female 1000000\n12    1397.   1276.      1529. 40-49     Male   1000000\n13    1605.   1514.      1701. 50-59     Female 1000000\n14     965.    879.      1059. 50-59     Male   1000000\n15     167.     41.7      667. 90-99     Female 1000000\n16    1042.    832.      1305. 90-99     Male   1000000\n17    1810.   1666.      1968. 11-18     Female 1000000\n18    1007.    912.      1112. 11-18     Male   1000000\n19    1320.   1090.      1598. 0-10      Female 1000000\n20    1668.   1382.      2015. 0-10      Male   1000000\n\n\nWe go ahead and graph these estimates. The result is virtually identical to the descriptive Figure 3. The error bars represent 95% CIs, and we note that the Poisson model returns very confident estimates for most subgroups.\n\n\nDraw Figure\npreds_poi |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_line() +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.5, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nFigure 6: Log-scaled usage rate estimates based on a Poisson regression model.\n\n\n\n\n\nThe Poisson model can be extended to account for the structure of the data, and for the fact that the usage rate of actually may very well vary from speaker to speaker, even within the same socio-demographic subgroup. Such extensions are sometimes referred to as Poisson mixture models, which include an extra parameter that captures the amount of observed between-speaker variation. A frequently used Poisson mixture is the negative binomial distribution. For a visual explanation, see this blog post. What matters for the present case study is the fact that this type of model knows about and adequately represents the structure of the data.\nWe can fit a negative binomial regression model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n    count ~ 1 + offset(log(total)) + gender * age_group, \n    data = d)\n\nPredictions can be calculated using {marginaleffects} in the same way as for the Poisson model:\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    age_group = unique, \n    gender = unique,\n    total = 1e6),\n  type = \"link\",\n  transform = exp) |&gt; \n  tidy()\n\nThen we can graph the estimates based on this model, including statistical uncertainty intervals:\n\n\nDraw Figure\npreds_nb |&gt; ggplot(\n  aes(x = age_group,\n      y = estimate, \n      color = gender, \n      group = gender, \n      linetype = gender,\n      shape = gender)) +\n  geom_line() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), linetype = 1, width = .2) +\n  geom_point(bg = \"white\") +\n  scale_shape_manual(values = c(19, 21)) +\n  scale_color_manual(values = c(\"black\", \"grey40\")) +\n  scale_y_log10(limits = c(20, 3500)) +\n  theme_classic_ls() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n  directlabels::geom_dl(aes(label = gender), method = list(\"smart.grid\", cex = .75, y = c(2.75, 4.3))) +\n  xlab(\"Age in decreasing order\") +\n  ylab(\"Estimated average rate of\\nactually (pmw, log-scaled)\") +\n  annotate(\"text\", x = 10.5, y = 25, label=\"Error bars denote 95% CIs\", size = 3, col = \"grey40\", adj=1)\n\n\n\n\n\n\n\n\nFigure 7: Log-scaled usage rate estimates based on a negative binomial regression model.\n\n\n\n\n\nThis yields a pattern that is very similar to Figure 4, i.e. which reflects mean speaker frequency (rather than subcorpus frequency). The subgroup estimates also form more regular patterns, rather than zig-zag profiles as in Figure 6. This appears much more plausible from a linguistic perspective. Finally, the statistical uncertaintiy intervals are appropriately wide, especially for the oldest cohorts. As our cross-tabulation above showed, very few speakers are found in these cells, which necessarily leads to imprecise estimates.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nIf the word count distribution across the speakers or texts in a corpus varies, the corpus frequency of a structure may be biased toward its occurrence rate in overrepresented units (i.e. texts or speakers).\nA simple double-check is to compare the corpus frequency to the mean text (or speaker) frequency.\nIn count regression modeling, the Poisson model produces corpus frequency estimates, while the negative binomial model yields scores that are similar to mean text frequency estimates.\nThe negative binomial model also gives more reasonable statistical uncertainty intervals.\n\n\n\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Unbalanced Distributions and Their Consequences: {Speakers}\n    in the {Spoken} {BNC2014}},\n  date = {2025-04-29},\n  url = {https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Unbalanced Distributions and Their\nConsequences: Speakers in the Spoken BNC2014.” April 29, 2025. https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/."
  },
  {
    "objectID": "posts/2025-05-01_clustering_uncertainty_intervals/index.html",
    "href": "posts/2025-05-01_clustering_uncertainty_intervals/index.html",
    "title": "Clustering in the data affects statistical uncertainty intervals",
    "section": "",
    "text": "The data points drawn from a corpus often come in groups: There are typically multiple observations from the same text (or speaker). If this kind of data layout is not represented in the analysis, this will often yield spuriously precise estimates of the quantities of interest. This blog post describes how this can interfere with replication efforts in corpus linguistics.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(corpora)         # to calculate a log-likelihood score\nlibrary(kableExtra)      # for drawing html tables\n\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nTerminology\nBefore we get started, let me briefly clarify how a number of terms are used in this blog post:\n\nOriginal study: The study whose results are subject to a replication effort\nReplication study: A study that uses new data and repeats the original work in the closest possible way, i.e. using the same research design and methods; this type of replication is often referred to as a direct/close/exact/literal replication\nOriginal/replication estimate: For the quantity of interest (sometimes referred to as an effect size), the point estimate returned by the original/replication study\n\n \n\n\nInterpretation of confidence intervals from a replication perspective\nThe key point of this blog post is that an inadequate analysis of corpus data can produce unrealistic expectations of a replication study. This may lead to erroneous judgments as to the compatibility of the original and the replication estimate, and therefore incorrect conclusions about the success of a replication study.\nWe therefore start by considering what kind of information confidence intervals (CIs) give us about consistency and replication. It is important to clarify this because misconceptions about the meaning and interpretation of CIs are widespread.\n\nInterpreting CI overlap\nA common misconception when comparing 95% confidence intervals of two independent groups is to assume that if the error bars overlap, the difference between the groups is “not statistically significant”. A nice paper by Cumming (2009) discusses this issue. It turns out that even if the overlap is moderate, the p-value for the comparison would be p &lt; .05. For CIs that do not overlap, it would be p &lt; .01.\nIn the figure below, the upper pair of estimates illustrates a situation where the arms of two CIs overlap. Moderate overlap means that the amount of overlap is less than half of the average arm length. This means that you need to mentally approximate the average length of the overlapping arms. This is easy in the example below, because they have the same length. Overlapping amounts to roughly half of the average arm length.\n\n\n\n\n\nWhile this insight is particularly relevant for comparisons made within a study, it also useful in the context of replication work. If we compare the CIs from an original and a replication study, we may see overlapping CIs. In some sense, the misconception then work in the opposite way: It invites lax judgments of replication success. Thus, when the original and replication CIs overlap, we may be tempted to conclude that the replication result is statistically compatible with the original one. As we have just seen however, this interpretation requires overlap by at least the average arm length.\n\n\nReplication information provided by CIs\nAnother misconception about CIs relevant to the present discussion concerns the information they provide about replication estimates. As Cumming, Williams, and Fidler (2004) noted, many researchers think that, given a 95% CI, the probability that the replication estimate will fall within the limits of this interval is also 95%. They refer to this probability as the average probability of capture (APC): The probability that a CI will capture a future replication estimate.\nIt turns out that the APC is smaller than the confidence level. In their appendix, Cumming, Williams, and Fidler (2004) provide the mathematical background. To develop our intuition, we consider the APC for a number of frequently used confidence levels. The mapping in the table below shows that the average probability of capture for a 95% CI, for instance, is 83%.\n\n\n\n\n\nConfidence level\nAverage probability of capture\n\n\n\n\n68.3% (± 1 standard error)\n52%\n\n\n90%\n76%\n\n\n95%\n83%\n\n\n99%\n93%\n\n\n\n\n\nWe keep the overlap and the replication interpretation of confidence intervals in mind as we turn to our illustrative analysis task.\n \n\n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur linguistic focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work.\nFurther, questions about diachronic trends in the frequency of modals have generated discussions about replicability in corpus linguistics. Based on a comparison of the Brown and Frown corpus, Leech (2003) concluded that the frequency of English modal verbs declined in the latter half of the 20th century. This finding was challenged by Millar (2009), which in turn prompted a response by Leech (2011). McEnery and Brezina (2022) also used data on English modals as a case study for discussing and illustrating key ideas about replication in corpus linguistics.\nEnglish modal verbs therefore have a special place in the corpus-linguistic discourse on replication and replicability. I therefore decided to set up a dedicated TROLLing post (Sönning 2024), which includes frequency information on the English modals from the Brown Family of corpora. Perhaps this resource may be of value in future discussion on the topic. An excerpt from this dataset is used in the current series of blog posts, which concentrate on statistical issues that may get in the way of replication attempts in corpus work.\nWe will concentrate on a subset of these data: the modal verb should in Brown and Frown, i.e. written American English. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\n \n\nData\nWe start by downloading the data directly from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe dataset we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nNext, we extract the data for should in Brown and Frown and prepare them for analysis.\n\n\nLoad and prepare data\nd_modals &lt;- subset(dat, corpus %in% c(\"Brown\", \"Frown\"))\n\nd_modals$time_period &lt;- factor(d_modals$time_period)\nd_modals$genre &lt;- factor(d_modals$genre)\n\ncontrasts(d_modals$genre) &lt;- contr.sum(4)\ncontrasts(d_modals$time_period) &lt;- contr.sum(2)\n\nshould_data &lt;- subset(d_modals, modal==\"should\")\nshould_Brown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Brown\")\nshould_Frown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Frown\")\nshould_learned &lt;- subset(d_modals, modal==\"should\" & genre==\"learned\")\n\n\nBrown and Frown each consist of 500 texts, which are sampled from four different genres. The following table shows the word count and number of texts for each genre in Brown:\n\n\n\n\n\nGenre\nWords\nTexts\n\n\n\n\nFiction\n295,779 (25.8%)\n126 (25.2%)\n\n\nGeneral prose\n470,726 (41.0%)\n206 (41.2%)\n\n\nLearned\n180,649 (15.7%)\n80 (16.0%)\n\n\nPress\n201,300 (17.5%)\n88 (17.6%)\n\n\n\n\n\n\n\n \n\n\nCrude answers to our research questions\nWe may obtain crude (but quick) answers to our questions as follows. To measure the frequency of should in Brown, we divide its corpus frequency by the size of the corpus. We can do the same for Frown. We will multiply these rates by thousand, to get normalized frequencies ‘per thousand words’.\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nAnd here they are, rounded to two decimal places:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\nFor Brown, we get a rate of 0.79 per thousand words, and for Frown the rate is 0.68 per thousand words.\nFor a quick answer to the second question, we divide the rate in Frown by that in Brown, which gives us a rate ratio of 0.86. This tells us that the frequency of should in the 1990s was only 86% as large as that in the 1960s:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\nTo see whether this frequency difference is “statistically significant”, a likelihood-ratio statistic may be computed. This score is based on a simple 2x2 table, which contains the frequency of should in each corpus, and the number of words in each corpus. We use the function keyness() in the R package {corpora} (Evert 2023) to calculate a likelihood-ratio test:\n\nkeyness(f1 = sum(should_Brown$n_tokens),\n        n1 = sum(should_Brown$n_words),\n        f2 = sum(should_Frown$n_tokens),\n        n2 = sum(should_Frown$n_words), \n        measure = \"G2\")\n\n[1] 9.255016\n\n\nThis returns a log-likelihood score of 9.3, which is close to the one reported by Leech (2003, 228) and indicates a “statistically significant” difference in normalized frequency between the two corpora.\nThese crude ways of assessing and testing frequencies and their differences in corpora are straightforward to carry out and therefore provide quick answers to our questions. We now look at how these estimates (and p-values) may be misleading, or may not answer the question we really had in mind.\nBefore we go further, however, I would like to should note that the following elaborations are not meant to discredit the work done by Geoffrey Leech in the early 2000s. In fact, Leech (2003) provides a balanced assessment of frequency changes in the English modal system. The diachronic patterns he observed were remarkably consistent across the 11 modal verbs, which strengthened the conclusion he drew. Further, his reference to log-likelihood scores for time differences explicitly noted that “too much should not be made of significance tests in comparative corpus studies” (2003, 228).\n \n\n\n\nClustering in the data affects statistical uncertainty intervals\nInterest in corpus-based work often centers on the frequency of a structure in language use, which is usually expressed as a normalized frequency (or occurrence rate), expressed, say, as ‘per million words’. Since any corpus is a sample of language use from a domain or language variety of interest, these normalized frequencies are sample statistics, which in turn often serve as estimates of population parameters.\n\nData structure\nThe Brown Corpus, for instance, contains a sample of written American English from the early 1960s, based on a purposefully compiled list of genres and sub-genres. If we look at the frequency of should in the Brown Corpus, it is unlikely that our linguistic interest is limited to the 500 texts (or text excerpts) in the corpus Rather, we would consider this as a sample from the population of interest – written American English in the 1960s.\nWhen extrapolating to this underlying language variety, our sample size is 500 (the number of texts in the corpus) rather than 1 million (the number of word tokens in Brown). In the sampling literature, the 500 texts would be considered the primary sampling units, and the 1 million word tokens in Brown are therefore clustered, or structured hierarchically. They are grouped by text file.\n\n\nData description\nLet’s take a look at the distribution of should in the Brown Corpus, by considering its occurrence rate at the level of the individual texts. This means that we first calculate normalized frequencies at the text level and then look at how they are distributed.\nFigure 1 shows the distribution of these text-level occurrence rates using a dot diagram. Each dot in the figure represents a text file. Since there are many texts with a frequency of 0 (n = 174, or 35%), the dot diagram is flipped: The y-axis shows the normalized frequency (expressed as ‘per thousand words’) and the dots form horizontal piles. We note that there are very few texts in which should occurs with a frequency greater than 5 per thousand words. Most texts (n = 382, or 76%) show at most 1 instance (i.e. a rate of roughly 2 ptw or lower).\n\n\nDraw Figure\nd_modals |&gt; filter(\n  corpus == \"Brown\",\n  modal == \"should\") |&gt; \n  mutate(rate_ptw = n_tokens / n_words * 1e3) |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_dotplot(method = \"histodot\", binwidth = .2, stackratio = 1) +\n  theme_dotplot_vertical() +\n  scale_y_continuous(expand = c(.004,0)) +\n  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +\n  xlab(\"Normalized frequency of should\\n(per 1,000 words)\") +\n  annotate(\"text\", y = .5, x = 5, label = \"Each dot denotes a text\", size = 3, col = \"grey30\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nFigure 1: Should in the Brown Corpus: Dot diagram showing the distribution of normalized frequencies across the 500 texts in the corpus.\n\n\n\n\n\nOur analyses will include Genre as a predictor – specifically, the broad text categories Fiction, General prose, Learned, and Press. We therefore break down the text-level occurrence rates by this variable. Figure 2 shows that occurrence rates tend to be lower in Fiction, and that the outliers with exceptionally high rates of should are found in General prose.\n\n\nDraw Figure\nd_modals |&gt; filter(\n  corpus == \"Brown\",\n  modal == \"should\") |&gt; \n  mutate(genre_nice = factor(\n    genre, \n    levels = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    labels = c(\"Fiction\\n\", \"General prose\\n\", \"Learned\\n\", \"Press\\n\"))) |&gt; \n  mutate(rate_ptw = n_tokens / n_words * 1e3) |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_dotplot(method = \"histodot\", binwidth = .2, stackratio = 1) +\n  theme_dotplot_vertical() +\n  facet_grid(. ~ genre_nice) +\n  scale_y_continuous(expand = c(.008,0)) +\n  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +\n  xlab(\"Normalized frequency of should\\n(per 1,000 words)\") +\n  ggh4x::force_panelsizes(cols = c(2,3.05,1.2,1.2)) +\n  coord_flip()\n\n\n\n\n\n\n\n\nFigure 2: Should in the Brown Corpus: Distribution of text-level normalized frequencies by Genre.\n\n\n\n\n\n \n\n\nStatistical modeling\nWe can use a statistical model to learn about the uncertainty surrounding our sample-based estimates. This uncertainty is often reported in the form of confidence intervals (or standard errors), which indicate the precision of estimates, based on the model and data. In order to arrive at reliable uncertainty estimates, a suitable model must be used. For instance, it must adequately represent the structure of the data – in our case, the fact that Brown is primarily a sample of 500 texts (rather than 1 million words) from the language variety of interest.\nThe use of an inadequate model, which does not take into account the clustered nature of the data, will usually suggest a higher level of precision than is warranted – in other words, we will get overconfidence intervals. This will happen if we analyze the current data with a Poisson model. This model does not account for the structure of the data in the sense that it makes no provision for the possibility that the usage rate of should may vary from text to text. Thus, it assumes that the underlying frequency of should is the same for each text, with observable variation in rates being exclusively due to sampling variation. The “underlying frequency” can be thought of as the propensity of the author(s) to use should in the particular context of language use represented by the text. This means that the model does not allow for the possibility that there may be inter-speaker variation.\nWe will compare two modeling approaches, which are also discussed and contrasted in Sönning and Krug (2022), in the context of a similar research task (the frequency of actually in conversational British speech, as represented in the Spoken BNC2014).\nLet’s fit a Poisson model to these data using the base R function glm(), where the code chunk offset(log(n_words)) represents the offset, which adjusts for the fact that text files differ (slightly) in length (for some background on this, see this blog post).\n\nm_poi &lt;- glm(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Brown, \n    family = poisson())\n\nWe also fit a negative binomial model to the data, which makes allowances for variation in occurrence rates across the 500 texts in the corpus. It does so via something similar to a standard deviation parameter, which expresses text-to-text variation in the normalized frequency of should. This blog post provides some background on the negative binomial distribution.\nWe fit a negative binomial regression model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Brown)\n\n \n\n\nModel-based predictions (i.e. estimates)\nThe next step is to calculate model-based estimates of the frequency of should. To this end, we use the predictions() function in the very helpful {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). As explained in some more detail in this blog post, we use the datagrid() function to define the condition(s) for which we wish to obtain estimates. In our case, these are the four genres. We also specify n_words = 1000 in datagrid(), to obtain rates per thousand words.\n\npreds_poi &lt;- predictions(\n  m_poi, \n  newdata = datagrid(\n    genre = c(\"press\", \"general_prose\", \"learned\", \"fiction\"),\n    n_words = 1000)) |&gt; \n  tidy()\n\npreds_nb &lt;- predictions(\n  m_nb, \n  newdata = datagrid(\n    genre = c(\"press\", \"general_prose\", \"learned\", \"fiction\"),\n    n_words = 1000)) |&gt; \n  tidy()\n\nHere is the (shortened) content of the output:\n\npreds_poi[,c(7,2,5,6)]\n\n# A tibble: 4 × 4\n  genre         estimate conf.low conf.high\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 press            0.864    0.745     1.00 \n2 general_prose    0.882    0.801     0.971\n3 learned          0.991    0.856     1.15 \n4 fiction          0.480    0.407     0.566\n\n\n\npreds_nb[,c(7,2,5,6)]\n\n# A tibble: 4 × 4\n  genre         estimate conf.low conf.high\n  &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 press            0.872    0.672     1.13 \n2 general_prose    0.884    0.746     1.05 \n3 learned          0.996    0.761     1.30 \n4 fiction          0.480    0.376     0.613\n\n\nFigure 3 compares estimates based on the Poisson and the negative binomial model graphically. We observe that the point estimates are virtually identical, but the negative binomial model returns much wider error intervals.\n\n\nDraw Figure\npreds_poi$model &lt;- \"Poisson\"\npreds_nb$model &lt;- \"Negative binomial\"\n\npreds_conditions &lt;- rbind(\n  preds_nb[,c(10,7,2,5,6)],\n  preds_poi[,c(10, 7,2,5,6)]\n)\n\npreds_conditions$genre_nice &lt;- rep(c(\"Press\", \"General\\nprose\", \"Learned\", \"Fiction\"), 2)\n\npreds_conditions |&gt; \n  ggplot(aes(x=genre_nice, y=estimate, group=model, shape=model)) + \n  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_shape_manual(values=c(21, 19)) +\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,\n                position = position_dodge(.3)) +\n  geom_point(position = position_dodge(.3), fill=\"white\") +\n  theme_classic_ls() +\n  theme(plot.subtitle = element_text(face = \"italic\")) +\n  annotate(\"text\", x = 4.6, y = .1, label = \"Error bars: 95% confidence intervals\", \n           adj=1, color = \"grey40\", size = 3) + \n  annotate(\"text\", x = 3, y = c(.35, .25), label = c(\"Negative binomial\", \"Poisson\"), \n           adj=0, color = \"grey40\", size = 3) + \n  annotate(\"point\", x = 2.8, y = c(.35, .25), shape = c(21, 19), \n           color = \"grey40\", size = 1.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3: Estimated normalized frequency of should by Genre: Comparison of estimates based on a Poisson and a negative binomial model.\n\n\n\n\n\n\n\n\nRelevance for replicability\nIn general, a statistical result is considered as having been replicated if a close replication yields statistical conclusions that are consistent with those of the original study. Uncertainty intervals serve as a basis for judging whether estimates based on different sets of data are consistent or not. As discussed above, a 95% CI has a 83% probability of capturing a replication estimate.\nThe problem with overconfident uncertainty intervals, then, is that they produce unreasonable expectations about what should happen in a replication study. Put differently, a replication estimate may appear to be inconsistent with the original result even though it isn’t. Had the original analysis (or perhaps both analyses) used a more adequate model, the uncertainty bounds as well as the replication expectations would have been wider.\nWe can illustrate this issue using our data on the modals. Since our corpora were compiled using the same sampling design, let us imagine the Frown Corpus were a direct replication of the Brown Corpus. Of course, the purpose of Frown, which records written American English in the early 1990s, was the documentation of diachronic trends in this variety. The goal was therefore to create a corpus that is as close to Brown as possible, apart from the difference in time. What this means is that if the Frown estimate (our “replication” estimate) is consistent with the Brown estimate (our “original” estimate), this will be interpreted as indicating no change over time; statistical differences, on the other hand, will be interpreted as reflecting a diachronic change in this variety.\nTo illustrate, we concentrate on the genre Learned and run two regression models, a Poisson and a negative binomial model. We fit these models, which include the variable Corpus as a predictor, in the same way as above:\n\nm_poi_learned &lt;- glm(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_learned, \n    family = poisson())\n\nm_nb_learned &lt;- glm.nb(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_learned)\n\nThen we use the {marginaleffects} package to generate predictions, again specifying n_words = 1000 in datagrid(), to obtain rates per thousand words.\n\npreds_poi_learned &lt;- predictions(\n  m_poi_learned, \n  newdata = datagrid(\n    corpus = unique,\n    n_words = 1000)) |&gt; \n  tidy()\n\npreds_nb_learned &lt;- predictions(\n  m_nb_learned, \n  newdata = datagrid(\n    corpus = unique,\n    n_words = 1000)) |&gt; \n  tidy()\n\nFigure 4 compares these model-based estimates visually. The question of interest is whether the frequency estimate from Frown is consistent with the one from Brown. Due the wider uncertainty intervals, the estimates based on the negative binomial model appear more consistent with one another then those from the Poisson model. In other words, a Poisson analysis of these data might lead us to believe that there is a diachronic decrease in the frequency of should.\n\n\nDraw Figure\ncomparison_learned &lt;- rbind(\n  preds_poi_learned[,c(7,2,5,6)],\n  preds_nb_learned[,c(7,2,5,6)])\n\ncomparison_learned$model &lt;- rep(c(\"Poisson\", \"Negative\\nbinomial\"), each = 2)\n\ncomparison_learned |&gt; \n  ggplot(aes(x=model, y=estimate, group=corpus, shape=corpus)) + \n  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_shape_manual(values=c(21, 19)) +\n  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,\n                position = position_dodge(.3)) +\n  geom_point(position = position_dodge(.3), fill=\"white\") +\n  theme_classic_ls() +\n  theme(plot.subtitle = element_text(face = \"italic\")) +\n  annotate(\"text\", x = 2.6, y = .1, label = \"Error bars: 95% CIs\", \n           adj=1, color = \"grey40\", size = 3) + \n  annotate(\"text\", x = 2.1, y = c(.35, .25), label = c(\"Brown\", \"Frown\"), \n           adj=0, color = \"grey40\", size = 3) + \n  annotate(\"point\", x = 2, y = c(.35, .25), shape = c(21, 19), \n           color = \"grey40\", size = 1.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 4: Estimated frequency of should in the genre Learned: Comparison of estimates based on a Poisson and a negative binomial model.\n\n\n\n\n\nIf we look at the CI overlap of the 95% confidence intervals for the Poisson estimates, we note that it is more than moderate, i.e. amounts to more than half the average arm length. An approximate interpretation is therefore that the two estimates do not differ “significantly”.\nFor reassurance, we take a closer look at the model output. Both models also produce an estimate of the difference between the occurrence rates in the two corpora. This difference is represented by the coefficient for the predictor Corpus. This coefficient is very similar in the two models:\n\ncoef(m_poi_learned)[2]\n\ncorpusFrown \n -0.1978386 \n\ncoef(m_nb_learned)[2]\n\ncorpusFrown \n -0.2054762 \n\n\nSince count regression models operate on the link scale of natural logarithms, these coefficients express differences on that scale. If we back-transform them using exponentiation, we obtain ratios. This means that the difference between the two corpora is expressed in relative terms. Back-transformation tells us that the normalized frequency of should in Frown is only around 80% as large as that in Frown:\n\nround(exp(coef(m_poi_learned)[2]), 2)\n\ncorpusFrown \n       0.82 \n\nround(exp(coef(m_nb_learned)[2]), 2)\n\ncorpusFrown \n       0.81 \n\n\nWe can obtain 95% CIs for these rate ratios using the function confint(). It returns the limits of a 95% CI on the log scale, which means we need to back-transform these confidence limits to obtain uncertainty bounds for our rate ratios:\n\nround(exp(confint(m_poi_learned, \"corpusFrown\")), 2)\n\n 2.5 % 97.5 % \n  0.66   1.02 \n\nround(exp(confint(m_nb_learned, \"corpusFrown\")), 2)\n\n 2.5 % 97.5 % \n  0.55   1.20 \n\n\nWe note that the statistical uncertainty surrounding the estimate based on the negative binomial model is considerably wider; it suggests that the 1990s rate of should could be as small as 55% of the 1960s rate, or as large as 120% of the 1960s rate. The Poisson model, in contrast, provides stronger indication of a decline in occurrence rate, which could amount to 66% or 102% of the rate in Brown.\nThe Poisson model is therefore more consistent with the interpretation of change over time. This is also reflected in the p-value associated with the coefficient for Corpus: In the Poisson model, it is .07, in the negative binomial model it is .30.\n\n\nSummary\nCorpus data are often structured hierarchically, with multiple data points from the same text. If this feature is not taken into account in the analysis, statistical inferences based on confidence intervals or p-values will usually be overoptimistic. As a result, the bounds for a successful replication, if evaluated on inferential grounds, are too narrow, and replication attempts may be too readily dismissed as unsuccessful. This has the potential to trigger unwarranted discussions about replication failures in our field.\nWe should arguably always consider the possibility that a failed replication may be due to issues surrounding the statistical analysis of clustered data. To rule out this possibility, the replication team needs access to the data from the original study. This is another reason why researchers should routinely make their data available to the community.\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nCumming, Geoff. 2009. “Inference by Eye: Reading the Overlap of Independent Confidence Intervals.” Statistics in Medicine 28 (2): 205–20. https://doi.org/10.1002/sim.3471.\n\n\nCumming, Geoff, Jennifer Williams, and Fiona Fidler. 2004. “Replication and Researchers’ Understanding of Confidence Intervals and Standard Error Bars.” Understanding Statistics 3 (4): 299–311. https://doi.org/10.1207/s15328031us0304_5.\n\n\nEvert, Stephanie. 2023. Corpora: Statistics and Data Sets for Corpus Frequency Data. https://doi.org/10.32614/CRAN.package.corpora.\n\n\nLeech, Geoffrey N. 2003. “Modality on the Move: The English Modal Auxiliaries 1961-1992.” In Modality in Contemporary English, 223–40. DE GRUYTER. https://doi.org/10.1515/9783110895339.223.\n\n\n———. 2011. “The Modals ARE Declining: Reply to Neil Millar’s ‘Modal Verbs in TIME: Frequency Changes 1923–2006,’ International Journal of Corpus Linguistics 14:2 (2009), 191–220.” International Journal of Corpus Linguistics 16 (4): 547–64. https://doi.org/10.1075/ijcl.16.4.05lee.\n\n\nMcEnery, Tony, and Vaclav Brezina. 2022. Fundamental Principles of Corpus Linguistics. Cambridge University Press. https://doi.org/10.1017/9781107110625.\n\n\nMillar, Neil. 2009. “Modal Verbs in TIME: Frequency Changes 1923–2006.” International Journal of Corpus Linguistics 14 (2): 191–220. https://doi.org/10.1075/ijcl.14.2.03mil.\n\n\nSönning, Lukas. 2024. “Background data for: Some obstacles to replication in corpus linguistics.” DataverseNO. https://doi.org/10.18710/7LNWJX.\n\n\nSönning, Lukas, and Manfred Krug. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Clustering in the Data Affects Statistical Uncertainty\n    Intervals},\n  date = {2025-05-02},\n  url = {https://lsoenning.github.io/posts/2025-05-01_clustering_uncertainty_intervals/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Clustering in the Data Affects Statistical\nUncertainty Intervals.” May 2, 2025. https://lsoenning.github.io/posts/2025-05-01_clustering_uncertainty_intervals/."
  },
  {
    "objectID": "posts/2025-05-03_imbalance_bias/index.html",
    "href": "posts/2025-05-03_imbalance_bias/index.html",
    "title": "Imbalance across predictor levels affects data summaries",
    "section": "",
    "text": "Corpus data are often characterized by a lack of balance. In contrast to experiments, where the researcher has (almost) full control over the distribution of data points across the conditions of interest, the spread of observations across the levels of relevant predictor variables is virtually always uneven in observational research. When the data break down into subgroups, many quantities of interest essentially represent some kind of average over these conditions. When dealing with unbalanced data, the researcher should actively decide whether they want subgroups to influence this average in proportion to their size, or whether a simple average is preferred. These are two different estimands, and in order for the estimates based on an original and a replication study to be comparable, they need to target the same estimand.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(corpora)         # to calculate a log-likelihood score\nlibrary(ggthemes)        # for colorblind color theme\nlibrary(kableExtra)      # for drawing html tables\nlibrary(ggthemes)        # for color-blind palette\n\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nTerminology\nBefore we get started, let me briefly clarify how a number of terms are used in this blog post:\n\nOriginal study: The study whose results are subject to a replication effort\nReplication study: A study that uses new data and repeats the original work in the closest possible way, i.e. using the same research design and methods; this type of replication is often referred to as a direct/close/exact/literal replication\nOriginal/replication estimate: For the quantity of interest (sometimes referred to as an effect size), the point estimate returned by the original/replication study\n\n\n\nEstimands, estimators, and estimates\nIn empirical work, the target of inquiry is sometimes referred to as the estimand. Estimands are formulated in substantive terms; in the language sciences, they represent the linguistic objectives of our research efforts.\nTo obtain linguistic insights from corpus data, we apply statistical procedures to calculate numerical summaries. A specific procedure (e.g. a formula) is referred to as an estimator. Well-known examples of estimators are the arithmetic mean and the median, which offer different ways of characterizing the typical unit under study (the estimand).\nFinally, the specific value we obtain by applying an estimator to a set of data is referred to as an estimate. Note that while the estimand hinges on our research objectives and a corresponding estimator is chosen by the researcher, the estimate we obtain will depend on the data at hand and therefore vary from study to study.\nAn excellent paper by Lundberg, Johnson, and Stewart (2021) discusses the importance of clearly defining the estimand of your study. While the authors go into much greater depth and consider the mapping between theoretical and empirical estimands, we will concentrate on what at first seems to be a rather superficial feature of estimands: The weight they give to different subgroups (or, more generally: conditions) in the data. It turns out, however, that this decision may not only affect the conclusions drawn from a study, but can also be challenging to motivate on linguistic grounds.\nReaders who are already familiar with this earlier blog post may skip to the section titled “Vague estimands”.\n \n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur linguistic focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work.\nFurther, questions about diachronic trends in the frequency of modals have generated discussions about replicability in corpus linguistics. Based on a comparison of the Brown and Frown corpus, Leech (2003) concluded that the frequency of English modal verbs declined in the latter half of the 20th century. This finding was challenged by Millar (2009), which in turn prompted a response by Leech (2011). McEnery and Brezina (2022) also used data on English modals as a case study for discussing and illustrating key ideas about replication in corpus linguistics.\nEnglish modal verbs therefore have a special place in the corpus-linguistic discourse on replication and replicability. I therefore decided to set up a dedicated TROLLing post (Sönning 2024), which includes frequency information on the English modals from the Brown Family of corpora. Perhaps this resource may be of value in future discussion on the topic. An excerpt from this dataset is used in the current series of blog posts, which concentrate on statistical issues that may get in the way of replication attempts in corpus work.\nWe will concentrate on a subset of these data: the modal verb should in Brown and Frown, i.e. written American English. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\n \n\n\nData\nWe start by downloading the data directly from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe dataset we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nNext, we extract the data for should in Brown and Frown and prepare them for analysis.\n\n\nLoad and prepare data\nd_modals &lt;- subset(dat, corpus %in% c(\"Brown\", \"Frown\"))\n\nd_modals$time_period &lt;- factor(d_modals$time_period)\nd_modals$genre &lt;- factor(d_modals$genre)\n\ncontrasts(d_modals$genre) &lt;- contr.sum(4)\ncontrasts(d_modals$time_period) &lt;- contr.sum(2)\n\nshould_data &lt;- subset(d_modals, modal==\"should\")\nshould_Brown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Brown\")\nshould_Frown &lt;- subset(d_modals, modal==\"should\" & corpus==\"Frown\")\nshould_learned &lt;- subset(d_modals, modal==\"should\" & genre==\"learned\")\n\n\nBrown and Frown each consist of 500 texts, which are sampled from four different genres. The following table shows the word count and number of texts for each genre in Brown:\n\n\n\n\nTable 1: Distribution of words and texts across the four braod genres in the Brown Corpus.\n\n\n\n\n\n\nGenre\nWords\nTexts\n\n\n\n\nFiction\n295,779 (25.8%)\n126 (25.2%)\n\n\nGeneral prose\n470,726 (41.0%)\n206 (41.2%)\n\n\nLearned\n180,649 (15.7%)\n80 (16.0%)\n\n\nPress\n201,300 (17.5%)\n88 (17.6%)\n\n\n\n\n\n\n\n\n\n\n \n\n\nVague estimands\nWhile the research questions we formulated above outline the general direction of our analysis, they leave considerable room for specifying an estimand. Thus, we may ask what exactly is meant by “the frequency of should in written American English of the early 1960s”.\nThe focus in this blog post is on how the four genres are to be handled when calculating a frequency estimate. This question is relevant because of their differential size in the corpus. When measuring the frequency of should in Brown, the question would be whether the four genres should be weighted proportionally to their size, or whether they should be given equal weights. In other words, are we interested in the frequency of should in a population of written American English where each genre is equally important, or a population where the weights differ, perhaps reflecting their prevalence, or currency, in the linguistic community.\nThe representation of the four genres in Brown is linguistically motivated. The manual states: “The list of main categories and their subdivisions was drawn up at a conference held at Brown University in February 1963. The participants in the conference [John B. Carroll, W. Nelson Francis, Philip B. Gove, Henry Kucera, Patricia O’Connor, and Randolph Quirk.] also independently gave their opinions as to the number of samples there should be in each category. These figures were averaged to obtain the preliminary set of figures used.” While the manual does not specify the rationale underlying participants’ preferences for the number of text samples per text category, we may assume that the weighting is meant to reflect the currency of these subvarieties in written American English.\nDifferent estimands require different estimators: The currency-informed estimand of the normalized frequency weights genres in proportion to their representation in the corpus. This is equivalent to simply obtaining the corpus frequency of should: Divide the number of occurrences by the corpus size. The equal-importance estimand, on the other hand, requires a simple average over four occurrence rates, one for each genre.\n\n\nCorpus frequencies are weighted averages\nThe typical way of answering the research questions we formulated above would be the following:\n\nObtain the normalized corpus frequency of should in Brown and then in Frown\nCompare the two normalized frequencies, e.g. by dividing the Frown rate by the Brown rate.\n\nHere is how we could do this in R (with normalized frequencies expressed as ‘per thousand words’):\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nFor Brown, we get a rate of 0.79 per thousand words, which is also what CQPweb returns:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\n\n\n\n\n\n\nFor Frown, we get a rate of 0.68 per thousand words, in line with the CQPweb report:\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\n\n\n\n\n\nFinally, the comparison of the normalized frequencies indicates that the Frown rate is 86% as large as that in Brown:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\nCorpus frequencies and their differences are weighted averages, which assign differential importance to the four genres. In the case of the Brown Family of corpora, where considerable thought has been given to the representation of the text categories, this differential weighting may be desirable. When working with other corpora, crude corpus frequencies may need to be treated more cautiously.\nA case in point are spoken corpora, where the word count (and implicit weight) may vary considerably across speakers and speaker groups. In written corpora, however, texts may also vary in length. In both cases (imbalance across speakers or texts), there are few situations in which differences in size reflect differences in importance. We would then like to avoid the implicit weighting implemented by plain corpus frequencies. As discussed by Egbert and Burch (2023), a different type of estimator, the mean text frequency, may then be preferable. It first calculates normalized frequencies at the text (or speaker) level, and then averages over these.\nIn the Brown Family, word counts are roughly balanced across texts, and we therefore need not worry about this kind of imbalance when measuring frequency. The mean text frequencies almost coincide with the corpus frequencies:\n\nround(\n  mean(\n    (should_Brown$n_tokens / should_Brown$n_words) *1000),\n  2)\n\n[1] 0.8\n\nround(\n  mean(\n    (should_Frown$n_tokens / should_Frown$n_words) *1000),\n  2)\n\n[1] 0.69\n\n\nThis blog post discusses situations where corpus units (texts or speakers) differ in size, and how this can affect frequency estimates.\nAs noted above, the Brown Family shows a different form of imbalance: The size of the four broad genres (Fiction, General Prose, Learned, Press) differs. Table 1 showed that the genre General prose accounts for 41% of the corpus size, while Learned and Press are relatively underrepresented. We now look at how this disproportion can affect estimates of frequencies and their differences.\n\n\nFrequency\nWe first consider the estimation of normalized frequencies in Brown and Frown, which we will approach from two angles: We start with descriptive data summaries and then look at model-based estimates.\n\nDescriptive data summaries\nWhen using descriptive statistics to summarize the data, the equal-importance estimand is obtained using a simple average over the genre-specific normalized frequencies. In R, we can use the {dplyr} package to calculate this simple average in two steps:\n\ncalculate genre-specific rates\naverage over these\n\n\nBrown_simple &lt;- should_Brown |&gt;                  #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(mean(rate_ptw))                      # (2)\n\nFrown_simple &lt;- should_Frown |&gt;                  #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(mean(rate_ptw))                      # (2)\n\nThe currency-informed average, on the other hand, can be obtained using plain corpus frequencies (see above). An alternative procedure, which is more flexible, uses the same two-step approach but calculates a weighted average in step 2. For this, we need a set of weights that reflect the representation of these text categories in the Brown Family. We will calculate these on the basis of both corpora (Brown and Frown):\n\nbrown_family_weights &lt;- should_data |&gt; \n  group_by(genre) |&gt; \n  summarize(\n    n_words_genre = sum(n_words)\n  ) |&gt; \n  mutate(\n    genre_weight = n_words_genre / sum(n_words_genre)\n  )\n\nHere they are (in alphabetical order: Fiction, General prose, Learned, Press):\n\nround(\n  brown_family_weights$genre_weight, \n  2)\n\n[1] 0.26 0.41 0.16 0.17\n\n\nNow we can apply the two-step procedure:\n\nBrown_weighted &lt;- should_Brown |&gt;                #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(weighted.mean(                       # (2)\n    rate_ptw,                                    #\n    w = brown_family_weights$genre_weight))      #\n\nFrown_weighted &lt;- should_Frown |&gt;                #\n  group_by(genre) |&gt;                             # (1)\n  summarize(                                     #\n    rate_ptw = mean(n_tokens / n_words)*1000) |&gt; #\n  summarize(weighted.mean(                       # (2)\n    rate_ptw,                                    #\n    w = brown_family_weights$genre_weight))      #\n\nNote that we could have included the calculation of weights into the code above, which would have been more error-tight in the present case. The specification of “external” weights, however, gives us more flexibility when calculating weighted averages. For instance, we may decide to use custom weights, or weights based on the data distribution in a different (reference) corpus.\nFigure 1 compares these frequency estimates visually. Proportionally scaled circles are used to show the differential representation of the genres in the corpus. The weighted averages appear in grey, the simple ones in black.\nFor Brown, the simple average (0.81 ptw) and the weighted average (0.80 ptw) are very similar, indicating that the imbalance of word counts across genres does not affect the frequency estimate much. In Frown, the discrepancy is greater, with a simple average of 0.72 ptw, and a weighted average of 0.69 ptw. This is because the genres Learned and Press, which show relatively large occurrence rates of should, gain weight when calculating a simple (instead of a weighted) average: The mass for Learned increases from .16 to .25, that for Press from .17 to .25.\n\n\nDraw Figure\nBrown_simple &lt;- as.numeric(Brown_simple)\nBrown_weighted &lt;- as.numeric(Brown_weighted)\n\nFrown_simple &lt;- as.numeric(Frown_simple)\nFrown_weighted &lt;- as.numeric(Frown_weighted)\n\n\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ggplot(aes(x = corpus, y = ptw, size = n_words, group = genre, color = genre)) +\n  geom_point(shape = 1) +\n  geom_point(shape = 16, size = 1) +\n  #geom_line(size = .5) +\n  scale_color_colorblind() +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +\n  scale_x_discrete(expand=c(.2,.2)) +\n  scale_size_area(max_size = 8) +\n  theme(legend.position = \"none\",\n        plot.margin = margin(.2, 2, .2, .2, \"cm\")) +\n  directlabels::geom_dl(aes(label = genre), method = list(\n    \"last.points\", cex = .75, x = 3.52, \n    label = c(\"Fiction\", \"General prose\", \"Learned\", \"Press\"))) +\n  \n  annotate(\"segment\", x = .9, xend = 1.1, y = Brown_weighted, yend = Brown_weighted, \n           linewidth = .8, color = \"grey50\") +\n  annotate(\"segment\", x = .9, xend = 1.1, y = Brown_simple, yend = Brown_simple,\n           linewidth = .8) +\n  annotate(\"segment\", x = 1.9, xend = 2.1, y = Frown_weighted, yend = Frown_weighted, \n           linewidth = .8, color = \"grey50\") +\n  annotate(\"segment\", x = 1.9, xend = 2.1, y = Frown_simple, yend = Frown_simple, \n           linewidth = .8) +\n  \n  annotate(\"segment\", x = 1.1, xend = 1.9, y = Brown_weighted, yend = Frown_weighted,\n           linewidth = .3, color = \"grey50\") +\n  annotate(\"segment\", x = 1.1, xend = 1.9, y = Brown_simple, yend = Frown_simple, \n           linewidth = .3) +\n  \n  annotate(\"text\", x = c(1.4, 1.6), y = c(.68, .85), label = c(\"weighted\", \"simple\"), \n           size = 3, color = c(\"grey50\", \"black\")) +\n  coord_cartesian(clip=\"off\")\n\nggsave(\"should_imbalance_brown.pdf\")\n\n\n\n\n\n\n\n\nFigure 1: Estimated frequency of should in Brown and Frown: Comparison of simple and weighted averages across genres.\n\n\n\n\n\n \n\n\nModel-based estimates\nWhen calculating model-based predictions, we can likewise decide whether we want to form simple or weighted averages. The default behavior in the {marginaleffects} package is to use the in-sample distribution of predictor variables to calculate average predictions. This is to say that, unless explicitly told to do otherwise, the functions in the package will usually calculate weighted averages.\nLet’s take a look at how to produce simple and weighted averages using a negative binomial model of should in Frown. The first step is to fit the model:\n\nm_nb_Frown &lt;- MASS::glm.nb(\n    n_tokens ~ genre + offset(log(n_words)), \n    data = should_Frown)\n\nThe function avg_predictions() calculates average predictions. Its default behavior for the data at hand returns a frequency estimate that is unlikely to be of interest to us. This is because it uses the in-sample mean text length (n_words) to adjust the predicted rate. The estimate of 1.58 is therefore the expected frequency ‘per 2,309 words’:\n\navg_predictions(\n  m_nb_Frown)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     1.58     0.0899 17.6   &lt;0.001 227.7   1.4   1.76\n\nType:  response \n\n\nWe must take control over the kind of normalized frequency we are getting. We prefer ‘per 1,000 words’ and therefore use the argument variables to specify n_words = 1000. Now we get a more interpretable estimate:\n\navg_predictions(\n  m_nb_Frown,\n  variables = list(\n    n_words = 1000))\n\n\n n_words Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    1000    0.686     0.0391 17.6   &lt;0.001 227.2  0.61  0.763\n\nType:  response \n\n\nWe note that this is close to the weighted mean we calculated above, which means that the genres are weighted in proportion to their size. This reflects the fact that the {marginaleffects} package by default averages over the estimation sample, and therefore propagates imbalances into the averages (which may be desirable or not). Specifically, the function avg_predictions() starts by calculating a model-based prediction for each text in the data, assuming it is 1,000 words long (as specified by variables = list(n_words = 1000))), and then averages over these 500 model-based estimates.\nAnother way of forming weighted predictions is to use the argument datagrid() to define the conditions over which we average, and then add another argument, wts, giving the weight of these conditions. This strategy is useful if we want to use an externally informed set of custom weights. The following returns (almost) the same results as the previous code:\n\navg_predictions(\n  m_nb_Frown,  \n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  wts = brown_family_weights$genre_weight)\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.685     0.0389 17.6   &lt;0.001 227.7 0.608  0.761\n\nType:  response \n\n\nIf we instead prefer a simple average, we can use the argument newdata to explicitly define the conditions to average over. This way we tell the function not to take the estimation sample as a basis for calculating predictions (and weighting), but instead define the reference grid over which to average. The following code asks for a simple average over four conditions, which represent different genres but have the same length. The result is close to the simple average we calculated above.\n\navg_predictions(\n  m_nb_Frown,  \n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000))\n\n\n Estimate Std. Error  z Pr(&gt;|z|)     S 2.5 % 97.5 %\n    0.715     0.0448 16   &lt;0.001 188.2 0.627  0.802\n\nType:  response \n\n\nThis shows that imbalances in the data can have an effect on simple data summaries such as estimates of average normalized frequencies. This is sometimes referred to as imbalance bias, and in the present case, we could refer to it as genre imbalance bias. This kind of distortion can occur when (i) there is imbalance across subgroups in the data and (ii) the quantity of interest varies from subgroup to subgroup. We have seen how to adjust for this form of bias using model-based predictions.  \n\n\n\nFrequency differences\nIn the same way, imbalance bias can affect frequency differences. Recall that a crude comparison of the corpora reveals that the frequency of should in Frown is only 86% as high as that in Brown. Since this comparison is based on two corpus frequencies, each of which is potentially affected by imbalance bias, the comparison may likewise be driven into the direction of more strongly represented genres.\n\nDescriptive data summaries\nFigure 2 shows the diachronic trends in the four genres. We note that while Press and Fiction show virtually no difference between Brown and Frown, a diachronic cline is apparent for Learned and General prose. Seeing that General prose is the most strongly represented genre, we would expect weighted differences to be pulled into its direction, meaning that a weighted frequency difference will be larger than a simple frequency difference, which would give the same weight to all genres.\n\n\nDraw Figure\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ggplot(aes(x = corpus, y = ptw, size = n_words, group = genre, color = genre)) +\n  geom_point(shape = 1) +\n  geom_point(shape = 16, size = 1) +\n  geom_line(size = .5) +\n  scale_color_colorblind() +\n  theme_classic_ls() +\n  ylab(\"Normalized frequency\\n(per 1,000 words)\") +\n  xlab(NULL) +\n  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +\n  scale_size_area(max_size = 8) +\n  theme(legend.position = \"none\",\n        plot.margin = margin(.2, 2, .2, .2, \"cm\")) +\n  directlabels::geom_dl(aes(label = genre), method = list(\"last.points\", cex = .75, x = 3.52, label = c(\"Fiction\", \"General prose\", \"Learned\", \"Press\"))) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 2: Diachronic trends by text category: Frequency of should in Frown vs. Brown, broken down by genre.\n\n\n\n\n\nWe can calculate descriptive frequency comparisons according to the two schemes, i.e. by either weighting all genres equivalently, or in proportion to their representation in the data. This returns two slightly different estimates: A frequency decline by 11% (simple comparison) or by 13% (weighted comparison).\n\nshould_data |&gt; \n  mutate(rate_ptw = (n_tokens/n_words)*1e3) |&gt; \n  group_by(corpus, genre) |&gt; \n  dplyr::summarize(\n    ptw = mean(rate_ptw),\n    n_words = sum(n_words)) |&gt; \n  ungroup() |&gt; \n  group_by(genre) |&gt; \n  dplyr::summarize(\n    freq_ratio_data = ptw[corpus == \"Frown\"]/ptw[corpus == \"Brown\"],\n    n_words_genre = sum(n_words)) |&gt; \n  mutate(\n    weight = n_words_genre/sum(n_words_genre)) |&gt; \n  dplyr::summarize(\n    simple_comparison = mean(freq_ratio_data),\n    weighted_comparison = weighted.mean(freq_ratio_data, w = weight)\n  ) |&gt; round(2)\n\n# A tibble: 1 × 2\n  simple_comparison weighted_comparison\n              &lt;dbl&gt;               &lt;dbl&gt;\n1              0.89                0.87\n\n\n \n\n\nModel-based estimates\nLet us again look at how to obtain these two types of comparison using a regression model. We start by fitting a negative binomial model that includes two predictors, Corpus and Genre, as well as their interaction.\n\nm_nb_corpus &lt;- MASS::glm.nb(\n    n_tokens ~ corpus * genre + offset(log(n_words)), \n    data = should_data)\n\nThis kind of model allows us to calculate frequency comparisons at the level of the individual genres (similar to what we saw in Figure 2 above). Alternatively, we may average over the four genres, to get a general estimate of how the frequency of should differs between the corpora.\nFor purposes of illustration, let’s use the {marginaleffects} package to get genre-level frequency comparisons. We use the function comparisons() to do so.\n\nThe argument variables specifies the focal variable(s), i.e. the one(s) whose levels are to be compared. In our case, this is the predictor Corpus.\nThe argument newdata allows us to specify the location in the predictor space at which to make comparisons. This means that it allows us to take control over the levels of the non-focal variables. Since we want a comparison for each genre, we specify all genres, and we also want to compare normalized frequencies ‘per 1,000 words’.\nFinally, by specifying transform = exp, we are asking comparisons() to exponentiate the log-scale differences, which yields rate ratios.\n\n\ncomparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  transform = exp)\n\n\n\n# A tibble: 4 × 5\n  genre         contrast      estimate conf.low conf.high\n  &lt;fct&gt;         &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 fiction       Frown - Brown     0.98     0.84      1.16\n2 general_prose Frown - Brown     0.82     0.68      0.99\n3 learned       Frown - Brown     0.83     0.59      1.17\n4 press         Frown - Brown     1.03     0.75      1.42\n\n\nWe can use the function avg_comparisons() to average over the four genres. The simple average is obtained as follows. Note that the code is almost identical to the one we used above, apart from replacing the function name:\n\navg_comparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  transform = exp)\n\n\n Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n    0.911    0.168 2.6 0.797   1.04\n\nTerm: corpus\nType:  response \nComparison: Frown - Brown\n\n\nUsing a simple average over the genres, the model-based estimate of the rate ratio is 91%: The normalized frequency in Frown is 91% as large as that in Brown. The regression model also provides a 95% confidence interval for this estimate, which ranges from 90% to 104%.\nTo obtain a model-based weighted average, we use the argument wts = brown_family_weights$genre_weight to specify the weights of the conditions in the reference grid. Our reference grid consists of four rows, so four weights are required, one for each genre.\n\navg_comparisons(\n  m_nb_corpus,  \n  variables = \"corpus\",\n  newdata = datagrid(\n    genre = c(\"fiction\", \"general_prose\", \"learned\", \"press\"),\n    n_words = 1000),\n  wts = brown_family_weights$genre_weight,\n  transform = exp)\n\n\n Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n    0.895   0.0648 3.9 0.796   1.01\n\nTerm: corpus\nType:  response \nComparison: Frown - Brown\n\n\nUsing a weighted average over the genres, the model-based estimate of the rate ratio is 89%, with the 95% CI ranging from 80% to 101%.\nWe observe that these model-based estimates differ from the descriptive ones reported above. Table 2 shows that the model-based estimates suggest slightly smaller differences between Brown and Frown. This is due to the fact that averaging was done on different scales: While the descriptive ratios were averaged on the data scale (i.e. ratios), the model-based estimates were averaged on the model scale (i.e. log ratios) and the average then back-transformed into a ratio. The discrepancy between these ways of forming averages will be discussed in a future blog post.\n\n\n\n\nTable 2: Comparison of simple and weighted summary statistics vs. model-based estimates.\n\n\n\n\n\n\nComparison\nDescriptive\nModel-based\n\n\n\n\nSimple\n89%\n91%\n\n\nWeighted\n87%\n89%\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelevance for replication\nIn order for the statistical results of an original and a replication study to be comparable, they need to be concerned with the same target quantity, or estimand. One issue that will always be relevant when working with observational data is the imbalance of observations across relevant subgroups, where “relevant” means that they show an association with the outcome variable. Corpus frequencies such as those reported in the CQPweb interface then represent weighted data summaries that reflect the disproportional representation of subgroups in the data.\nDue to the directly parallel design of Brown and Frown, we saw relatively minor differences between currency-based and equal-importance (i.e. weighted and simple) normalized frequencies. Nevertheless, the choice of estimand did have an effect on our estimates and statistical uncertainty intervals.\nThis issue will be more serious when we compare results across corpora that differ in composition. It is then arguably negligent to work with plain corpus frequencies. When comparing the BNC and COCA, for instance, comparisons should be adjusted for differences in genre coverage and representation (see Sönning and Schlüter 2022, 29–31 for a worked example).\nIn general, it is therefore necessary for any type of corpus analysis to think carefully about imbalances in the data and how these will propagate into the statistical results. If the disproportional representation of subgroups in the data is a meaningful feature of the population of interest, the researcher may wish to preserve it in their data summaries. A replication study, however, should be based on the same weighting scheme as the original study, which means that adjustments to the way in which weighted averages are formed will be necessary.\nThis issue should prompt us to generally think more carefully about imbalances in corpus data. Here, we have dealt with a single variable, genre, whose distribution in the data can usually be recovered from the corpus documentation. The issue, however, applies to any predictor variable that (i) is relevant, i.e. shows an association with the outcome; and (ii) whose distribution in the data is out of balance. Whether adjustments should be made for imbalances then depends on whether distributional asymmetries reflect a meaningful feature of the population of interest, or rather a nuisance.\nWe discuss the question of weighting model-based estimates in some more detail in Sönning and Grafmiller (2024, 163–69), where we suggest that a useful default approach may be to retain sample-based weights for internal (or linguistic) variables in the data, whose distribution cannot be controlled during corpus compilation. External variables, on the other hand, which primarily reflect corpus design (e.g. characteristics of the speaker or text) may be considered as candidates for adjustment. Note that this does not necessarily mean that they are assigned equivalent weights.\n\n\nSummmary\nDue to their observational nature, corpus data are often unbalanced. If imbalances affect variables that show an association with the outcome, a study may target different estimands, depending on how imbalances are handled when summarizing the data. While weighted averages propagate imbalances into our summaries, simple averages assign equal weight (and importance) to subgroups. In this blog post, we used the Brown Family of corpora, which is unbalanced by design. Genres differ in size, and when measuring and comparing normalized frequencies, we need to decide how to work with this asymmetry. Importantly, corpus frequencies, which are reported in corpus analysis software, are always weighted averages reflecting the composition of the corpus. The choice of estimand is a linguistic decision – it depends on the nature of the (hypothetical) population we are interested in. In replication work, however, it is also a methodological decision: A replication study must ensure that it targets the same estimand as the original study.\n\n\n\n\n\nReferences\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nLeech, Geoffrey N. 2003. “Modality on the Move: The English Modal Auxiliaries 1961-1992.” In Modality in Contemporary English, 223–40. DE GRUYTER. https://doi.org/10.1515/9783110895339.223.\n\n\n———. 2011. “The Modals ARE Declining: Reply to Neil Millar’s ‘Modal Verbs in TIME: Frequency Changes 1923–2006,’ International Journal of Corpus Linguistics 14:2 (2009), 191–220.” International Journal of Corpus Linguistics 16 (4): 547–64. https://doi.org/10.1075/ijcl.16.4.05lee.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMcEnery, Tony, and Vaclav Brezina. 2022. Fundamental Principles of Corpus Linguistics. Cambridge University Press. https://doi.org/10.1017/9781107110625.\n\n\nMillar, Neil. 2009. “Modal Verbs in TIME: Frequency Changes 1923–2006.” International Journal of Corpus Linguistics 14 (2): 191–220. https://doi.org/10.1075/ijcl.14.2.03mil.\n\n\nSönning, Lukas. 2024. “Background data for: Some obstacles to replication in corpus linguistics.” DataverseNO. https://doi.org/10.18710/7LNWJX.\n\n\nSönning, Lukas, and Jason Grafmiller. 2024. “Seeing the Wood for the Trees: Predictive Margins for Random Forests.” Corpus Linguistics and Linguistic Theory 20 (1): 153–81. https://doi.org/10.1515/cllt-2022-0083.\n\n\nSönning, Lukas, and Julia Schlüter. 2022. “Comparing Standard Reference Corpora and Google Books Ngrams: Strengths, Limitations and Synergies in the Contrastive Study of Variable h- in British and American English.” In Data and Methods in Corpus Linguistics, 17–45. Cambridge University Press. https://doi.org/10.1017/9781108589314.002.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Imbalance Across Predictor Levels Affects Data Summaries},\n  date = {2025-05-04},\n  url = {https://lsoenning.github.io/posts/2025-05-03-imbalance_bias/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Imbalance Across Predictor Levels Affects\nData Summaries.” May 4, 2025. https://lsoenning.github.io/posts/2025-05-03-imbalance_bias/."
  },
  {
    "objectID": "posts/2023-12-13_negative_binomial_parameterization/index.html",
    "href": "posts/2023-12-13_negative_binomial_parameterization/index.html",
    "title": "Different parameterizations of the negative binomial distribution",
    "section": "",
    "text": "R setup\nlibrary(lattice)\nlibrary(tidyverse)\nlibrary(MASS)\nlibrary(gamlss)\nlibrary(COUNT)\nlibrary(brms)\nlibrary(rstanarm)\nlibrary(knitr)\nlibrary(kableExtra)\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\nOne complication that arises when working with the negative binomial distribution is the fact that it can be written down in two different ways. These different parameterizations yield the same results for the mean of the distribution, i.e. the expected count or rate. Thus, if we are only interested in the occurrence rate (or normalized frequency) of an item, including a 95% statistical uncertainty interval, no problems arise. If we are interested in the negative binomial dispersion parameter, however, we must know which parameterization is implemented in the R package or R function we are using. The main purpose of this blog post is to divide R functions into two groups, depending on which version of the negative binomial distribution they use."
  },
  {
    "objectID": "posts/2023-12-13_negative_binomial_parameterization/index.html#footnotes",
    "href": "posts/2023-12-13_negative_binomial_parameterization/index.html#footnotes",
    "title": "Different parameterizations of the negative binomial distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor completeness, we provide the mathematical definition of the negative binomial distribution (more specifically, the NB2). The probability of a count y given x is \\(\\small{\\Pr(y\\;|\\;x) = \\frac{\\gamma(y+\\phi)}{y!\\;\\gamma(\\phi)} \\left(\\frac{\\phi}{\\phi + \\mu}\\right)^{\\phi} \\left(\\frac{\\mu}{\\phi + \\mu}\\right)^{y}}\\) where \\(\\small{\\gamma(\\dots)}\\) is the gamma function and \\(\\small{\\phi}\\) is the dispersion parameter. In R, we can use the function dNBI() in the package gamlss.↩︎\nDon’t be confused if the way the gamma distribution is introduced here does not correspond to descriptions you find in the literature. The distribution is actually characterized by two parameters \\(\\small{p_1}\\) and \\(\\small{p_2}\\) (see, e.g. Gelman et al. 2013, 578–79). For the negative binomial distribution, we need a gamma distribution that is centered at 1, which makes one of these parameters (i.e. \\(\\small{p_1}\\) or \\(\\small{p_2}\\)) redundant. The mean, or expected value, of a gamma distribution is given by \\(\\small{p_1/p_2}\\). In order for the mean to be 1, we have to set \\(\\small{p_1=p_2}\\), so the version of the gamma distribution we need for the NB distribution only has one parameter.↩︎"
  },
  {
    "objectID": "posts/2025-05-05_binomial_overdispersion/index.html",
    "href": "posts/2025-05-05_binomial_overdispersion/index.html",
    "title": "Modeling clustered binomial data",
    "section": "",
    "text": "A typical feature of corpus data is their hierarchical layout. Observations are usually clustered, which is the case if multiple data points are from the same text (or speaker). Observations from the same source are usually more similar to one another, reflecting idiosyncracies of the author/speaker or particularities of the context of language use. For binary outcome variables, there are different options for modeling such data. This blog post builds on a paper by Anderson (1988) and contrasts approaches that differ in the way they represent (or account for) the non-independence of data points.\n\n\nR setup\nlibrary(tidyverse)         # for data wrangling and visualization\nlibrary(marginaleffects)   # to compute model-based estimates\nlibrary(corpora)           # for data on passives\nlibrary(kableExtra)        # for drawing html tables\nlibrary(lattice)           # for data visualization\nlibrary(likelihoodExplore) # for drawing the binomial likelihood\nlibrary(gamlss)            # to fit a variant of the quasi-binomial model\nlibrary(aod)               # to fit a beta-binomial model\nlibrary(PropCIs)           # to calculate Wilson score CIs\nlibrary(doBy)              # to convert data from short to long format\nlibrary(lme4)              # to fit mixed-effects regression models\n\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nData: Passives in academic writing\nWe use data on the frequency of the passive in the Brown Family of corpora, which is part of the {corpora} package (Evert 2023). We concentrate on the genre Learned and consider texts from Brown and Frown.\n\nd &lt;- PassiveBrownFam |&gt; \n  filter(\n    genre == \"learned\",\n    corpus %in% c(\"Brown\", \"Frown\")) |&gt; \n  select(id, corpus, act, pass, verbs)\n\nThis leaves us with 160 texts:\n\nstr(d)\n\n'data.frame':   160 obs. of  5 variables:\n $ id    : chr  \"brown_J01\" \"brown_J02\" \"brown_J03\" \"brown_J04\" ...\n $ corpus: Factor w/ 5 levels \"BLOB\",\"Brown\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ act   : int  88 66 134 117 119 65 95 108 40 192 ...\n $ pass  : int  43 73 61 46 53 65 46 67 84 26 ...\n $ verbs : int  131 139 195 163 172 130 141 175 124 218 ...\n\n\nThere is one row per text and the following variables are relevant for our analyses:\n\nid text identifier\ncorpus source corpus (“Brown” vs. “Frown”)\nact number of active verb phrases in the text\npass number of passive verb phrases in the text\nverbs total number of verb phrases in the text\n\nFor each text, the frequency of the passive can be expressed as a proportion: the proportion of verb phrases that are in the passive voice. We add this variable to the data frame:\n\nd$prop_passive &lt;- d$pass/d$verbs\n\nWe use a dot diagram to inspect the distribution of these proportions across the 160 texts. In Figure 1, each dot represents a text, and the scores reflect the share of passive verb phrases among all verb phrases in the text document. We will refer to this quantity as the text-specific proportion of passive verb phrases.\n\n\ndraw figure\nd |&gt; \n  ggplot(aes(x = prop_passive)) +\n  geom_dotplot(method = \"histodot\", binwidth = .015, dotsize=.8) +\n  theme_dotplot() +\n  scale_x_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .25, .5, .75, 1),\n    labels = c(\"0\", \".25\", \".50\", \".75\", \"1\")) +\n  xlab(\"Proportion of passive verb phrases\")\n\n\n\n\n\n\n\n\nFigure 1: Dot diagram showing the proportion of passive verb phrases in the 160 texts.\n\n\n\n\n\nThe 160 texts also differ in the number of verb phrases they contain, so let us also look at this distribution. Figure 2 shows that this count varies between roughly 100 and 250.\n\n\ndraw figure\nd |&gt; \n  ggplot(aes(x = verbs)) +\n  geom_dotplot(method = \"histodot\", binwidth = 3, dotsize = .7) +\n    theme_dotplot() +\n    xlab(\"Number of verb phrases\")\n\n\n\n\n\n\n\n\nFigure 2: Dot diagram showing the distirbution of the number of verb phrases per text file.\n\n\n\n\n\nThe 160 texts can be considered a sample of academic prose from the language variety of interest, written American English in the second half of the 20th century. In selecting (or sampling) these 160 academic texts, the corpus compilers essentially selected a set of authors, or speakers, of this language variety. In some sense, these individuals represent the primary sampling units: Our sample size for making inference about a larger population of speakers is 160.\nEach text in the Brown Family of corpora is around 2,000 words long. A text excerpt, and the verb phrases it contains, can be considered as a sample from a (hypothetical) population, the academic prose produced by a specific author. At this level, the language use (or writing style) of this individual is the population of interest. The 2,000 words, (or, e.g., 160 verb phrases) then represent the secondary sampling units.\nThis means that we can use the information in the text to make inferences about the underlying propensity of the author(s) to use the passive voice in their academic writing. Texts with fewer verb phrases provide less information, and – due to sampling variation – we would expect smaller samples to yield more variable proportions (see Sönning and Schlüter 2022 for an illustration).\nThis is indeed the case for the present data. The point cloud in the Figure 3 below shows a trumpet-like shape: the highest proportions are from the texts with the fewest verb phrases. We should note, however, that other factors may contribute to this pattern: Thus, texts with fewer verb phrases necessarily feature longer (and presumably more elaborate) sentences, an indicator of abstract writing style that is also associated with passive usage.\n\n\ndraw figure\nd |&gt; ggplot(aes(x = verbs, y = prop_passive)) +\n  geom_point() +\n  theme_classic() +\n  scale_y_continuous(\n    limits = c(0,1), expand = c(0,0),\n    breaks = c(0, .5, 1),\n    labels = c(\"0\", \".5\", \"1\")) +\n  ylab(\"Proportion of passives\") +\n  xlab(\"Number of verb phrases in the text\")\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot showing the relation between the proportion of passives and the sample size (number of verb phrases in the text).\n\n\n\n\n\nTo emphasize the two-stage sampling design involved in corpus compilation, let us make visual inferences about the language use of the individual authors. To this end, we can construct a 95% confidence interval for each text-specific estimate. We will use the package {PropCIs} (Scherer 2018) to calculate 95% Wilson score confidence intervals for each of the 160 texts.\nFigure 4 presents text-level estimates of the proportion of passives with a 95% confidence interval. The degree of overlap among the 160 intervals can be interpreted as giving an indication of the heterogeneity of the individual authors. If the 160 authors showed a similar inclination toward the passive, we would observe considerable overlap among the error bars. Judging from the figure below, however, there seems to be appreciably heterogeneity.\n\n\ndraw figure\nci_upper &lt;- NA\nci_lower &lt;- NA\n\nfor(i in 1:160){\n  ci_lower[i] &lt;- scoreci(x = d$pass[i], n = d$verbs[i], conf.level = .95)$conf.int[1]\n  ci_upper[i] &lt;- scoreci(x = d$pass[i], n = d$verbs[i], conf.level = .95)$conf.int[2]\n}\n\nxyplot(1~1, type = \"n\", ylim=c(0,1), xlim = c(0,163),\n       par.settings = my_settings, axis = axis_left,\n       scales = list(\n         y = list(\n               at = c(0, .5, 1),\n               label = c(\"0\", \".5\", \"1\"))),\n       ylab = \"Proportion of passives\", xlab = NULL,\n       panel = function(x,y){\n         panel.points(x=c(1:80, 83:162), y = d$prop_passive)\n         panel.segments(x0=c(1:80, 83:162), x1 = c(1:80, 83:162), y0 = ci_lower, y1 = ci_upper)\n         panel.segments(x0=1, x1=80, y0=0, y1=0)\n         panel.segments(x0=83, x1=162, y0=0, y1=0)\n         panel.text(x=c(40.5, 122.5), y = -.1, label = c(\"Brown\", \"Frown\"))\n         panel.text(x=162, y = -.25, label=\"Error bars: 95% CIs\", col = \"grey40\", cex = .8, adj=1)\n       })\n\n\n\n\n\n\n\n\nFigure 4: Estimated proportion of passives for all texts in the data (grouped by corpus), presented with a 95% confidence interval.\n\n\n\n\n\n\n\nBinomial model\nWe start with a simple binomial model, which basically ignores the structure of the data. It uses a single parameter to express the mean proportion of the passive in the dataset. This means that it essentially treats all verb phrases in the data (n = 26772) as an unstructured sample from the population of interest, each one drawn independently of the other ones. The way the data are presented to the model, with one row per text, does not matter to the binomial model – it produces the same result if we supply just one row, with verbs representing the total number of verb phrases and pass the total number of passives in the data.\nSince the clustering variable Text is not taken into account, the model rests on the assumption that the 160 texts share the same underlying relative frequency of the passive. This means that the authors are assumed to be perfectly homogeneous with respect to their stylistic preferences. Under this model, the observed variability in proportions is merely a result of sampling variation, which in turn depends on (i) the number of verb phrases in the text, and (ii) the overall proportion of the passive.\nPoint (ii) deserves some more comment. For binomial data, sampling variation is smaller the closer we get to 0 and 1. This is due to the boundedness of the scale – near 0 or 1, there is less room for variation. Statistically speaking, the variance of the binomial distribution depends on its mean. As Figure 5 illustrates, it is greatest at .50 and decreases toward the endpoints of the proportion scale. This means that the variability of observed proportions depends on the mean of the binomial distribution.\n\n\ndraw figure\nxyplot(\n  1~1, type = \"n\", xlim=c(0,1), ylim = c(0,.25),\n  par.settings = my_settings, axis = axis_L,\n  scales = list(y = list(\n    at = c(0, .1, .2, .3, .4, .5),\n    label = c(\"0\", \".1\", \".2\", \".3\", \".4\", \".5\")),\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      labels = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  ylab = \"Variance\",\n  xlab = \"Mean\",\n  panel = function(x,y){\n    panel.points(x = seq(0, 1, .01),\n                 y = (seq(0, 1, .01)*(1-seq(0, 1, .01))),\n                 type=\"l\")\n       })\n\n\n\n\n\n\n\n\nFigure 5: Mean-variance relationship in the binomial model.\n\n\n\n\n\nWe can fit this model in R using the glm() function:\n\nm &lt;- glm(\n  cbind(pass, act) ~ 1, \n  data = d, \n  family = \"binomial\")\n\nThe model intercept is -1.37, which is the mean probability of the passive, expressed on the log odds scale. We can use the function plogis() to back-transform to the proportion scale:\n\nround(\n  plogis(coef(m)), 3)\n\n(Intercept) \n      0.203 \n\n\nA 95% CI can be constructed using the function confint():\n\nplogis(confint(m))\n\n    2.5 %    97.5 % \n0.1981468 0.2077818 \n\n\nModel-based estimates on the proportion scale are easy to obtain using the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024). The function avg_predictions() returns a model-based prediction of the mean probability of a passive verb phrase in the population of interest, along with a 95% CI.\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.198     0.208\n\n\nThe estimated proportion of .203 comes with a crisp 95% CI, which ranges from .198 to .208. Apparently, the model is very confident in its predicted probability of the passive voice.\nTo check how well the binomial model fits the data, we can ask it to “retrodict” the data, i.e. to tell us what it thinks the distribution of the 160 text-level proportions looks like. For this task, the binomial model uses the estimated overall proportion (.203) and the sample size for each text, i.e. the number of verb phrases it contains. It can then produce a density curve for each text, which is centered on .203 and spread out in accordance with the sample size: for a text with more verb phrases, the density curve is more peaked, and for a text with fewer verb phrases it is spread more widely around .203. Importantly, however, all density curves are centered on .203, the estimate of the population proportion.\nFigure 6 draws the 160 density curves against the observed distribution of text-specific proportions. Apparently, the model fails to capture the heterogeneity among texts.\n\n\ndraw figure\nxyplot(\n  1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.16),\n  par.settings = my_settings, axis = axis_bottom,\n  scales = list(\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab.top = \"Binomial model\\n\",\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.dotdiagram(d$prop_passive, scale_y = .006, n_bins=50, set_col=\"grey\")\n    for(i in 1:160){\n      panel.points(\n        x = seq(0, 1, length = 100),\n        y = exp(likbinom(\n          x = round(d$verbs[i]*.203), \n          size = d$verbs[i], \n          prob = seq(0,1, length = 100), log = TRUE)) /\n          sum(exp(likbinom(\n            x = round(d$verbs[i]*.203), \n            size = d$verbs[i], \n            prob = seq(0,1, length = 100), log = TRUE))),\n        type = \"l\", alpha = .1)\n      }\n    panel.text(x = .4, y = .06, label = \"Observed distribution\", col = \"grey40\", cex = .9, adj = 0)\n    panel.text(x = .25, y = .13, label = \"Expected distribution\", col = 1, cex = .9, adj = 0)\n    })\n\n\n\n\n\n\n\n\nFigure 6: Fit between the binomial model and the data.\n\n\n\n\n\nIf the observed data show greater variation than anticipated by a statistical model, the data are said to be overdispersed relative to this model. Alternative modeling approaches take into account this overdispersion, or heterogeneity. As we will see, however, they do so in different ways.\n\n\nQuasi-binomial model including a heterogeneity parameter\nA quasi-binomial model includes a second parameter that explicitly captures the excess variation in the data (see Agresti 2013, 150–51). This dispersion parameter adjusts the variance of the binomial distribution and is often denoted as \\(\\phi\\). It is estimated on the basis of a global \\(\\chi^2\\) fit statistic for the model.\nIn this way, the quasi-binomial model allows the standard deviation of observed proportions to be greater than anticipated by the simple dependency on the mean of the distribution, which we saw in Figure 5 above. Essentially, the dispersion parameter is a multiplicative factor that adjusts the variance of the binomial distribution upwards. If the dispersion parameter is 1, the model reduces to the binomial model discussed above. The dispersion parameter also affects inferences from the model: standard errors are multiplied by \\(\\sqrt{\\phi}\\).\nWe can fit a quasi-binomial model using the glm() function in R:\n\nm &lt;- glm(\n  cbind(pass, act) ~ 1, \n  data = d, \n  family = \"quasibinomial\")\n\nThen we use the {marginaleffects} package to obtain the model-based predicted probability of a passive verb phrase (+ 95% CI):\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.186      0.22\n\n\nThis produces the same estimate as above, but with a wider uncertainty interval. The model intercept, once back-transformed to the probability scale, yields the same estimate:\n\nround(\n  plogis(coef(m)), 3)\n\n(Intercept) \n      0.203 \n\n\nThe heterogeneity factor adjusts the variance of the binomial distribution to align the model with the excess variability in the observed proportions. However, the model still assumes a constant underlying proportion for all authors. The heterogeneity parameter basically states that some perturbation, perhaps caused by omitted predictors or positively correlated (i.e. non-independent) observations in each row of the table, leads to greater variability of the observed proportions. The model does not point to a specific source of the overdispersion.\nSince the variance is increased proportionally to the overdisersion parameter, the bell-shaped curves are spread out more widely. This is illustrated in the figure below. We see that the fit between model and data is much better now.\n\n\ndraw figure\nxyplot(\n  1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.06),\n  par.settings = my_settings, axis = axis_bottom,\n  scales = list(\n    x = list(\n      at = c(0, .25, .5, .75, 1),\n      label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab.top = \"Quasi-binomial model\\n\",\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.dotdiagram(d$prop_passive, scale_y = .004, n_bins=50, set_col=\"grey\")\n    for(i in 1:160){\n      interpolated_to_100_steps &lt;- approx(\n        x = (0:d$verbs[i])/d$verbs[i],\n        y = dDBI(0:d$verbs[i], mu=.203, sigma=11, bd=d$verbs[i]),\n        xout = seq(0, 1, length=100))\n      \n      panel.points(x = interpolated_to_100_steps$x,\n                   y = interpolated_to_100_steps$y/sum(interpolated_to_100_steps$y),\n                   type = \"l\", alpha = .1)\n    }\n    panel.text(x = .48, y = .03, label = \"Observed distribution\", \n               col = \"grey40\", cex = .9, adj = 0)\n    panel.text(x = .25, y = .055, label = \"Expected distribution\", \n               col = 1, cex = .9, adj = 0)\n  })\n\n\n\n\n\n\n\n\nFigure 7: Fit between the quasi-binomial model and the data.\n\n\n\n\n\nHowever, the quasi-binomial model does not represent the structure in our data. It does not attribute the excess variation in proportions to the fact that we are looking at 160 different texts, from speakers who may very well show different stylistic attitudes toward passive usage. Rather, it states that some noise variable increased the sampling variation when drawing verb phrases from each speaker, with speakers nevertheless being actually homogeneous with respect to their underlying usage rate of the passive.\nWhile the quasi-binomial model effectively adjusts inferences for the non-independence of observations, the way in which this is achieved may not be appropriate in all situations (see Finney 1971, 72). In particular, if the data are clustered, this information should be explicitly taken into account. The models we consider next embrace the data structure and introduce parameters that describe between-cluster variation, thereby linking overdispersion to a specific source. As noted by Agresti (2013, 151), this approach is preferable, because it actually models the observed heterogeneity.\n\n\nBeta-binomial model\nThe beta-binomial model also includes a second parameter, but this parameter has a different function (and interpretation). It explicitly allows for the possibility that the texts in the data differ in the underlying probability of passive usage. This parameter aims to represent the distribution of text-specific proportions, which means that it actively takes into account the clustering variable Text. If texts vary considerably, reflecting large overdispersion relative to the binomial model, the parameter describing the text-to-text variation will be large. If there is no evidence for surplus variation among texts, the beta-binomial model reduces to the binomial model. The relationship between the binomial mean and variance (see Figure 5) therefore remains unaltered.\nSince the text-specific proportions are bounded between 0 and 1, a distribution that respects these limits must be used. In the case of the beta-binomial model, this is the beta distribution. As discussed in more detail in this blog post, the beta distribution has two parameterizations. It can be defined using two so-called shape parameters, or it can be defined using a mean and a standard deviation parameter. The mean, in our case, is the model-based overall mean proportion of passive verb phrases.\nWe can fit a beta-binomial model using the function betabin() in the R package {aod} (Lesnoff et al. 2012):\n\nm &lt;- betabin(\n  cbind(pass, act) ~ 1, \n  ~ 1, \n  data = d)\n\nThe parameter controlling the spread of the text-specific proportions is termed \\(\\phi\\). It can be extracted from the model object as follows:\n\nm@random.param\n\nphi.(Intercept) \n      0.0649405 \n\n\nThe \\(\\phi\\) parameter returned by aod::betabin() is the reciprocal of the standard deviation of the beta distribution, so we convert it:\n\nsd_beta &lt;- 1/m@random.param\n\nThis gives us the mean and standard deviation of the beta distribution that describes the variability among texts. To graph this distribution, we need to translate these parameters into shape parameters (see this blog post):\n\nmuphi_to_shapes &lt;- function(mu, phi) {\n  shape1 &lt;- mu * phi\n  shape2 &lt;- (1 - mu) * phi\n  return(list(shape1 = shape1, shape2 = shape2))\n}\n\nshape_parameters &lt;- muphi_to_shapes(\n  mu = plogis(coef(m)), \n  phi = 1/m@random.param)\n\nWe can now graph the beta distribution:\n\n\ndraw figure\nxyplot(\n  1~1, type = \"n\", xlim=c(0,1), ylim = c(0,5),\n  par.settings = my_settings, axis = axis_bottom,\n  scales = list(x = list(\n      at = c(0, .25, .5, .75, 1),\n      labels = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n  xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n  panel = function(x,y){\n    panel.points(x = seq(0, 1, .01),\n                 y = dbeta(seq(0, 1, .01), \n                           shape1 = shape_parameters$shape1,\n                           shape2 = shape_parameters$shape2),\n                 type=\"l\")\n    panel.text(x=.4, y=5, label=\"Beta distribution with parameters:\", \n               col = \"grey40\", cex=.8, adj=0)\n    panel.text(x=.45, y = 3.5, label = \"\\u2022 Mean = 0.21; SD = 15.4\",\n               col = \"grey40\", cex=.8, adj=0)\n    panel.text(x=.45, y = 2.5, label = \"\\u2022 Shape 1 = 3.3; Shape 2 = 12.1\",\n               col = \"grey40\", cex=.8, adj=0)\n       })\n\n\n\n\n\n\n\n\nFigure 8: Beta density describing the distirbution of text-specific proportions.\n\n\n\n\n\nThe intercept of the beta-binomial model translates into a slightly higher mean probability of the passive:\n\nplogis(coef(m))\n\n(Intercept) \n  0.2137407 \n\n\nThe function avg_predictions() returns the same estimate, along with an appropriately wide confidence interval:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.214    0.197     0.231\n\n\nFinally, we check the fit between model and data visually (see Figure 9). The beta distribution appears to capture the spread of the observed text-specific proportions quite well. We should note, however, that the density curve does not capture the additional variation among the observed proportions that is due to sampling variation. The dots are therefore expected to be spread out more widely, albeit only slightly so due to the large sample sizes (see Figure 2).\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = my_settings, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Beta-binomial model\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = -.125,, seq_max = 1)\n         panel.points(x = seq(0, 1, length=100),\n                      y = dbeta(x = seq(0, 1, length=100),\n                                shape1 = shape_parameters$shape1, \n                                shape2 = shape_parameters$shape2)/200, type = \"l\")\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 9: Fit between the beta-binomial model and the data.\n\n\n\n\n\nSince the variation in text-specific rates is represented using a probability distribution, we can use the beta-binomial model to describe this variation in informative ways. For instance, we may be interested in the interquartile range, i.e. the spread of the central 50% of the proportions.\n\nqbeta(\n  c(.25, .75), \n  shape1 = shape_parameters$shape1, \n  shape2 = shape_parameters$shape2) |&gt; \n  round(3)\n\n[1] 0.138 0.277\n\n\nOr, seeing that the the model estimate is the mean over the text-specific proportions, we may instead be interested in the median proportion:\n\nqbeta(\n  .5, \n  shape1 = shape_parameters$shape1, \n  shape2 = shape_parameters$shape2) |&gt; \n  round(3)\n\n[1] 0.201\n\n\nWe now move on to a class of models that may be more familiar to many researchers: Mixed-effects regression models.\n\n\nRandom-effects model with identity link\nWe start with an ordinary random-effects regression model, which models the data on the proportion scale. This kind of model fails to respect the scale limits and does not account for the relationship between the binomial mean and variance (see Figure 5).\nTo run this model, we first need to convert the data from frequency to case form, so that each row in the data represents a verb phrase:\n\nd_long &lt;- binomial_to_bernoulli_data(\n  response_name = \"passive\",\n  data = d, \n  y = pass,\n  size = verbs, \n  type = \"total\"  \n)\nd_long$passive &lt;- as.numeric(d_long$passive) - 1\n\nNow we fit the model using the function lmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- lmer(\n  passive ~ (1|id), \n  data = d_long)\n\nLet us first look at model-based predictions, which look fine:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.185     0.221\n\n\nHowever, these differ from the model intercept, which is higher:\n\nfixef(m)\n\n(Intercept) \n  0.2123103 \n\n\nWe will return to this discrepancy, which is due to the fact that these are two different means, at the end of this section.\nLet us look at the fit between model and data. The ordinary random-intercept model assumes that the text-specific proportions are distributed normally around the overall mean. It describes this distribution using a standard deviation parameter, which can be obtained as follows:\n\nsummary(m)$varcor$id[1] |&gt; \n  sqrt()\n\n[1] 0.11225\n\n\nIn Figure 10, the density curve shows what the model thinks the data look like, which does not match the observed distribution. In fact, the symmetric bell-shaped curve provides a rather poor fit to the text-specific proportions: It expects negative proportions (dotted part of the curve), and it fails to capture the scale-induced asymmetry of the distribution.\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = my_settings, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Ordinary regression with random intercepts\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = -.125,, seq_max = 1)\n         panel.points(x = seq(-.2, 0, length=20),\n                      y = dnorm(x = seq(-.2, 0, length=20),\n                                mean = .203, sd = .1122)/190, type = \"l\", lty = \"13\")\n         \n         panel.points(x = seq(0, 1, length=100),\n                      y = dnorm(x = seq(0, 1, length=100),\n                                mean = .203, sd = .1122)/190, type = \"l\")\n\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 10: Fit between the ordinary random-effects model and the data.\n\n\n\n\n\nFigure 10 helps us understand the discrepancy between the back-transformed model intercept (.212) and the mean prediction produced by the {marginaleffects} package (.203). The model intercept represents the center of the bell-shaped curve in Figure 10. If the text proportions were indeed distributed symmetrically around this center, the intercept would coincide with the mean prediction. This is because the default way in which the {marginaleffects} package calculates mean predictions from a model with random effects proceeds in two steps: First, a prediction is made for each cluster (here: text), based on the cluster-specific random intercept. In step 2, these cluster predictions are averaged.\nIf the cluster-specific means form a symmetric distribution, they will tend to cancel out, leading to no (or very minor) discrepancies between the two types of prediction. In the present case, the text-specific predictions do not form a symmetric pile. The center of gravity is below .212, and the mean over the estimated asymmetric distribution of cluster proportions therefore lower.\nWe can retrieve both types of means using the {marginaleffects} package. The model intercept is equivalent to ignoring (or “turning off”) the clustering variable when making predictions. This can be done as follows:\n\navg_predictions(\n  m, \n  newdata = datagrid(\n    id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.194      0.23\n\n\nThe following code, in contrast, works the estimated distribution of cluster-level proportions into the predictions:\n\navg_predictions(\n  m, \n  re.form = ~(1|id)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.203    0.185     0.221\n\n\n\n\nRandom-effects model with logit link\nFinally, we fit a logistic random-effects model, which uses a logit link function to respect scale constraints. This means that the proportions are not modeled directly, but instead on the unbounded logistic (or log-odds) scale.\nWe can fit the model with the glmer() function in the {lme4} package:\n\nm &lt;- glmer(\n  cbind(pass, act) ~ 1 + (1|id), \n  data = d, \n  family = binomial)\n\nHere is the mean prediction we get using the {marginaleffects} package:\n\navg_predictions(m) |&gt; \n  tidy() |&gt; \n  select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.195     0.229\n\n\nAgain, we note that it differs from the back-transformed model intercept, which is now lower:\n\nplogis(fixef(m))\n\n(Intercept) \n  0.1933644 \n\n\nWe are again dealing with two different types of means. Before we consider how this discrepancy arises, we take a closer look at the structure of the model.\nIn this model, the random-intercept SD represents the variation between texts on the logit scale. We can extract the SD parameter as follows:\n\nsummary(m)$varcor$id[1] |&gt; \n  sqrt()\n\n[1] 0.6599597\n\n\nThe model therefore assumes that the distribution of the random intercepts is symmetric on the logit scale. We can graph the text-specific proportions on the logit scale and overlay the expected distribution. The distribution of logits seems to be quite well approximated by the normal density curve.\n\n\ndraw figure\nxyplot(1 ~ 1, type = \"n\", xlim=c(-3.5, 1.1), ylim = c(0,.02),\n       par.settings = my_settings, axis = axis_bottom,\n       xlab.top = \"Logistic regression with random intercepts\\n\",\n       xlab = \"Text-specific logit of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(qlogis(d$prop_passive), scale_y = .0012, n_bins=50,\n                          set_col=\"grey60\", seq_min = -3.5, seq_max = .9)\n         panel.points(x = seq(-3.5, .8, .1),\n                      y = dnorm(seq(-3-5, .8, .1), mean = fixef(m), sd = .66)/50,\n                      type = \"l\")\n         panel.text(x = -2, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 1)\n         panel.text(x = -.75, y = .011, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 11: Fit between text-specific logits and the random-intercept distribution assumed by the model.\n\n\n\n\n\nLet us also look at the fit between model and data on the proportion scale. Figure 12 shows that the match is also pretty good on this scale.\n\n\ndraw figure\ninterpolated_to_100_steps &lt;- approx(\n        x = plogis(seq(-4, 1, length = 300)),\n        y = dnorm(seq(-4, 1, length = 300), mean=-1.42830, sd=.66),\n        xout = seq(0, 1, length=100))\n\nxyplot(1 ~ 1, type = \"n\", xlim=c(0,1), ylim = c(0,.03),\n       par.settings = my_settings, axis = axis_bottom,\n       scales = list(\n         x = list(\n               at = c(0, .25, .5, .75, 1),\n               label = c(\"0\", \".25\", \".50\", \".75\", \"1\"))),\n       xlab.top = \"Logistic regression with random intercepts\\n\",\n       xlab = \"Text-specific proportion of passive verb phrases\", ylab = NULL,\n       panel = function(x,y){\n         panel.dotdiagram(d$prop_passive, scale_y = .002, n_bins=80, set_col=\"grey60\", seq_min = 0, seq_max = 1)\n         panel.points(x = interpolated_to_100_steps$x,\n                      y = interpolated_to_100_steps$y/30, type = \"l\")\n         panel.text(x = .48, y = .015, label = \"Observed distribution\", \n                    col = \"grey40\", cex = .8, adj = 0)\n         panel.text(x = .25, y = .03, label = \"Expected distribution\", \n                    col = 1, cex = .8, adj = 0)\n       })\n\n\n\n\n\n\n\n\nFigure 12: Fit between the logistic random-effetcs model and the data.\n\n\n\n\n\nWe can again use this probability distribution to summarize text-to-text variation. The interquartile range based on this model is\n\nplogis(\n  qnorm(\n    c(.25, .75), \n    mean = fixef(m), \n    sd = .66)) |&gt; \n  round(3)\n\n[1] 0.133 0.272\n\n\nThe median proportion coincides with the back-transformed model intercept:\n\nplogis(\n  qnorm(\n    .5, \n    mean = fixef(m), \n    sd = .66)) |&gt; \n  round(3)\n\n[1] 0.193\n\n\nLet us now consider the discrepancy between the back-transformed model intercept and the mean prediction returned by the {marginaleffects} package.\nThe model intercept represents the center of the bell-shaped curve in Figure 11. This means that by-text random intercepts are averaged on the logit scale, and this mean over text-specific logits is then back-transformed to the proportion scale. This estimate is often referred to as a conditional/cluster-specific estimate.\nIn contrast, the default average prediction returned by {marginaleffects} first back-transforms the text-specific logits to the proportion scale and then calculates a mean over text-specific proportions, i.e. the distribution in Figure 12. This estimate is often referred to as a marginal/population-averaged estimate. The difference between these means (or estimates) is whether the averaging over clusters (here: texts) was done on the logit or the proportion scale.\nWe can use the {marginaleffects} package to produce both kinds of averages. We get the mean over text-specific logits by ignoring (or “turning off”) the clustering variable when making predictions. This can be done as follows:\n\navg_predictions(\n  m, \n  newdata = datagrid(\n    id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.193    0.177      0.21\n\n\nThe following code, in contrast, works the estimated distribution of cluster-level proportions into the predictions:\n\navg_predictions(\n  m, \n  re.form = ~(1|id)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(3)\n\n# A tibble: 1 × 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.212    0.195     0.229\n\n\n\n\nComparison\nFigure 13 brings together the estimates from the different models. The first thing we note is that, in terms of proposed statistical precision, they form two groups: The binomial model is the odd one out, with a very narrow confidence interval on the estimated proportion. All other models produce confidence intervals that are very similar in length.\nThe second thing to note is that estimates form three groups:\n\nThe lowest estimate is the conditional mean based on the logistic random-effects model. It is the mean over cluster-specific logits, back-transformed to the proportion scale.\nThe marginal mean from the same model, which is the mean over cluster-specific proportions (nearly) coincides with the mean proportion returned by the beta-binomial model, and (interestingly) the conditional estimate from the ordinary random-effects model.\nThe third group, which yields an intermediate predicted proportion, is formed by the binomial, the quasi-binomial, and the marginal estimate from the ordinary random-effects model.\n\n\n\ndraw figure\ncomp_models &lt;- tibble(\n  model = c(\"Binomial\", \"Quasi-binomial\", \"Beta-binomial\", \n                \"Ordinary random-effects (conditional)\",\n                \"Ordinary random-effects (marginal)\",\n                \"Logistic random-effects (conditional)\",\n                \"Logistic random-effects (marginal)\"),\n  estimate = c(pred_binomial$estimate,\n               pred_quasibin$estimate,\n               pred_betabin$estimate,\n               pred_ord_ranef_c$estimate,\n               pred_ord_ranef_m$estimate,\n               pred_log_ranef_c$estimate,\n               pred_log_ranef_m$estimate),\n  ci_lower = c(pred_binomial$conf.low,\n               pred_quasibin$conf.low,\n               pred_betabin$conf.low,\n               pred_ord_ranef_c$conf.low,\n               pred_ord_ranef_m$conf.low,\n               pred_log_ranef_c$conf.low,\n               pred_log_ranef_m$conf.low),\n  ci_upper = c(pred_binomial$conf.high,\n               pred_quasibin$conf.high,\n               pred_betabin$conf.high,\n               pred_ord_ranef_c$conf.high,\n               pred_ord_ranef_m$conf.high,\n               pred_log_ranef_c$conf.high,\n               pred_log_ranef_m$conf.high)\n)\n\ncomp_models$model &lt;- factor(\n  comp_models$model,\n  levels = c(\"Binomial\", \"Quasi-binomial\", \"Beta-binomial\", \n                \"Ordinary random-effects (conditional)\",\n                \"Ordinary random-effects (marginal)\",\n                \"Logistic random-effects (conditional)\",\n                \"Logistic random-effects (marginal)\"),\n  ordered = TRUE\n)\n\ncomp_models |&gt; \n  ggplot(aes(y = model, x = estimate)) +\n  geom_vline(xintercept = c(.1935, .203, .213), color = \"grey95\", lwd=3) +\n  geom_point() +\n  geom_linerange(aes(xmin = ci_lower, xmax = ci_upper)) +\n  scale_x_continuous(breaks = seq(.18, .23, .01), \n                     labels = c(\".18\", \".19\", \".20\", \".21\", \".22\", \".23\")) +\n  theme_classic_ls() +\n  xlab(\"Estimated proportion of passives\") +\n  ylab(NULL)\n\n\n\n\n\n\n\n\nFigure 13: Comparison of model-based mean predictions.\n\n\n\n\n\n\n\nSummary\nWe discussed different approaches to modeling clustered binomial data. These differ in the way they address the resulting non-independence of observations in the data. If a clustering variable is present, it is generally preferable to use a model that links the observed non-independence to this source. These (proper) modeling approaches represent the observed variation across clusters in different ways, and they yield different types of estimates for the mean proportion in the population of interest. We saw how the {marginaleffects} package can be used to construct these mean predictions, which mainly differ in terms of the scale on which they average over cluster-specific quantities.\n\n\n\n\n\nReferences\n\nAgresti, Alan. 2013. Categorical Data Analysis. Hoboken, NJ: Wiley.\n\n\nAnderson, Dorothy A. 1988. “Some Models for Overdispersed Data.” Australian Journal of Statistics 30 (2): 125–48. https://doi.org/10.1111/j.1467-842x.1988.tb00844.x.\n\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nEvert, Stephanie. 2023. Corpora: Statistics and Data Sets for Corpus Frequency Data. https://doi.org/10.32614/CRAN.package.corpora.\n\n\nFinney, David J. 1971. Probit Analysis. New York: Cambridge University Press.\n\n\nLesnoff, M., Lancelot, and R. 2012. Aod: Analysis of Overdispersed Data. https://cran.r-project.org/package=aod.\n\n\nScherer, Ralph. 2018. PropCIs: Various Confidence Interval Methods for Proportions. https://doi.org/10.32614/CRAN.package.PropCIs.\n\n\nSönning, Lukas, and Julia Schlüter. 2022. “Comparing Standard Reference Corpora and Google Books Ngrams: Strengths, Limitations and Synergies in the Contrastive Study of Variable h- in British and American English.” In Data and Methods in Corpus Linguistics, 17–45. Cambridge University Press. https://doi.org/10.1017/9781108589314.002.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Binomial Data},\n  date = {2025-05-09},\n  url = {https://lsoenning.github.io/posts/2025-05-05_binomial_overdispersion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Binomial Data.”\nMay 9, 2025. https://lsoenning.github.io/posts/2025-05-05_binomial_overdispersion/."
  },
  {
    "objectID": "posts/2025-05-12_poisson_random_intercepts/index.html",
    "href": "posts/2025-05-12_poisson_random_intercepts/index.html",
    "title": "Frequency estimates based on random-intercept Poisson models",
    "section": "",
    "text": "When modeling frequency counts, the Poisson model is often inappropriate since the observed variation from text to text (or speaker to speaker) is greater than anticipated by this simple model. The observed overdispersion can be addressed using Poisson mixture models, which include an additional parameter that captures the variation among texts. A frequently used variant is the negative binomial model (also called a Poisson-gamma mixture model), which represents text-to-text variation using a gamma distribution. This blog post discusses another option, a random-effects Poisson regression model (also called a Poisson-lognormal mixture model). We look at the structure of this model and how it represents text-to-text variation, and draw comparisons with the negative binomial model. We will see that a Poisson random-intercept model yields two different types of average frequencies.\n\n\nR setup\nlibrary(tidyverse)          # for data wrangling and visualization\nlibrary(dataverse)          # for downloading data from TROLLing\nlibrary(marginaleffects)    # to compute model-based estimates\nlibrary(MASS)               # to fit a negative binomial regression model\nlibrary(kableExtra)         # for drawing html tables\nlibrary(lme4)               # to fit mixed-effects regression models\nlibrary(lattice)            # for data visualization\nlibrary(gamlss)             # for drawing gamma densities\n\n# pak::pak(\"lsoenning/uls\") # install package \"uls\"\nlibrary(uls)                # for ggplot2 dotplot theme\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative data records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). For more information on the dataset, please refer to Sönning and Krug (2021).\nWe start by downloading the data from TROLLing and rename a few variables for clarity.\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nd &lt;- dat |&gt; \n  dplyr::select(-c(Exact_age, age_bins)) |&gt; \n  rename(\n    n_actually = count,\n    n_words = total,\n    age_group = Age_range,\n    gender = Gender\n  )\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age and gender is missing.\n\nd &lt;- d |&gt; \n  filter(\n    n_words &gt; 100,\n    !(is.na(gender)))\n\nIn this blog post, we will concentrate on speakers between 19 and 29 years of age.\n\nd &lt;- d |&gt; \n  filter(age_group == \"19-29\") |&gt; \n  droplevels()\n\nWe add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per thousand words’:\n\nd$rate_ptw &lt;- (d$n_actually / d$n_words) * 1000\n\nThe data subset used in the present blog post includes 247 speakers and the following variables:\n\nan ID (speaker)\nthe number of times they used actually (n_actually)\nthe age group (age_group)\nself-reported gender (gender)\nthe total number of words contributed to the corpus by the speaker (n_words), and\nthe usage rate of actually, expressed as ‘per thousand words’ (rate_ptw)\n\n\nstr(d)\n\n'data.frame':   247 obs. of  6 variables:\n $ speaker   : chr  \"S0002\" \"S0003\" \"S0007\" \"S0009\" ...\n $ n_actually: int  21 8 6 0 0 1 21 39 1 0 ...\n $ age_group : chr  \"19-29\" \"19-29\" \"19-29\" \"19-29\" ...\n $ gender    : chr  \"Female\" \"Female\" \"Male\" \"Female\" ...\n $ n_words   : int  8535 1893 11276 533 473 605 8209 13449 417 304 ...\n $ rate_ptw  : num  2.46 4.226 0.532 0 0 ...\n\n\n \n\n\nFocus of analysis\nThe key interest in the following is in the usage rate of actually (expressed as a normalized frequency) among young British adults in conversational speech. In the following, the terms normalized frequency, occurrence rate, and usage rate will be used interchangeably.\n\n\nData description\nWe start by inspecting some key characteristics of the data. First we examine the distribution of speakers by Gender. The ratio of female and male speakers is 3 to 2:\n\ntable(d$gender)\n\n\nFemale   Male \n   148     99 \n\n\nNext, we consider the distribution of word counts across speakers (i.e. the total number of word tokens each person contributed to the corpus). Figure 1 shows a very skewed profile, with a few speakers showing disproportionately high word counts.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = n_words)) + \n  geom_dotplot(binwidth = 3000, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 200000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers from the Spoken BNC2014 aged 19 to 29, excluding individuals who contribute fewer than 100 words to the corpus.\n\n\n\n\n\nTo see how the outcome variable is distributed at the speaker level, we draw a dot diagram of the speaker-specific usage rate of actually, expressed as “per thousand words”. Figure 2 shows a skewed arrangement, with a few individuals using the word at an exceptionally high rate.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .1, stackratio = 1, method = \"histodot\", dotsize = .8) +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0.002, 0.002)) +\n  scale_x_continuous(expand = c(0.002, 0.002)) +\n  xlab(\"Speaker-specific usage rate of actually (per thousand words)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words.\n\n\n\n\n\n\n\nNegative binomial regression\nFor a point of reference, we start by fitting a negative binomial regression model. This model takes into account the speakers, and represents the observed variability in the usage rate of actually using a probability distribution. The model therefore includes an additional parameter that represents the variability of usage rates. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates.\nThis is illustrated in Figure 3, which shows considerable variation among speakers. The vertical line marks a ratio of 1, which represents speakers whose usage rate coincides with the model-based average usage rate of actually (which , as we will see shortly, is 1.6 per thousand words). The density curve shows the distribution of speakers across multiplicative factors ranging from 0 to 3. A ratio of 0.5 represents a speaker whose usage rate of actually is only half as large as the overall average, and a ratio of 2 refers to speakers whose usage rate is twice as large as the overall average.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(0, 3.1), ylim=c(0,1),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,1,2,3,4))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1, col=1)\n    panel.points(x = seq(.01, 4.2, length=1000),\n                 y = dGA(seq(.01, 4.2, length=1000), mu=1, sigma=(1/sqrt(3.4363))),\n                 type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 3: The gamma distribution describing between-speaker variability (in multiplicative terms) in the usage rate of actually.\n\n\n\n\n\nSince this is a probability distribution, we can summarize the estimated distribution of speakers. The following code finds the quartiles of the distribution:\n\nqGA(\n  p = c(.25, .5, .75), \n  mu = 1, \n  sigma = 1/sqrt(3.4363)) |&gt; \n  round(2)\n\n[1] 0.60 0.90 1.29\n\n\nThis tells us that ratios of 0.60 and 1.29 mark the interquartile range: The central 50% of the speakers are within this interval. Interestingly, and perhaps counterintuitively, the median of this gamma distribution is 0.90 (rather than 1), meaning that half of the speakers have a ratio below this mark. Let us also see how many speakers have ratio above and below 1:\n\npGA(\n  q = 1, \n  mu = 1, \n  sigma = 1/sqrt(3.4363)) |&gt; \n  round(2)\n\n[1] 0.57\n\n\n57% of the speakers have a ratio below 1, meaning that more than half of the speakers actually show a usage rate below the estimated mean. We will return to this rather puzzling feature of the negative binomial model further below.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm_nb &lt;- MASS::glm.nb(\n  n_actually ~ 1 + offset(log(n_words)),\n  data = d)\n\nThis produces the following regression table:\n\nsummary(m_nb)\n\n\nCall:\nMASS::glm.nb(formula = n_actually ~ 1 + offset(log(n_words)), \n    data = d, init.theta = 3.436299959, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.43610    0.04266  -150.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.4363) family taken to be 1)\n\n    Null deviance: 275.14  on 246  degrees of freedom\nResidual deviance: 275.14  on 246  degrees of freedom\nAIC: 1516.1\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.436 \n          Std. Err.:  0.462 \n\n 2 x log-likelihood:  -1512.118 \n\n\nThe intercept of negative binomial model represents its estimate of the average frequency of actually:\n\ncoef(m_nb)\n\n(Intercept) \n  -6.436104 \n\n\nWe can back-transform it to the scale of normalized frequencies (per thousand words):\n\nround(\n  exp(coef(m_nb)) *1000, 2)\n\n(Intercept) \n        1.6 \n\n\nWe can also retrieve frequency estimates using the function avg_predictions() in the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024), which also returns a 95% confidence interval:\n\navg_predictions(\n  m_nb, \n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.6\n1.47\n1.74\n\n\n\n\n\n \n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model is similar to the negative binomial since it also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms, using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution.\n\nModel fitting\nWe will illustrate this once we have fit our model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm_pr &lt;- glmer(\n    n_actually ~ 1 + offset(log(n_words)) + (1|speaker), \n    data = d,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nHere is a condensed regression table:\n\narm::display(m_pr)\n\nglmer(formula = n_actually ~ 1 + offset(log(n_words)) + (1 | \n    speaker), data = d, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\ncoef.est  coef.se \n   -6.58     0.04 \n\nError terms:\n Groups   Name        Std.Dev.\n speaker  (Intercept) 0.55    \n Residual             1.00    \n---\nnumber of obs: 247, groups: speaker, 247\nAIC = 1519.2, DIC = -1277.5\ndeviance = 118.9 \n\n\n \n\n\nRepresentation of between-speaker variation\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing between-speaker variation, is 0.55. Figure 4 shows the inferred distribution of speaker intercepts on the log scale. At the top of the figure, a second x-axis is drawn, which shows the normalized frequencies corresponding to these natural logs. The equidistant logs translate into a warped, non-linear spacing of the occurrence rates per thousand words.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(-8.2, -4.9), ylim=c(0,1.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(\n    at=-9:-3,\n    label = c(\"\\u22129\", \"\\u22128\", \"\\u22127\",\"\\u22126\", \"\\u22125\", \"\\u22124\", \"\\u22123\"))),\n  ylab=NULL, xlab=\"Model scale: Natural logarithm\",\n  panel=function(x,y,...){\n    panel.segments(x0=0, x1=0, y0=0, y1=.9, col=1)\n    panel.points(x = seq(-8.2, -4.9, length=1000),\n                 y = dnorm(seq(-8.2, -4.9, length=1000), \n                           mean = fixef(m_pr), sd = .55),\n                 type=\"l\")\n    panel.segments(x0=-8.2, x1=-4.9, y0=1, y1=1)\n    panel.segments(x0 = log(seq(.001, .007, .001)),\n                   x1 = log(seq(.001, .007, .001)),\n                   y0 = 1, y1 = 1.05)\n    panel.segments(x0 = log(seq(.0005, .0075, .001)),\n                   x1 = log(seq(.0005, .0075, .001)),\n                   y0 = 1, y1 = 1.03)\n    panel.text(x = log(seq(.001, .007, .001)), \n               y = 1.17, \n               label = seq(.001, .007, .001)*1000)\n    panel.text(x = fixef(m_pr), y = 1.45, label = \"Occurrences per thousand words\")\n    })\n\n\n\n\n\n\n\n\nFigure 4: The normal distribution describing between-speaker variability in the usage rate of actually on the scale of natural logarithms.\n\n\n\n\n\nFigure 5 shows what this distribution looks like on the scale of normalized frequencies. Now the occurrence rates per thousand words are equidistant and the natural logs assume a non-linear spacing. The distribution we are looking at is a log-normal distribution, which consists of positive values only, and which is skewed toward large values.\n\n\ndraw figure\nx_seq &lt;- seq(0, .006, length = 100)\n\nxyplot(\n  1~1, type=\"n\", xlim=c(0, exp(-4.99)), ylim=c(0,1.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=(0:7)/1000, label=0:7)),\n  ylab=\"Density           \", xlab=\"Occurrences per thousand words\",\n  panel=function(x,y,...){\n    panel.segments(x0 = 0, x1=0, y0=0, y1=.8)\n    panel.points(x = x_seq,\n                 y = dlnorm(x_seq, \n                            meanlog = fixef(m_pr), \n                            sdlog = 0.55)/800,\n                 type=\"l\")\n    \n    panel.segments(x0=exp(-8), x1=exp(-5), y0=1, y1=1)\n    panel.segments(x0 = exp(-8:-5),\n                   x1 = exp(-8:-5),\n                   y0 = 1, y1 = 1.07)\n    panel.segments(x0 = exp(-7.5:-5.5),\n                   x1 = exp(-7.5:-5.5),\n                   y0 = 1, y1 = 1.045)\n    \n    panel.segments(x0 = exp(seq(-8, 5, .1)),\n                   x1 = exp(seq(-8, 5, .1)),\n                   y0 = 1, y1 = 1.03)\n    panel.text(x = exp(-8:-5), \n               y = 1.2, \n               label = c(\"\\u22128\", \"\\u22127\",\"\\u22126\", \"\\u22125\"))\n    panel.text(x=.0035, y=1.5, label=\"Natural logarithm\")\n    })\n\n\n\n\n\n\n\n\nFigure 5: The log-normal distribution describing between-speaker variability in the usage rate of actually on the normlized frequency scale.\n\n\n\n\n\nTo clarify the relation between normalized frequencies (the data scale) and natural logarithms (the model scale), let us consider a group of 250 hypothetical speakers. These speakers, and their individual average usage rates of actually, are generated in perfect accordance with the Poisson random-intercept model. Since the model operates on the log scale, each speaker is characterized by their log usage rate of actually.\nFigure 6 shows that, according to the model, the speaker-specific log usage rates form a symmetric, bell-shaped pile, which resembles a normal distribution. The center of this pile of dots is the mean log usage rate in the sample of speakers. It is the mean over the 250 log rates.\n\n\ndraw figure\nsample_speakers &lt;- dnorm_to_dots(\n  n_dots = 250, \n  mean = fixef(m_pr), \n  sd = .55)\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(-8.3, -4.8), ylim=c(0,2.2),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=-9:-3)),\n  ylab=NULL, xlab=\"Model scale: Log usage rate (base e)\",\n  panel=function(x,y,...){\n    panel.segments(x0 = fixef(m_pr), x1 = fixef(m_pr), y0 = 0, y1 = 2.1, col = \"grey\")\n    panel.dotdiagram(sample_speakers, scale_y = .07, n_bins = 34)\n    panel.text(x = fixef(m_pr)-.2, y = 1.9, label=\"Mean over\\nspeaker-specific\\nlog usage rates\", lineheight = .8, adj=1)\n    panel.text(x = fixef(m_pr)+.2, y = 1.9, label=\"Fixed intercept of\\nrandom-intercept\\nPoisson model\", lineheight = .8, adj=0)\n    panel.text(x = fixef(m_pr), y = 1.9, label = \"=\")\n    panel.text(x = fixef(m_pr)+.4, y = 1.2, label=\"\\u22126.58\", adj=0)\n    panel.text(x = fixef(m_pr)+1.2, y = 1.2, label=\"Back-transformation:\", adj=0, col = \"grey40\")\n    panel.text(x = fixef(m_pr)+1.4, y = .95, label=\"exp(\\u22126.58)\", adj=0, col = \"grey40\")\n    panel.text(x = fixef(m_pr)+1.4, y = .7, label=\"= 1.4 per thousand words\", adj=0, col = \"grey40\")\n    })\n\nprint(p1, position = c(0,0,.69,1))\n\n\n\n\n\n\n\n\nFigure 6: Dot diagram showing an idealized distribution of 250 speakers on the model scale (log normalized frequencies), based on the parameters of the Poisson random-intercept model.\n\n\n\n\n\nIn the Poisson random-intercept model, this mean log usage rate is represented by the fixed intercept of the model. We saw this fixed intercept in the regression table above. We can retrieve it from the model using the function fixef():\n\nfixef(m_pr)\n\n(Intercept) \n   -6.57786 \n\n\nTo make sense of this value, we back-transform this log normalized frequency to the scale of occurrence rates via exponentiation. To get occurrences ‘per thousand words’, we multiply this rate by 1,000, and round the result to one decimal place:\n\nround(\n  exp(fixef(m_pr)) * 1000,\n  1)\n\n(Intercept) \n        1.4 \n\n\nThe mean log normalized frequency, then, which is represented by the fixed intercept of the model, corresponds to a rate of 1.4 per thousand words.\n\n\nTwo types of average frequencies\nThe normalized frequency of 1.4 per thousand words is one kind of average usage rate we can report based on a random-intercept Poisson model. To recognize that there is a second type of average normalized frequency, let us consider the distribution of the 250 speaker-specific usage rates on the data scale of normalized frequencies. Figure 7 shows that the pile is no longer symmetric – it is skewed toward the right, with a longer upper tail. Note how this dot diagram matches the density curve shown in Figure 5 above.\n\n\ndraw figure\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, .0071), ylim=c(0,2.5),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=(0:7)/1000, label=0:7)),\n  ylab=NULL, xlab=\"Data scale: Usage rate (per thousand words)\",\n  panel=function(x,y,...){\n    panel.segments(x0 = exp(fixef(m_pr) + (.55^2)/2), x1 = exp(fixef(m_pr) + (.55^2)/2), y0 = 0, y1 = 2.5)\n    panel.segments(x0 = exp(fixef(m_pr)), x1 = exp(fixef(m_pr)), y0 = 0, y1 = 2.5, col = \"grey\")\n    #panel.segments(x0 = 0, x1 = 0, y0 = 0, y1 = .9, col = 1)\n    panel.dotdiagram(exp(sample_speakers), scale_y = .06, n_bins = 45)\n    panel.text(x = exp(fixef(m_pr) + (.55^2)/2)+.0003, y = 2.3, label=\"Mean over\\nspeaker-specific\\nusage rates\", lineheight = .8, adj=0)\n    panel.text(x = exp(fixef(m_pr))-.0003, y = 2.3, label=\"Back-transformed\\nintercept of\\nPoisson model\", lineheight = .8, adj=1, col=\"grey40\")\n    panel.text(x = exp(fixef(m_pr) + (.55^2)/2)+.0003, y = 1.8, label=\"1.6 per thousand words\", lineheight = .8, adj=0)\n    panel.text(x = exp(fixef(m_pr))-.0003, y = 1.8, label=\"1.4 per thousand words\", adj=1, col=\"grey40\")\n    #panel.text(x = fixef(m_pr), y = 1.7, label = \"=\")\n    panel.points(x = c(exp(fixef(m_pr)), exp(fixef(m_pr) + (.55^2)/2)), y = 2.5, pch = 19, col = c(\"grey40\", \"black\"))\n    })\n\nprint(p1, position = c(.2,0,1,1))\n\n\n\n\n\n\n\n\nFigure 7: Dot diagram showing an idealized distribution of 250 speakers on the data scale (normalized frequencies), based on the parameters of the Poisson random-intercept model.\n\n\n\n\n\nCharacterizing the distribution of usage rates on this scale is more challenging due to the asymmetry. If we want to summarize the pile of dots by referring to the “typical”, or “average” occurrence rate in the group of speakers, we could use the mean or median usage rate. The mean is sensitive to outliers – the few speakers with unusually high usage rate will therefore pull it upwards slightly. The median, on the other hand, is the usage rate in the middle of the distribution and not affected by outliers – half the speakers are above, and half below the median rate.\nIf we calculate the mean usage rate based on the pile of dots in Figure 7, we obtain a normalized frequency of 1.6 per thousand words. This “average” is marked in Figure 7 using a black needle. For comparison, the back-transformed mean log usage rate of 1.4 per thousand words, which we calculated above, appears as a grey needle.\nIt is important to note that these two types of averages differ. Let us therefore repeat what they represent:\n\nThe grey average (1.4 ptw) is the mean over the speaker-specific log usage rates (see Figure 6), back-transformed into a normalized frequency.\nThe black average (1.6 ptw) is the mean over the speaker-specific usage rates (see Figure 7), i.e. over the back-transformed log usage rates.\n\nThe two averages represent two different measures of central tendency:\n\n1.6 ptw, in black, is the mean usage rate over the 250 speakers.\n1.4 ptw, in grey, is the median usage rate over the 250 speakers.\n\nIn other words, upon back-transforming the fixed intercept in a random-intercept Poisson regression model, we obtain the median normalized frequency.\nIf we look at Figure 7, we note that the median (grey) arguably does a better job at locating the typical occurrence rate in the group of 250 speakers – the mean (black) seems a bit too high, as most of the dots are below 1.6 ptw. This is consistent with the advice found in statistical textbooks: the median is often a better summary measure for skewed distributions.\n\n\nObtaining model-based estimates of the different average frequencies\nBoth kinds of average frequencies can be constructed based on a Poisson model with random-intercepts. We will look at two approaches: (i) an analytic approach based on the model parameters, and (ii) a predictive approach using the {marginaleffects} package.\nAs we saw above, the median normalized frequency is represented by the model intercept, and we can retrieve it as follows:\n\nround(\n  exp(fixef(m_pr)) * 1000,\n  2)\n\n(Intercept) \n       1.39 \n\n\nThe mean normalized frequency can be calculated based on the model intercept and the variance of the normal distribution describing between-speaker variation. This is the formula:\n\\[\n\\textrm{mean normalized frequency} = \\textrm{exp}(\\textrm{intercept} + \\frac{\\textrm{random-intercept variance}}{2})\n\\]\nTo apply this formula, we first extract the random-intercept variance from the model object:\n\nintercept_variance &lt;- as.numeric(\n  summary(m_pr)$varcor$speaker)\n\nNow we can apply the formula above to obtain the mean normalized frequency of actually:\n\nround(\n  exp(fixef(m_pr) + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.61 \n\n\nWe can also obtain these two types of average frequency using the {marginaleffects} package. To get the median normalized frequency of actually (i.e. the back-transformed mean log rate), we run the following code. The argument re.form = NA tells the function to ignore between-speaker variation:\n\navg_predictions(\n  m_pr, \n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.39\n1.27\n1.51\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m_pr, \n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(estimate, conf.low, conf.high) |&gt; \n  round(2) |&gt; \n  kable()\n\n\n\n\nestimate\nconf.low\nconf.high\n\n\n\n\n1.57\n1.43\n1.7\n\n\n\n\n\nThe result is not identical to the one we got above due to shrinkage: The speaker intercepts are partially pooled, and their variability is therefore smaller than implied by the random-intercept standard deviation.\n\n\n\nComparison: Poisson random-intercept vs. negative binomial model\nWe can now compare the two types of regression models in terms of (i) how they represent between-speaker variation and (ii) the kind of frequency estimates they return.\nThe two models describe the variation among speakers using different probability distributions. These are compared in Figure 8, which shows that they provide quite similar, though not identical, representations of the distribution of speaker-specific normalized frequencies.\n\nThe negative binomial model uses the gamma distribution to express between-speaker variation; it is therefore also referred to as a Poisson-gamma mixture model.\nPoisson regression with random intercepts uses the lognormal distribution to express between-speaker variation; it is therefore also referred to as a Poisson-lognormal mixture model.\n\n\n\ndraw figure\nx_seq &lt;- seq(0, .006, length = 100)\n\np1 &lt;- xyplot(\n  1~1, type=\"n\", xlim=c(0, .006), ylim=c(0,1.1),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=(0:6)/1000, label=c(0,1,2,3,4,5,6))),\n  ylab=\"Density\", xlab=\"Ocurrences per thousand words\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1.5, col=1)\n    panel.points(x = seq(.01, 5, length=200)*exp(coef(m_nb)),\n                 y = dGA(seq(.01, 5, length=200), mu=1, sigma=1/sqrt(3.4363)),\n                 type=\"l\", col = \"grey40\", lty = \"32\", lineend = \"butt\")\n    \n    panel.points(x = x_seq,\n                 y = dlnorm(x_seq, \n                            meanlog = fixef(m_pr), \n                            sdlog = 0.55)/600,\n                 type=\"l\")\n    panel.segments(x0=0, x1=.006, y0=0, y1=0)\n    panel.text(x = 4, y = .2, label=\"Log-normal distribution\", adj=0, cex=.9)\n    panel.text(x = 1.5, y = .45, label=\"Gamma distribution\", adj=0, col = \"grey50\", cex=.9)\n\n    panel.abline(v = 1)\n    panel.text(x = .0008, y = 1.25, label = \"Poisson-lognormal model\\n(Poisson regression with random intercepts)\", adj=0, cex=.9, lineheight = .8)\n    panel.text(x = .0025, y = .6, label = \"Poisson-gamma model\\n(Negative binomial regression)\", adj=0, cex=.9, col = \"grey40\", lineheight = .8)\n    })\n\nprint(p1, position = c(0,0,1,.85))\n\n\n\ncairo_pdf(\"actually_gamma_lognormal_comparison.pdf\", width = 4, height = 2)\nprint(p1, position = c(0,0,1,.85))\ndev.off()\n\n\npng \n  2 \n\n\n\n\n\n\n\n\nFigure 8: The log-normal distribution (black) vs. the gamma distribution (grey) describing between-speaker variability in the usage rate of actually.\n\n\n\n\n\nAs for the model predictions, the intercepts in the two models represent different average frequencies:\n\nThe intercept in a negative binomial model represents the mean normalized frequency\nThe intercept in the Poisson random-intercept model represents the median normalized frequency\n\nThe following code therefore returns different frequency estimates:\n\nround(\n  exp(fixef(m_pr)) * 1e3, 2)\n\n(Intercept) \n       1.39 \n\n\n\nround(\n  exp(coef(m_nb)) * 1e3, 2)\n\n(Intercept) \n        1.6 \n\n\n\n\nSummary\nThe Poisson regression model with random intercepts is a strategy for modeling clustered frequency data. This model captures text-to-text (or speaker-to-speaker) variation in the occurrence rate (i.e. normalized frequency) of interest using a lognormal distribution. It is therefore also referred to as a Poisson-lognormal mixture model. This model is capable of producing two types of frequency estimates: the median normalized frequency across texts, and the mean normalized frequency across texts. It is important to recognize the difference between these measures, as they represent alternative ways of expressing the typical (or “average”) occurrence rate in the population of interest. Further, it allows us to make sense of model-based predictions and how they may differ from those produced by other count regression models (such as the negative binomial model).\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Frequency Estimates Based on Random-Intercept {Poisson}\n    Models},\n  date = {2025-05-13},\n  url = {https://lsoenning.github.io/posts/2025-05-12_poisson_random_intercept/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Frequency Estimates Based on\nRandom-Intercept Poisson Models.” May 13, 2025. https://lsoenning.github.io/posts/2025-05-12_poisson_random_intercept/."
  },
  {
    "objectID": "posts/2025-05-10_counts_overdispersion_unbalanced/index.html",
    "href": "posts/2025-05-10_counts_overdispersion_unbalanced/index.html",
    "title": "Modeling clustered frequency data II: Texts of disproportionate length",
    "section": "",
    "text": "When describing or modeling corpus-based frequency data, the fact that a corpus is divided into text files has consequences for statistical modeling. For count variables (which corpus linguists often summarize using normalized frequencies), there are several options. This blog post contrasts different regression approaches to clustered count data and clarifies how they deal with unequal text lengths.\n\n\nR setup\nlibrary(tidyverse)       # for data wrangling and visualization\nlibrary(dataverse)       # for downloading data from TROLLing\nlibrary(marginaleffects) # to compute model-based estimates\nlibrary(MASS)            # to fit a negative binomial regression model\nlibrary(kableExtra)      # for drawing html tables\nlibrary(lme4)            # to fit mixed-effects regression models\nlibrary(lattice)         # for data visualization\nlibrary(gamlss)          # to draw the density of the gamma distribution\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: Actually in the Spoken BNC2014\nOur illustrative data records the distribution of actually in the Spoken BNC2014 (Love et al. 2017), which was analyzed in Sönning and Krug (2022). For more information on the dataset, please refer to Sönning and Krug (2021).\nWe start by downloading the data from TROLLing:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"actually_data_2014.tab\",\n    dataset   = \"10.18710/A3SATC\",\n    server    = \"dataverse.no\",\n    .f        = read.csv,\n    original  = TRUE\n  )\n\nIn line with Sönning and Krug (2022), we remove speakers who contributed fewer than 100 words to the corpus, and for whom information on age and gender is missing.\n\nd &lt;- dat |&gt; \n  filter(\n    total &gt; 100,\n    Age_range != \"Unknown\",\n    !(is.na(Gender)))\n\nIn this blog post, we will concentrate on speakers aged 70 or older.\n\nd &lt;- d |&gt; \n  filter(Age_range %in% c(\"70-79\", \"80-89\", \"90-99\")) |&gt; \n  droplevels()\n\nWe add a new variable to the data frame: the speaker-specific normalized frequency of actually, expressed as ‘per thousand words’:\n\nd$rate_ptw &lt;- (d$count / d$total) * 1000\n\nWe reduce the data frame to the variables we need for analysis and rename a few columns for consistency and clarity.\n\nd &lt;- d |&gt; dplyr::select(\n  speaker, Gender, Age_range, count, total, rate_ptw) |&gt; \n  dplyr::rename(\n    age_group = Age_range,\n    gender = Gender,\n    n_tokens = count,\n    n_words = total)\n\nThe data subset used in the present blog post includes 56 speakers and the following variables:\n\nan ID (speaker)\nthe number of times they used actually (n_actually)\nthe age group (age_group)\nself-reported gender (gender)\nthe total number of words contributed to the corpus by the speaker (n_words), and\nthe usage rate of actually, expressed as ‘per thousand words’ (rate_ptw)\n\n\nstr(d)\n\n'data.frame':   56 obs. of  6 variables:\n $ speaker  : chr  \"S0005\" \"S0006\" \"S0012\" \"S0017\" ...\n $ gender   : chr  \"Female\" \"Male\" \"Male\" \"Male\" ...\n $ age_group: chr  \"80-89\" \"80-89\" \"70-79\" \"70-79\" ...\n $ n_tokens : int  3 1 69 4 1 1 3 6 2 2 ...\n $ n_words  : int  1449 4804 277953 7377 2084 7029 2497 3420 3460 6822 ...\n $ rate_ptw : num  2.07 0.208 0.248 0.542 0.48 ...\n\n\n \n\n\nFocus of analysis\nThe key interest in the following is in the usage rate of actually (expressed as a normalized frequency) in conversational speech. Two subgroups of British speakers are compared: Male speakers aged 70 or older (“Male 70+”), and female speakers aged 70 or older (“Female 70+”). The questions guiding our analyses are:\n\nWhat is the normalized frequency of actually in the two groups?\nDoes the usage rate of actually differ between the groups?\n\n\n\nData description\nWe start by inspecting some key characteristics of the data. First we examine the distribution of speakers across the groups, which turns out to be roughly balanced:\n\ntable(d$gender)\n\n\nFemale   Male \n    25     31 \n\n\nNext, we consider the distribution of word counts across speakers (i.e. the total number of word tokens each individual contributed to the corpus). Figure 1 shows a very skewed profile, with one speaker showing a disproportionately high word count.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = n_words)) + \n  geom_dotplot(binwidth = 3000, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_x_continuous(labels = scales::label_comma(), expand = c(.01, .01)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  annotate(\"text\", x = 150000, y = .5, label = \"Each dot represents a speaker\", color = \"grey30\", size = 3.5) +\n  xlab(\"Number of word tokens contributed to the corpus\")\n\n\n\n\n\n\n\n\nFigure 1: Distribution of word counts across speakers from the Spoken BNC2014 aged 70 or older, excluding individuals who contributed fewer than 100 words to the corpus.\n\n\n\n\n\nTo see how the outcome variable is distributed at the speaker level, we draw a dot diagram of the speaker-specific usage rate of actually, expressed as “per thousand words”. Figure 2 shows a skewed arrangement, with a few individuals using the word at an exceptionally high rate.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .1, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0, 0)) +\n  xlab(\"Speaker-specific usage rate of * (per thousand words)\")\n\n\n\n\n\n\n\n\nFigure 2: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words.\n\n\n\n\n\nDue to the skew in the distribution, we use a square-root transformation for visual group comparisons. Figure 3 reassures us that this effectively removes the skew.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = rate_ptw)) + \n  geom_dotplot(binwidth = .05, stackratio = .9, method = \"histodot\") +\n  theme_dotplot() + \n  scale_y_continuous(expand = c(0, 0)) +\n  scale_x_sqrt(breaks = c(0,2,4,6,8)) +\n  xlab(\"Speaker-specific usage rate of actually\\n(per thousand words, square-root-scaled)\")\n\n\n\n\n\n\n\n\nFigure 3: Distribution of speaker-specific usage rates of actually in our data subset, per thousand words, square-root-scaled.\n\n\n\n\n\nNow we inspect the (square-root-scaled) distribution of speaker-specific rates of actually by Gender. Figure 4 shows that the median rate is very similar in the two groups. Between-speaker variation, as indicated by the height of the boxes, is slightly larger among male speakers.\n\n\nDraw Figure\nd |&gt; \n  ggplot(aes(x = gender, y = rate_ptw)) +\n  geom_boxplot() +\n  scale_y_sqrt(breaks = 0:10) +\n  theme_classic() +\n  ylab(\"Normalized frequency of actually\\n(per thousand words, square-root-scaled)\\n\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nFigure 4: Boxplot showing the distribution of speaker-specific usage rates of actually (per thousand words, square-root-scaled) by Gender.\n\n\n\n\n\nLet us also draw a bubble chart, which simultaneously takes into account the speaker-specific (i) word count and (ii) usage rate of actually. This means that we look at the distribution of the data points behind the boxplot.\nIn Figure 5, each individual appears as a circle and the size of this circle is proportional to the speaker word count. Individuals contributing an overabundance of words to the corpus (and our data subset) appear as big circles. We observe that the person with the highest word count (the biggest circle) is male, with a relatively low rate of actually. Among female speakers, the two individuals with the largest word counts also show the highest usage rates.\n\n\ndraw figure\nset.seed(7)\n\nd |&gt; \n  ggplot(aes(x = gender, y = rate_ptw, size = n_words)) +\n  geom_jitter(shape = 1, width = .25, alpha=.7) +\n  scale_y_sqrt(breaks = 0:10) +\n  theme_classic() +\n  ylab(\"Normalized frequency of actually\\n(per thousand words, square-root-scaled)\\n\") +\n  scale_size_area(max_size = 15) +\n  theme(legend.position = \"none\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nFigure 5: Bubble chart showing the distribution of speaker-specific usage rates of actually (per thousand words, square-root-scaled) by Gender, with the size of circles reflecting the total word count for a speaker.\n\n\n\n\n\nA key insight that will emerge from our comparison of modeling approaches will be that they respond differently to this data feature, i.e. the combination of disproportionately high word counts and relatively high or low occurrences rates, for specific texts or speakers. Before we turn to regression analysis, however, let us jot down numerical summaries for the data.\n\n\nDescriptive measures: Subcorpus frequencies and mean speaker frequency\nThere are two straightforward ways of summarizing the frequencies in the two subgroups. Egbert and Burch (2023, 105) refer to these as corpus frequency and mean text frequency. In the present setting, we will talk about subcorpus frequencies (Male 70+ subcorpus vs. Female 70+ subcorpus) and mean speaker frequencies.\nTo obtain the subcorpus frequency of actually in each group, we divide the total number of actually-tokens by the subcorpus size. We multiply this rate by 1,000 to obtain a normalized frequency of ‘per thousand words’:\n\nd |&gt; \n  group_by(gender) |&gt; \n  dplyr::summarize(\n    n_actually = sum(n_tokens),\n    corpus_size = sum(n_words),\n    subcorpus_frequency = round(n_actually/corpus_size*1000, 2)\n  ) |&gt; kable()\n\n\n\n\ngender\nn_actually\ncorpus_size\nsubcorpus_frequency\n\n\n\n\nFemale\n731\n257786\n2.84\n\n\nMale\n403\n522671\n0.77\n\n\n\n\n\nThis gives us a subcorpus frequency of 2.84 ptw for female speakers and 0.77 ptw for male speakers. We get the same estimates when using CQPweb (Hardie 2012) to run a restricted corpus query:\n \nAnother way of estimating the average rate of actually in each subgroup is to proceed in two steps: We first determine the speaker-specific normalized frequencies (i.e. the variable rate_ptw) and then we average over these within each group. This yields much more similar frequency estimates, which is consistent with what we saw in Figure 4 above.\n\nd |&gt; \n  group_by(gender) |&gt; \n  dplyr::summarize(\n    mean_speaker_frequency = round(\n      mean(rate_ptw), 2)\n  ) |&gt; kable()\n\n\n\n\ngender\nmean_speaker_frequency\n\n\n\n\nFemale\n1.21\n\n\nMale\n1.23\n\n\n\n\n\nThe difference between these two ways of measuring frequency is that while the mean speaker frequency gives the same weight to each person, the corpus frequency weights speakers in proportion to the number of words they contribute to the corpus. In the present case, there is no reason why certain individuals should inform our frequency estimate more than others, so we clearly prefer the mean speaker frequency.\nWe keep these differences in mind as we consider alternative ways of modeling the data.\n \n\n\nPoisson regression\nWe start with a Poisson regression model, which does not take into account the grouping structure of the data. This means that it turns a blind eye on the speakers in our data and considers the actually tokens (and the corpus) as an unstructured bag of words.\nWe can fit a Poisson model with the glm() function:\n\nm &lt;- glm(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d,\n  family = \"poisson\")\n\nHere is the regression table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ gender + offset(log(n_words)), family = \"poisson\", \n    data = d)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.86547    0.03699 -158.59   &lt;2e-16 ***\ngenderMale  -1.30230    0.06204  -20.99   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1615.2  on 55  degrees of freedom\nResidual deviance: 1148.4  on 54  degrees of freedom\nAIC: 1321.2\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe use the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024) to calculate model-based predictions for male and female speakers. These coincide with the corpus frequencies reported above:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n2.84\n2.64\n3.05\n\n\nMale\n0.77\n0.70\n0.85\n\n\n\n\n\nThe function comparisons() in the {marginaleffects} package allows us to compare the two groups in relative terms: The usage rate of male speakers is estimated to be only 27% as large as that of female speakers:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n0.27\n0.24\n0.3\n\n\n\n\n\n \n\n\nQuasi-Poisson regression\nA Quasi-Poisson model includes a dispersion parameter, which adjust inferences to account for the lack of fit of the simple Poisson model. The dispersion parameter \\(\\phi\\) is estimated on the basis of a global \\(\\chi^2\\) statistic of model (mis)fit, and it is then used to adjust the standard errors returned by the model, which are multiplied by \\(\\sqrt{\\phi}\\). For some more background on this way of accounting for overdispersion, see this blog post.\nWe can run a Quasi-Poisson model as follows:\n\nm &lt;- glm(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d,\n  family = \"quasipoisson\")\n\nThe model is summarized in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ gender + offset(log(n_words)), family = \"quasipoisson\", \n    data = d)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.8655     0.1861 -31.521  &lt; 2e-16 ***\ngenderMale   -1.3023     0.3121  -4.172  0.00011 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 25.31282)\n\n    Null deviance: 1615.2  on 55  degrees of freedom\nResidual deviance: 1148.4  on 54  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe regression table tells us that the dispersion parameter is estimated to be roughly 25, which means that the standard errors for the Quasi-Poisson model should be 5 times (\\(\\sqrt{25}\\)) larger than in the Poisson model.\nImportantly, however, the regression coefficients themselves do not change, and neither do the model-based predictions. We get the same point estimates, though with (appropriately) wider confidence intervals:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n2.84\n1.97\n4.08\n\n\nMale\n0.77\n0.47\n1.26\n\n\n\n\n\nThe Quasi-Poisson model also returns the same relative difference between the groups:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n0.27\n0.11\n0.44\n\n\n\n\n\n \n\n\nNegative binomial regression\nNegative binomial regression explicitly takes into account the speakers, and models the observed variability in the usage rate of actually using a probability distribution. The model therefore includes an additional parameter that represents the variability of usage rates. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates. For some more background, see this blog post.\nThis is illustrated in Figure 6, which shows high variability among speakers.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=c(0, 4.2), ylim=c(0,1.5),\n  par.settings=my_settings, axis=axis_L,\n  scales=list(y=list(at=0), x=list(at=c(0,1,2,3,4))),\n  ylab=\"Density\", xlab=\"Multiplicative factor\",\n  panel=function(x,y,...){\n    panel.segments(x0=1, x1=1, y0=0, y1=1.5, col=1)\n    panel.points(x = seq(.01, 4.2, length=1000),\n                 y = dGA(seq(.01, 4.2, length=1000), mu=1, sigma=(1/sqrt(0.9347))),\n                 type=\"l\")\n    })\n\n\n\n\n\n\n\n\nFigure 6: The gamma distribution describing between-speaker variability in the usage rate of actually.\n\n\n\n\n\nSince this is a probability distribution, we can summarize the estimated distribution of speakers around their subgroup means. The following code finds the quartiles of the distribution:\n\nqGA(\n  p = c(.25, .5, .75), \n  mu = 1, \n  sigma = 1/sqrt(0.9347)) |&gt; \n  round(2)\n\n[1] 0.27 0.67 1.39\n\n\nThis tells us that ratios of 0.27 and 1.39 mark the interquartile range: The central 50% of the speakers are within this interval. Interestingly, and perhaps counterintuitively, the median of this gamma distribution is 0.67, meaning that half of the speakers have a ratio below this mark. Let us also see how many speakers have ratio above and below 1:\n\npGA(\n  q = 1, \n  mu = 1, \n  sigma = 1/sqrt(0.9347)) |&gt; \n  round(2)\n\n[1] 0.64\n\n\n64% of the speakers have a ratio below 1, meaning that around two-thirds of the speakers actually show a usage rate below the estimated subgroup mean. We will return to this rather puzzling feature of the negative binomial model further below.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm &lt;- MASS::glm.nb(\n  n_tokens ~ gender + offset(log(n_words)),\n  data = d)\n\nThis produces the following regression table:\n\nsummary(m)\n\n\nCall:\nMASS::glm.nb(formula = n_tokens ~ gender + offset(log(n_words)), \n    data = d, init.theta = 0.9346725065, link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.66312    0.22513 -29.596   &lt;2e-16 ***\ngenderMale   0.02303    0.30582   0.075     0.94    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9347) family taken to be 1)\n\n    Null deviance: 60.329  on 55  degrees of freedom\nResidual deviance: 60.323  on 54  degrees of freedom\nAIC: 339.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.935 \n          Std. Err.:  0.210 \n\n 2 x log-likelihood:  -333.121 \n\n\nFrequency estimates based on this model are much closer to the mean speaker frequencies we reported above:\n\npredictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n1.28\n0.82\n1.99\n\n\nMale\n1.31\n0.87\n1.96\n\n\n\n\n\nAccordingly, the estimated relative difference between the groups is negligible:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n1.02\n0.41\n1.64\n\n\n\n\n\n \n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model is similar to the negative binomial since it also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution. For a more detailed discussion of the structure of this model, see this blog post.\nWe will illustrate this once we have fit our model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- glmer(\n    n_tokens ~ gender + offset(log(n_words)) + (1|speaker), \n    data = d,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nHere is a condensed regression table:\n\narm::display(m)\n\nglmer(formula = n_tokens ~ gender + offset(log(n_words)) + (1 | \n    speaker), data = d, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\n            coef.est coef.se\n(Intercept) -7.27     0.25  \ngenderMale   0.15     0.33  \n\nError terms:\n Groups   Name        Std.Dev.\n speaker  (Intercept) 1.05    \n Residual             1.00    \n---\nnumber of obs: 56, groups: speaker, 56\nAIC = 338.4, DIC = -294.8\ndeviance = 18.8 \n\n\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing between-speaker variation, is 1.05. Figure 7 shows the inferred distribution of speaker intercepts on the log scale.\n\n\ndraw figure\nxyplot(\n  1~1, type=\"n\", xlim=fixef(m)[1] + c(-3.4, 3.5), ylim=c(0,.45),\n  par.settings=my_settings, axis=axis_bottom,\n  scales=list(y=list(at=0), x=list(at=-10:-4)),\n  ylab=\"Density\", xlab=\"Natural logarithm\",\n  panel=function(x,y,...){\n    panel.segments(x0=0, x1=0, y0=0, y1=.45, col=1)\n    panel.points(x = fixef(m)[1] +  seq(-3.5, 3.5, length=1000),\n                 y = dnorm(fixef(m)[1] + seq(-3.5, 3.5, length=1000), mean = fixef(m)[1], sd = 1.05),\n                 type=\"l\", lty=\"23\", lineend=\"square\")\n    })\n\n\n\n\n\n\n\n\nFigure 7: The normal distribution describing between-speaker variability in the usage rate of actually on the scale of natural logarithms.\n\n\n\n\n\nAs discussed in detail in this blog post, there are two types of predictions we can calculate for the random-intercept Poisson model. Seeing that we are interested in the occurrence rate of actually rather than its natural logarithm, we will want to back-transform model-based predictions to the scale of normalized frequencies. Since there is between-speaker variation, our model-based estimate will have to somehow average over speakers. The question is whether we want to average over speakers on the scale of natural logarithms (the model scale) or on the scale of normalized frequencies (the data scale).\n\nBy averaging on the data scale of normalized frequencies, we obtain the mean usage rate across speakers.\nBy averaging on the model scale of log normalized frequencies, and then back-transforming this mean log rate, we obtain the median usage rate across speakers.\n\nThis blog post provides a detailed illustration of these two types of frequency estimates.\nThrough appropriate combination of the regression coefficients for the fixed effects, we obtain averages over speakers on the model scale. This is the estimated mean log rate of actually in the population of interest. We can back-transform this into a normalized frequency. This summary measure, however, does not represent the mean over normalized frequencies, since the averaging was done on another scale (the model scale).\nIn our model, the intercept represents the mean log rate for female speakers. If we add the coefficient for the predictor Gender, we get the mean log rate for male speakers. Back-transforming these values gives us:\n\n# female\nround(exp(fixef(m)[1]) * 1e3, 2)\n\n(Intercept) \n        0.7 \n\n# male\nround(exp(fixef(m)[1] + fixef(m)[2]) * 1e3, 2)\n\n(Intercept) \n       0.81 \n\n\nThese frequency estimates are lower than the ones we have obtained above. This is because they represent the median usage rate of actually, and in a distribution that skewed toward large values, the median is always smaller than the mean.\nAs illustrated in this blog post, we can also use the model to calculate mean normalized frequencies, using the model intercept and the random-effects variance:\n\\[\n\\textrm{mean normalized frequency} = \\textrm{exp}(\\textrm{intercept} + \\frac{\\textrm{random-intercept variance}}{2})\n\\] We first extract the random-intercept variance from the model object:\n\nintercept_variance &lt;- as.numeric(\n  summary(m)$varcor$speaker)\n\nAnd then calculate the mean normalized frequency of actually in the two groups:\n\n# female\nround(\n  exp(fixef(m)[1] + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.21 \n\n# female\nround(\n  exp(fixef(m)[1] + fixef(m)[2] + intercept_variance/2) * 1e3, 2)\n\n(Intercept) \n       1.41 \n\n\nWe can also obtain these two types of estimates using the {marginaleffects} package. To get the mean log rate of actually, back-transformed to the normalized frequency scale, we run the following code. The argument re.form = NA tells the function to ignore the between-speaker variation:\n\navg_predictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n0.70\n0.36\n1.04\n\n\nMale\n0.81\n0.45\n1.17\n\n\n\n\n\nThe corresponding relative difference between the groups can be retrieved as follows:\n\ncomparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = NA),\n  re.form = NA,\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nMale / Female\n1.16\n0.41\n1.92\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(gender, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ngender\nestimate\nconf.low\nconf.high\n\n\n\n\nFemale\n1.15\n0.58\n1.71\n\n\nMale\n1.33\n0.74\n1.93\n\n\n\n\n\nThe result is not identical to the one we got above due to shrinkage: The speaker intercepts are partially pooled, and their variability is therefore smaller than implied by the random-intercept standard deviation.\nThe relative difference between the groups remains the same:\n\navg_comparisons(\n  m, \n  variables = \"gender\",\n  newdata = datagrid(\n    n_words = 1000,\n    speaker = unique),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Male) / mean(Female)\n1.16\n0.41\n1.92\n\n\n\n\n\n\n\nComparison\nFigure 8 compares the estimated average predictions we have collected in this blog post. For a point of reference, our descriptive summaries are shown in grey: The dotted lines are the two (sub)corpus frequencies, and the solid lines – which are almost identical in the groups – are the mean speaker frequencies.\nOur first observation is that estimates based on the Poisson and Quasi-Poisson model coincide with the plain subcorpus frequencies – as a result, they suffer from the imbalanced word counts across speakers. Just like the corpus frequency, both models give much greater weight to speakers who contributed a large number of words to the corpus. As we have noted above, this is undesirable in the present case. We therefore conclude that the Poisson and Quasi-Poisson model are inadequate for the data at hand, since they do not guard against imbalances.\nThe other models produce estimates that are close(r) to the mean speaker frequencies. The three models agree in the statement that the difference between the two groups, “Male 70+” and “Female 70+”, are minor. The estimates from the negative binomial model and the mean normalized frequency predicted by the Poisson random-intercept model are virtually indistinguishable from the mean speaker frequencies.\nFinally, the median normalized frequency predicted by the Poisson random-intercept model is considerably lower.\n\n\ndraw figure\npred_models &lt;- tibble(\n  model = rep(c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                \"Random-intercept\\nPoisson\\n(median usage rate)\"), each = 2),\n  gender = rep(c(\"Female\", \"Male\"), 5),\n  estimate = c(pred_poisson$estimate,\n               pred_quaspoi$estimate,\n               pred_negbin$estimate,\n               pred_ranef_m$estimate,\n               pred_ranef_c$estimate),\n  ci_lower = c(pred_poisson$conf.low,\n               pred_quaspoi$conf.low,\n               pred_negbin$conf.low,\n               pred_ranef_m$conf.low,\n               pred_ranef_c$conf.low),\n  ci_upper = c(pred_poisson$conf.high,\n               pred_quaspoi$conf.high,\n               pred_negbin$conf.high,\n               pred_ranef_m$conf.high,\n               pred_ranef_c$conf.high)\n)\n\npred_models$model &lt;- factor(\n  pred_models$model,\n  levels = c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n  ordered = TRUE\n)\n\nann_text &lt;- data.frame(\n  estimate = 3.3,\n  lab = \"Corpus frequency\",\n  gender = \"Female\",\n  model = factor(\"Random-intercept\\nPoisson\\n(median usage rate)\",\n                 levels = c(\"Poisson\", \"Quasi-Poisson\",\n                            \"Negative\\nbinomial\", \n                            \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                            \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n                 ordered = TRUE))\n\nann_text2 &lt;- data.frame(\n  estimate = 1.5,\n  lab = \"Mean speaker frequency\",\n  gender = \"Female\",\n  model = factor(\"Random-intercept\\nPoisson\\n(median usage rate)\",\n                 levels = c(\"Poisson\", \"Quasi-Poisson\",\n                            \"Negative\\nbinomial\", \n                            \"Random-intercept\\nPoisson\\n(mean usage rate)\",\n                            \"Random-intercept\\nPoisson\\n(median usage rate)\"),\n                 ordered = TRUE))\n\n\npred_models |&gt; \n  ggplot(aes(x = gender, y = estimate, group = model)) +\n  geom_hline(yintercept = c(1.21, 1.23), col = \"grey\") +\n  geom_hline(yintercept = c(.77, 2.84), col = \"grey\", lty = \"22\", linetype=\"square\") +\n  geom_point() +\n  geom_line() +\n  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  facet_grid(. ~ model) +\n  theme_classic_ls() +\n  scale_y_sqrt(limits = c(0, 4.5), expand = c(0,0)) +\n  ylab(\"Usage rate of actually\\n(ptw, square-root-scaled)\\n\") +\n  xlab(NULL) +\n  geom_text(data = ann_text, label = \"                    Corpus frequency\", col=\"grey50\", size=3) +\n  geom_text(data = ann_text2, label = \"          Mean speaker frequency\", col=\"grey50\", size=3) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 8: Comparison of model-based predictions for the average usage rate of actually in the two subgroups.\n\n\n\n\n\n\n\nSummary\nThis blog post compared different approaches to modeling corpus-based frequency data. The regression models we considered address the non-independence of observations in the data in different ways and therefore return different estimates of average normalized frequencies. Differences between these estimates correspond to differences between two broad ways of measuring frequency: corpus frequency and mean (or median) text frequency. Models that account for the clustering in the data yield analogues of mean text frequencies, which are more suitable if texts differ in length, or speakers differ in the number of word tokens they contribute to a corpus. Models in this second group differ, however, in the way they average over speaker- (or text-)specific frequencies. Thus, we can summarize a distribution of frequencies on the log scale, and then transform this mean log rate into a normalized frequency. Or we can summarize the distribution on the scale of normalized frequencies. From the viewpoint of interpretation, it is essential to realize that these two estimates represent the median and the mean of the distribution of text-level normalized frequencies. We saw how the {marginaleffects} package can be used to construct both types of predictions.\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nEgbert, Jesse, and Brent Burch. 2023. “Which Words Matter Most? Operationalizing Lexical Prevalence for Rank-Ordered Word Lists.” Applied Linguistics 44 (1): 103–26. https://doi.org/10.1093/applin/amac030.\n\n\nHardie, Andrew. 2012. “CQPweb — Combining Power, Flexibility and Usability in a Corpus Analysis Tool.” International Journal of Corpus Linguistics 17 (3): 380–409. https://doi.org/10.1075/ijcl.17.3.04har.\n\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Frequency Data {II:} {Texts} of\n    Disproportionate Length},\n  date = {2025-05-15},\n  url = {https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion_unbalanced/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Frequency Data II: Texts\nof Disproportionate Length.” May 15, 2025. https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion_unbalanced/."
  },
  {
    "objectID": "posts/2025-05-09_counts_overdispersion/index.html",
    "href": "posts/2025-05-09_counts_overdispersion/index.html",
    "title": "Modeling clustered frequency data I: Texts of similar length",
    "section": "",
    "text": "When describing or modeling corpus-based frequency counts, the fact that a corpus is divided into texts has consequences for statistical modeling. For count variables, there are different options for modeling such data. This blog post contrasts approaches that differ in the way they represent (or account for) the non-independence of data points and looks at a setting where texts are very similar in length.\n\n\nR setup\nlibrary(tidyverse)          # for data wrangling and visualization\nlibrary(dataverse)          # for downloading data from TROLLing\nlibrary(marginaleffects)    # to compute model-based estimates\nlibrary(MASS)               # to fit a negative binomial regression model\nlibrary(corpora)            # to calculate a log-likelihood score\nlibrary(kableExtra)         # for drawing html tables\nlibrary(lme4)               # to fit mixed-effects regression models\nlibrary(lattice)            # for data visualization\nlibrary(gamlss)             # for drawing gamma densities\n\n# pak::pak(\"lsoenning/uls\") # install package \"uls\"\nlibrary(uls)                # for ggplot2 dotplot theme\n\nsource(\"C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R\")\n\n\n\nCase study: The frequency of should in written AmE of the 1960s and 1990s\nOur focus will be on the frequency of the modal verb should in written American English, and we will rely on data from the Brown and Frown Corpus. This allows us to work with straightforward research questions about normalized frequencies and their comparison, which are quite common in corpus work. The following questions guide our analysis:\n\nWhat is the frequency of should in written American English of the early 1960s and early 1990s?\nHas its frequency changed over time?\n\nWe will consider the imbalance across genres in the Brown Family of corpora a meaningful feature of the population of interest and therefore not adjust our estimates for the differential representation of these text categories. For an alternative approach, see this blog post.\nWe start by downloading the data from the TROLLing archive:\n\ndat &lt;- get_dataframe_by_name(\n    filename  = \"modals_freq_form.tsv\",\n    dataset   = \"10.18710/7LNWJX\",\n    server    = \"dataverse.no\",\n    .f        = read_tsv,\n    original  = TRUE\n  )\n\nThe table we have downloaded contains text-level frequencies for nine modal verbs from six members of the Brown Family (Brown, Frown, LOB, FLOB, BE06, AmE06). It includes the following variables:\n\ntext_id: The text ID used in the Brown Family corpora (“A01”, “A02”, …)\nmodal: the modal verb\nn_tokens: number of occurrences of the modal verb in the text\ncorpus: member of the Brown Family\ngenre: broad genre (Fiction, General prose, Learned, Press)\ntext_category: subgenre\nn_words: length of the text (number of word tokens)\ntime_period: time period represented by the corpus\nvariety: variety of English represented by the corpus\n\n\nstr(dat)\n\n\n\n'data.frame':   27000 obs. of  9 variables:\n $ text_id      : chr  \"A01\" \"A01\" \"A01\" \"A01\" ...\n $ modal        : chr  \"can\" \"could\" \"may\" \"might\" ...\n $ n_tokens     : num  1 0 1 1 3 0 6 14 9 4 ...\n $ corpus       : chr  \"Brown\" \"Brown\" \"Brown\" \"Brown\" ...\n $ genre        : chr  \"press\" \"press\" \"press\" \"press\" ...\n $ text_category: chr  \"press_reportage\" \"press_reportage\" \"press_reportage\" \"press_reportage\" ...\n $ n_words      : num  2206 2206 2206 2206 2206 ...\n $ time_period  : num  1961 1961 1961 1961 1961 ...\n $ variety      : chr  \"AmE\" \"AmE\" \"AmE\" \"AmE\" ...\n\n\nWe extract the data for should in Brown and Frown:\n\nshould_data &lt;- dat |&gt; \n  filter(\n    corpus %in% c(\"Brown\", \"Frown\"),\n    modal == \"should\"\n  )\n\nshould_Brown &lt;- should_data |&gt; \n  filter(\n    corpus == \"Brown\")\n\nshould_Frown &lt;- should_data |&gt; \n  filter(\n    corpus == \"Frown\")\n\n \n\n\nData description\nLet us start by summarizing key features of the data. There are 500 texts in each corpus:\n\ntable(should_data$corpus)\n\n\nBrown Frown \n  500   500 \n\n\nLet’s also take a look at the distribution of occurrence rates across texts. We first add a new variable that expresses the text-level frequency of should as a normalized frequency (per thousand words):\n\nshould_data$rate_ptw &lt;- should_data$n_tokens/should_data$n_words*1000\n\nThen we draw a histogram showing the distribution of these rates by Corpus.\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_histogram(binwidth = .15) +\n  facet_grid(corpus ~ .) +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(\"Number of texts\")\n\n\n\n\n\n\n\n\nFigure 1: Histogram showing the distribution of text-level occurrence rates by Corpus.\n\n\n\n\n\nSeeing that the normalized frequencies are skewed, we try a square-root-transformation, which somewhat mitigates the skew:\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw)) +\n  geom_histogram(binwidth = .05) +\n  facet_grid(corpus ~ .) +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(\"Number of texts\") +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\nFigure 2: Histogram showing the distribution of text-level occurrence rates by Corpus, using a square-root-scale trnasformation to mitigate the skew.\n\n\n\n\n\nThen we compare the distributions with a boxplot:\n\n\ndraw figure\nshould_data |&gt; \n  ggplot(aes(x = rate_ptw, y = corpus)) +\n  geom_boxplot() +\n  theme_classic_ls() +\n  xlab(\"Normalized frequency of should (per thousand words)\") +\n  ylab(NULL) +\n  scale_x_sqrt()\n\n\n\n\n\n\n\n\nFigure 3: Boxplot comparing the distribution of text-level occurrence rates in Brown and Frown, using a square-root trnasformation.\n\n\n\n\n\nAs for numerical summaries, a quick measure of the frequency of should in Brown can be calculated by dividing its corpus frequency by the size of the corpus. We can do the same for Frown. We will multiply these rates by 1,000, to get normalized frequencies ‘per thousand words’.\n\nfreq_should_Brown &lt;- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000\nfreq_should_Frown &lt;- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000\n\nAnd here they are, rounded to two decimal places:\n\nround(freq_should_Brown, 2)\n\n[1] 0.79\n\nround(freq_should_Frown, 2)\n\n[1] 0.68\n\n\nFor Brown, we get a rate of 0.79 per thousand words, and for Frown the rate is 0.68 per thousand words.\nFor a quick answer to the second question, we divide the rate in Frown by that in Brown, which gives us a rate ratio of 0.86. This tells us that the frequency of should in the 1990s was only 86% as large as that in the 1960s:\n\nround(freq_should_Frown / freq_should_Brown, 2)\n\n[1] 0.86\n\n\n\n\nPoisson regression\nWe start with a Poisson regression model, which does not take into account the fact that each corpus breaks down into 500 texts. Rather, the corpus is treated as an unstructured bag of words.\nWe can fit a Poisson model with the glm() function:\n\nm &lt;- glm(\n  n_tokens ~ corpus + offset(log(n_words)),\n  data = should_data,\n  family = \"poisson\")\n\nA model summary appears in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ corpus + offset(log(n_words)), family = \"poisson\", \n    data = should_data)\n\nCoefficients:\n            Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -7.14048    0.03315 -215.416  &lt; 2e-16 ***\ncorpusFrown -0.14774    0.04864   -3.037  0.00239 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2671.7  on 999  degrees of freedom\nResidual deviance: 2662.5  on 998  degrees of freedom\nAIC: 4352.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe use the {marginaleffects} package (Arel-Bundock, Greifer, and Heiss 2024) to calculate model-based predictions for the frequency of should in each corpus. These coincide with the plain corpus frequencies reported above. We specify the n_words = 1000 to get normalized frequencies ‘per thousand words’.\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.79\n0.74\n0.84\n\n\nFrown\n0.68\n0.64\n0.73\n\n\n\n\n\nThe function comparisons() in the {marginaleffects} package allows us to compare the two corpora in relative terms, in the form of a frequency ratio. The rate of should in Frown is only 86% of that in Brown, suggesting a decrease of 14 percentage points.\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\n\n\nQuasi-Poisson regression\nA Quasi-Poisson model introduces a dispersion parameter to adjust inferences for the non-independence of the data points. This parameter, \\(\\phi\\), is estimated on the basis of a global \\(\\chi^2\\) statistic of model (mis)fit, and it is then used to adjust the standard errors returned by the model, which are multiplied by \\(\\sqrt{\\phi}\\). For some more background on this way of accounting for overdispersion, see this blog post.\nWe can run a Quasi-Poisson model as follows:\n\nm &lt;- glm(\n  n_tokens ~ corpus + offset(log(n_words)),\n  data = should_data,\n  family = \"quasipoisson\")\n\nThe model is summarized in the following table:\n\nsummary(m)\n\n\nCall:\nglm(formula = n_tokens ~ corpus + offset(log(n_words)), family = \"quasipoisson\", \n    data = should_data)\n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -7.14048    0.06479 -110.212   &lt;2e-16 ***\ncorpusFrown -0.14774    0.09507   -1.554    0.121    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 3.820299)\n\n    Null deviance: 2671.7  on 999  degrees of freedom\nResidual deviance: 2662.5  on 998  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe regression table tells us that the dispersion parameter is about 3.8, which means that the standard errors for the Quasi-Poisson model should be 1.95 times (\\(\\sqrt{3.8}\\)) larger than in the Poisson model.\nThe regression coefficients themselves do not change, and neither do the model-based predictions. We get the same point estimates, though with (appropriately) wider confidence intervals:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.79\n0.69\n0.89\n\n\nFrown\n0.68\n0.59\n0.78\n\n\n\n\n\nThe Quasi-Poisson model also returns the same relative difference between the corpora:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.7\n1.02\n\n\n\n\n\n\n\nNegative binomial regression\nNegative binomial regression explicitly takes into account the texts in the data, and represents the observed text-to-text variability in the frequency of should using a probability distribution. The model therefore has an additional parameter that represents the variability of text-level frequencies. As discussed in more detail in this blog post, this parameter controls the shape of a gamma distribution, which in turn describes the multiplicative variation in speaker-specific rates. More background is provided in this blog post.\nWe can fit a negative binomial model using the function glm.nb() in the {MASS} package (Venables and Ripley 2002):\n\nm &lt;- MASS::glm.nb(\n    n_tokens ~ corpus + offset(log(n_words)), \n    data = should_data)\n\nHere is the regression table for this model:\n\nsummary(m)\n\n\nCall:\nMASS::glm.nb(formula = n_tokens ~ corpus + offset(log(n_words)), \n    data = should_data, init.theta = 0.9278339214, link = log)\n\nCoefficients:\n            Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept) -7.13436    0.05699 -125.175   &lt;2e-16 ***\ncorpusFrown -0.15053    0.08166   -1.843   0.0653 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9278) family taken to be 1)\n\n    Null deviance: 1043.2  on 999  degrees of freedom\nResidual deviance: 1039.8  on 998  degrees of freedom\nAIC: 3569\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.9278 \n          Std. Err.:  0.0720 \n\n 2 x log-likelihood:  -3562.9510 \n\n\nModel-based predictions are again virtually identical to the ones from the Poisson and Quasi-Poisson model, with wider uncertainty intervals:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.80\n0.71\n0.89\n\n\nFrown\n0.69\n0.61\n0.76\n\n\n\n\n\nThe negative binomial model also returns the same relative difference between the corpora:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.72\n1\n\n\n\n\n\n\n\nPoisson regression with random intercepts\nAnother way of accounting for the structure in the data is to use a Poisson regression model with random intercepts on Speaker. This model also represents the observed variation among speakers using a probability distribution. Between-speaker variation is modeled on the scale of natural logarithms using a normal distribution. On the scale of the actual occurrence rates, this translates into a log-normal distribution. For a more detailed discussion of the structure of this model, see this blog post.\nWe can fit this model using the function glmer() in the R package {lme4} (Bates et al. 2015).\n\nm &lt;- lme4::glmer(\n    n_tokens ~ corpus + offset(log(n_words)) + (1 | text_id), \n    data = should_data,\n    family = \"poisson\",\n    control = glmerControl(optimizer=\"bobyqa\"))\n\nWe print a condensed regression table:\n\narm::display(m)\n\nlme4::glmer(formula = n_tokens ~ corpus + offset(log(n_words)) + \n    (1 | text_id), data = should_data, family = \"poisson\", control = glmerControl(optimizer = \"bobyqa\"))\n            coef.est coef.se\n(Intercept) -7.43     0.05  \ncorpusFrown -0.15     0.05  \n\nError terms:\n Groups   Name        Std.Dev.\n text_id  (Intercept) 0.76    \n Residual             1.00    \n---\nnumber of obs: 1000, groups: text_id, 500\nAIC = 3820.2, DIC = -1122.2\ndeviance = 1346.0 \n\n\nThe table tells us that the standard deviation of the random intercepts, i.e. the parameter describing the spread of the normal distribution representing text-to-text variation in the occurrence rate of should, is 0.76.\nAs discussed in detail in this blog post, there are two types of predictions we can calculate for the random-intercept Poisson model. Seeing that we are interested in the normalized frequency of should rather than its natural logarithm, we will want to back-transform model-based predictions to the scale of normalized frequencies. Since there is between-speaker variation, our model-based estimate will have to somehow average over speakers. The question is whether we want to average over speakers on the scale of natural logarithms (the model scale) or on the scale of normalized frequencies (the data scale).\n\nBy averaging on the data scale of normalized frequencies, we obtain the mean occurrence rate across texts.\nBy averaging on the model scale of log normalized frequencies, and then back-transforming this mean log rate, we obtain the median occurrence rate across texts.\n\nThis blog post provides a detailed illustration of these two types of frequency estimates.\nThrough appropriate combination of the regression coefficients for the fixed effects, we get averages over texts on the model scale. This is the estimated mean log rate of should in the population of interest. We can back-transform this into a normalized frequency. This summary measure, however, does not represent the mean over normalized frequencies, since the averaging was done on another scale (the model scale).\nIn our model, the intercept represents the mean log rate for Brown. If we add the coefficient for the predictor Corpus, we get the mean log rate for Frown. Back-transforming these values gives us:\n\n# Brown\nround(exp(fixef(m)[1]) * 1e3, 2)\n\n(Intercept) \n       0.59 \n\n# Frown\nround(exp(fixef(m)[1] + fixef(m)[2]) * 1e3, 2)\n\n(Intercept) \n       0.51 \n\n\nThese frequency estimates are lower than the ones we have obtained above. This is because they represent the median occurrence rate of should, and in a distribution that is skewed toward large values (see Figure 1), the median is always smaller than the mean.\nWe can also calculate these two types of estimates using the {marginaleffects} package. To get the mean log rate of should, back-transformed to the normalized frequency scale, we run the following code. The argument re.form = NA tells the function to ignore the between-speaker variation:\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = NA),\n  re.form = NA) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.59\n0.53\n0.65\n\n\nFrown\n0.51\n0.46\n0.56\n\n\n\n\n\nThe corresponding relative difference between the corpora can be retrieved as follows:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = NA),\n  re.form = NA,\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\nTo get (something close to) the mean normalized frequencies we calculated above, we can ask the function avg_predictions() to average predictions over the speakers in the sample. This means that the by-speaker random intercepts are incorporated into the model predictions. The model-based speaker intercepts are used to get a predicted normalized frequency for each speaker, and these are then averaged.\n\navg_predictions(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = unique)) |&gt; \n  tidy() |&gt; \n  dplyr::select(corpus, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncorpus\nestimate\nconf.low\nconf.high\n\n\n\n\nBrown\n0.77\n0.69\n0.85\n\n\nFrown\n0.66\n0.59\n0.73\n\n\n\n\n\nThe relative difference between the groups remains the same:\n\navg_comparisons(\n  m, \n  variables = \"corpus\",\n  newdata = datagrid(\n    n_words = 1000,\n    text_id = unique),\n  comparison = \"ratio\") |&gt; \n  tidy() |&gt; \n  dplyr::select(contrast, estimate, conf.low, conf.high) |&gt; \n  mutate(across(2:4, \\(x) round(x, 2))) |&gt; \n  kable()\n\n\n\n\ncontrast\nestimate\nconf.low\nconf.high\n\n\n\n\nmean(Frown) / mean(Brown)\n0.86\n0.78\n0.94\n\n\n\n\n\n\n\nComparison\nFigure 4 compares the estimated average predictions we have collected in this blog post. Two points are noteworthy:\n\nThe uncertainty intervals suggested by the Poisson model are narrower than the ones based on the other models. This is because the other model explicitly take into account the non-independence of observations.\nExcept for the median occurrence rate estimate based on the random-intercept Poisson model, all models return nearly identical estimates of the normalized frequency of should in Brown and Frown.\n\n\n\ndraw figure\npred_models &lt;- tibble(\n  model = rep(c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean rate)\",\n                \"Random-intercept\\nPoisson\\n(median rate)\"), each = 2),\n  corpus = rep(c(\"Brown\", \"Frown\"), 5),\n  estimate = c(pred_poisson$estimate,\n               pred_quaspoi$estimate,\n               pred_negbin$estimate,\n               pred_ranef_m$estimate,\n               pred_ranef_c$estimate),\n  ci_lower = c(pred_poisson$conf.low,\n               pred_quaspoi$conf.low,\n               pred_negbin$conf.low,\n               pred_ranef_m$conf.low,\n               pred_ranef_c$conf.low),\n  ci_upper = c(pred_poisson$conf.high,\n               pred_quaspoi$conf.high,\n               pred_negbin$conf.high,\n               pred_ranef_m$conf.high,\n               pred_ranef_c$conf.high)\n)\n\npred_models$model &lt;- factor(\n  pred_models$model,\n  levels = c(\"Poisson\", \"Quasi-Poisson\", \"Negative\\nbinomial\", \n                \"Random-intercept\\nPoisson\\n(mean rate)\",\n                \"Random-intercept\\nPoisson\\n(median rate)\"),\n  ordered = TRUE\n)\n\n\npred_models |&gt; \n  ggplot(aes(x = corpus, y = estimate, group = model)) +\n  geom_point() +\n  geom_line() +\n  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  facet_grid(. ~ model) +\n  theme_classic_ls() +\n  scale_y_continuous(limits = c(.4, .95), expand = c(0,0)) +\n  ylab(\"Occurrence rate of should\\n(per thousand words)\\n\") +\n  xlab(NULL) +\n  coord_cartesian(clip=\"off\")\n\n\n\n\n\n\n\n\nFigure 4: Comparison of model-based predictions for the average occurrence rate of should in the two corpora.\n\n\n\n\n\n\n\nSummary\nThis blog post contrasted different approaches to modeling clustered frequency counts. If count data are grouped by text (or speaker), a Poisson regression model is usually too restrictive. This is because it is insensitive to the very likely possibility that the normalized frequency of interest varies among texts (or speakers). We looked at different alternatives, and observed that all of these yielded very similar results. This will be the case in situations where texts (or speakers) are similar in length. We also noted that the random-intercept Poisson model is capable of producing two different average frequency estimates: The mean or the median occurrence rate (i.e. normalized frequency) of the item across texts (or speakers).\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent, Noah Greifer, and Andrew Heiss. 2024. “How to Interpret Statistical Models Using marginaleffects for R and Python.” Journal of Statistical Software 111 (9): 1–32. https://doi.org/10.18637/jss.v111.i09.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Modeling Clustered Frequency Data {I:} {Texts} of Similar\n    Length},\n  date = {2025-05-14},\n  url = {https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Modeling Clustered Frequency Data I: Texts\nof Similar Length.” May 14, 2025. https://lsoenning.github.io/posts/2025-05-09_counts_overdispersion/."
  },
  {
    "objectID": "posts/2025-05-19_data_vis_workflow/index.html",
    "href": "posts/2025-05-19_data_vis_workflow/index.html",
    "title": "Exporting R graphics: A basic workflow",
    "section": "",
    "text": "When it comes to exporting graphs from R, it took me some time to develop a workflow that I am happy with. In this blog post, I provide a brief run-down and illustrate the tools I use. The general steps are the following:\n\nDraw the figure in R, with the result almost looking the way I want it to\nSave as a PDF file\nIf necessary, do some polishing using Adobe Acrobat\nIf needed, create a PNG/JPG version of the PDF file\n\n\n\nR setup\n# These packages may need to be installed first:\n# pak::pak(\"lsoenning/uls\")\n# pak::pak(\"lsoenning/wls\")\n\nlibrary(wls)       # for illustrative data\nlibrary(uls)       # for customizing lattice plots\nlibrary(tidyverse) # for data wrangling and visualization\nlibrary(lattice)   # for data visualization\n\n\n\nStep 1: Draw the figure in R\nI almost exclusively use the R packages {lattice} (Sarkar 2008) and {ggplot2} (Wickham 2016) for data visualization. Both generate graphs that are very close to publication quality. Depending on how much extra formatting is needed, it probably makes sense to do parts of the fine-tuning with different software (see below). This is because certain modifications may require pretty involved code. For instance, if I want to italicize a single word in a longer axis title, I don’t do this in R.\nThe following code draws a histogram using {ggplot2}. The data show the usage rate of actually (per million words) for speakers in the Spoken BNC2014 (Love et al. 2017). (These data were analyzed in Sönning and Krug 2022; see Sönning and Krug 2021 for details).\n\ndata_actually |&gt; \n  ggplot(aes(x = rate_pmw)) +\n  geom_histogram() +\n  xlab(\"Usage rate of actually (per million words)\") +\n  ylab(\"Number of speakers\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNext, we draw a similar plot using {lattice}:\n\nhistogram(\n  ~ rate_pmw, \n  data = data_actually, \n  col = \"grey\",\n  par.settings = lattice_ls,\n  axis = axis_L,\n  nint = 35,\n  xlab = \"Usage rate of actually (per million words)\",\n  ylab = \"Number of speakers\")\n\n\n\n\n\n\n\n\n\n\nStep 2: Export as PDF\nI always save graphs as a PDF – this is for several reasons:\n\nIf the figure ends up in a publication, a vector image provides the best resolution. I always forward figures as PDF files to print production.\nPDF files require relatively little storage space.\nThe figure can be polished using graphics software, including Adobe Acrobat.\n\nThe function ggsave() makes it very convenient to write images created using {ggplot2} to file. Simply running ggsave() including the storage path saves the file using the current specifications for height and width. These settings can be modified using arguments of the same name. It is good practice to store images in the (manually created) “figures” sub-directory of the project folder.\n\ndata_actually |&gt; \n  ggplot(aes(x = rate_pmw)) +\n  geom_histogram() +\n  xlab(\"Usage rate of actually (per million words)\") +\n  ylab(\"Number of speakers\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggsave(\"figures/histogram_ggplot.pdf\")\n\nGraphs drawn with {lattice} can be saved using the function cairo_pdf(), which works in a similar way. We start by creating a graph object (p1 for plot 1) and then write this object to file. cairo_pdf() opens what is called a “device”, which we need to close afterwards using dev.off():\n\np1 &lt;- histogram(\n  ~ rate_pmw, \n  data = data_actually, \n  col = \"grey\",\n  par.settings = lattice_ls,\n  axis = axis_L,\n  nint = 35,\n  xlab = \"Usage rate of actually (per million words)\",\n  ylab = \"Number of speakers\")\n\ncairo_pdf(\"figures/histogram_lattice.pdf\", width = 4, height = 2)\np1\ndev.off()\n\npng \n  2 \n\n\nBoth PDF files can now be found in the folder “figures”.\n\n\nStep 3: Polishing in Adobe Acrobat\nIf I want to add final touches to the image, I use Adobe Acrobat. I am sure there is better software out there, but this alternative is pre-installed on my work laptop and good enough for my purposes. Regardless of the software you use, two things are important:\n\nBefore you invest time into fine-tuning, make sure that you are really looking at the final version of the graph. If you decide to alter the figure (or if the data change), you will have to redo all of the manual touches.\nStore the modified PDF under a different name – otherwise it may be overwritten when you (accidentally) rerun your code. I always add the suffix “_modified” to the file name, to protect the new version of the PDF file.\n\nIn Adobe Acrobat, click the tab “Edit” to start editing the PDF image:\n\n\n\n\n\nItalicizing the word “actually” in the x-axis title is then pretty straightforward:\n\n\n\n\n\nThere are a few things I sometimes change in PDF images:\n\nMove elements (e.g. text boxes, lines, etc.)\nAdd annotations in the form of text boxes (this only works by copying an existing text box and then modifying the text)\nChange font size and/or color\nCrop page to remove white figure margins\n\n\n\nStep 4: Create PNG/JPG version if necessary\nThere are two purposes for which I need raster-based versions of my graphs:\n\nWhen writing a paper, to include graphs into the Word document, and\nfor presentation slides (which I create using PowerPoint).\n\nThe quick way to get a raster-based version is via a screenshot:\n\nOpen the PDF\nZoom in for sufficient resolution\nTake a screenshot\nPaste the screenshot into the software IrfanView\nUse the mouse to draw a frame around the part of the screenshot I want to use\nHit Ctrl + C (to copy)\nPaste into Word or PowerPoint using Ctrl + V\n\n\n\n\n\n\nAlternatively, the PDF file can be properly converted into a PNG file using Adobe Acrobat (All tools &gt; Export a PDF &gt; Image format). The resolution (pixels/inch) can then be specified manually, which is attractive if the PNG file needs to be high(er)-resolution.\n\n\n\n\n\nReferences\n\nLove, Robbie, Claire Dembry, Andrew Hardie, Vaclav Brezina, and Tony McEnery. 2017. “The Spoken BNC2014: Designing and Building a Spoken Corpus of Everyday Conversations.” International Journal of Corpus Linguistics, 319–44. https://doi.org/10.1075/ijcl.22.3.02lov.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with r. New York: Springer.\n\n\nSönning, Lukas, and Manfred Krug. 2021. “Actually in contemporary British speech: Data from the Spoken BNC corpora.” DataverseNO. https://doi.org/10.18710/A3SATC.\n\n\n———. 2022. “Comparing Study Designs and down-Sampling Strategies in Corpus Analysis: The Importance of Speaker Metadata in the BNCs of 1994 and 2014.” In Data and Methods in Corpus Linguistics, 127–60. Cambridge University Press. https://doi.org/10.1017/9781108589314.006.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. New York: Springer.\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Exporting {R} Graphics: {A} Basic Workflow},\n  date = {2025-05-20},\n  url = {https://lsoenning.github.io/posts/2025-05-19_data_vis_workflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Exporting R Graphics: A Basic\nWorkflow.” May 20, 2025. https://lsoenning.github.io/posts/2025-05-19_data_vis_workflow/."
  },
  {
    "objectID": "posts/2025-06-16_dendrogram_grouping_cues/index.html",
    "href": "posts/2025-06-16_dendrogram_grouping_cues/index.html",
    "title": "Color-coded dendrograms using the R function A2Rplot()",
    "section": "",
    "text": "Dendrograms are exploratory tools that help identify clusters, i.e. groups of relatively similar units, in multivariate data sets. Unfortunately, the resulting tree-like representations provide weak visual cues to the clusters in the data. The R function A2Rplot(), which was written by Romain Francois, uses color to distinguish a user-specified number of clusters in a dendrogram. Importantly, it not only colors the labels sitting at the final nodes, but also the branches that connect the members of a cluster. This provides much stronger visual cues to the groups in the data, and it allows us to recognize their degree of internal (dis)similarity more easily. In the following, I briefly describe the basic use of this function.\n\nPreparation: Clustering analysis\nFor illustration, we use the built-in dataset mtcars in R. We start by converting the data frame into a distance matrix:\n\nmtcars_dist &lt;- dist(mtcars)\n\nThen we carry out a clustering analysis:\n\nmtcars_hclust &lt;- hclust(mtcars_dist)\n\n\n\nStandard dendrogram\nThe plot() function can be used to produce a standard dendrogram:\n\nplot(mtcars_hclust, xlab = \"\", main = \"\")\n\n\n\n\n\n\n\n\n\n\nDendrogram using A2Rplot()\nWe download the R function from Romain Francois’ (old) website:\n\nsource(\"http://addictedtor.free.fr/packages/A2R/lastVersion/R/code.R\")\n\nThe function A2Rplot() can now be used to draw a colored dendrogram. The argument k specifies the number of clusters that should be distinguished using different colors:\n\nA2Rplot(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE)\n\n\n\n\n\n\n\n\nNote how the the colored branches allow us to quickly recognize the degree of dissimilarity among the members of a cluster: The higher the horizontal line joining branches, the greater the dis(!)similarity between the conjoined units. This means that the green cluster is the most homogeneous one.\nYou can change the colors in the plot using the following arguments:\n\ncol.up The branches above the groups\ncol.down Colors for the groups\n\nThe following code backgrounds the branches above the clusters and uses a colorblind-friendly set of hues for the clusters:\n\nA2Rplot(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE,\n  col.up = \"grey\", \n  col.down = c(\"#E69F00\", \"#56B4E9\", \"#009E73\"))\n\n\n\n\n\n\n\n\nA slightly modified version of the function makes a few minor changes to the appearance of the plot (no title and annotation, thinner branches, more room for labels, avoidance of dashed line patterns). It can be downloaded from my website:\n\nsource(\"https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/A2Rplot_modified.R\")\n\nHere is the resulting alternative version of the dendrogram:\n\nA2Rplot_modified(\n  mtcars_hclust, \n  k = 3, \n  boxes = FALSE,\n  col.up = \"grey\", \n  col.down = c(\"#E69F00\", \"#56B4E9\", \"#009E73\"))\n\n\n\n\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{sönning2025,\n  author = {Sönning, Lukas},\n  title = {Color-Coded Dendrograms Using the {R} Function {`A2Rplot()`}},\n  date = {2025-06-16},\n  url = {https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSönning, Lukas. 2025. “Color-Coded Dendrograms Using the R\nFunction `A2Rplot()`.” June 16, 2025. https://lsoenning.github.io/posts/2025-06-16_dendrogram_grouping_cues/."
  }
]