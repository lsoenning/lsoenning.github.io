---
title: "Clustering in the data affects statistical uncertainty intervals"
subtitle: "Obstacles to replication in corpus linguistics"
description: "This blog post is part of a small series on obstacles to replication in corpus linguistics. It deals with a prevalent issue in corpus data analysis: the non-independence of data points that results from clustered (or hierarchical) data layouts. I show how an inadequate analysis can produce unduly narrow expectations of a replication study and therefore raise the bar too high."
date: 2025-05-01
categories: [corpus linguistics, replication, models]
citation: 
  url: https://lsoenning.github.io/posts/2023-11-08-modals_replication/ 
draft: true
---

This blog post discusses two features of corpus data that make it more complicated to talk about replicability in corpus linguistics. The first one concerns the issue of non-independence of observations: Tokens extracted from a corpus are almost always clustered by source (author or speaker). The second one concerns the lack of balance in corpus data. In contrast to experiments, where the researcher has (almost) full control over the distribution of data points across the conditions of interest, the spread of corpus hits across the levels of relevant predictors is uneven.

```{r}
#| warning: false
#| message: false
#| code-fold: true
#| code-summary: "Load R packages"

library(tidyverse)       # for data wrangling and visualization
library(dataverse)       # for downloading data from TROLLing
library(marginaleffects) # to compute model-based estimates
library(MASS)            # to fit a negative binomial regression model
library(corpora)         # to calculate a log-likelihood score
library(ggthemes)


source("C:/Users/ba4rh5/Work Folders/My Files/R projects/my_utils_website.R")
```

### Case study: The frequency of *should* in written AmE of the 1960s and 1990s

We will consider, as an illustrative set of data, the frequency of modals in recent British and American English. These make for an interesting case study, not only because they have generated an exchange on replicability in the recent literature [@Leech2003; @Millar2009] and they have been used by @McEnery_Brezina2022 to illustrate key ideas about replication. The data on which the following (selective) discussion is based are available from TROLLing [@Soenning2024], which should facilitate follow-up discussions.


The following questions guide our analysis:

- What is the frequency of *should* in written American English of the 1960s and 1990s?
- Has the frequency of *should* changed over time?


#### Data

The illustrative data used in this blog post is available from *TROLLing* [@Soenning2024]. We start by downloading the data directly from this archive:

```{r}
#| message: false
#| warning: false

dat <- get_dataframe_by_name(
    filename  = "modals_freq_form.tsv",
    dataset   = "10.18710/7LNWJX",
    server    = "dataverse.no",
    .f        = read_tsv,
    original  = TRUE
  )
```

```{r}
str(dat)
```

Next, we restrict our attention to the Brown and Frown corpus and prepare the data for analysis.

```{r}
#| code-fold: true
#| code-summary: "Load and prepare data"
 
d_modals <- subset(dat, corpus %in% c("Brown", "Frown"))

d_modals$time_period <- factor(d_modals$time_period)
d_modals$genre <- factor(d_modals$genre)

contrasts(d_modals$genre) <- contr.sum(4)
contrasts(d_modals$time_period) <- contr.sum(2)

should_data <- subset(d_modals, modal=="should")
should_Brown <- subset(d_modals, modal=="should" & corpus=="Brown")
should_Frown <- subset(d_modals, modal=="should" & corpus=="Frown")

should_learned <- subset(d_modals, modal=="should" & genre=="learned")
should <- subset(d_modals, modal=="should")
```

Brown and Frown each consist of 500 texts, which are sampled from four different genres. The following table shows the word count and number of texts for each genre:

```{r}
should_Brown |> 
  group_by(genre) |> 
  dplyr::summarize(
    n_words = sum(n_words),
    n_texts = n()) |> 
  mutate(p_words = round(n_words/sum(n_words), 3),
         p_texts = round(n_texts/sum(n_texts), 3)) |> 
  transmute(
    Genre = c("Fiction", "General prose", "Learned", "Press"),
    Words = paste0(format(round(as.numeric(n_words), 0), nsmall=0, big.mark=","), " (", format(round(p_words*100, 1), nsmall = 1), "%)"),
    Texts = paste0(n_texts, " (", format(round(p_texts*100, 1), nsmall = 1), "%)")) |> 
  kbl() |> 
  kable_paper("hover", full_width = F)
```

#### Crude answers to our research questions

We may obtain a quick (and dirty) answer to our questions as follows. To measure the frequency of *should* in Brown, we divide the coprus frequency of the item by the size of the corpus. We can do the same for Frown. We will multiply these rates by 1,000, to get normalized frequencies 'per 1,000 words':

```{r}
freq_should_Brown <- sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000
freq_should_Frown <- sum(should_Frown$n_tokens) / sum(should_Frown$n_words) * 1000

round(freq_should_Brown, 2)
round(freq_should_Frown, 2)
```

For Brown, we get a rate of 0.79 per thousand words, and for Frown the rate of occurrence is 0.68 per thousand words.

To also get a quick answer to the second question, we divide the rate in Frown by that in Brown, which gives us a rate ratio of 0.86. This tells us that the frequency of *should* in the 1990s was only 86% as large as that in the 1960s:

```{r}
round(freq_should_Frown / freq_should_Brown, 2)
```

To see whether this frequency difference is "statistically significant", a likelihood-ratio statistic may be computed. This score is based on a simple 2x2 table, which contains the frequency of *should* in each corpus, and the number of words in each corpus. We use the function `keyness()` in the R package `{corpora}` [@Evert2023] to calculate a likelihood-ratio test:

```{r}
keyness(f1 = sum(should_Brown$n_tokens),
        n1 = sum(should_Brown$n_words),
        f2 = sum(should_Frown$n_tokens),
        n2 = sum(should_Frown$n_words), 
        measure = "G2")
```

This returns a log-likelihood score of 9.3, which is close to the one reported by @Leech2003 [p. 228] and indicates a "statistically significant" difference in normalized frequency between the two corpora.[^The difference in log-likelihood scores is most likely due to the different corpus sizes underlying these analyses. CQPweb reports larger corpus sizes.]

These crude ways of assessing and testing frequencies and their differences in corpora are straightforward to carry out and therefore provide quick answers to our questions. We now look at how these estimates (and *p*-values) may be misleading, or may not answer the question we really had in mind. Before we go further, however, we should note that the following elaborations are not meant to discredit the work done by Geoffrey Leech in the early 2000s. In fact, @Leech2003 provides a balanced consideration of frequency changes in the English modal system. The diachronic patterns he observed were remarkably consistent across the 11 modal verbs he studied, which strengthened his conclusion. Further, when briefly referring to the log-likelihood scores for time differences, he explicitly noted that "too much
should not be made of significance tests in comparative corpus studies" [-@Leech2003, p. 228].



### Clustering in the data affects statistical uncertainty intervals

Interest in corpus-based work often centers on the frequency of a structure in language use. This feature is usually expressed as a normalized frequency (or occurrence rate), expressed, say, as 'per million words'. Since any corpus is a sample of language use from the domain(s) of interest, these normalized frequencies are sample statistics, which in turn often serve as estimates of population parameters.

#### Data structure

The Brown corpus, for instance, contains a sample of written American English from the early 1960s, which represents a purposefully compiled list of genres and sub-genres. If we look at the frequency of *should* in the Brown corpus, it is unlikely that our linguistic interest is limited to the 500 texts (or text excerpts) that are found in Brown. Rather, we would consider this as a sample from the population of interest -- written American English in the 1960s.

When extrapolating to this underlying language variety, our sample size is 500 (the number of texts in the corpus) rather than 1 million (the number of word tokens in Brown). In the sampling literature, the 500 texts would be considered the primary sampling units, and the roughly 2,000 words per text file in turn constitute the secondary sampling units (if a text file represents an except from a longer text). The 1 million word tokens in Brown are therefore clustered, or structured hierarchically. They a grouped by text file.


#### Data description

If we take a look at the distribution of *should* in the Brown Corpus, we should consider its occurrence rate at the level of the individual texts. This means that we first calculate normlaized frequencies at the level of the text files and then look at the distribution of these.

@fig-should-dotdiagram shows the distribution of these text-level occurrence rates using a dot diagram. Each dot in the figure represents a text file. Since there are many texts with a frequency of 0 (*n* = 174, or 35%), the dot diagram is flipped: The y-axis shows the normalized frequency (expressed as 'per 1,000 words') and the dots form horizontal piles. We note that there are very few texts in which *should* occurs with a frequency greater than 5 per thousand words. Most texts (*n* = 382, or 76%) show at most 2 instances (i.e. a rate of roughly 2.5 ptw or lower).

```{r}
#| fig-width: 7
#| fig-height: 2.3
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-dotdiagram
#| fig-cap: "*Should* in the Brown Corpus: Dot diagram showing the distribution of normalized frequencies across the 500 texts in the corpus."
#| message: false
#| warning: false

d_modals |> filter(
  corpus == "Brown",
  modal == "should") |> 
  mutate(rate_ptw = n_tokens / n_words * 1e3) |> 
  ggplot(aes(x = rate_ptw)) +
  geom_dotplot(method = "histodot", binwidth = .2, stackratio = .9) +
  theme_dotplot_vertical() +
  scale_y_continuous(expand = c(.004,0)) +
  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +
  xlab("Normalized frequency of should\n(per 1,000 words)") +
  annotate("text", y = .5, x = 5, label = "Each dot denotes a text", size = 3, col = "grey30") +
  coord_flip()
```

Our analyses will include Genre as a predictor -- specifically, the broad text categories "Fiction", "General prose", "Learned", and "Press". We therefore break down the text-level occurrence rates by Genre. @fig-should-dotdiagram-genre shows that occurernce rates tend to be lower in Fiction, and that the outliers, with exceptionally high rates of *should* are found in General prose.

```{r}
#| fig-width: 7
#| fig-height: 2.7
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-dotdiagram-genre
#| fig-cap: "*Should* in the Brown Corpus: Distribution of text-level normalized frequencies by Genre."
#| message: false
#| warning: false

d_modals |> filter(
  corpus == "Brown",
  modal == "should") |> 
  mutate(genre_nice = factor(
    genre, 
    levels = c("fiction", "general_prose", "learned", "press"),
    labels = c("Fiction\n", "General prose\n", "Learned\n", "Press\n"))) |> 
  mutate(rate_ptw = n_tokens / n_words * 1e3) |> 
  ggplot(aes(x = rate_ptw)) +
  geom_dotplot(method = "histodot", binwidth = .2, stackratio = .9) +
  theme_dotplot_vertical() +
  facet_grid(. ~ genre_nice) +
  scale_y_continuous(expand = c(.008,0)) +
  scale_x_continuous(expand = c(0,0), breaks = c(0, 5, 10)) +
  xlab("Normalized frequency of should\n(per 1,000 words)") +
  ggh4x::force_panelsizes(cols = c(2,3,1.2,1.2)) +
  coord_flip()
```

#### Statistical modeling

We can use a statistical model to learn about the uncertainty surrounding our sample-based estimates. This uncertainty is often reported in terms of confidence intervals (or standard errors), which indicate the precision of estimates, based on the model and data. In order to arrive at reliable uncertainty estimates, a suitable model must be used. For instance, it must adequately represent the structure of the data -- in our case, the fact that Brown is primarily a sample of 500 texts (rather than 1 million words) from the language variety of interest.

The use of an inadequate model, which does not take into account the clustered nature of the data, will usually suggest a higher level of precision than is warranted -- in other words, we will get overconfidence intervals. This will happen if we analyze the current data with a Poisson model. This model does not account for the structure of the data in the sense that it makes no provision for the possibility that the usage rate of *should* may vary from text to text. Thus, it assumes that the underlying frequency of *should* is the same for each text, with observable variation in rates being exclusively due to sampling variation. The "underlying frequency" can be thought of as the propensity of the author(s) to use *should* in the particular context of language use represented by the text. This means that the model does not allow for the possibility that there my be inter-speaker variation and variation depending on the context of language use (such as the genre). 

We will compare two modeling approaches, which are also discussed and contrasted in @Soenning_Krug2022, in the context of a similar research task (the frequency of *actually* in conversational British speech, as represented in the Spoken BNC2014).

Let's fit a Poisson model to these data using the base R function `glm()`, where the code chunk `offset(log(n_words))` represents the offset, which adjusts for the fact that text files differ (slightly) in length (for some background on this, see [this blog post](https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/){target="_blank"}). 

```{r}
#| results: false

m_poi <- glm(
	n_tokens ~ genre + offset(log(n_words)), 
	data = should_Brown, 
	family = poisson())

```

We also fit a negative binomial model to the data, which makes allowances for variation in occurrence rates across the 500 texts in the corpus. It does so via something similar to a standard deviation parameter, which expresses text-to-text variation in the normalized frequency of *should*. [This blog post](https://lsoenning.github.io/posts/2023-11-16_negative_binomial/){target="_blank"} provides some background on the negative binomial distribution.

We fit a negative binomial regression model using the function `glm.nb()` in the `{MASS}` package [@Venables_Ripley2002]:

```{r}
#| results: false

m_nb <- MASS::glm.nb(
	n_tokens ~ genre + offset(log(n_words)), 
	data = should_Brown)
```


#### Model-based predictions (i.e. estimates)

The next step is to calculate model-based estimates of the frequency of *should*. To this end, we use the `predictions()` function in the very helpful `{marginaleffects}` package [@ArelBundock_etal2024]. As explained in some more detail in [this blog post](https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/){target="_blank"}, we use the `datagrid()` function to define the condition for which we wish to obtain estimates. In our case, these are the four genres. We also specify `n_words = 1000` in `datagrid()`, to obtain rates per 1,000 words. 

```{r}
preds_poi <- predictions(
  m_poi, 
  newdata = datagrid(
    genre = c("press", "general_prose", "learned", "fiction"),
    n_words = 1000)) |> 
  tidy()

preds_nb <- predictions(
  m_nb, 
  newdata = datagrid(
    genre = c("press", "general_prose", "learned", "fiction"),
    n_words = 1000)) |> 
  tidy()
```

Here is the (shortened) content of the output:

```{r}
preds_poi[,c(7,2,5,6)]
```

```{r}
preds_nb[,c(7,2,5,6)]
```

@fig-should-estimates compares these model-based estimates graphically. We observe that the point estimates are virtually identical, but the negative binomial model returns much wider error intervals.

```{r}
#| fig-width: 3.2
#| fig-height: 2.3
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-estimates
#| fig-cap: "Estimated normalized frequency of *should* by Genre: Comparison of estimates based on a Poisson and a negative binomial model."
#| message: false
#| warning: false

preds_poi$model <- "Poisson"
preds_nb$model <- "Negative binomial"

preds_conditions <- rbind(
  preds_nb[,c(10,7,2,5,6)],
  preds_poi[,c(10, 7,2,5,6)]
)

preds_conditions$genre_nice <- rep(c("Press", "General\nprose", "Learned", "Fiction"), 2)

preds_conditions |> 
  ggplot(aes(x=genre_nice, y=estimate, group=model, color=genre_nice, shape=model)) + 
  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +
  ylab("Normalized frequency\n(per 1,000 words)") +
  xlab(NULL) +
  scale_color_colorblind() +
  scale_shape_manual(values=c(21, 19)) +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,
                position = position_dodge(.3)) +
  geom_point(position = position_dodge(.3), fill="white") +
  theme_classic_ls() +
  theme(plot.subtitle = element_text(face = "italic")) +
  annotate("text", x = 4.6, y = .1, label = "Error bars: 95% confidence intervals", 
           adj=1, color = "grey40", size = 3) + 
  annotate("text", x = 3, y = c(.35, .25), label = c("Negative binomial", "Poisson"), 
           adj=0, color = "grey40", size = 3) + 
  annotate("point", x = 2.8, y = c(.35, .25), shape = c(21, 19), 
           color = "grey40", size = 1.5) +
  theme(legend.position = "none")
```

#### Relevance for replicability

In general, a statistical result is considered as having been replicated if a close replication, i.e. an exact repetition of a study using new data yields statistical conclusions that are consistent with those of the original study. Uncertainty intervals serve as a basis for judging whether the statistical results based on different sets of data are consistent or not. In fact, a 95% confidence interval raises expectations about likely values of replication estimates, i.e. estimates from a replication study. While the degree of consistency can also be evaluated statistically, it is useful to know that a 95% CI has a 83% probability of capturing the replication estimate (see Calin-Jageman & Cumming 2016: 116-117). This gives us some intuition about where replication estimates are likely to fall and guard against the erroneous expectation that the replication estimate should lie within the limits of a 95% CI.

The problem with overconfident error intervals, then, is that they produce unreasonable expectations about what should happen in a replication study. Put differently, a replication estimate may appear to be inconsistent with the original result even though it isn't. Had the original analysis used a more adequate model, the uncertainty bounds as well as the replication expectations would have been wider.

We can illustrate this issue using our data on the modals. We may consider the Frown Corpus a direct replication of the Brown Corpus, in the sense that is was was compiled using the same sampling design. Of course, the purpose of Frown, which records written American English in the early 1990s, was the documentation of diachronic trends in this variety. The goal was therefore to create a corpus that is a close to Brown as possible, apart from the difference in time. What this means is that Frown estimates that are consistent with those from the 1960s will be interpreted as indicating no change over time, while statistical differences will be interpreted as reflecting a diachronic change in this variety.

To illustrate, we fit two regression models, a Poisson and a negative binomial model, to a subset of the modals data, which includes all texts in the genre Learned from Brown and Frown. These model will have the same structure and include the variable Corpus as a predictor. 

We fit these models in the same way as above:

```{r}
#| results: false

m_poi_learned <- glm(
	n_tokens ~ corpus + offset(log(n_words)), 
	data = should_learned, 
	family = poisson())

m_nb_learned <- glm.nb(
	n_tokens ~ corpus + offset(log(n_words)), 
	data = should_learned)
```

Then we use the `{marginaleffects}` package to generate predictions.

```{r}
preds_poi_learned <- predictions(
  m_poi_learned, 
  newdata = datagrid(
    corpus = unique,
    n_words = 1000)) |> 
  tidy()

preds_nb_learned <- predictions(
  m_nb_learned, 
  newdata = datagrid(
    corpus = unique,
    n_words = 1000)) |> 
  tidy()
```

@fig-should-corpus compares these model-based estimates visually. The question of interest is whether the frequency estimate from Frown is consistent with the one from Brown. Due the wider uncertainty intervals, the estimates based on the negative binomial model appear more consistent with one another then those from the Poisson model. In other words, a Poisson analysis of these data might lead us to believ that there is a diachronic decrease in the frequency of *should*. 

```{r}
#| fig-width: 2.5
#| fig-height: 2.3
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-corpus
#| fig-cap: "Estimated frequency of *should* in the genre Learned: Comparison of estimates based on a Poisson and a negative binomial model."
#| message: false
#| warning: false

comparison_learned <- rbind(
  preds_poi_learned[,c(7,2,5,6)],
  preds_nb_learned[,c(7,2,5,6)])

comparison_learned$model <- rep(c("Poisson", "Negative binomial"), each = 2)

comparison_learned |> 
  ggplot(aes(x=corpus, y=estimate, group=model, shape=model)) + 
  scale_y_continuous(limits=c(0,NA), expand=c(0,0), breaks = c(0, .5, 1)) +
  ylab("Normalized frequency\n(per 1,000 words)") +
  xlab(NULL) +
  scale_color_colorblind() +
  scale_shape_manual(values=c(21, 19)) +
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high), width=.1, alpha=.5,
                position = position_dodge(.3)) +
  geom_point(position = position_dodge(.3), fill="white") +
  theme_classic_ls() +
  theme(plot.subtitle = element_text(face = "italic")) +
  annotate("text", x = 2.6, y = .1, label = "Error bars: 95% CIs", 
           adj=1, color = "grey40", size = 3) + 
  annotate("text", x = 1.5, y = c(.35, .25), label = c("Negative binomial", "Poisson"), 
           adj=0, color = "grey40", size = 3) + 
  annotate("point", x = 1.4, y = c(.35, .25), shape = c(21, 19), 
           color = "grey40", size = 1.5) +
  theme(legend.position = "none")
```
Both models also produce an estimate of the difference between the occurrence rates in the two corpora. This difference is represented by the coefficient for the predictor Corpus, which is very similar in th etwo models:

```{r}
coef(m_poi_learned)[2]
coef(m_nb_learned)[2]
```

Since count regression models operate on the link scale of natural logarithms, these coefficients express differences on the log scale. If we back-transform them using exponentiation, we obtain ratios. This means that the difference between the two corpora is expressed in relative terms. The normalized frequency of *should* in Frown is only 80% as large as that in Frown.

```{r}
round(exp(coef(m_poi_learned)[2]), 2)
round(exp(coef(m_nb_learned)[2]), 2)
```

We can obtain 95% CIs for these rate ratios using the function `confint()`. It returns the limits of a 95% CI on the log scale, which means we need to back-transform it to obtain rate ratios:

```{r}
round(exp(confint(m_poi_learned, "corpusFrown")), 2)
round(exp(confint(m_nb_learned, "corpusFrown")), 2)
```

We note that the statistical uncertainty surrounding the estimate based on the negative binomial model is considerably wider, meaning that the 1990 rate of *should* could be as small as 55% or as large as 120% of the 1960s rate. The Poisson model, in contrast, suggests that there is stronger indication of a decline in occurrence rate, which could amount to 66% or 102% of the rate in Brown. 

We note that the Poisson model is much more consistent with the interpretation of change over time.



### Imbalance across predictor levels affects data summaries

@Leech2003 [p. 228] looked at the frequency of different modals in the Brown family corpora. For *should* in Brown, he reports a raw frequency of 910 instances, which is consistent with the current output of CQPweb. Compared against the total number of words in Brown (1,148,454 words), this corresponds to a normalized frequency of 0.79 per thousand words (ptw). This occurrence rate is obtained in the following way: We divide the number of times *should* occurs in Brown by the total number of word tokens in the corpus, and then multiply by our preferred *N*, to express the rate as 'per *N* words'.

```{r}
round(
  sum(should_Brown$n_tokens) / sum(should_Brown$n_words) * 1000,
  2)
```

#### Frequency: Simple vs. weighted averages

This kind of frequency estimate, which is widespread in corpus linguistics, is referred to as the *corpus frequency* of an item or structure. @Egbert_Burch2023 contrasts this with a different type of estimate, the *mean text frequency*, which is found by first calculating normalized frequencies at the text level, and then averaging over these. Since the 500 texts in Brown are all around 2,000 words long, the mean text frequency almost coincides with the corpus frequency:

```{r}
round(
  mean(
    (should_Brown$n_tokens / should_Brown$n_words) *1000),
  2)
```

Since the word count is roughly balanced across texts, we need not worry about unbalanced distributions when forming frequency estimates. [This blog post](https://lsoenning.github.io/posts/2025-04-28_disproportionate_representation_speaker/){target="_blank"} discusses situations where corpus units (texts or speakers) differ in size, and how this can affect frequency estimates.

The Brown Corpus shows a different form of imbalance, however: The size of the four broad genres (Fiction, General Prose, Learned, Press) differs. The following table lists the word count, the number of *should* tokens, and the derived normalized (subcorpus) frequency of *should* for each genre. We note that the genre General prose accounts for 41% of the corpus size, while Learned and Press are relatively underrepresented.

```{r}
genre_rates <- should_Brown |> 
  mutate(rate_ptw = (n_tokens/n_words)*1e3) |> 
  group_by(genre) |> 
  dplyr::summarize(
    ptw = format(round(mean(rate_ptw), 3), nsmall = 3),
    corpus = unique(corpus),
    Tokens = sum(n_tokens),
    n_words = sum(n_words)) |> 
  mutate(p_words = n_words/sum(n_words))

genre_rates$Percent <- paste0("(", round(genre_rates$p_words*100), "%)")
genre_rates$Rate <- paste0(genre_rates$ptw, " ptw")
genre_rates$Words <- paste0(format(round(as.numeric(genre_rates$n_words), 0), nsmall=0, big.mark=","), " (", round(genre_rates$p_words*100), "%)")
genre_rates$Genre <- c("Fiction", "General prose", "Learned", "Press")

genre_rates |> dplyr::select(c(Genre, Words, Tokens, Rate)) |> 
  kbl() |> 
  kable_paper("hover", full_width = F)
```

This means that both the corpus frequency and the mean text frequency of *should* in Brown are pulled into the direction of its normalized frequency in General prose. To avoid this, we could instead weight all genres equally and simply average over the four rates listed in the table above. For Brown, this gives us the following frequency estimate:

```{r}
mean(c(0.480, 0.885, 0.999, 0.876))
```

We will refer to this as the *simple average* over genres. "Simple" just means unweighted. The corpus frequency (or mean text frequency) reported above is instead weighted by the representation of each genre in the corpus. We could recover this rate by calculating a weighted average, with the weights depending on genre size:

```{r}
round(
  weighted.mean(
    c(0.480, 0.885, 0.999, 0.876),
    w = c(.26, .41, .16, .18)),
  2)
```

For *should* in Brown, the *simple average* and the *weighted average* are very similar, indicating the the imbalance of word counts across genres does not affect the frequency estimate much.

Let's also look at *should* in Frown, starting with a tabular overview. The genres show the same representation in Frown, but the normalized frequencies of *should* differ from those in Brown.

```{r}
genre_rates <- should_Frown |> 
  mutate(rate_ptw = (n_tokens/n_words)*1e3) |> 
  group_by(genre) |> 
  dplyr::summarize(
    ptw = format(round(mean(rate_ptw), 3), nsmall = 3),
    corpus = unique(corpus),
    Tokens = sum(n_tokens),
    n_words = sum(n_words)) |> 
  mutate(p_words = n_words/sum(n_words))

genre_rates$Percent <- paste0("(", round(genre_rates$p_words*100), "%)")
genre_rates$Rate <- paste0(genre_rates$ptw, " ptw")
genre_rates$Words <- paste0(format(round(as.numeric(genre_rates$n_words), 0), nsmall=0, big.mark=","), " (", round(genre_rates$p_words*100), "%)")
genre_rates$Genre <- c("Fiction", "General prose", "Learned", "Press")

genre_rates |> dplyr::select(c(Genre, Words, Tokens, Rate)) |> 
  kbl() |> 
  kable_paper("hover", full_width = F)
```


Based on the information listed in the table, we can determine the simple average and the weighted average:

```{r}
round(
  mean(c(0.463, 0.684, 0.811, 0.903)), 
  2)

round(
  weighted.mean(c(0.463, 0.684, 0.811, 0.903), 
                w = c(.26, .41, .16, .17)),
  2)
```

In Frown, the discrepancy between these frequency estimates is greater. This is because the genres Learned and Press, which show relatively large occurrence rates of *should*, gain weight when calculating a simple (instead of a weighted) average: The mass for Learned increases from .16 to .25, that for Press from .17 to .25. @fig-should-Frown-imbalance provides a visual illustration of the differential weights and frequencies.

```{r}
#| fig-width: 3
#| fig-height: 2.3
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-Frown-imbalance
#| fig-cap: "Estimated frequency of *should* in the genre Learned: Comparison of estimates based on a Poisson and a negative binomial model."
#| message: false
#| warning: false

genre_rates$Genre_nice <- c("Fiction", "General\nprose", "Learned", "Press")

genre_rates |> 
  ggplot(aes(x = Genre_nice, y = as.numeric(ptw), size = n_words)) +
  geom_point(shape = 1) +
  theme_classic_ls() +
  ylab("Normalized frequency\n(per 1,000 words)") +
  xlab(NULL) +
  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +
  scale_size_area(max_size = 8) +
  geom_hline(yintercept = .72) +
  geom_hline(yintercept = .68, color = "grey40") +
  annotate("text", x = 4.6, y = c(.79, .62), label = c("simple", "weighted"), size = 3, color = c("black", "grey40"), adj = 1) +
  theme(legend.position = "none")
```



#### Model-based frequency estimates

When calculating model-based predictions, we can decide whether we want to form simple or weighted averages. The default behavior in the `{marginaleffects}` package is to use the in-sample distribution of predictor variables to calculate average predictions. This is to say that, unless explicitly tols to do otherwise, the functions in the package will average predictions over the cases in the estimation sample.

Let's take a look at how to produce simple and weighted averages using a negative binomial model of *should* in Frown. The first step is to fit the model:

```{r}
m_nb_Frown <- MASS::glm.nb(
	n_tokens ~ genre + offset(log(n_words)), 
	data = should_Frown)
```

The function `avg_predictions()` calculates average predictions. Its default behavior for the data at hand returns a frequency estimate that is unlikely to be of interest to us. This is because it uses the in-sample mean text length (`n_words`) to adjust the predicted rate. The estimate of 1.58 is therefore the expected frequency 'per 2,309 words':

```{r}
avg_predictions(
  m_nb_Frown)
```

At the very least, we should take control over the kind of normalized frequency we are getting. We prefer 'per 1,000 words' and therefore use the argument `variables` to specify `n_words = 1000`. Now we get a more interpretable estimate:

```{r}
avg_predictions(
  m_nb_Frown,
  variables = list(
    n_words = 1000))
```

We note that this is close to the weighted mean we calculated above, which means that the genres are weighted in proportion to their size. This reflects the fact that the `{marginaleffects}` package by default averages over the estimation sample, and therefore propagates imbalances into the averages. Specifically, the function `avg_predictions()` starts by calculating a model-based prediction for each text in the data, assuming it is 1,000 words long (as specified by `variables = list(n_words = 1000))`), and then averages over these 500 model-bases estimates.

```{r}
avg_predictions(
  m_nb_Frown,
  variables = list(
    n_words = 1000),
  type = "response")
```


If we instead prefer a simple average, we can use the argument `newdata` to explicitly define the conditions to average over. This way we tell the function not to take the estimation sample as a basis for calculating predictions and weighting, but instead define the reference grid over which to average. The following code asks for a simple average over four conditions, which represent different genres but have the same length. The result is close to the simple average we calculated above.


```{r}
avg_predictions(
  m_nb_Frown,
  variables = list(
    n_words = 1000),
  newdata = datagrid(
    genre = c("press", "general_prose", "learned", "fiction")))

```

This shows that imbalances in the data can have an effect on simple data summaries such as estimates of average normalized frequencies. This is sometimes referred to as *imbalance bias*, and in the present case, we could refer to it as genre imbalance bias. This kind of distortion can occur when (i) there is imbalance across subgroups in the data and (ii) the quantity of interest varies from subgroup to subgroup. We have seen how to adjust for this form of bias using model-based predictions.


#### Frequency differences: Simple vs. weighted averages

We have seen that imbalance bias can affect simple averages, such as frequency estimates. The same is true for frequency differences. Recall that a crude comparison of the corpora reveals that the frequency of *should* in Frown is only as high as 86% of that in Brown. Since this comparison is based on two corpus frequencies, each of which is potentially affected by imbalance bias, the comparison may likewise be driven into the direction of more strongly represented genres.

@fig-should-imbalance-comparison shows the diachronic trends in the four genres. We note that while Press and Fiction show virtually no difference between Brown and Frown, a diachronic cline is apparent for Learned and General prose. Seeing that General prose is the most strongly represented genre, we would expect weighted differences to be pulled into its direction, meaning that a weighted frequency difference will be larger than a simple frequency difference, which would give the same weight to all genres. 

```{r}
#| fig-width: 3
#| fig-height: 2.3
#| code-fold: true
#| code-summary: "Draw Figure" 
#| label: fig-should-imbalance-comparison
#| fig-cap: "Estimated frequency of *should* in the genre Learned: Comparison of estimates based on a Poisson and a negative binomial model."
#| message: false
#| warning: false


should |> 
  mutate(rate_ptw = (n_tokens/n_words)*1e3) |> 
  group_by(corpus, genre) |> 
  dplyr::summarize(
    ptw = mean(rate_ptw),
    n_words = sum(n_words)) |> 
  ggplot(aes(x = corpus, y = ptw, size = n_words, group = genre, color = genre)) +
  geom_point(shape = 1) +
  geom_point(shape = 16, size = 1) +
  geom_line(size = .5) +
  scale_color_colorblind() +
  theme_classic_ls() +
  ylab("Normalized frequency\n(per 1,000 words)") +
  xlab(NULL) +
  scale_y_continuous(limits = c(0, 1.1), breaks = c(0, .5, 1), expand = c(0,0)) +
  scale_size_area(max_size = 8) +
  theme(legend.position = "none",
        plot.margin = margin(.2, 2, .2, .2, "cm")) +
  directlabels::geom_dl(aes(label = genre), method = list("last.points", cex = .75, x = 3.52, label = c("Fiction", "General prose", "Learned", "Press"))) +
  coord_cartesian(clip="off")

```


We can calculate descriptive frequency comparisons according to the two schemes, i.e. by either weighting all genres equally, or in proportion to their representation in the data. This returns two slightly different estimates: A frequency decline by 11% (simple comparison) or by 13% (weighted comparison).

```{r}
should |> 
  mutate(rate_ptw = (n_tokens/n_words)*1e3) |> 
  group_by(corpus, genre) |> 
  dplyr::summarize(
    ptw = mean(rate_ptw),
    n_words = sum(n_words)) |> 
  ungroup() |> 
  group_by(genre) |> 
  dplyr::summarize(
    freq_ratio_data = ptw[corpus == "Frown"]/ptw[corpus == "Brown"],
    n_words_genre = sum(n_words)) |> 
  mutate(
    weight = n_words_genre/sum(n_words_genre)) |> 
  dplyr::summarize(
    simple_comparison = mean(freq_ratio),
    weighted_comparison = weighted.mean(freq_ratio, w = weight)
  ) |> round(2)
```





#### Model-based estimates of frequency differences 

Let us again look at how to obtain these two types of comparisons using a regression model. We start by fitting a negative binomial model that includes two predictors, Corpus and Genre, as well as their interaction.

```{r}
m_nb_corpus <- MASS::glm.nb(
	n_tokens ~ corpus * genre + offset(log(n_words)), 
	data = should)
```

This kind of model allows us to calculate frequency comparisons at the level of the individual genres (similar to what we saw in @fig-should-imbalance-comparison above). Alternatively, we may average over the four genres, to get a general estimat of how the frequency of *should* differs between the corpora. 

For purposes of illustration, let's use the `{marginaleffects}` package to get genre-level frequency comparisons. We use the function `comparisons()` to do so. 

- The argument `variables` specifies the focal variable(s), i.e. the one(s) whose levels are to be compared. In our case, this is the predictor Corpus. 
- The argument `newdata` allows us to specify the location in the predictor space at which to make comparisons. This means that it allows us to take control over the levels of the non-focal variables. Since we want a comparisons for each genre, we specify all genres, and we also want to compare normalized frequencies 'per 1,000 words'.
- Finally, by specifying `transform = exp`, we are asking `comparisons()` to exponentiate the log-scale differences, which yields rate ratios.

```{r}
comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  transform = exp) |> 
  tidy() |> 
  select(genre, contrast, estimate, conf.low, conf.high) |> 
    mutate(across(where(is.numeric), round, 2))
```


Try: Averaging on the model scale vs. the data scale

```{r}
comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  comparison = "difference") |> 
  tidy() |> 
  dplyr::summarize(
    data_scale_simple = mean(exp(estimate)),
    data_scale_weight = weighted.mean(exp(estimate), w = c(.26, .41, .16, .17)),
    model_scale_simple = exp(mean(estimate)),
    model_scale_weight = exp(weighted.mean(estimate, w = c(.26, .41, .16, .17)))) |> 
    mutate(across(where(is.numeric), round, 3)) |> 
  t()

```




#### Average comparisons

Weighted average (model scale)

```{r}
avg_comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  wts = c(.26, .41, .16, .17),
  comparison = "difference",
  transform = exp)
```

Simple average (model scale)

```{r}
avg_comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  comparison = "difference",
  transform = exp)
```



```{r}
comp_data_scale <- function(hi, lo){
  mean(exp(hi - lo))
}

avg_comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  comparison = comp_data_scale)
```

```{r}
comp_data_scale_wtd <- function(hi, lo, wts = c(.26, .41, .16, .17)){
  weighted.mean(exp(hi - lo), w = wts)
}

avg_comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  comparison = comp_data_scale_wtd)
```


```{r}
avg_comparisons(
  m_nb_corpus,  
  variables = "corpus",
  newdata = datagrid(
    genre = c("fiction", "general_prose", "learned", "press"),
    n_words = 1000),
  wts = c(.26, .41, .16, .17),
  comparison = "ratio")
```




